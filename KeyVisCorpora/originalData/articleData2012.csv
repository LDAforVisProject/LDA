Paper ID	Conference	Year	Title	DOI	Link	First Page	Last Page	IEEE Xplore Article Number	INSPEC Controlled	INSPEC Non-Controlled	IEEE Terms	Author Keywords	Abstract	Author Names	Author Affiliations	Author IDs
	SciVis+InfoVis+some VAST	Dec. 2012	VisWeek 2012 Keynote Speaker	10.1109/TVCG.2012.290	http://dx.doi.org/10.1109/TVCG.2012.290	xxi	xxi	6327202					"A professional biography of Mary Czerwinski of Microsoft Research is presented. Her speech, which is not included, was entitled: ""Trends and Topics from the Last 17 Years at Microsoft Research."""			
	SciVis+InfoVis+some VAST	Dec. 2012	VisWeek 2012 Capstone Speaker	10.1109/TVCG.2012.289	http://dx.doi.org/10.1109/TVCG.2012.289	xxii	xxii	6327302	data visualisation;photography	graphics;photography;visualization			What you are doing as visualization researchers and developers is critical and, in fact, your role is more important than ever in this age of massive data. I and many others desperately want to use your work, but sometimes I just cannot seem to wrap my head around what you are showing-even if it really looks cool. Cool doesn't cut it for me. This talk will give examples from my own successes and failures in photography and graphics and suggest, with a little imagination and open minds, there might be some lessons learned from my own commitment to delving into and communicating information.	Frankel, F.		38489079400
	SciVis+InfoVis+some VAST	Dec. 2012	Generalized Topological Simplification of Scalar Fields on Surfaces	10.1109/TVCG.2012.228	http://dx.doi.org/10.1109/TVCG.2012.228	2005	2013	6327204	C++ language;computational geometry;iterative methods;mesh generation	C++ implementation;combinatorial algorithm;critical points;data-fitting purpose;feature selection;generalized topological simplification;geometry processing;input function topology;iterative algorithm;multisaddles;numerical noise;numerical solvers;scalar field design;standard ϵ-simplification;sublevel sets;sur-level sets;surface mesh;topological noise robust pruning;triangulated surfaces	Algorithm design and analysis;Geometry;Level set;Noise measurement;Robustness;Topology	Scalar field visualization;scalar field design;topological simplification		Tierny, J.;Pascucci, V.	Telecom ParisTech, Paris, France|c|;	37298870300;37284312600
	SciVis+InfoVis+some VAST	Dec. 2012	Computing Morse-Smale Complexes with Accurate Geometry	10.1109/TVCG.2012.209	http://dx.doi.org/10.1109/TVCG.2012.209	2014	2022	6327205	computational geometry;data analysis;data visualisation;mesh generation;natural sciences computing;probability;randomised algorithms	MS complex;Morse-Smale complex computation;deterministic variant;domain mesh resolution;function ordered traversals;geometry;probability;randomized algorithm;real-world data;scalar field discrete gradient;scientific data analysis;scientific data visualization;standard deviation;synthetic data;topological techniques	Algorithm design and analysis;Geometry;Manifolds;Robustness;Standards;Topology;Vectors	Morse-Smale complex;Topology;topological methods	Topological techniques have proven highly successful in analyzing and visualizing scientific data. As a result, significant efforts have been made to compute structures like the Morse-Smale complex as robustly and efficiently as possible. However, the resulting algorithms, while topologically consistent, often produce incorrect connectivity as well as poor geometry. These problems may compromise or even invalidate any subsequent analysis. Moreover, such techniques may fail to improve even when the resolution of the domain mesh is increased, thus producing potentially incorrect results even for highly resolved functions. To address these problems we introduce two new algorithms: (i) a randomized algorithm to compute the discrete gradient of a scalar field that converges under refinement; and (ii) a deterministic variant which directly computes accurate geometry and thus correct connectivity of the MS complex. The first algorithm converges in the sense that on average it produces the correct result and its standard deviation approaches zero with increasing mesh resolution. The second algorithm uses two ordered traversals of the function to integrate the probabilities of the first to extract correct (near optimal) geometry and connectivity. We present an extensive empirical study using both synthetic and real-world data and demonstrates the advantages of our algorithms in comparison with several popular approaches.	Gyulassy, A.;Bremer, P.;Pascucci, V.	SCI Inst., Univ. of Utah, Salt Lake City, UT, USA|c|;;	37870001700;37564112000;37284312600
	SciVis+InfoVis+some VAST	Dec. 2012	Visualization of Temporal Similarity in Field Data	10.1109/TVCG.2012.284	http://dx.doi.org/10.1109/TVCG.2012.284	2023	2032	6327206	data analysis;data visualisation;inspection;interactive systems;matrix algebra	correlation extraction;field data temporal variation;interactive technique;quasiperiodic behavior extraction;similarity matrices;spatial relationships;temporal relationships;temporal similarity visualization approach;time-dependent data analysis techniques;univariate function temporal similarity;visual exploration;visual inspection;visual interaction	Context awareness;Correlation;Data visualization;Information analysis;Machine learning;Smoothing methods	Time-dependent fields;comparative visualization;interactive recurrence analysis;similarity analysis	This paper presents a visualization approach for detecting and exploring similarity in the temporal variation of field data. We provide an interactive technique for extracting correlations from similarity matrices which capture temporal similarity of univariate functions. We make use of the concept to extract periodic and quasiperiodic behavior at single (spatial) points as well as similarity between different locations within a field and also between different data sets. The obtained correlations are utilized for visual exploration of both temporal and spatial relationships in terms of temporal similarity. Our entire pipeline offers visual interaction and inspection, allowing for the flexibility that in particular time-dependent data analysis techniques require. We demonstrate the utility and versatility of our approach by applying our implementation to data from both simulation and measurement.	Frey, S.;Sadlo, F.;Ertl, T.	Visualization Res. Center (VISUS), Univ. of Stuttgart, Stuttgart, Germany|c|;;	37553894500;37282541900;37268023800
	SciVis+InfoVis+some VAST	Dec. 2012	Visualizing Nuclear Scission through a Multifield Extension of Topological Analysis	10.1109/TVCG.2012.287	http://dx.doi.org/10.1109/TVCG.2012.287	2033	2040	6327207	data visualisation;density functional theory;digital simulation;fission;nuclear engineering computing	DFT simulations;JCN visualization;aesthetic criteria analysis;atomic nucleus;complex interaction modelling;density functional theory;fission;joint contour net;multivariate scalar field topological analysis;nuclear science;nuclear scission detection;nuclear scission visualization;topological analysis multifield extension;univariate fields	Approximation methods;Data visualization;Discrete Fourier transforms;Nuclear physics;Topology;Trajectory	Topology;multifields;scalar fields	In nuclear science, density functional theory (DFT) is a powerful tool to model the complex interactions within the atomic nucleus, and is the primary theoretical approach used by physicists seeking a better understanding of fission. However DFT simulations result in complex multivariate datasets in which it is difficult to locate the crucial `scission' point at which one nucleus fragments into two, and to identify the precursors to scission. The Joint Contour Net (JCN) has recently been proposed as a new data structure for the topological analysis of multivariate scalar fields, analogous to the contour tree for univariate fields. This paper reports the analysis of DFT simulations using the JCN, the first application of the JCN technique to real data. It makes three contributions to visualization: (i) a set of practical methods for visualizing the JCN, (ii) new insight into the detection of nuclear scission, and (iii) an analysis of aesthetic criteria to drive further work on representing the JCN.	Duke, D.;Carr, H.;Knoll, A.;Schunck, N.;Hai Ah Nam;Staszczak, A.	Sch. of Comput., Univ. of Leeds, Leeds, UK|c|;;;;;	37282576200;37282624500;37692671900;38489803900;37410378500;38489287300
	SciVis+InfoVis+some VAST	Dec. 2012	Augmented Topological Descriptors of Pore Networks for Material Science	10.1109/TVCG.2012.200	http://dx.doi.org/10.1109/TVCG.2012.200	2041	2050	6327208	X-ray microscopy;biomineralisation;carbon capture and storage;carbon compounds;computerised tomography;data visualisation;environmental science computing;feature extraction;flow through porous media;flow visualisation;glass;materials science computing;permeability;porosity;synchrotrons	augmented topological descriptors;biomineralization analysis;captured carbon dioxide geologic storage;carbon dioxide concentration reduction;carbon sequestration;experimental data processing;feature extraction;flow through porous networks;geometric descriptors;glass beads;jammed packed bead beds;material permeability;material science;media porosity;monodispersive material;multiscale topological analysis;permeability estimates;pore networks;pore structure visualization;porous networks;segmentation;subsurface process;synchrotron-based X-ray computed microtomography;topological descriptors;underground rock formations	Algorithm design and analysis;Carbon dioxide;Geophysical measurements;Image segmentation;Information analysis;Microscopy;Sequestration	Reeb graph;geometric algorithms;microscopy;persistent homology;segmentation;topological data analysis		Ushizima, D.;Morozov, D.;Weber, G.H.;Bianchi, A.G.C.;Sethian, J.A.;Bethel, E.W.	Comput. Res. Div., Lawrence Berkeley Nat. Lab., Berkeley, CA, USA|c|;;;;;	37393897000;38488906200;37411444100;38490099700;38345723900;38198872100
	SciVis+InfoVis+some VAST	Dec. 2012	KnotPad: Visualizing and Exploring Knot Theory with Fluid Reidemeister Moves	10.1109/TVCG.2012.242	http://dx.doi.org/10.1109/TVCG.2012.242	2051	2060	6327209	data visualisation;interactive systems;mathematics computing;user interfaces	KnotPad;exploratory interface;fluid Reidemeister moves;interactive paper-like system;interactive techniques;knot diagrams;math-aware deformation methods;mathematical knot theory exploration;mathematical knot theory visualization;navigation enhancements;supplementary interface elements;topological drawing	Haptic interfaces;Media;Mice;Rendering (computer graphics);Shape;User interfaces;Visualization	Knot Theory;Math Visualization	We present KnotPad, an interactive paper-like system for visualizing and exploring mathematical knots; we exploit topological drawing and math-aware deformation methods in particular to enable and enrich our interactions with knot diagrams. Whereas most previous efforts typically employ physically based modeling to simulate the 3D dynamics of knots and ropes, our tool offers a Reidemeister move based interactive environment that is much closer to the topological problems being solved in knot theory, yet without interfering with the traditional advantages of paper-based analysis and manipulation of knot diagrams. Drawing knot diagrams with many crossings and producing their equivalent is quite challenging and error-prone. KnotPad can restrict user manipulations to the three types of Reidemeister moves, resulting in a more fluid yet mathematically correct user experience with knots. For our principal test case of mathematical knots, KnotPad permits us to draw and edit their diagrams empowered by a family of interactive techniques. Furthermore, we exploit supplementary interface elements to enrich the user experiences. For example, KnotPad allows one to pull and drag on knot diagrams to produce mathematically valid moves. Navigation enhancements in KnotPad provide still further improvement: by remembering and displaying the sequence of valid moves applied during the entire interaction, KnotPad allows a much cleaner exploratory interface for the user to analyze and study knot equivalence. All these methods combine to reveal the complex spatial relationships of knot diagrams with a mathematically true and rich user experience.	Hui Zhang;Jianguang Weng;Lin Jing;Yiwen Zhong	Pervasive Technol. Inst., Indiana Univ., Bloomington, IN, USA|c|;;;	38490442700;38490274900;38489766500;38490597900
	SciVis+InfoVis+some VAST	Dec. 2012	Visualization of Electrostatic Dipoles in Molecular Dynamics of Metal Oxides	10.1109/TVCG.2012.282	http://dx.doi.org/10.1109/TVCG.2012.282	2061	2068	6327210	alumina;cracks;data visualisation;digital simulation;electronic engineering computing;integrated circuits;molecular dynamics method;oxygen	alumina;aluminum oxide;ceramic;computer simulation;crack propagation simulation;electrostatic dipole visualization;electrostatic property;fractional anisotropy value;glyph representation;induced electric dipole moment visualization;material behavior reproduction;mechanical force;metal oxide;microelectronic device;molecular dynamics simulation;oxygen atom	Anisotropic magnetoresistance;Computational modeling;Data models;Data visualization;Electrostatics;Image color analysis;Surface cracks	Visualization in physical sciences and engineering;glyph-based techniques;point-based data;time-varying data	Metal oxides are important for many technical applications. For example alumina (aluminum oxide) is the most commonly-used ceramic in microelectronic devices thanks to its excellent properties. Experimental studies of these materials are increasingly supplemented with computer simulations. Molecular dynamics (MD) simulations can reproduce the material behavior very well and are now reaching time scales relevant for interesting processes like crack propagation. In this work we focus on the visualization of induced electric dipole moments on oxygen atoms in crack propagation simulations. The straightforward visualization using glyphs for the individual atoms, simple shapes like spheres or arrows, is insufficient for providing information about the data set as a whole. As our contribution we show for the first time that fractional anisotropy values computed from the local neighborhood of individual atoms of MD simulation data depict important information about relevant properties of the field of induced electric dipole moments. Iso surfaces in the field of fractional anisotropy as well as adjustments of the glyph representation allow the user to identify regions of correlated orientation. We present novel and relevant findings for the application domain resulting from these visualizations, like the influence of mechanical forces on the electrostatic properties.	Grottel, S.;Beck, P.;Muller, C.;Reina, G.;Roth, J.;Trebin, H.-R.;Ertl, T.	;;;;;;	37590934100;38489110000;37665809300;37287276500;38489822300;38490641500;37268023800
	SciVis+InfoVis+some VAST	Dec. 2012	Cumulative Heat Diffusion Using Volume Gradient Operator for Volume Analysis	10.1109/TVCG.2012.210	http://dx.doi.org/10.1109/TVCG.2012.210	2069	2077	6327211	chemical engineering computing;computer graphics;diffusion;feature extraction;gradient methods;image classification;shape recognition	LBO;VGO;cumulative heat diffusion;data-driven operator;feature classification;half gradient;heat value;local shape information;shape-based feature extraction;shape-based volume analysis;volume gradient operator;voxel intensity	Diffusion processes;Equations;Heating;Histograms;Shape analysis;Volume measurement	Heat diffusion;classification;shape-based volume analysis;transfer function;volume gradient operator	We introduce a simple, yet powerful method called the Cumulative Heat Diffusion for shape-based volume analysis, while drastically reducing the computational cost compared to conventional heat diffusion. Unlike the conventional heat diffusion process, where the diffusion is carried out by considering each node separately as the source, we simultaneously consider all the voxels as sources and carry out the diffusion, hence the term cumulative heat diffusion. In addition, we introduce a new operator that is used in the evaluation of cumulative heat diffusion called the Volume Gradient Operator (VGO). VGO is a combination of the LBO and a data-driven operator which is a function of the half gradient. The half gradient is the absolute value of the difference between the voxel intensities. The VGO by its definition captures the local shape information and is used to assign the initial heat values. Furthermore, VGO is also used as the weighting parameter for the heat diffusion process. We demonstrate that our approach can robustly extract shape-based features and thus forms the basis for an improved classification and exploration of features based on shape.	Gurijala, K.C.;Lei Wang;Kaufman, A.	Stony Brook Univ., Stony Brook, NY, USA|c|;;	38490235700;38240962400;37268052800
	SciVis+InfoVis+some VAST	Dec. 2012	A Novel Approach to Visualizing Dark Matter Simulations	10.1109/TVCG.2012.187	http://dx.doi.org/10.1109/TVCG.2012.187	2078	2087	6327212	N-body problems;N-body simulations (astronomical);ab initio calculations;cosmology;dark matter;data visualisation;gravity;interpolation;rendering (computer graphics)	GPU-assisted rendering;N-body simulation data;SPH-code;Universe;ab initio studies;big bang;cosmic web;cosmological N-body dark matter simulation;dark matter particle position;dark matter simulation visualization;density fluctuation;galaxy formation;gravitational force;gravity;image quality;kernel interpolation;mesh vertices;phase-space information;physical quantities;point based rendering;smoothed particle hydrodynamics-code;spatial distribution;splatting approach;structuere formation;tetrahedral cells;tetrahedral tessellation;time-dependent particle based simulation	Computational modeling;Data models;Equations;Graphics processing unit;Mathematical model;Rendering (computer graphics)	Astrophysics;dark matter;n-body simulations;tetrahedral grids	In the last decades cosmological N-body dark matter simulations have enabled ab initio studies of the formation of structure in the Universe. Gravity amplified small density fluctuations generated shortly after the Big Bang, leading to the formation of galaxies in the cosmic web. These calculations have led to a growing demand for methods to analyze time-dependent particle based simulations. Rendering methods for such N-body simulation data usually employ some kind of splatting approach via point based rendering primitives and approximate the spatial distributions of physical quantities using kernel interpolation techniques, common in SPH (Smoothed Particle Hydrodynamics)-codes. This paper proposes three GPU-assisted rendering approaches, based on a new, more accurate method to compute the physical densities of dark matter simulation data. It uses full phase-space information to generate a tetrahedral tessellation of the computational domain, with mesh vertices defined by the simulation's dark matter particle positions. Over time the mesh is deformed by gravitational forces, causing the tetrahedral cells to warp and overlap. The new methods are well suited to visualize the cosmic web. In particular they preserve caustics, regions of high density that emerge, when several streams of dark matter particles share the same location in space, indicating the formation of structures like sheets, filaments and halos. We demonstrate the superior image quality of the new approaches in a comparison with three standard rendering techniques for N-body simulation data.	Kaehler, R.;Hahn, O.;Abel, T.	;;	37550810200;38489890700;37955451000
	SciVis+InfoVis+some VAST	Dec. 2012	Visual Data Analysis as an Integral Part of Environmental Management	10.1109/TVCG.2012.278	http://dx.doi.org/10.1109/TVCG.2012.278	2088	2094	6327213	contamination;data visualisation;environmental science computing;parallel processing;waste management	DOE/EM;contaminant transport;decision-making process;environmental management;high-performance computing;large-scale simulation;nuclear contaminant;nuclear waste site;parallel processing;visual data analysis;visualization software	Computational modeling;Data models;Data visualization;Environmental management;Google;Monitoring;Pollution measurement;Visual analytics	Visual analytics;data management;environmental management;high-performance computing;parallel rendering	The U.S. Department of Energy's (DOE) Office of Environmental Management (DOE/EM) currently supports an effort to understand and predict the fate of nuclear contaminants and their transport in natural and engineered systems. Geologists, hydrologists, physicists and computer scientists are working together to create models of existing nuclear waste sites, to simulate their behavior and to extrapolate it into the future. We use visualization as an integral part in each step of this process. In the first step, visualization is used to verify model setup and to estimate critical parameters. High-performance computing simulations of contaminant transport produces massive amounts of data, which is then analyzed using visualization software specifically designed for parallel processing of large amounts of structured and unstructured data. Finally, simulation results are validated by comparing simulation results to measured current and historical field data. We describe in this article how visual analysis is used as an integral part of the decision-making process in the planning of ongoing and future treatment options for the contaminated nuclear waste sites. Lessons learned from visually analyzing our large-scale simulation runs will also have an impact on deciding on treatment measures for other contaminated sites.	Meyer, J.;Bethel, E.W.;Horsman, J.L.;Hubbard, S.S.;Krishnan, H.;Romosan, A.;Keating, E.H.;Monroe, L.;Strelitz, R.;Moore, P.;Taylor, G.;Torkian, B.;Johnson, T.C.;Gorton, I.	Lawrence Berkeley Nat. Lab., Berkeley, CA, USA|c|;;;;;;;;;;;;;	38490041400;38198872100;37937460100;37975586200;38489112000;37442417700;37860980800;37930840900;38488975000;38489021100;38489471300;38490632600;38488916700;37270267800
	SciVis+InfoVis+some VAST	Dec. 2012	Evaluation of Fast-Forward Video Visualization	10.1109/TVCG.2012.222	http://dx.doi.org/10.1109/TVCG.2012.222	2095	2103	6327214	data visualisation;image motion analysis;video signal processing	adaptive fast-forward;fast-forward video visualization evaluation;frame-skipping;motion perception;object identification;object trail visualization;object trajectory;playback speed visualizations;predictive trajectory visualization;successive frames temporal blending;video frames	Acceleration;Data visualization;Image color analysis;Rendering (computer graphics);Trajectory;Video recording;Visualization	Video visualization;adaptive fast-forward;controlled laboratory user study	We evaluate and compare video visualization techniques based on fast-forward. A controlled laboratory user study (n = 24) was conducted to determine the trade-off between support of object identification and motion perception, two properties that have to be considered when choosing a particular fast-forward visualization. We compare four different visualizations: two representing the state-of-the-art and two new variants of visualization introduced in this paper. The two state-of-the-art methods we consider are frame-skipping and temporal blending of successive frames. Our object trail visualization leverages a combination of frame-skipping and temporal blending, whereas predictive trajectory visualization supports motion perception by augmenting the video frames with an arrow that indicates the future object trajectory. Our hypothesis was that each of the state-of-the-art methods satisfies just one of the goals: support of object identification or motion perception. Thus, they represent both ends of the visualization design. The key findings of the evaluation are that object trail visualization supports object identification, whereas predictive trajectory visualization is most useful for motion perception. However, frame-skipping surprisingly exhibits reasonable performance for both tasks. Furthermore, we evaluate the subjective performance of three different playback speed visualizations for adaptive fast-forward, a subdomain of video fast-forward.	Hoferlin, M.;Kurzhals, K.;Hoferlin, B.;Heidemann, G.;Weiskopf, D.	;;;;	37680797500;38489885500;37571601300;37266102100;37268045000
	SciVis+InfoVis+some VAST	Dec. 2012	Human Computation in Visualization: Using Purpose Driven Games for Robust Evaluation of Visualization Algorithms	10.1109/TVCG.2012.234	http://dx.doi.org/10.1109/TVCG.2012.234	2104	2113	6327215	computer games;data visualisation;human computer interaction	Disguise;HC;human brain;human cognition;human computation;human perception;hybrid computational loop;purpose driven games;sensory system;visualization algorithm robust evaluation	Color;Computation theory;Decision support systems;Games;Handheld computers;Human factors	Human computation;color blending;evaluation;perception	Due to the inherent characteristics of the visualization process, most of the problems in this field have strong ties with human cognition and perception. This makes the human brain and sensory system the only truly appropriate evaluation platform for evaluating and fine-tuning a new visualization method or paradigm. However, getting humans to volunteer for these purposes has always been a significant obstacle, and thus this phase of the development process has traditionally formed a bottleneck, slowing down progress in visualization research. We propose to take advantage of the newly emerging field of Human Computation (HC) to overcome these challenges. HC promotes the idea that rather than considering humans as users of the computational system, they can be made part of a hybrid computational loop consisting of traditional computation resources and the human brain and sensory system. This approach is particularly successful in cases where part of the computational problem is considered intractable using known computer algorithms but is trivial to common sense human knowledge. In this paper, we focus on HC from the perspective of solving visualization problems and also outline a framework by which humans can be easily seduced to volunteer their HC resources. We introduce a purpose-driven game titled “Disguise” which serves as a prototypical example for how the evaluation of visualization algorithms can be mapped into a fun and addicting activity, allowing this task to be accomplished in an extensive yet cost effective way. Finally, we sketch out a framework that transcends from the pure evaluation of existing visualization methods to the design of a new one.	Ahmed, N.;Ziyi Zheng;Mueller, K.	Comput. Sci. Dept., Stony Brook Univ., Stony Brook, NY, USA|c|;;	38021380500;37599599100;37273119700
	SciVis+InfoVis+some VAST	Dec. 2012	Evaluation of Multivariate Visualization on a Multivariate Task	10.1109/TVCG.2012.223	http://dx.doi.org/10.1109/TVCG.2012.223	2114	2121	6327216	data analysis;data visualisation	attribute blocks;baseline case;data analyst;data sets dimensionality;data-driven spots;grayscale images;high-dimensional data sets;integrated techniques;multiple variables;multivariate task;multivariate visualization evaluation;numerical analysis;oriented slivers;response time;rigorous analysis;subjective workload ratings;utility ranking;visual representation	Analysis of variance;Data visualization;Gray-scale;Image color analysis;Quantitative evaluation;Shape analysis;Time factors	Quantitative evaluation;multivariate visualization;texture perception;visual task design	Multivariate visualization techniques have attracted great interest as the dimensionality of data sets grows. One premise of such techniques is that simultaneous visual representation of multiple variables will enable the data analyst to detect patterns amongst multiple variables. Such insights could lead to development of new techniques for rigorous (numerical) analysis of complex relationships hidden within the data. Two natural questions arise from this premise: Which multivariate visualization techniques are the most effective for high-dimensional data sets? How does the analysis task change this utility ranking? We present a user study with a new task to answer the first question. We provide some insights to the second question based on the results of our study and results available in the literature. Our task led to significant differences in error, response time, and subjective workload ratings amongst four visualization techniques. We implemented three integrated techniques (Data-driven Spots, Oriented Slivers, and Attribute Blocks), as well as a baseline case of separate grayscale images. The baseline case fared poorly on all three measures, whereas Datadriven Spots yielded the best accuracy and was among the best in response time. These results differ from comparisons of similar techniques with other tasks, and we review all the techniques, tasks, and results (from our work and previous work) to understand the reasons for this discrepancy.	Livingston, M.A.;Decker, J.W.;Zhuming Ai	;;	37300434300;37681593700;37330342700
	SciVis+InfoVis+some VAST	Dec. 2012	A Data-Driven Approach to Hue-Preserving Color-Blending	10.1109/TVCG.2012.186	http://dx.doi.org/10.1109/TVCG.2012.186	2122	2129	6327217	Internet;colour graphics;data visualisation;knowledge based systems;rendering (computer graphics);support vector machines;user interfaces	KAV paradigm;Porter-Duff over operator;Web-based user surveys;color mapping;data-driven approach;hue-preserving color-blending;information visualization;knowledge-assisted visualization;semitransparent layering;support vector machine;visual depth ordering;visualization scenarios;volume rendering	Color;Image color analysis;Rendering (computer graphics);Standards;Support vector machines;Vectors	Color blending;hue preservation;knowledge-assisted visualization;parallel coordinates;volume rendering	Color mapping and semitransparent layering play an important role in many visualization scenarios, such as information visualization and volume rendering. The combination of color and transparency is still dominated by standard alpha-compositing using the Porter-Duff over operator which can result in false colors with deceiving impact on the visualization. Other more advanced methods have also been proposed, but the problem is still far from being solved. Here we present an alternative to these existing methods specifically devised to avoid false colors and preserve visual depth ordering. Our approach is data driven and follows the recently formulated knowledge-assisted visualization (KAV) paradigm. Preference data, that have been gathered in web-based user surveys, are used to train a support-vector machine model for automatically predicting an optimized hue-preserving blending. We have applied the resulting model to both volume rendering and a specific information visualization technique, illustrative parallel coordinate plots. Comparative renderings show a significant improvement over previous approaches in the sense that false colors are completely removed and important properties such as depth ordering and blending vividness are better preserved. Due to the generality of the defined data-driven blending operator, it can be easily integrated also into other visualization frameworks.	Kuhne, L.;Giesen, J.;Zhiyuan Zhang;Sungsoo Ha;Mueller, K.	;;;;	38489403700;37325999500;38025124400;38021621900;37273119700
	SciVis+InfoVis+some VAST	Dec. 2012	Effects of Stereo and Screen Size on the Legibility of Three-Dimensional Streamtube Visualization	10.1109/TVCG.2012.216	http://dx.doi.org/10.1109/TVCG.2012.216	2130	2139	6327218	biomedical MRI;brain;data visualisation;medical image processing;screens (display);stereo image processing	DMRI;FA;body-scale unit;brain lesion marking;bundle naming;completion time reduction;diffusion magnetic resonance imaging;display characteristics;fiber tract endpoint;fractional anisotropy;scientific visualization applications;screen size;stereo effects;task accuracy;three-dimensional streamtube visualization legibility;visual reasoning	Data visualization;Lesions;Magnetic resonance imaging;Retina;Stereo image processing;Virtual environments	Display characteristics;diffusion tensor MRI;virtual environment	We report the impact of display characteristics (stereo and size) on task performance in diffusion magnetic resonance imaging (DMRI) in a user study with 12 participants. The hypotheses were that (1) adding stereo and increasing display size would improve task accuracy and reduce completion time, and (2) the greater the complexity of a spatial task, the greater the benefits of an improved display. Thus we expected to see greater performance gains when detailed visual reasoning was required. Participants used dense streamtube visualizations to perform five representative tasks: (1) determine the higher average fractional anisotropy (FA) values between two regions, (2) find the endpoints of fiber tracts, (3) name a bundle, (4) mark a brain lesion, and (5) judge if tracts belong to the same bundle. Contrary to our hypotheses, we found the task completion time was not improved by the use of the larger display and that performance accuracy was hurt rather than helped by the introduction of stereo in our study with dense DMRI data. Bigger was not always better. Thus cautious should be taken when selecting displays for scientific visualization applications. We explored the results further using the body-scale unit and subjective size and stereo experiences.	Jian Chen;Haipeng Cai;Auchus, A.P.;Laidlaw, D.H.	;;;	38489588600;38490505100;38489087400;37275712600
	SciVis+InfoVis+some VAST	Dec. 2012	Analysis of Streamline Separation at Infinity Using Time-Discrete Markov Chains	10.1109/TVCG.2012.198	http://dx.doi.org/10.1109/TVCG.2012.198	2140	2148	6327219	Lyapunov methods;Markov processes;combinatorial mathematics;data visualisation;matrix algebra	combinatorial methods;data visualization;finite-time-Lyapunov-exponents;grid-independent algorithm;infinite-time-evaluation;local area;particle distributions;probabilistic methods;steady planar vector fields;streamline separation analysis;time direction;time-discrete Markov chains;transition matrix	Approximation methods;Eigenvalues and eigenfunctions;Markov processes;Sparse matrices;Topology;Transmission line matrix methods;Vectors	Vector field topology;feature extraction;flow visualization;uncertainty	Existing methods for analyzing separation of streamlines are often restricted to a finite time or a local area. In our paper we introduce a new method that complements them by allowing an infinite-time-evaluation of steady planar vector fields. Our algorithm unifies combinatorial and probabilistic methods and introduces the concept of separation in time-discrete Markov-Chains. We compute particle distributions instead of the streamlines of single particles. We encode the flow into a map and then into a transition matrix for each time direction. Finally, we compare the results of our grid-independent algorithm to the popular Finite-Time-Lyapunov-Exponents and discuss the discrepancies.	Reich, W.;Scheuermann, G.	Univ. of Leipzig, Leipzig, Germany|c|;	38489040200;37282574800
	SciVis+InfoVis+some VAST	Dec. 2012	Derived Metric Tensors for Flow Surface Visualization	10.1109/TVCG.2012.211	http://dx.doi.org/10.1109/TVCG.2012.211	2149	2158	6327220	computational fluid dynamics;continuum mechanics;data visualisation;deformation;flow visualisation;tensors	continuum mechanics;derived metric tensors;displacement visualization;domain segmentation;flow information;flow surface rendering;flow surface visualization;flow visualization;fluid transport;integral flow surfaces;local surface deformations;metric tensor field;shearing;surface querying;velocity gradient	Deformation;Shape analysis;Surface treatment;Tensile stress;Trajectory;Velocity measurement	Vector field;continuum mechanics;deformation;integral surfaces;metric tensor;velocity gradient	Integral flow surfaces constitute a widely used flow visualization tool due to their capability to convey important flow information such as fluid transport, mixing, and domain segmentation. Current flow surface rendering techniques limit their expressiveness, however, by focusing virtually exclusively on displacement visualization, visually neglecting the more complex notion of deformation such as shearing and stretching that is central to the field of continuum mechanics. To incorporate this information into the flow surface visualization and analysis process, we derive a metric tensor field that encodes local surface deformations as induced by the velocity gradient of the underlying flow field. We demonstrate how properties of the resulting metric tensor field are capable of enhancing present surface visualization and generation methods and develop novel surface querying, sampling, and visualization techniques. The provided results show how this step towards unifying classic flow visualization and more advanced concepts from continuum mechanics enables more detailed and improved flow analysis.	Obermaier, H.;Joy, K.I.	Inst. for Data Anal. & Visualization (IDAV), Univ. of California, Davis, CA, USA|c|;	38228357000;37267811400
	SciVis+InfoVis+some VAST	Dec. 2012	Lagrangian Coherent Structures for Design Analysis of Revolving Doors	10.1109/TVCG.2012.243	http://dx.doi.org/10.1109/TVCG.2012.243	2159	2168	6327221	HVAC;building management systems;computational fluid dynamics;data visualisation;design engineering;doors;fast Fourier transforms;structural engineering computing	FTLE computation;LCS;Lagrangian coherent structures;air curtains;air exchange;air flow visual analysis;building entrances;building interior;computational fluid dynamics;design analysis;energy-efficient buildings;energy-efficient design;fast Fourier transform;finite-time Lyapunov exponent;flow barriers;recirculation behavior;revolving doors;ridge definition;room air flow;scale-space derivatives	Atmospheric modeling;Energy efficiency;Flow control;Heating;Lyapunov methods;Meteorology;Tensile stress	Visualization in physical sciences and engineering;topology-based techniques;vector field data	Room air flow and air exchange are important aspects for the design of energy-efficient buildings. As a result, simulations are increasingly used prior to construction to achieve an energy-efficient design. We present a visual analysis of air flow generated at building entrances, which uses a combination of revolving doors and air curtains. The resulting flow pattern is challenging because of two interacting flow patterns: On the one hand, the revolving door acts as a pump, on the other hand, the air curtain creates a layer of uniformly moving warm air between the interior of the building and the revolving door. Lagrangian coherent structures (LCS), which by definition are flow barriers, are the method of choice for visualizing the separation and recirculation behavior of warm and cold air flow. The extraction of LCS is based on the finite-time Lyapunov exponent (FTLE) and makes use of a ridge definition which is consistent with the concept of weak LCS. Both FTLE computation and ridge extraction are done in a robust and efficient way by making use of the fast Fourier transform for computing scale-space derivatives.	Schindler, B.;Fuchs, R.;Barp, S.;Waser, J.;Pobitzer, A.;Carnecky, R.;Peikert, R.	ETH Zurich, Zurich, Switzerland|c|;;;;;;	38102461400;38099765400;38489598400;38111592300;38233238000;38233238600;37282541100
	SciVis+InfoVis+some VAST	Dec. 2012	Turbulence Visualization at the Terascale on Desktop PCs	10.1109/TVCG.2012.274	http://dx.doi.org/10.1109/TVCG.2012.274	2169	2177	6327475	computational fluid dynamics;data visualisation;encoding;entropy;graphics processing units;ray tracing;turbulence	GPU memory;GPU system;bandwidth capacities;brick-based volume ray-casting;compressed flow field representation;desktop computers;entropy encoding;feature-based turbulence visualization;interactive selection;large-scale turbulent motions;memory capacities;run-length;simultaneous visualization;small-scale turbulent motions;spatio-temporal resolution;terascale on desktop PCs;turbulence properties;turbulence small-scale structure;unsteady turbulence simulations;velocity gradient tensor;visually guided exploration;wavelet-based compression scheme	Data visualization;Encoding;Graphics processing unit;Memory management;Rendering (computer graphics);Tensile stress;Vectors	Visualization system and toolkit design;data compression;data streaming;vector fields;volume rendering	Despite the ongoing efforts in turbulence research, the universal properties of the turbulence small-scale structure and the relationships between small- and large-scale turbulent motions are not yet fully understood. The visually guided exploration of turbulence features, including the interactive selection and simultaneous visualization of multiple features, can further progress our understanding of turbulence. Accomplishing this task for flow fields in which the full turbulence spectrum is well resolved is challenging on desktop computers. This is due to the extreme resolution of such fields, requiring memory and bandwidth capacities going beyond what is currently available. To overcome these limitations, we present a GPU system for feature-based turbulence visualization that works on a compressed flow field representation. We use a wavelet-based compression scheme including run-length and entropy encoding, which can be decoded on the GPU and embedded into brick-based volume ray-casting. This enables a drastic reduction of the data to be streamed from disk to GPU memory. Our system derives turbulence properties directly from the velocity gradient tensor, and it either renders these properties in turn or generates and renders scalar feature volumes. The quality and efficiency of the system is demonstrated in the visualization of two unsteady turbulence simulations, each comprising a spatio-temporal resolution of 10244. On a desktop computer, the system can visualize each time step in 5 seconds, and it achieves about three times this rate for the visualization of a scalar feature volume.	Treib, M.;Burger, K.;Reichl, F.;Meneveau, C.;Szalay, A.;Westermann, R.	Tech. Univ. Munchen, Munich, Germany|c|;;;;;	38490343400;37587634700;38490078000;37591319300;37294404700;37444424000
	SciVis+InfoVis+some VAST	Dec. 2012	Automatic Detection and Visualization of Qualitative Hemodynamic Characteristics in Cerebral Aneurysms	10.1109/TVCG.2012.202	http://dx.doi.org/10.1109/TVCG.2012.202	2178	2187	6327222	computational fluid dynamics;data visualisation;haemodynamics;medical image processing;object detection	CFD;automatic detection;automatic visualization;blood flow measurements;cerebral aneurysms;computational fluid dynamics;impingement zone;inflow jet boundary contour;minimal visual clutter;occlusions;pathological vessel dilatation;qualitative flow characteristics;qualitative hemodynamic characteristics;quantitative hemodynamic information;risk of rupture	Aneurysm;Data visualization;Hemodynamics;Rendering (computer graphics);Surface morphology	CFD;Cerebral aneurysm;hemodynamic;visualization	Cerebral aneurysms are a pathological vessel dilatation that bear a high risk of rupture. For the understanding and evaluation of the risk of rupture, the analysis of hemodynamic information plays an important role. Besides quantitative hemodynamic information, also qualitative flow characteristics, e.g., the inflow jet and impingement zone are correlated with the risk of rupture. However, the assessment of these two characteristics is currently based on an interactive visual investigation of the flow field, obtained by computational fluid dynamics (CFD) or blood flow measurements. We present an automatic and robust detection as well as an expressive visualization of these characteristics. The detection can be used to support a comparison, e.g., of simulation results reflecting different treatment options. Our approach utilizes local streamline properties to formalize the inflow jet and impingement zone. We extract a characteristic seeding curve on the ostium, on which an inflow jet boundary contour is constructed. Based on this boundary contour we identify the impingement zone. Furthermore, we present several visualization techniques to depict both characteristics expressively. Thereby, we consider accuracy and robustness of the extracted characteristics, minimal visual clutter and occlusions. An evaluation with six domain experts confirms that our approach detects both hemodynamic characteristics reasonably.	Gasteiger, R.;Lehmann, D.J.;van Pelt, R.;Janiga, G.;Beuing, O.;Vilanova, A.;Theisel, H.;Preim, B.	Dept. of Simulation & Graphics, Univ. of Magdeburg, Magdeburg, Germany|c|;;;;;;;	38017015100;37601992200;37390973400;37833255000;38017002900;37282551500;37266875400;37424645300
	SciVis+InfoVis+some VAST	Dec. 2012	Visualization of Astronomical Nebulae via Distributed Multi-GPU Compressed Sensing Tomography	10.1109/TVCG.2012.281	http://dx.doi.org/10.1109/TVCG.2012.281	2188	2197	6327223	astronomy computing;compressed sensing;data visualisation;graphics processing units;nebulae;rendering (computer graphics)	3D visualization;Earth;Hubble Space Telescope;astronomical imagery;astronomical nebulae visualization;axial symmetry;compressed sensing reconstruction;direct volume rendering;distributed multiGPU compressed sensing tomography;distributed multiGPU implementation;high-resolution datasets;linear equality constraints;position-dependent volumetric regularization;single 2D projection;spherical symmetry;tomographic approach;volumetric visualizations	Compressed sensing;Graphics processing unit;Image reconstruction;Memory management;Reconstruction algorithms	Astronomical visualization;direct volume rendering;distributed volume reconstruction	The 3D visualization of astronomical nebulae is a challenging problem since only a single 2D projection is observable from our fixed vantage point on Earth. We attempt to generate plausible and realistic looking volumetric visualizations via a tomographic approach that exploits the spherical or axial symmetry prevalent in some relevant types of nebulae. Different types of symmetry can be implemented by using different randomized distributions of virtual cameras. Our approach is based on an iterative compressed sensing reconstruction algorithm that we extend with support for position-dependent volumetric regularization and linear equality constraints. We present a distributed multi-GPU implementation that is capable of reconstructing high-resolution datasets from arbitrary projections. Its robustness and scalability are demonstrated for astronomical imagery from the Hubble Space Telescope. The resulting volumetric data is visualized using direct volume rendering. Compared to previous approaches, our method preserves a much higher amount of detail and visual variety in the 3D visualization, especially for objects with only approximate symmetry.	Wenger, S.;Ament, M.;Guthe, S.;Lorenz, D.;Tillmann, A.;Weiskopf, D.;Magnor, M.	Inst. fur Computergraphik, Tech. Univ. Braunschweig, Braunschweig, Germany|c|;;;;;;	37409205100;37393968500;37728224400;37410118300;38489508600;37268045000;37273816400
	SciVis+InfoVis+some VAST	Dec. 2012	Visualization of Flow Behavior in Earth Mantle Convection	10.1109/TVCG.2012.283	http://dx.doi.org/10.1109/TVCG.2012.283	2198	2207	6327224	computational fluid dynamics;data visualisation;flow;geophysical fluid dynamics;geophysics computing;rendering (computer graphics);shells (structures)	4D problem;caching;characterize flow;constant opacity;direct volume rendering;domain experts;earth mantle convection model;finite element model CitcomS;fluid flow behavior;geophysical flow simulation;geophysicists;geophysics;geoscientist;information overload;integrated processing pipeline;intelligent indexing;mantle homogenization;mixing model;occlusion;passive tracers;self-gravitating 3D spherical shell;smooth animation;time dependent simulation data;tracer concentration statistics;visualization	Computational modeling;Data models;Data visualization;Earth;Flow control;Geophysical measurements;Rendering (computer graphics)	Earth mantle;Geophysics;convection;flow visualization;large data system;tracer concentration	A fundamental characteristic of fluid flow is that it causes mixing: introduce a dye into a flow, and it will disperse. Mixing can be used as a method to visualize and characterize flow. Because mixing is a process that occurs over time, it is a 4D problem that presents a challenge for computation, visualization, and analysis. Motivated by a mixing problem in geophysics, we introduce a combination of methods to analyze, transform, and finally visualize mixing in simulations of convection in a self-gravitating 3D spherical shell representing convection in the Earth's mantle. Geophysicists use tools such as the finite element model CitcomS to simulate convection, and introduce massless, passive tracers to model mixing. The output of geophysical flow simulation is hard to analyze for domain experts because of overall data size and complexity. In addition, information overload and occlusion are problems when visualizing a whole-earth model. To address the large size of the data, we rearrange the simulation data using intelligent indexing for fast file access and efficient caching. To address information overload and interpret mixing, we compute tracer concentration statistics, which are used to characterize mixing in mantle convection models. Our visualization uses a specially tailored version of Direct Volume Rendering. The most important adjustment is the use of constant opacity. Because of this special area of application, i. e. the rendering of a spherical shell, many computations for volume rendering can be optimized. These optimizations are essential to a smooth animation of the time-dependent simulation data. Our results show how our system can be used to quickly assess the simulation output and test hypotheses regarding Earth's mantle convection. The integrated processing pipeline helps geoscientists to focus on their main task of analyzing mantle homogenization.	Schroder, S.;Peterson, J.A.;Obermaier, H.;Kellogg, L.H.;Joy, K.I.;Hagen, H.	Comput. Graphics & HCI Group, Univ. of Kaiserslautern, Kaiserslautern, Germany|c|;;;;;	38489665700;38488843200;38228357000;38489193200;37267811400;37282578800
	SciVis+InfoVis+some VAST	Dec. 2012	Interactive Retro-Deformation of Terrain for Reconstructing 3D Fault Displacements	10.1109/TVCG.2012.239	http://dx.doi.org/10.1109/TVCG.2012.239	2208	2215	6327225	Mars;astronomy computing;data visualisation;geology;geophysics computing;graphics processing units;real-time systems	3D fault displacement reconstruction;3D kinematics;3D terrain visualization;GPU;Mars;Noctis Labyrinthus region;San Andreas fault;active fault;characteristic surface displacement;displacement interpolation;faulted topography;geological process;geometric model;geometry shader;graben structure;human-in-the-loop approach;interactive retro-deformation;planetary topography;point-to-point reconstruction;real-time system;surface-rupturing earthquake;visual feedback	Image reconstruction;Rendering (computer graphics);Solid modeling;Surface reconstruction;Surface topography;Terrain mapping	Terrain rendering;fault simulation;interactive;mesh deformation	Planetary topography is the result of complex interactions between geological processes, of which faulting is a prominent component. Surface-rupturing earthquakes cut and move landforms which develop across active faults, producing characteristic surface displacements across the fault. Geometric models of faults and their associated surface displacements are commonly applied to reconstruct these offsets to enable interpretation of the observed topography. However, current 2D techniques are limited in their capability to convey both the three-dimensional kinematics of faulting and the incremental sequence of events required by a given reconstruction. Here we present a real-time system for interactive retro-deformation of faulted topography to enable reconstruction of fault displacement within a high-resolution (sub 1m/pixel) 3D terrain visualization. We employ geometry shaders on the GPU to intersect the surface mesh with fault-segments interactively specified by the user and transform the resulting surface blocks in realtime according to a kinematic model of fault motion. Our method facilitates a human-in-the-loop approach to reconstruction of fault displacements by providing instant visual feedback while exploring the parameter space. Thus, scientists can evaluate the validity of traditional point-to-point reconstructions by visually examining a smooth interpolation of the displacement in 3D. We show the efficacy of our approach by using it to reconstruct segments of the San Andreas fault, California as well as a graben structure in the Noctis Labyrinthus region on Mars.	Westerteiger, R.;Compton, T.;Cowgill, E.;Gwinner, K.;Hamann, B.;Gerndt, A.;Hagen, H.	German Aerosp. Center, Univ. of Kaiserslautern, Kaiserslautern, Germany|c|;;;;;;	38108787400;38489370900;38489757700;37846171100;37282068700;37938209100;37282578800
	SciVis+InfoVis+some VAST	Dec. 2012	A Visual Analysis Concept for the Validation of Geoscientific Simulation Models	10.1109/TVCG.2012.190	http://dx.doi.org/10.1109/TVCG.2012.190	2216	2225	6327226	Earth;data visualisation;geophysics computing	Earth system modelers;data samples;geosciences;geoscientific simulation models;glacial isostatic adjustments;imprecise observation data;model parameter space;sea levels indicators;sparse observation data;spatiotemporal dimension;twofold interactive drill-down;validation methods;visual analysis concept;visualization components	Analytical models;Computational modeling;Data models;Data visualization;Geophysical measurements;Sea level	Earth science visualization;coordinated multiple views;model validation;sea level indicators;spatio-temporal visualization	Geoscientific modeling and simulation helps to improve our understanding of the complex Earth system. During the modeling process, validation of the geoscientific model is an essential step. In validation, it is determined whether the model output shows sufficient agreement with observation data. Measures for this agreement are called goodness of fit. In the geosciences, analyzing the goodness of fit is challenging due to its manifold dependencies: 1) The goodness of fit depends on the model parameterization, whose precise values are not known. 2) The goodness of fit varies in space and time due to the spatio-temporal dimension of geoscientific models. 3) The significance of the goodness of fit is affected by resolution and preciseness of available observational data. 4) The correlation between goodness of fit and underlying modeled and observed values is ambiguous. In this paper, we introduce a visual analysis concept that targets these challenges in the validation of geoscientific models - specifically focusing on applications where observation data is sparse, unevenly distributed in space and time, and imprecise, which hinders a rigorous analytical approach. Our concept, developed in close cooperation with Earth system modelers, addresses the four challenges by four tailored visualization components. The tight linking of these components supports a twofold interactive drill-down in model parameter space and in the set of data samples, which facilitates the exploration of the numerous dependencies of the goodness of fit. We exemplify our visualization concept for geoscientific modeling of glacial isostatic adjustments in the last 100,000 years, validated against sea levels indicators - a prominent example for sparse and imprecise observation data. An initial use case and feedback from Earth system modelers indicate that our visualization concept is a valuable complement to the range of validation methods.	Unger, A.;Schulte, S.;Klemann, V.;Dransch, D.	GFZ German Reserach Center For Geosci., Potsdam, Germany|c|;;;	37628385900;38489773300;38490271000;38490186800
	SciVis+InfoVis+some VAST	Dec. 2012	SeiVis: An Interactive Visual Subsurface Modeling Application	10.1109/TVCG.2012.259	http://dx.doi.org/10.1109/TVCG.2012.259	2226	2235	6327227	data visualisation;fossil fuels;geophysics computing;hydrocarbon reservoirs;interactive systems;natural gas technology;optimisation;seismology;solid modelling	SeiVis;ambiguous seismic data;depth-converted model;domain expert evaluation;ecological risks;economic risks;energy demands;fossil fuels;global optimization;hydrocarbon reservoirs;interactive horizon extraction technique;interactive visual subsurface modeling application;measured reflection seismics;natural gas;oil;spatial ground truth data;time-depth visualization;unphysical velocity model;velocity modeling	Biological system modeling;Computational modeling;Cost function;Data visualization;Rendering (computer graphics)	Seismic visualization;exploded views;seismic interpretation;volume deformation	The most important resources to fulfill today's energy demands are fossil fuels, such as oil and natural gas. When exploiting hydrocarbon reservoirs, a detailed and credible model of the subsurface structures is crucial in order to minimize economic and ecological risks. Creating such a model is an inverse problem: reconstructing structures from measured reflection seismics. The major challenge here is twofold: First, the structures in highly ambiguous seismic data are interpreted in the time domain. Second, a velocity model has to be built from this interpretation to match the model to depth measurements from wells. If it is not possible to obtain a match at all positions, the interpretation has to be updated, going back to the first step. This results in a lengthy back and forth between the different steps, or in an unphysical velocity model in many cases. This paper presents a novel, integrated approach to interactively creating subsurface models from reflection seismics. It integrates the interpretation of the seismic data using an interactive horizon extraction technique based on piecewise global optimization with velocity modeling. Computing and visualizing the effects of changes to the interpretation and velocity model on the depth-converted model on the fly enables an integrated feedback loop that enables a completely new connection of the seismic data in time domain and well data in depth domain. Using a novel joint time/depth visualization, depicting side-by-side views of the original and the resulting depth-converted data, domain experts can directly fit their interpretation in time domain to spatial ground truth data. We have conducted a domain expert evaluation, which illustrates that the presented workflow enables the creation of exact subsurface models much more rapidly than previous approaches.	Hollt, T.;Freiler, W.;Gschwantner, F.;Doleisch, H.;Heinemann, G.;Hadwiger, M.	King Adbullah Univ. of Sci. & Technol., Thuwal, Saudi Arabia|c|;;;;;	37846851700;38017008700;37846849900;37546620400;37846848900;37394809600
	SciVis+InfoVis+some VAST	Dec. 2012	WYSIWYP: What You See Is What You Pick	10.1109/TVCG.2012.292	http://dx.doi.org/10.1109/TVCG.2012.292	2236	2244	6327228	data analysis;data visualisation;medical image processing;radiology;rendering (computer graphics)	3D data analysis;3D rendering;WYSIWYP;medical imaging data;radiologists;slice-based visualizations;volume picking;what you see is what you pick	Biomedical imaging;Data visualization;Equations;Geometry;Image color analysis;Rendering (computer graphics);Transfer functions	Picking;WYSIWYG;volume rendering	Scientists, engineers and physicians are used to analyze 3D data with slice-based visualizations. Radiologists for example are trained to read slices of medical imaging data. Despite the numerous examples of sophisticated 3D rendering techniques, domain experts, who still prefer slice-based visualization do not consider these to be very useful. Since 3D renderings have the advantage of providing an overview at a glance, while 2D depictions better serve detailed analyses, it is of general interest to better combine these methods. Recently there have been attempts to bridge this gap between 2D and 3D renderings. These attempts include specialized techniques for volume picking in medical imaging data that result in repositioning slices. In this paper, we present a new volume picking technique called WYSIWYP (“what you see is what you pick”) that, in contrast to previous work, does not require pre-segmented data or metadata and thus is more generally applicable. The positions picked by our method are solely based on the data itself, the transfer function, and the way the volumetric rendering is perceived by the user. To demonstrate the utility of the proposed method, we apply it to automated positioning of slices in volumetric scalar fields from various application areas. Finally, we present results of a user study in which 3D locations selected by users are compared to those resulting from WYSIWYP. The user study confirms our claim that the resulting positions correlate well with those perceived by the user.	Wiebel, A.;Vos, F.M.;Foerster, D.;Hege, H.-C.	Zuse Inst. Berlin (ZIB), Berlin, Germany|c|;;;	37565763400;37271678400;38489178700;37282272000
	SciVis+InfoVis+some VAST	Dec. 2012	Efficient Structure-Aware Selection Techniques for 3D Point Cloud Visualizations with 2DOF Input	10.1109/TVCG.2012.217	http://dx.doi.org/10.1109/TVCG.2012.217	2245	2254	6327229	Boolean algebra;data visualisation	2DOF input;3D particle cloud;3D point cloud datasets;3D point cloud visualizations;Boolean operations;TeddySelection;bounding selection surface;complex multistep selection;data selection;direct-touch input;efficient structure-aware selection;flexible CloudLasso technique;spatial selection;standard cylinder-based selection	Data visualization;Estimation;Shape analysis;Three dimensional displays	3D interaction;direct-touch interaction;spatial selection	Data selection is a fundamental task in visualization because it serves as a pre-requisite to many follow-up interactions. Efficient spatial selection in 3D point cloud datasets consisting of thousands or millions of particles can be particularly challenging. We present two new techniques, TeddySelection and CloudLasso, that support the selection of subsets in large particle 3D datasets in an interactive and visually intuitive manner. Specifically, we describe how to spatially select a subset of a 3D particle cloud by simply encircling the target particles on screen using either the mouse or direct-touch input. Based on the drawn lasso, our techniques automatically determine a bounding selection surface around the encircled particles based on their density. This kind of selection technique can be applied to particle datasets in several application domains. TeddySelection and CloudLasso reduce, and in some cases even eliminate, the need for complex multi-step selection processes involving Boolean operations. This was confirmed in a formal, controlled user study in which we compared the more flexible CloudLasso technique to the standard cylinder-based selection technique. This study showed that the former is consistently more efficient than the latter - in several cases the CloudLasso selection time was half that of the corresponding cylinder-based selection.	Lingyun Yu;Efstathiou, K.;Isenberg, P.;Isenberg, T.	Univ. of Groningen, Groningen, Netherlands|c|;;;	37593898200;37390025000;37591317800;37297057400
	SciVis+InfoVis+some VAST	Dec. 2012	Sketching Uncertainty into Simulations	10.1109/TVCG.2012.261	http://dx.doi.org/10.1109/TVCG.2012.261	2255	2264	6327230	data visualisation;decision making;emergency services;floods	boundary conditions;decision making;flood management;flood response personnel;rendering;simulation models;simulation steering;sketch based input approach;sketching uncertainty;special visualizations;traditional manipulation;visual feedback	Mobile communication;Numerical models;Rendering (computer graphics);Shape analysis;Splines (mathematics);Visualization	Emergency/disaster management;ensemblesimulation steering;flood management;integrated visualization system;interaction design;sketch-based steering;uncertainty visualization	In a variety of application areas, the use of simulation steering in decision making is limited at best. Research focusing on this problem suggests that most user interfaces are too complex for the end user. Our goal is to let users create and investigate multiple, alternative scenarios without the need for special simulation expertise. To simplify the specification of parameters, we move from a traditional manipulation of numbers to a sketch-based input approach. Users steer both numeric parameters and parameters with a spatial correspondence by sketching a change onto the rendering. Special visualizations provide immediate visual feedback on how the sketches are transformed into boundary conditions of the simulation models. Since uncertainty with respect to many intertwined parameters plays an important role in planning, we also allow the user to intuitively setup complete value ranges, which are then automatically transformed into ensemble simulations. The interface and the underlying system were developed in collaboration with experts in the field of flood management. The real-world data they have provided has allowed us to construct scenarios used to evaluate the system. These were presented to a variety of flood response personnel, and their feedback is discussed in detail in the paper. The interface was found to be intuitive and relevant, although a certain amount of training might be necessary.	Ribicic, H.;Waser, J.;Gurbat, R.;Sadransky, B.;Groller, M.E.	VRVis Vienna, Vienna, Austria|c|;;;;	38229386600;38111592300;38489417000;38490409700;37282552200
	SciVis+InfoVis+some VAST	Dec. 2012	A Perceptual-Statistics Shading Model	10.1109/TVCG.2012.188	http://dx.doi.org/10.1109/TVCG.2012.188	2265	2274	6327231	data visualisation;statistics;visual perception	2D screens;3D surface visualization;distal stimulus;influencing factors;perception error;perceptual-statistics shading model;proximal stimulus;publicly-available datasets;surface perception;visualization technique	Computational modeling;Mathematical model;Observers;Rendering (computer graphics);Shape analysis;Surface reconstruction	Shading;evaluation;perception;statistical analysis;surface slant	The process of surface perception is complex and based on several influencing factors, e.g., shading, silhouettes, occluding contours, and top down cognition. The accuracy of surface perception can be measured and the influencing factors can be modified in order to decrease the error in perception. This paper presents a novel concept of how a perceptual evaluation of a visualization technique can contribute to its redesign with the aim of improving the match between the distal and the proximal stimulus. During analysis of data from previous perceptual studies, we observed that the slant of 3D surfaces visualized on 2D screens is systematically underestimated. The visible trends in the error allowed us to create a statistical model of the perceived surface slant. Based on this statistical model we obtained from user experiments, we derived a new shading model that uses adjusted surface normals and aims to reduce the error in slant perception. The result is a shape-enhancement of visualization which is driven by an experimentally-founded statistical model. To assess the efficiency of the statistical shading model, we repeated the evaluation experiment and confirmed that the error in perception was decreased. Results of both user experiments are publicly-available datasets.	Solteszova, V.;Turkay, C.;Price, M.C.;Viola, I.	Dept. of Inf., Univ. of Bergen, Bergen, Norway|c|;;;	38489116100;37567685600;38489546000;37282726800
	SciVis+InfoVis+some VAST	Dec. 2012	Visual Steering and Verification of Mass Spectrometry Data Factorization in Air Quality Research	10.1109/TVCG.2012.280	http://dx.doi.org/10.1109/TVCG.2012.280	2275	2284	6327232	aerosols;data visualisation;formal verification;mass spectroscopy;matrix decomposition	aerosol composition;air quality research;black box schemes;dimensionality reduction;interactive visual framework;lower-dimensional basis transformations;mass spectrometry data factorization;non-negative matrix factorization;particle mass spectrometry data;visual steering;visual verification	Aerosols;Atmospheric measurements;Data visualization;Error analysis;Mass spectroscopy;Optimization	Dimension reduction;mass spectrometry data;matrix factorization;multidimensional data visualization;visual encodings of numerical error metrics	The study of aerosol composition for air quality research involves the analysis of high-dimensional single particle mass spectrometry data. We describe, apply, and evaluate a novel interactive visual framework for dimensionality reduction of such data. Our framework is based on non-negative matrix factorization with specifically defined regularization terms that aid in resolving mass spectrum ambiguity. Thereby, visualization assumes a key role in providing insight into and allowing to actively control a heretofore elusive data processing step, and thus enabling rapid analysis meaningful to domain scientists. In extending existing black box schemes, we explore design choices for visualizing, interacting with, and steering the factorization process to produce physically meaningful results. A domain-expert evaluation of our system performed by the air quality research experts involved in this effort has shown that our method and prototype admits the finding of unambiguous and physically correct lower-dimensional basis transformations of mass spectrometry data at significantly increased speed and a higher degree of ease.	Engel, D.;Greff, K.;Garth, C.;Bein, K.;Wexler, A.;Hamann, B.;Hagen, H.	Univ. of Kaiserslautern, Kaiserslautern, Germany|c|;;;;;;	38489527100;38489177000;37282573700;38490554500;38227083700;37282068700;37282578800
	SciVis+InfoVis+some VAST	Dec. 2012	Interactive Volume Exploration of Petascale Microscopy Data Streams Using a Visualization-Driven Virtual Memory Approach	10.1109/TVCG.2012.240	http://dx.doi.org/10.1109/TVCG.2012.240	2285	2294	6327233	data acquisition;data visualisation;electron microscopes;microscopy;virtual storage	2D microscope image tiles;3D blocks;3D multiresolution representation;anisotropic petascale volume;best-of-breed system;cache misses;continuous stream;data acquisition;decouples construction;high resolution electron microscopy image;interactive volume exploration;microscopes;multiresolution hierarchy;multiresolution virtual memory architecture;neuroscience;octree;petascale microscopy data streams;petascale volumes;ray-casting;real microscopy data;system design;visible volume data;visualization-driven virtual memory;volume ray casting;volume visualization system	Data visualization;Graphics processing unit;Image resolution;Microscopy;Neuroscience;Octrees;Rendering (computer graphics)	Petascale volume exploration;high-resolution microscopy;high-throughput imaging;neuroscience	This paper presents the first volume visualization system that scales to petascale volumes imaged as a continuous stream of high-resolution electron microscopy images. Our architecture scales to dense, anisotropic petascale volumes because it: (1) decouples construction of the 3D multi-resolution representation required for visualization from data acquisition, and (2) decouples sample access time during ray-casting from the size of the multi-resolution hierarchy. Our system is designed around a scalable multi-resolution virtual memory architecture that handles missing data naturally, does not pre-compute any 3D multi-resolution representation such as an octree, and can accept a constant stream of 2D image tiles from the microscopes. A novelty of our system design is that it is visualization-driven: we restrict most computations to the visible volume data. Leveraging the virtual memory architecture, missing data are detected during volume ray-casting as cache misses, which are propagated backwards for on-demand out-of-core processing. 3D blocks of volume data are only constructed from 2D microscope image tiles when they have actually been accessed during ray-casting. We extensively evaluate our system design choices with respect to scalability and performance, compare to previous best-of-breed systems, and illustrate the effectiveness of our system for real microscopy data from neuroscience.	Hadwiger, M.;Beyer, J.;Won-Ki Jeong;Pfister, H.	;;;	37394809600;37409575800;38490133400;37275698100
	SciVis+InfoVis+some VAST	Dec. 2012	An Adaptive Prediction-Based Approach to Lossless Compression of Floating-Point Volume Data	10.1109/TVCG.2012.194	http://dx.doi.org/10.1109/TVCG.2012.194	2295	2304	6327234	data compression;data visualisation;interpolation;natural sciences computing;polynomials	adaptive combined encoder;adaptive polynomial encoder;adaptive prediction;computer simulations;efficient visualization;floating-point values;interpolating polynomials;linear predictors;lossless compression;lossless floating-point compressors;medical floating-point volume data;observational measurements;polynomial predictors;prediction-based compression method;progressive representation;scientific data;scientific floating-point volume data;switched prediction scheme	Data models;Data visualization;Entropy coding;Floating-point arithmetic;Image coding;Polynomials	Volume compression;floating-point compression;lossless compression	In this work, we address the problem of lossless compression of scientific and medical floating-point volume data. We propose two prediction-based compression methods that share a common framework, which consists of a switched prediction scheme wherein the best predictor out of a preset group of linear predictors is selected. Such a scheme is able to adapt to different datasets as well as to varying statistics within the data. The first method, called APE (Adaptive Polynomial Encoder), uses a family of structured interpolating polynomials for prediction, while the second method, which we refer to as ACE (Adaptive Combined Encoder), combines predictors from previous work with the polynomial predictors to yield a more flexible, powerful encoder that is able to effectively decorrelate a wide range of data. In addition, in order to facilitate efficient visualization of compressed data, our scheme provides an option to partition floating-point values in such a way as to provide a progressive representation. We compare our two compressors to existing state-of-the-art lossless floating-point compressors for scientific data, with our data suite including both computer simulations and observational measurements. The results demonstrate that our polynomial predictor, APE, is comparable to previous approaches in terms of speed but achieves better compression rates on average. ACE, our combined predictor, while somewhat slower, is able to achieve the best compression rate on all datasets, with significantly better rates on most of the datasets.	Fout, N.;Kwan-Liu Ma	UC Davis, Davis, CA, USA|c|;	37945247900;37275869400
	SciVis+InfoVis+some VAST	Dec. 2012	On the Interpolation of Data with Normally Distributed Uncertainty for Visualization	10.1109/TVCG.2012.249	http://dx.doi.org/10.1109/TVCG.2012.249	2305	2314	6327235	Gaussian distribution;data visualisation;interpolation;learning (artificial intelligence)	Gaussian distributions;data interpolation;geostatistics;machine learning;normally distributed uncertainty;uncertain behavior;visualization purposes	Data models;Data visualization;Distributed databases;Gaussian processes;Interpolation;Random variables;Uncertainty	Gaussian process;interpolation;uncertainty	In many fields of science or engineering, we are confronted with uncertain data. For that reason, the visualization of uncertainty received a lot of attention, especially in recent years. In the majority of cases, Gaussian distributions are used to describe uncertain behavior, because they are able to model many phenomena encountered in science. Therefore, in most applications uncertain data is (or is assumed to be) Gaussian distributed. If such uncertain data is given on fixed positions, the question of interpolation arises for many visualization approaches. In this paper, we analyze the effects of the usual linear interpolation schemes for visualization of Gaussian distributed data. In addition, we demonstrate that methods known in geostatistics and machine learning have favorable properties for visualization purposes in this case.	Schlegel, S.;Korn, N.;Scheuermann, G.	Univ. of Leipzig, Leipzig, Germany|c|;;	38490141400;38489846500;37282574800
	SciVis+InfoVis+some VAST	Dec. 2012	Coherency-Based Curve Compression for High-Order Finite Element Model Visualization	10.1109/TVCG.2012.206	http://dx.doi.org/10.1109/TVCG.2012.206	2315	2324	6327236	data compression;data reduction;data visualisation;finite element analysis;ray tracing;rendering (computer graphics)	coherency-based curve compression;computational complexity;computes view-independent proxy rays;data reduction;decoupling method;engineering;geosciences;high-order basis functions;high-order finite element model visualization;interactive data exploration;interactive exploration;interactive rendering;life sciences;medicine;nonuniform grid;ray casting;regular grid structure;rendering stage;time-consuming simulations;uniformly distributed sample points;viewing ray;volumetric data sets;world-to-material space transformation	Computational modeling;Finite element methods;Rendering (computer graphics);Splines (mathematics)	Finite element visualization;GPU-based ray-casting	Finite element (FE) models are frequently used in engineering and life sciences within time-consuming simulations. In contrast with the regular grid structure facilitated by volumetric data sets, as used in medicine or geosciences, FE models are defined over a non-uniform grid. Elements can have curved faces and their interior can be defined through high-order basis functions, which pose additional challenges when visualizing these models. During ray-casting, the uniformly distributed sample points along each viewing ray must be transformed into the material space defined within each element. The computational complexity of this transformation makes a straightforward approach inadequate for interactive data exploration. In this paper, we introduce a novel coherency-based method which supports the interactive exploration of FE models by decoupling the expensive world-to-material space transformation from the rendering stage, thereby allowing it to be performed within a precomputation stage. Therefore, our approach computes view-independent proxy rays in material space, which are clustered to facilitate data reduction. During rendering, these proxy rays are accessed, and it becomes possible to visually analyze high-order FE models at interactive frame rates, even when they are time-varying or consist of multiple modalities. Within this paper, we provide the necessary background about the FE data, describe our decoupling method, and introduce our interactive rendering algorithm. Furthermore, we provide visual results and analyze the error introduced by the presented approach.	Bock, A.;Sunden, E.;Bingchen Liu;Wunsche, B.;Ropinski, T.	Sci. Visualization Group, Linkoping Univ., Linkoping, Sweden|c|;;;;	38489536800;38243026500;38489425700;37571592800;37295281400
	SciVis+InfoVis+some VAST	Dec. 2012	ElVis: A System for the Accurate and Interactive Visualization of High-Order Finite Element Solutions	10.1109/TVCG.2012.218	http://dx.doi.org/10.1109/TVCG.2012.218	2325	2334	6327237	Galerkin method;data visualisation;finite element analysis;graphics processing units;mathematics computing;parallel architectures;partial differential equations;ray tracing;rendering (computer graphics)	CUDA;ElVis;NVIDIA OptiX GPU-based ray-tracing engine;PDE;accelerated ray traversal;aerodynamic simulations;data visualization;debugging effort;element visualizer;high-order discontinuous Galerkin finite element solution;high-order finite element solution;high-order geometry;interactive visualization;linear function;linear method;linear primitive;open-source scientific visualization system;parallel evaluation;pixel-exact representation;pixel-exact visualization;visual artifact;visualization error minimisation;volume rendering	Data models;Finite element methods;Geometry;Isosurfaces;Polynomials;Rendering (computer graphics)	High-order finite elements;contours;cut surface extraction;discontinuous Galerkin;fluid flow simulation;isosurfaces;spectral/hp elements	This paper presents the Element Visualizer (ElVis), a new, open-source scientific visualization system for use with high-order finite element solutions to PDEs in three dimensions. This system is designed to minimize visualization errors of these types of fields by querying the underlying finite element basis functions (e.g., high-order polynomials) directly, leading to pixel-exact representations of solutions and geometry. The system interacts with simulation data through runtime plugins, which only require users to implement a handful of operations fundamental to finite element solvers. The data in turn can be visualized through the use of cut surfaces, contours, isosurfaces, and volume rendering. These visualization algorithms are implemented using NVIDIA's OptiX GPU-based ray-tracing engine, which provides accelerated ray traversal of the high-order geometry, and CUDA, which allows for effective parallel evaluation of the visualization algorithms. The direct interface between ElVis and the underlying data differentiates it from existing visualization tools. Current tools assume the underlying data is composed of linear primitives; high-order data must be interpolated with linear functions as a result. In this work, examples drawn from aerodynamic simulations-high-order discontinuous Galerkin finite element solutions of aerodynamic flows in particular-will demonstrate the superiority of ElVis' pixel-exact approach when compared with traditional linear-interpolation methods. Such methods can introduce a number of inaccuracies in the resulting visualization, making it unclear if visual artifacts are genuine to the solution data or if these artifacts are the result of interpolation errors. Linear methods additionally cannot properly visualize curved geometries (elements or boundaries) which can greatly inhibit developers' debugging efforts. As we will show, pixel-exact visualization exhibits none of these issues, removing the visualization scheme as a source of - ncertainty for engineers using ElVis.	Nelson, B.;Liu, E.;Kirby, R.M.;Haimes, R.	Sch. of Comput., Univ. of Utah, Salt Lake City, UT, USA|c|;;;	37557887400;38490364600;37275716100;37282898700
	SciVis+InfoVis+some VAST	Dec. 2012	Fuzzy Volume Rendering	10.1109/TVCG.2012.227	http://dx.doi.org/10.1109/TVCG.2012.227	2335	2344	6327238	data visualisation;fuzzy set theory;reliability;rendering (computer graphics);uncertainty handling	fuzzy transform;fuzzy volume rendering;possibility-based representation;reliable computing;self-validating computational model;uncertainty-aware rendering;visualization;volume data	Computational modeling;Data visualization;Rendering (computer graphics);Transforms;Uncertainty;Volume measurement	Uncertainty visualization;verifiable visualization;volume rendering	In order to assess the reliability of volume rendering, it is necessary to consider the uncertainty associated with the volume data and how it is propagated through the volume rendering algorithm, as well as the contribution to uncertainty from the rendering algorithm itself. In this work, we show how to apply concepts from the field of reliable computing in order to build a framework for management of uncertainty in volume rendering, with the result being a self-validating computational model to compute a posteriori uncertainty bounds. We begin by adopting a coherent, unifying possibility-based representation of uncertainty that is able to capture the various forms of uncertainty that appear in visualization, including variability, imprecision, and fuzziness. Next, we extend the concept of the fuzzy transform in order to derive rules for accumulation and propagation of uncertainty. This representation and propagation of uncertainty together constitute an automated framework for management of uncertainty in visualization, which we then apply to volume rendering. The result, which we call fuzzy volume rendering, is an uncertainty-aware rendering algorithm able to produce more complete depictions of the volume data, thereby allowing more reliable conclusions and informed decisions. Finally, we compare approaches for self-validated computation in volume rendering, demonstrating that our chosen method has the ability to handle complex uncertainty while maintaining efficiency.	Fout, N.;Kwan-Liu Ma	;	37945247900;37275869400
	SciVis+InfoVis+some VAST	Dec. 2012	Automatic Tuning of Spatially Varying Transfer Functions for Blood Vessel Visualization	10.1109/TVCG.2012.203	http://dx.doi.org/10.1109/TVCG.2012.203	2345	2354	6327239	blood vessels;computerised tomography;data visualisation;medical image processing;optimisation;rendering (computer graphics);transfer functions	CT angiography datasets;automatic tuning;blood stream;blood vessel visualization;blood vessels;clinical routine;computed tomography angiography;direct volume rendering;enhanced image contrast;image data;mixture concentration;optimization criterion;spatially varying transfer functions;vascular diseases;vesselness descriptor	Biomedical imaging;Blood vessels;Data visualization;Optimization;Rendering (computer graphics)	Direct volume rendering;transfer functions;vessel visualization	Computed Tomography Angiography (CTA) is commonly used in clinical routine for diagnosing vascular diseases. The procedure involves the injection of a contrast agent into the blood stream to increase the contrast between the blood vessels and the surrounding tissue in the image data. CTA is often visualized with Direct Volume Rendering (DVR) where the enhanced image contrast is important for the construction of Transfer Functions (TFs). For increased efficiency, clinical routine heavily relies on preset TFs to simplify the creation of such visualizations for a physician. In practice, however, TF presets often do not yield optimal images due to variations in mixture concentration of contrast agent in the blood stream. In this paper we propose an automatic, optimization-based method that shifts TF presets to account for general deviations and local variations of the intensity of contrast enhanced blood vessels. Some of the advantages of this method are the following. It computationally automates large parts of a process that is currently performed manually. It performs the TF shift locally and can thus optimize larger portions of the image than is possible with manual interaction. The method is based on a well known vesselness descriptor in the definition of the optimization criterion. The performance of the method is illustrated by clinically relevant CT angiography datasets displaying both improved structural overviews of vessel trees and improved adaption to local variations of contrast concentration.	Lathen, G.;Lindholm, S.;Lenz, R.;Persson, A.;Borga, M.	Center for Med. Image Sci. & Visualization (CMIV), Linkoping Univ., Linkoping, Sweden|c|;;;;	38187724100;37591265100;37270605100;37604521200;37294429200
	SciVis+InfoVis+some VAST	Dec. 2012	Hierarchical Exploration of Volumes Using Multilevel Segmentation of the Intensity-Gradient Histograms	10.1109/TVCG.2012.231	http://dx.doi.org/10.1109/TVCG.2012.231	2355	2363	6327240	data visualisation;gradient methods;image segmentation;information theory	data-driven coarse-to-fine hierarchy;exploration hierarchy;hierarchical volume exploration;histogram segmentation;information-theoretic measures;intensity-gradient 2D histogram;intensity-gradient histogram;normalized-cut multilevel segmentation;user exploration behavior;visual exploration;visual segmentation;volumetric data segment;volumetric dataset	Entropy;Histograms;Image segmentation;Shape analysis;Transfer functions;Visualization;Volume measurement	Information-guided exploration;Volume exploration;normalized cut;volume classification	Visual exploration of volumetric datasets to discover the embedded features and spatial structures is a challenging and tedious task. In this paper we present a semi-automatic approach to this problem that works by visually segmenting the intensity-gradient 2D histogram of a volumetric dataset into an exploration hierarchy. Our approach mimics user exploration behavior by analyzing the histogram with the normalized-cut multilevel segmentation technique. Unlike previous work in this area, our technique segments the histogram into a reasonable set of intuitive components that are mutually exclusive and collectively exhaustive. We use information-theoretic measures of the volumetric data segments to guide the exploration. This provides a data-driven coarse-to-fine hierarchy for a user to interactively navigate the volume in a meaningful manner.	Cheuk Yiu Ip;Varshney, A.;JaJa, J.	Inst. for Adv. Comput. Studies, Univ. of Maryland, College Park, MD, USA|c|;;	37586231900;37282560200;37276261200
	SciVis+InfoVis+some VAST	Dec. 2012	Historygrams: Enabling Interactive Global Illumination in Direct Volume Rendering using Photon Mapping	10.1109/TVCG.2012.232	http://dx.doi.org/10.1109/TVCG.2012.232	2364	2371	6327241	interactive systems;lighting;rendering (computer graphics)	DVR;TF;advanced global illumination techniques;arbitrary phase functions;bidirectional reflectance distribution functions;direct volume rendering;historygram approach;interactive transfer function;interactive volumetric global illumination;light sources;light transport equations;material editing;parameter change;photon mapping techniques;photon media interactions;photon scattering events;property change;view-ray	Lighting;Photonics;Rendering (computer graphics);Scattering;Volume measurement	Volume rendering;global illumination;participating media;photon mapping	In this paper, we enable interactive volumetric global illumination by extending photon mapping techniques to handle interactive transfer function (TF) and material editing in the context of volume rendering. We propose novel algorithms and data structures for finding and evaluating parts of a scene affected by these parameter changes, and thus support efficient updates of the photon map. In direct volume rendering (DVR) the ability to explore volume data using parameter changes, such as editable TFs, is of key importance. Advanced global illumination techniques are in most cases computationally too expensive, as they prevent the desired interactivity. Our technique decreases the amount of computation caused by parameter changes, by introducing Historygrams which allow us to efficiently reuse previously computed photon media interactions. Along the viewing rays, we utilize properties of the light transport equations to subdivide a view-ray into segments and independently update them when invalid. Unlike segments of a view-ray, photon scattering events within the volumetric medium needs to be sequentially updated. Using our Historygram approach, we can identify the first invalid photon interaction caused by a property change, and thus reuse all valid photon interactions. Combining these two novel concepts, supports interactive editing of parameters when using volumetric photon mapping in the context of DVR. As a consequence, we can handle arbitrarily shaped and positioned light sources, arbitrary phase functions, bidirectional reflectance distribution functions and multiple scattering which has previously not been possible in interactive DVR.	Jonsson, D.;Kronander, J.;Ropinski, T.;Ynnerman, A.	Linkoping Univ., Linkoping, Sweden|c|;;;	38228659100;38230927900;37295281400;37284192000
	SciVis+InfoVis+some VAST	Dec. 2012	Structure-Aware Lighting Design for Volume Visualization	10.1109/TVCG.2012.267	http://dx.doi.org/10.1109/TVCG.2012.267	2372	2381	6327242	data visualisation;rendering (computer graphics);transfer functions	detail perception;direct volume rendering;external lighting;feature information;human visual system;image information;lighting similarity;lighting stability;multiple light sources;shape perception;structural information enhancement;structure-aware automatic lighting design;transfer function;volume data sets;volume visualization	Entropy;Light sources;Lighting;Measurement;Rendering (computer graphics);Shape analysis;Stability analysis	Automatic lighting design;lighting similarity;lighting stability;structural dissimilarity;volume rendering	Lighting design is a complex, but fundamental, problem in many fields. In volume visualization, direct volume rendering generates an informative image without external lighting, as each voxel itself emits radiance. However, external lighting further improves the shape and detail perception of features, and it also determines the effectiveness of the communication of feature information. The human visual system is highly effective in extracting structural information from images, and to assist it further, this paper presents an approach to structure-aware automatic lighting design by measuring the structural changes between the images with and without external lighting. Given a transfer function and a viewpoint, the optimal lighting parameters are those that provide the greatest enhancement to structural information - the shape and detail information of features are conveyed most clearly by the optimal lighting parameters. Besides lighting goodness, the proposed metric can also be used to evaluate lighting similarity and stability between two sets of lighting parameters. Lighting similarity can be used to optimize the selection of multiple light sources so that different light sources can reveal distinct structural information. Our experiments with several volume data sets demonstrate the effectiveness of the structure-aware lighting design approach. It is well suited to use by novices as it requires little technical understanding of the rendering parameters associated with direct volume rendering.	Yubo Tao	State Key Lab. of CAD&amp;CG, Zhejiang Univ., Hangzhou, China|c|	
	SciVis+InfoVis+some VAST	Dec. 2012	Multivariate Data Analysis Using Persistence-Based Filtering and Topological Signatures	10.1109/TVCG.2012.248	http://dx.doi.org/10.1109/TVCG.2012.248	2382	2391	6327243	data analysis;data structures;data visualisation;history;information filtering;pattern classification;topology	algebraic topology;analysis pipeline;arbitrary high-dimensional data sets;central structures;cultural heritage;data points classification;hierarchical manner;high-dimensional real-world data sets;interaction capability;interactive visualization techniques;large data sets;multivariate data analysis;noisy data;parameter space evaluation;persistence rings;persistence-based filtering algorithm;persistent homology;problem complexity;relevant structures;significant structures extraction;stopping criterion;synthetic data sets;topological data analysis;topological features;topological objects;topological signatures;visualization pipeline	Clustering methods;Multivariate data sets;Network topology	Topological persistence;clustering;multivariate data	The extraction of significant structures in arbitrary high-dimensional data sets is a challenging task. Moreover, classifying data points as noise in order to reduce a data set bears special relevance for many application domains. Standard methods such as clustering serve to reduce problem complexity by providing the user with classes of similar entities. However, they usually do not highlight relations between different entities and require a stopping criterion, e.g. the number of clusters to be detected. In this paper, we present a visualization pipeline based on recent advancements in algebraic topology. More precisely, we employ methods from persistent homology that enable topological data analysis on high-dimensional data sets. Our pipeline inherently copes with noisy data and data sets of arbitrary dimensions. It extracts central structures of a data set in a hierarchical manner by using a persistence-based filtering algorithm that is theoretically well-founded. We furthermore introduce persistence rings, a novel visualization technique for a class of topological features-the persistence intervals-of large data sets. Persistence rings provide a unique topological signature of a data set, which helps in recognizing similarities. In addition, we provide interactive visualization techniques that assist the user in evaluating the parameter space of our method in order to extract relevant structures. We describe and evaluate our analysis pipeline by means of two very distinct classes of data sets: First, a class of synthetic data sets containing topological objects is employed to highlight the interaction capabilities of our method. Second, in order to affirm the utility of our technique, we analyse a class of high-dimensional real-world data sets arising from current research in cultural heritage.	Rieck, B.;Mara, H.;Leitte, H.	Interdiscipl. Center for Sci. Comput. (IWR), Heidelberg Univ., Heidelberg, Germany|c|;;	38489062800;37298496400;38488927600
	SciVis+InfoVis+some VAST	Dec. 2012	Surface-Based Structure Analysis and Visualization for Multifield Time-Varying Datasets	10.1109/TVCG.2012.269	http://dx.doi.org/10.1109/TVCG.2012.269	2392	2401	6327244	data visualisation;iterative methods	feature analysis;iterative algorithm;large-scale time-varying simulation;multifield feature definition;multifield time-varying dataset visualization;nonrigid surface registration;salient feature;surface-based structure analysis;surface-centric model;visual analysis	Context awareness;Correlation;Data visualization;Feature extraction;Shape analysis	Multifield;surface structures;time-varying	This paper introduces a new feature analysis and visualization method for multifield datasets. Our approach applies a surface-centric model to characterize salient features and form an effective, schematic representation of the data. We propose a simple, geometrically motivated, multifield feature definition. This definition relies on an iterative algorithm that applies existing theory of skeleton derivation to fuse the structures from the constitutive fields into a coherent data description, while addressing noise and spurious details. This paper also presents a new method for non-rigid surface registration between the surfaces of consecutive time steps. This matching is used in conjunction with clustering to discover the interaction patterns between the different fields and their evolution over time. We document the unified visual analysis achieved by our method in the context of several multifield problems from large-scale time-varying simulations.	Barakat, S.S.;Rutten, M.;Tricoche, X.	;;	38328697800;37437412100;37282575100
	SciVis+InfoVis+some VAST	Dec. 2012	How Capacity Limits of Attention Influence Information Visualization Effectiveness	10.1109/TVCG.2012.233	http://dx.doi.org/10.1109/TVCG.2012.233	2402	2410	6327245	data visualisation	information visualization effectiveness;visible objects;visual elements;visual feature type	Accuracy;Color;Data visualization;Image color analysis;Layout;Time factors;Visualization	Perception;attention;color;goal-oriented design;layout;motion;nominal axis;user study	In this paper, we explore how the capacity limits of attention influence the effectiveness of information visualizations. We conducted a series of experiments to test how visual feature type (color vs. motion), layout, and variety of visual elements impacted user performance. The experiments tested users' abilities to (1) determine if a specified target is on the screen, (2) detect an odd-ball, deviant target, different from the other visible objects, and (3) gain a qualitative overview by judging the number of unique categories on the screen. Our results show that the severe capacity limits of attention strongly modulate the effectiveness of information visualizations, particularly the ability to detect unexpected information. Keeping in mind these capacity limits, we conclude with a set of design guidelines which depend on a visualization's intended use.	Haroz, S.;Whitney, D.	Univ. of California, Davis, CA, USA|c|;	37697696900;38489136700
	SciVis+InfoVis+some VAST	Dec. 2012	Different Strokes for Different Folks: Visual Presentation Design between Disciplines	10.1109/TVCG.2012.214	http://dx.doi.org/10.1109/TVCG.2012.214	2411	2420	6327246	computer aided instruction;data visualisation;technical presentation	PCA;academic disciplines;assistive tools;charts;design differences;design guidelines;diagrams;eigenslides;electronic slideshows;ethnographic study;formal sciences;human-centered authoring tools;humanities;image-based analysis;information visualization;natural sciences;social sciences;visual information;visual presentation design;visual presentations;visualization ethnography;whiteboard chalk talks	Buildings;Cognitive science;Educational institutions;Encoding;Principal component analysis;Semantics;Visualization	Presentations;design;information visualization;visual analysis	We present an ethnographic study of design differences in visual presentations between academic disciplines. Characterizing design conventions between users and data domains is an important step in developing hypotheses, tools, and design guidelines for information visualization. In this paper, disciplines are compared at a coarse scale between four groups of fields: social, natural, and formal sciences; and the humanities. Two commonplace presentation types were analyzed: electronic slideshows and whiteboard “chalk talks”. We found design differences in slideshows using two methods - coding and comparing manually-selected features, like charts and diagrams, and an image-based analysis using PCA called eigenslides. In whiteboard talks with controlled topics, we observed design behaviors, including using representations and formalisms from a participant's own discipline, that suggest authors might benefit from novel assistive tools for designing presentations. Based on these findings, we discuss opportunities for visualization ethnography and human-centered authoring tools for visual information.	Gomez, S.R.;Jianu, R.;Ziemkiewicz, C.;Hua Guo;Laidlaw, D.H.	Brown Univ., Providence, RI, USA|c|;;;;	38489539100;37887934300;37548028800;38489241200;37275712600
	SciVis+InfoVis+some VAST	Dec. 2012	Does an Eye Tracker Tell the Truth about Visualizations?: Findings while Investigating Visualizations for Decision Making	10.1109/TVCG.2012.215	http://dx.doi.org/10.1109/TVCG.2012.215	2421	2430	6327247	cognitive systems;data visualisation;decision making;eye;information retrieval	SimulSort;browsing behavior;cognitive process;decision making;eye movements;eye tracker;information visualization;peripheral vision;tabular visualization	Crowdsourcing;Data visualization;Decision making;Market research;Research and development;Tracking	Visualized decision making;crowdsourcing;eye tracking;limitations;peripheral vision;quantitative empirical study	For information visualization researchers, eye tracking has been a useful tool to investigate research participants' underlying cognitive processes by tracking their eye movements while they interact with visual techniques. We used an eye tracker to better understand why participants with a variant of a tabular visualization called `SimulSort' outperformed ones with a conventional table and typical one-column sorting feature (i.e., Typical Sorting). The collected eye-tracking data certainly shed light on the detailed cognitive processes of the participants; SimulSort helped with decision-making tasks by promoting efficient browsing behavior and compensatory decision-making strategies. However, more interestingly, we also found unexpected eye-tracking patterns with Simul- Sort. We investigated the cause of the unexpected patterns through a crowdsourcing-based study (i.e., Experiment 2), which elicited an important limitation of the eye tracking method: incapability of capturing peripheral vision. This particular result would be a caveat for other visualization researchers who plan to use an eye tracker in their studies. In addition, the method to use a testing stimulus (i.e., influential column) in Experiment 2 to verify the existence of such limitations would be useful for researchers who would like to verify their eye tracking results.	Sung-Hee Kim;Zhihua Dong;Hanjun Xian;Upatising, B.;Ji Soo Yi	Sch. ofIndustrial Eng., Purdue Univ., West Lafayette, IN, USA|c|;;;;	38489894700;38489279100;37399307600;38489813800;38238292000
	SciVis+InfoVis+some VAST	Dec. 2012	Design Study Methodology: Reflections from the Trenches and the Stacks	10.1109/TVCG.2012.213	http://dx.doi.org/10.1109/TVCG.2012.213	2431	2440	6327248	data visualisation	design study methodology;information location axis;problem-driven visualization;task clarity axis;visualization design guideline	Algorithm design and analysis;Collaboration;Data visualization;Design methodology;Logic gates;Visualization	Design study;framework;methodology;visualization	Design studies are an increasingly popular form of problem-driven visualization research, yet there is little guidance available about how to do them effectively. In this paper we reflect on our combined experience of conducting twenty-one design studies, as well as reading and reviewing many more, and on an extensive literature review of other field work methods and methodologies. Based on this foundation we provide definitions, propose a methodological framework, and provide practical guidance for conducting design studies. We define a design study as a project in which visualization researchers analyze a specific real-world problem faced by domain experts, design a visualization system that supports solving this problem, validate the design, and reflect about lessons learned in order to refine visualization design guidelines. We characterize two axes - a task clarity axis from fuzzy to crisp and an information location axis from the domain expert's head to the computer - and use these axes to reason about design study contributions, their suitability, and uniqueness from other approaches. The proposed methodological framework consists of 9 stages: learn, winnow, cast, discover, design, implement, deploy, reflect, and write. For each stage we provide practical guidance and outline potential pitfalls. We also conducted an extensive literature survey of related methodological approaches that involve a significant amount of qualitative field work, and compare design study methodology to that of ethnography, grounded theory, and action research.	Sedlmair, M.;Meyer, M.;Munzner, T.	;;	37590945600;37564728700;37349490300
	SciVis+InfoVis+some VAST	Dec. 2012	Graphical Tests for Power Comparison of Competing Designs	10.1109/TVCG.2012.230	http://dx.doi.org/10.1109/TVCG.2012.230	2441	2448	6327249	computer graphics	Amazon Mechanical Turk;MTurk;cartesian coordinates;coordinate system;distributional assumptions;graphical designs;graphical findings;graphical tests;lineups;plot designs;polar coordinates;power comparison;spotting patterns;standard statistical inference tests;visual testing	Accuracy;Data models;Inference mechanisms;Observers;Statistical analysis;Visual analytics	Efficiency of displays;Lineups;Power comparison;Visual inference	Lineups [4, 28] have been established as tools for visual testing similar to standard statistical inference tests, allowing us to evaluate the validity of graphical findings in an objective manner. In simulation studies [12] lineups have been shown as being efficient: the power of visual tests is comparable to classical tests while being much less stringent in terms of distributional assumptions made. This makes lineups versatile, yet powerful, tools in situations where conditions for regular statistical tests are not or cannot be met. In this paper we introduce lineups as a tool for evaluating the power of competing graphical designs. We highlight some of the theoretical properties and then show results from two studies evaluating competing designs: both studies are designed to go to the limits of our perceptual abilities to highlight differences between designs. We use both accuracy and speed of evaluation as measures of a successful design. The first study compares the choice of coordinate system: polar versus cartesian coordinates. The results show strong support in favor of cartesian coordinates in finding fast and accurate answers to spotting patterns. The second study is aimed at finding shift differences between distributions. Both studies are motivated by data problems that we have recently encountered, and explore using simulated data to evaluate the plot designs under controlled conditions. Amazon Mechanical Turk (MTurk) is used to conduct the studies. The lineups provide an effective mechanism for objectively evaluating plot designs.	Hofmann, H.;Follett, L.;Majumder, M.;Cook, D.	Stat., Iowa State Univ., Ames, IA, USA|c|;;;	37592793000;38489858600;38489014500;37358631700
	SciVis+InfoVis+some VAST	Dec. 2012	A User Study on Curved Edges in Graph Visualization	10.1109/TVCG.2012.189	http://dx.doi.org/10.1109/TVCG.2012.189	2449	2456	6327250	data visualisation;graph theory	common neighbor;connectivity determination;edge curvature;edge variation;graph readability;graph task;graph visualization;node degree;node-link diagram;shortest path;subjective rating	Analysis of variance;Educational institutions;Layout;Optimization;Software;User interfaces;Visualization	Graph;curved edges;evaluation;visualization	Recently there has been increasing research interest in displaying graphs with curved edges to produce more readable visualizations. While there are several automatic techniques, little has been done to evaluate their effectiveness empirically. In this paper we present two experiments studying the impact of edge curvature on graph readability. The goal is to understand the advantages and disadvantages of using curved edges for common graph tasks compared to straight line segments, which are the conventional choice for showing edges in node-link diagrams. We included several edge variations: straight edges, edges with different curvature levels, and mixed straight and curved edges. During the experiments, participants were asked to complete network tasks including determination of connectivity, shortest path, node degree, and common neighbors. We also asked the participants to provide subjective ratings of the aesthetics of different edge types. The results show significant performance differences between the straight and curved edges and clear distinctions between variations of curved edges.	Kai Xu;Rooney, C.;Passmore, P.;Dong-Han Ham;Nguyen, P.H.	Middlesex Univ., London, UK|c|;;;;	38489059400;38488869200;38180787500;38489463400;38490354300
	SciVis+InfoVis+some VAST	Dec. 2012	Compressed Adjacency Matrices: Untangling Gene Regulatory Networks	10.1109/TVCG.2012.208	http://dx.doi.org/10.1109/TVCG.2012.208	2457	2466	6327251	biology computing;data visualisation;genetics;matrix algebra;network theory (graphs)	compressed adjacency matrices;directed networks;gene regulatory networks;motifs;neatly-arranged visualization;network characteristics;node-link diagrams;rearrangement clustering;scale-free distribution;sparse network;standard adjacency matrix;standard visualization;structural characteristics;visualization domain	Bismuth;Computer aided manufacturing;Layout;Proteins;Sparse matrices;Standards;Visualization	Network;adjacency matrix;gene regulation;scale-free	We present a novel technique-Compressed Adjacency Matrices-for visualizing gene regulatory networks. These directed networks have strong structural characteristics: out-degrees with a scale-free distribution, in-degrees bound by a low maximum, and few and small cycles. Standard visualization techniques, such as node-link diagrams and adjacency matrices, are impeded by these network characteristics. The scale-free distribution of out-degrees causes a high number of intersecting edges in node-link diagrams. Adjacency matrices become space-inefficient due to the low in-degrees and the resulting sparse network. Compressed adjacency matrices, however, exploit these structural characteristics. By cutting open and rearranging an adjacency matrix, we achieve a compact and neatly-arranged visualization. Compressed adjacency matrices allow for easy detection of subnetworks with a specific structure, so-called motifs, which provide important knowledge about gene regulatory networks to domain experts. We summarize motifs commonly referred to in the literature, and relate them to network analysis tasks common to the visualization domain. We show that a user can easily find the important motifs in compressed adjacency matrices, and that this is hard in standard adjacency matrix and node-link diagrams. We also demonstrate that interaction techniques for standard adjacency matrices can be used for our compressed variant. These techniques include rearrangement clustering, highlighting, and filtering.	Dinkla, K.;Westenberg, M.A.;van Wijk, J.J.	;;	38489573400;38185191700;37267249200
	SciVis+InfoVis+some VAST	Dec. 2012	Visualizing Network Traffic to Understand the Performance of Massively Parallel Simulations	10.1109/TVCG.2012.286	http://dx.doi.org/10.1109/TVCG.2012.286	2467	2476	6327252	data visualisation;laser beams;mainframes;network topology;parallel processing;physics computing;plasma simulation;plasma-beam interactions	2D view;3D view;IBM Blue Gene-P system;compute nodes;data visualization;hardware interconnect;laser interaction;network structure;network traffic visualization;packet flow;parallel application developers;parallel multiphysics code pF3D;parallel simulation performance;physical network topology;plasma interaction;supercomputers	Computational modeling;Data visualization;Hardware;Layout;Network topology;Performance evaluation;Supercomputers	Performance analysis;network traffic visualization;projected graph layouts	The performance of massively parallel applications is often heavily impacted by the cost of communication among compute nodes. However, determining how to best use the network is a formidable task, made challenging by the ever increasing size and complexity of modern supercomputers. This paper applies visualization techniques to aid parallel application developers in understanding the network activity by enabling a detailed exploration of the flow of packets through the hardware interconnect. In order to visualize this large and complex data, we employ two linked views of the hardware network. The first is a 2D view, that represents the network structure as one of several simplified planar projections. This view is designed to allow a user to easily identify trends and patterns in the network traffic. The second is a 3D view that augments the 2D view by preserving the physical network topology and providing a context that is familiar to the application developers. Using the massively parallel multi-physics code pF3D as a case study, we demonstrate that our tool provides valuable insight that we use to explain and optimize pF3D's performance on an IBM Blue Gene/P system.	Landge, A.G.;Levine, J.A.;Bhatele, A.;Isaacs, K.E.;Gamblin, T.;Schulz, M.;Langer, S.H.;Bremer, P.-T.;Pascucci, V.	;;;;;;;;	38490116900;37853884500;37572974300;38490580100;37892085700;37290317700;37297252300;37564112000;37284312600
	SciVis+InfoVis+some VAST	Dec. 2012	Memorability of Visual Features in Network Diagrams	10.1109/TVCG.2012.245	http://dx.doi.org/10.1109/TVCG.2012.245	2477	2485	6327253	data visualisation;graph theory;interactive systems;network theory (graphs)	cognitive impact;graphs;interactive network-based visualisations;network diagrams;parallel edges;static network-based visualisations;task enabled visual processing;visceral level;visual feature memorability	Algorithm design and analysis;Educational institutions;Image edge detection;Layout;Shape;Topology;Visualization	Network diagrams;diagram recall;experiment;graph layout;perceptual theories;visual features	We investigate the cognitive impact of various layout features-symmetry, alignment, collinearity, axis alignment and orthogonality - on the recall of network diagrams (graphs). This provides insight into how people internalize these diagrams and what features should or shouldn't be utilised when designing static and interactive network-based visualisations. Participants were asked to study, remember, and draw a series of small network diagrams, each drawn to emphasise a particular visual feature. The visual features were based on existing theories of perception, and the task enabled visual processing at the visceral level only. Our results strongly support the importance of visual features such as symmetry, collinearity and orthogonality, while not showing any significant impact for node-alignment or parallel edges.	Marriott, K.;Purchase, H.;Wybrow, M.;Goncu, C.	Monash Univ., Melbourne, VIC, Australia|c|;;;	37354163600;37329672500;37867777700;38490359600
	SciVis+InfoVis+some VAST	Dec. 2012	Interactive Level-of-Detail Rendering of Large Graphs	10.1109/TVCG.2012.238	http://dx.doi.org/10.1109/TVCG.2012.238	2486	2495	6327254	rendering (computer graphics)	common graphics hardware;density-based node aggregation;edge cumulation;graph data;interactive level-of-detail rendering;straight-line graph drawings	Aggregates;Data visualization;Image color analysis;Image edge detection;Rendering (computer graphics)	Graph visualization;OpenGL;edge aggregation	We propose a technique that allows straight-line graph drawings to be rendered interactively with adjustable level of detail. The approach consists of a novel combination of edge cumulation with density-based node aggregation and is designed to exploit common graphics hardware for speed. It operates directly on graph data and does not require precomputed hierarchies or meshes. As proof of concept, we present an implementation that scales to graphs with millions of nodes and edges, and discuss several example applications.	Zinsmaier, M.;Brandes, U.;Deussen, O.;Strobelt, H.	;;;	38489526500;37550836200;37266781000;38108845300
	SciVis+InfoVis+some VAST	Dec. 2012	Visual Semiotics &amp;amp; Uncertainty Visualization: An Empirical Study	10.1109/TVCG.2012.279	http://dx.doi.org/10.1109/TVCG.2012.279	2496	2505	6327255	data visualisation	iconic representations;intuitive abstract;map reading task;representation intuitiveness;typology;uncertainty visualization;visual semiotics;visual variable	Semiotics;Syntactics;Uncertainty;Visual analytics	Uncertainty visualization;semiotics;uncertainty categories;visual variables	This paper presents two linked empirical studies focused on uncertainty visualization. The experiments are framed from two conceptual perspectives. First, a typology of uncertainty is used to delineate kinds of uncertainty matched with space, time, and attribute components of data. Second, concepts from visual semiotics are applied to characterize the kind of visual signification that is appropriate for representing those different categories of uncertainty. This framework guided the two experiments reported here. The first addresses representation intuitiveness, considering both visual variables and iconicity of representation. The second addresses relative performance of the most intuitive abstract and iconic representations of uncertainty on a map reading task. Combined results suggest initial guidelines for representing uncertainty and discussion focuses on practical applicability of results.	MacEachren, A.M.;Roth, R.E.;O'Brien, J.;Li, B.;Swingley, D.;Gahegan, M.	;;;;;	37374699000;38489149900;38490350000;38490350500;38489925400;38490482800
	SciVis+InfoVis+some VAST	Dec. 2012	Comparing Clusterings Using Bertin&#39;s Idea	10.1109/TVCG.2012.207	http://dx.doi.org/10.1109/TVCG.2012.207	2506	2515	6327256	computer graphics;diagrams;pattern clustering	circos diagrams;clusterings;contingency tables;fluctuation diagrams;graphical displays;logical simplification;mosaicplots;nominal classification variables;parallel coordinates plots;parallel sets plots;tableplots;top-down partitioning	Classification;Clustering algorithms;Graphics;Optimization;Stress measurement	Order optimization;classification;fluctuation diagrams;seriation	Classifying a set of objects into clusters can be done in numerous ways, producing different results. They can be visually compared using contingency tables [27], mosaicplots [13], fluctuation diagrams [15], tableplots [20] , (modified) parallel coordinates plots [28], Parallel Sets plots [18] or circos diagrams [19]. Unfortunately the interpretability of all these graphical displays decreases rapidly with the numbers of categories and clusterings. In his famous book A Semiology of Graphics [5] Bertin writes “the discovery of an ordered concept appears as the ultimate point in logical simplification since it permits reducing to a single instant the assimilation of series which previously required many instants of study”. Or in more everyday language, if you use good orderings you can see results immediately that with other orderings might take a lot of effort. This is also related to the idea of effect ordering [12], that data should be organised to reflect the effect you want to observe. This paper presents an efficient algorithm based on Bertin's idea and concepts related to Kendall's t [17], which finds informative joint orders for two or more nominal classification variables. We also show how these orderings improve the various displays and how groups of corresponding categories can be detected using a top-down partitioning algorithm. Different clusterings based on data on the environmental performance of cars sold in Germany are used for illustration. All presented methods are available in the R package extracat which is used to compute the optimized orderings for the example dataset.	Pilhofer, A.;Gribov, A.;Unwin, A.	Univ. of Augsburg, Augsburg, Germany|c|;;	38489815800;37326693200;38489067600
	SciVis+InfoVis+some VAST	Dec. 2012	Perception of Visual Variables on Tiled Wall-Sized Displays for Information Visualization Applications	10.1109/TVCG.2012.251	http://dx.doi.org/10.1109/TVCG.2012.251	2516	2525	6327257	colour displays;colour graphics;computer displays;data visualisation;human factors;visual perception	angles perception;areas perception;distortion effect estimation;free motion;information visualization applications;lengths perception;perception accuracy;perception estimation tasks;static viewpoint;tiled high-resolution wall-sized displays;visual variables perception;visualization design	Data visualization;Information analysis;Navigation;Visual analytics	Information visualization;perception;wall-displays	We present the results of two user studies on the perception of visual variables on tiled high-resolution wall-sized displays. We contribute an understanding of, and indicators predicting how, large variations in viewing distances and viewing angles affect the accurate perception of angles, areas, and lengths. Our work, thus, helps visualization researchers with design considerations on how to create effective visualizations for these spaces. The first study showed that perception accuracy was impacted most when viewers were close to the wall but differently for each variable (Angle, Area, Length). Our second study examined the effect of perception when participants could move freely compared to when they had a static viewpoint. We found that a far but static viewpoint was as accurate but less time consuming than one that included free motion. Based on our findings, we recommend encouraging viewers to stand further back from the display when conducting perception estimation tasks. If tasks need to be conducted close to the wall display, important information should be placed directly in front of the viewer or above, and viewers should be provided with an estimation of the distortion effects predicted by our work-or encouraged to physically navigate the wall in specific ways to reduce judgement error.	Bezerianos, A.;Isenberg, P.	;	37413699200;37591317800
	SciVis+InfoVis+some VAST	Dec. 2012	Visualizing Flow of Uncertainty through Analytical Processes	10.1109/TVCG.2012.285	http://dx.doi.org/10.1109/TVCG.2012.285	2526	2535	6327258	data analysis;data visualisation	analysis pipeline;analytical processes;data transformation;data transformations;data-intensive applications;multivariate data analysis;uncertainty flow visualization;uncertainty-aware visualization;visual analytics process;visual metaphor	Covariance matrix;Data visualization;Ellipsoids;Uncertainty;Visual analytics	Uncertainty visualization;error ellipsoids;uncertainty fusion;uncertainty propagation;uncertainty quantification	Uncertainty can arise in any stage of a visual analytics process, especially in data-intensive applications with a sequence of data transformations. Additionally, throughout the process of multidimensional, multivariate data analysis, uncertainty due to data transformation and integration may split, merge, increase, or decrease. This dynamic characteristic along with other features of uncertainty pose a great challenge to effective uncertainty-aware visualization. This paper presents a new framework for modeling uncertainty and characterizing the evolution of the uncertainty information through analytical processes. Based on the framework, we have designed a visual metaphor called uncertainty flow to visually and intuitively summarize how uncertainty information propagates over the whole analysis pipeline. Our system allows analysts to interact with and analyze the uncertainty information at different levels of detail. Three experiments were conducted to demonstrate the effectiveness and intuitiveness of our design.	Yingcai Wu;Guo-Xun Yuan;Kwan-Liu Ma	Univ. of California, Davis, CA, USA|c|;;	38489887500;38468805500;37275869400
	SciVis+InfoVis+some VAST	Dec. 2012	Assessing the Effect of Visualizations on Bayesian Reasoning through Crowdsourcing	10.1109/TVCG.2012.199	http://dx.doi.org/10.1109/TVCG.2012.199	2536	2545	6327259	belief networks;data visualisation;inference mechanisms;psychology;statistical analysis	Bayesian problem;Bayesian reasoning;area-proportional Euler diagram;crowdsourcing;glyph representation;numerical value;psychology;statistical information;textual representation;visual design;visual representation;visualization	Bayesian methods;Breast cancer;Crowdsourcing;Sociology;Statistical analysis;Visualization	Bayesian reasoning;Euler diagrams;base rate fallacy;crowdsourcing;glyphs;probabilistic judgment	People have difficulty understanding statistical information and are unaware of their wrong judgments, particularly in Bayesian reasoning. Psychology studies suggest that the way Bayesian problems are represented can impact comprehension, but few visual designs have been evaluated and only populations with a specific background have been involved. In this study, a textual and six visual representations for three classic problems were compared using a diverse subject pool through crowdsourcing. Visualizations included area-proportional Euler diagrams, glyph representations, and hybrid diagrams combining both. Our study failed to replicate previous findings in that subjects' accuracy was remarkably lower and visualizations exhibited no measurable benefit. A second experiment confirmed that simply adding a visualization to a textual Bayesian problem is of little help, even when the text refers to the visualization, but suggests that visualizations are more effective when the text is given without numerical values. We discuss our findings and the need for more such experiments to be carried out on heterogeneous populations of non-experts.	Micallef, L.;Dragicevic, P.;Fekete, J.	INRIA, Sophia Antipolis, France|c|;;	38490161300;37590932700;37407972900
	SciVis+InfoVis+some VAST	Dec. 2012	Organizing Search Results with a Reference Map	10.1109/TVCG.2012.250	http://dx.doi.org/10.1109/TVCG.2012.250	2546	2555	6327260	computational geometry;digital libraries;graph theory;knowledge based systems;pattern clustering;query formulation	Voronoi treemap;digital libraries;dynamic graph layout;hierarchical clustering;interrelated items;knowledge bases;multidimensional scaling layout;newspaper articles;query hits;reference map;search result organization;user mental map preservation	Edge detection;Query processing;Search methods;Tree data structures	Search results;dynamic graph layout;edge bundling;mental map;multidimensional scaling;voronoi treemaps	We propose a method to highlight query hits in hierarchically clustered collections of interrelated items such as digital libraries or knowledge bases. The method is based on the idea that organizing search results similarly to their arrangement on a fixed reference map facilitates orientation and assessment by preserving a user's mental map. Here, the reference map is built from an MDS layout of the items in a Voronoi treemap representing their hierarchical clustering, and we use techniques from dynamic graph layout to align query results with the map. The approach is illustrated on an archive of newspaper articles.			
	SciVis+InfoVis+some VAST	Dec. 2012	Spatial Text Visualization Using Automatic Typographic Maps	10.1109/TVCG.2012.264	http://dx.doi.org/10.1109/TVCG.2012.264	2556	2564	6327261	Web services;cartography;computational geometry;data visualisation	OpenStreetMap project;Web service;automatic typographic maps;crime rate;demographic data;geographic map;graphical features;polygons;polylines;spatial data;spatial text visualization;text data;traffic density;user-defined visual attributes;visual representation	Cities and towns;Data visualization;Geospatial analysis;Rendering (computer graphics);Spatial databases	Geovisualization;label placement;spatial data;text visualization	We present a method for automatically building typographic maps that merge text and spatial data into a visual representation where text alone forms the graphical features. We further show how to use this approach to visualize spatial data such as traffic density, crime rate, or demographic data. The technique accepts a vector representation of a geographic map and spatializes the textual labels in the space onto polylines and polygons based on user-defined visual attributes and constraints. Our sample implementation runs as a Web service, spatializing shape files from the OpenStreetMap project into typographic maps for any region.	Afzal, S.;Maciejewski, R.;Yun Jang;Elmqvist, N.;Ebert, D.S.	Purdue Univ. in West Lafayette, West Lafayette, IN, USA|c|;;;;	37601862300;37396008400;37557344300;37295438200;37282598900
	SciVis+InfoVis+some VAST	Dec. 2012	Stacking-Based Visualization of Trajectory Attribute Data	10.1109/TVCG.2012.265	http://dx.doi.org/10.1109/TVCG.2012.265	2565	2574	6327262	cartography;data visualisation;geographic information systems;solid modelling	2D map;2D time graph;2D-3D display;3D navigation;color mapping;dynamic query mechanism;maritime navigation;radiation surveillance;spatial context;spatio-temporal context;stacked 3D trajectory band;stacking-based visualization;traffic analysis;trajectory attribute data;trajectory visualization	Data visualization;Image color analysis;Navigation;Trajectory	Visualization;exploratory analysis;interaction;spatio-temporal data;trajectory attribute data	Visualizing trajectory attribute data is challenging because it involves showing the trajectories in their spatio-temporal context as well as the attribute values associated with the individual points of trajectories. Previous work on trajectory visualization addresses selected aspects of this problem, but not all of them. We present a novel approach to visualizing trajectory attribute data. Our solution covers space, time, and attribute values. Based on an analysis of relevant visualization tasks, we designed the visualization solution around the principle of stacking trajectory bands. The core of our approach is a hybrid 2D/3D display. A 2D map serves as a reference for the spatial context, and the trajectories are visualized as stacked 3D trajectory bands along which attribute values are encoded by color. Time is integrated through appropriate ordering of bands and through a dynamic query mechanism that feeds temporally aggregated information to a circular time display. An additional 2D time graph shows temporal information in full detail by stacking 2D trajectory bands. Our solution is equipped with analytical and interactive mechanisms for selecting and ordering of trajectories, and adjusting the color mapping, as well as coordinated highlighting and dedicated 3D navigation. We demonstrate the usefulness of our novel visualization by three examples related to radiation surveillance, traffic analysis, and maritime navigation. User feedback obtained in a small experiment indicates that our hybrid 2D/3D solution can be operated quite well.	Tominski, C.;Schumann, H.;Andrienko, G.;Andrienko, N.	;;;	37283236000;37283240400;37283047100;37283047700
	SciVis+InfoVis+some VAST	Dec. 2012	Adaptive Composite Map Projections	10.1109/TVCG.2012.192	http://dx.doi.org/10.1109/TVCG.2012.192	2575	2582	6327263	Web services;cartography;data visualisation;geographic information systems	Web Mercator projection;Web mapping services;adaptive composite map projections;areal distortion;cartographic visualization;erroneous distances impression;geographic area;geographic region;information visualization;map geometry;map height-to-width ratio	Continents;Decision trees;Earth;Interpolation;Mapping;Shape analysis	HTML5 Canvas;Multi-scale map;web Mercator;web cartography;web map projection;web mapping	All major web mapping services use the web Mercator projection. This is a poor choice for maps of the entire globe or areas of the size of continents or larger countries because the Mercator projection shows medium and higher latitudes with extreme areal distortion and provides an erroneous impression of distances and relative areas. The web Mercator projection is also not able to show the entire globe, as polar latitudes cannot be mapped. When selecting an alternative projection for information visualization, rivaling factors have to be taken into account, such as map scale, the geographic area shown, the map's height-to-width ratio, and the type of cartographic visualization. It is impossible for a single map projection to meet the requirements for all these factors. The proposed composite map projection combines several projections that are recommended in cartographic literature and seamlessly morphs map space as the user changes map scale or the geographic region displayed. The composite projection adapts the map's geometry to scale, to the map's height-to-width ratio, and to the central latitude of the displayed area by replacing projections and adjusting their parameters. The composite projection shows the entire globe including poles; it portrays continents or larger countries with less distortion (optionally without areal distortion); and it can morph to the web Mercator projection for maps showing small regions.	Jenny, B.	Oregon State Univ., Corvallis, OR, USA|c|	38488917300
	SciVis+InfoVis+some VAST	Dec. 2012	Algorithms for Labeling Focus Regions	10.1109/TVCG.2012.193	http://dx.doi.org/10.1109/TVCG.2012.193	2583	2592	6327264	computer graphics;curve fitting;diagrams;pattern clustering	Bezier curve;boundary labeling problem;diagram;facility-location perspective;focus region labeling;leader layout;mapping service;point site labeling;polylines;site clustering;straight-line segment;total leader length	Clustering methods;Data visualization;Geospatial analysis;Gravity;Labels;Ubiquitous computing;Visual analytics	Focus+context techniques;data clustering;geographic/geospatial visualization;mobile and ubiquitous visualization	In this paper, we investigate the problem of labeling point sites in focus regions of maps or diagrams. This problem occurs, for example, when the user of a mapping service wants to see the names of restaurants or other POIs in a crowded downtown area but keep the overview over a larger area. Our approach is to place the labels at the boundary of the focus region and connect each site with its label by a linear connection, which is called a leader. In this way, we move labels from the focus region to the less valuable context region surrounding it. In order to make the leader layout well readable, we present algorithms that rule out crossings between leaders and optimize other characteristics such as total leader length and distance between labels. This yields a new variant of the boundary labeling problem, which has been studied in the literature. Other than in traditional boundary labeling, where leaders are usually schematized polylines, we focus on leaders that are either straight-line segments or Bezier curves. Further, we present algorithms that, given the sites, find a position of the focus region that optimizes the above characteristics. We also consider a variant of the problem where we have more sites than space for labels. In this situation, we assume that the sites are prioritized by the user. Alternatively, we take a new facility-location perspective which yields a clustering of the sites. We label one representative of each cluster. If the user wishes, we apply our approach to the sites within a cluster, giving details on demand.	Fink, M.;Haunert, J.-H.;Schulz, A.;Spoerhase, J.;Wolff, A.	Inst. fur Inf., Univ. Wurzburg, Wurzburg, Germany|c|;;;;	38490470100;38017278900;38489531200;38489875600;37398094700
	SciVis+InfoVis+some VAST	Dec. 2012	Capturing the Design Space of Sequential Space-Filling Layouts	10.1109/TVCG.2012.205	http://dx.doi.org/10.1109/TVCG.2012.205	2593	2602	6327265	data visualisation	InfoViz toolkits;JavaScript prototype;design space;dice layouts;dimensional stacking;five dimensional space;independent dimensions;layout component;mosaic plots;order dimension;phrase dimension;pivot layouts;rectangular area;recurse dimension;score dimension;sequential space-filling layouts;size dimension;slice layouts;spiral treemaps;squarified layouts;stacked bar charts;statistics visualizations;strip layouts;tables layouts;tree layouts	Algorithm design and analysis;Layout;Spirals;Tree data structures	Layout;dimensional stacking;grids;mosaic plots;squarified and pivot variations);strip;tables &amp; tree layouts;treemaps (slice and dice;visualization models	We characterize the design space of the algorithms that sequentially tile a rectangular area with smaller, fixed-surface, rectangles. This space consist of five independent dimensions: Order, Size, Score, Recurse and Phrase. Each of these dimensions describe a particular aspect of such layout tasks. This class of layouts is interesting, because, beyond encompassing simple grids, tables and trees, it also includes all kinds of treemaps involving the placement of rectangles. For instance, Slice and dice, Squarified, Strip and Pivot layouts are various points in this five dimensional space. Many classic statistics visualizations, such as 100% stacked bar charts, mosaic plots and dimensional stacking, are also instances of this class. A few new and potentially interesting points in this space are introduced, such as spiral treemaps and variations on the strip layout. The core algorithm is implemented as a JavaScript prototype that can be used as a layout component in a variety of InfoViz toolkits.			
	SciVis+InfoVis+some VAST	Dec. 2012	Taxonomy-Based Glyph Design&amp;#8212;with a Case Study on Visualizing Workflows of Biological Experiments	10.1109/TVCG.2012.271	http://dx.doi.org/10.1109/TVCG.2012.271	2603	2612	6327266	biology computing;data visualisation;design	application-specific abstraction;biological experiment;concept categorization;discriminative capacity ordering;glyph-based visualization;icon design;metaphor;multivariate information presentation;public archive;taxonomy-based glyph design;visual channel;visual search;workflow visualization	Data visualization;Glyph design	Glyph-based techniques;bioinformatics visualization;design methodologies;taxonomies	Glyph-based visualization can offer elegant and concise presentation of multivariate information while enhancing speed and ease in visual search experienced by users. As with icon designs, glyphs are usually created based on the designers' experience and intuition, often in a spontaneous manner. Such a process does not scale well with the requirements of applications where a large number of concepts are to be encoded using glyphs. To alleviate such limitations, we propose a new systematic process for glyph design by exploring the parallel between the hierarchy of concept categorization and the ordering of discriminative capacity of visual channels. We examine the feasibility of this approach in an application where there is a pressing need for an efficient and effective means to visualize workflows of biological experiments. By processing thousands of workflow records in a public archive of biological experiments, we demonstrate that a cost-effective glyph design can be obtained by following a process of formulating a taxonomy with the aid of computation, identifying visual channels hierarchically, and defining application-specific abstraction and metaphors.	Maguire, E.;Rocca-Serra, P.;Sansone, S.-A.;Davies, J.;Min Chen	Dept. of Comput. Sci., Univ. of Oxford, Oxford, UK|c|;;;;	38489729700;38489532000;38489722500;37320294300;38489316600
	SciVis+InfoVis+some VAST	Dec. 2012	An Empirical Model of Slope Ratio Comparisons	10.1109/TVCG.2012.196	http://dx.doi.org/10.1109/TVCG.2012.196	2613	2620	6327267	data visualisation;graphs	Cleveland experimental design;classic design guideline;comparing slopes;empirical model;fundamental graph reading task;slope ratio comparisons;slope ratio estimation strategies	Approximation methods;Data models;Estimation;Market research;Predictive models;Slope analysis	Banking to 45 degrees;aspect ratio selection;orientation resolution;slope perception	Comparing slopes is a fundamental graph reading task and the aspect ratio chosen for a plot influences how easy these comparisons are to make. According to Banking to 45°, a classic design guideline first proposed and studied by Cleveland et al., aspect ratios that center slopes around 45° minimize errors in visual judgments of slope ratios. This paper revisits this earlier work. Through exploratory pilot studies that expand Cleveland et al.'s experimental design, we develop an empirical model of slope ratio estimation that fits more extreme slope ratio judgments and two common slope ratio estimation strategies. We then run two experiments to validate our model. In the first, we show that our model fits more generally than the one proposed by Cleveland et al. and we find that, in general, slope ratio errors are not minimized around 45°. In the second experiment, we explore a novel hypothesis raised by our model: that visible baselines can substantially mitigate errors made in slope judgments. We conclude with an application of our model to aspect ratio selection.	Talbot, J.;Gerth, J.;Hanrahan, P.	;;	37604514900;37840462500;37349803800
	SciVis+InfoVis+some VAST	Dec. 2012	Representative Factor Generation for the Interactive Visual Analysis of High-Dimensional Data	10.1109/TVCG.2012.256	http://dx.doi.org/10.1109/TVCG.2012.256	2621	2630	6327268	biomedical imaging;brain;data analysis;data visualisation;iterative methods;medical computing	analytical procedure;brain imaging study;computational analysis;computational tools;high-dimensional data;interactive visual analysis;iterative analysis;representative factor generation	Correlation;Data mining;Data visualization;Gaussian distribution;Principal component analysis;Reliability	Interactive visual analysis;high-dimensional data analysis	Datasets with a large number of dimensions per data item (hundreds or more) are challenging both for computational and visual analysis. Moreover, these dimensions have different characteristics and relations that result in sub-groups and/or hierarchies over the set of dimensions. Such structures lead to heterogeneity within the dimensions. Although the consideration of these structures is crucial for the analysis, most of the available analysis methods discard the heterogeneous relations among the dimensions. In this paper, we introduce the construction and utilization of representative factors for the interactive visual analysis of structures in high-dimensional datasets. First, we present a selection of methods to investigate the sub-groups in the dimension set and associate representative factors with those groups of dimensions. Second, we introduce how these factors are included in the interactive visual analysis cycle together with the original dimensions. We then provide the steps of an analytical procedure that iteratively analyzes the datasets through the use of representative factors. We discuss how our methods improve the reliability and interpretability of the analysis process by enabling more informed selections of computational tools. Finally, we demonstrate our techniques on the analysis of brain imaging study results that are performed over a large group of subjects.	Turkay, C.;Lundervold, A.;Lundervold, A.J.;Hauser, H.	Dept. of Inf., Univ. of Bergen, Bergen, Norway|c|;;;	37567685600;37373564100;38489275800;37274158800
	SciVis+InfoVis+some VAST	Dec. 2012	Graphical Overlays: Using Layered Elements to Aid Chart Reading	10.1109/TVCG.2012.229	http://dx.doi.org/10.1109/TVCG.2012.229	2631	2638	6327269	charts;data visualisation	World Wide Web;aggregating numerical values;aid chart reading;automated system;bar charts;chart bitmaps;chart reading task;cognitive processes;descriptive text;interactive overlays;layered elements;line charts;numerical data label;pie charts;redundant encoding;summary statistics;user chosen graphical overlays;visual elements;visual marks;visualization	Bars;Data mining;Data visualization;Encoding;Image color analysis;Market research;Visualization	Visualization;graph comprehension;graphical perception;overlays	Reading a visualization can involve a number of tasks such as extracting, comparing or aggregating numerical values. Yet, most of the charts that are published in newspapers, reports, books, and on the Web only support a subset of these tasks. In this paper we introduce graphical overlays-visual elements that are layered onto charts to facilitate a larger set of chart reading tasks. These overlays directly support the lower-level perceptual and cognitive processes that viewers must perform to read a chart. We identify five main types of overlays that support these processes; the overlays can provide (1) reference structures such as gridlines, (2) highlights such as outlines around important marks, (3) redundant encodings such as numerical data labels, (4) summary statistics such as the mean or max and (5) annotations such as descriptive text for context. We then present an automated system that applies user-chosen graphical overlays to existing chart bitmaps. Our approach is based on the insight that generating most of these graphical overlays only requires knowing the properties of the visual marks and axes that encode the data, but does not require access to the underlying data values. Thus, our system analyzes the chart bitmap to extract only the properties necessary to generate the desired overlay. We also discuss techniques for generating interactive overlays that provide additional controls to viewers. We demonstrate several examples of each overlay type for bar, pie and line charts.	Kong, N.;Agrawala, M.	Comput. Sci. Div., UC Berkeley, Berkeley, CO, USA|c|;	37599492800;37282718200
	SciVis+InfoVis+some VAST	Dec. 2012	Facilitating Discourse Analysis with Interactive Visualization	10.1109/TVCG.2012.226	http://dx.doi.org/10.1109/TVCG.2012.226	2639	2648	6327270	computational linguistics;data visualisation;document handling;grammars;interactive systems;iterative methods;natural language processing;tree data structures	DAViewer;computational linguistics researchers;data structures;dialogue generation;discourse analysis;discourse parser;document;interactive visualization system;iterative user-centered design process;natural language processing system;parsing algorithms;question answering;rhetorical structure tree;text summarization	Algorithm design and analysis;Computational linguistics;Data visualization;Image color analysis;Prototypes;Standards;Visualization	Discourse structure;computational linguisitics;interaction techniques;tree comparison;visual analytics	A discourse parser is a natural language processing system which can represent the organization of a document based on a rhetorical structure tree-one of the key data structures enabling applications such as text summarization, question answering and dialogue generation. Computational linguistics researchers currently rely on manually exploring and comparing the discourse structures to get intuitions for improving parsing algorithms. In this paper, we present DAViewer, an interactive visualization system for assisting computational linguistics researchers to explore, compare, evaluate and annotate the results of discourse parsers. An iterative user-centered design process with domain experts was conducted in the development of DAViewer. We report the results of an informal formative study of the system to better understand how the proposed visualization and interaction techniques are used in the real research environment.	Jian Zhao;Chevalier, F.;Collins, C.;Balakrishnan, R.	Univ. of Toronto, Toronto, ON, Canada|c|;;;	38024987300;37395495900;38490026700;37394495600
	SciVis+InfoVis+some VAST	Dec. 2012	Whisper: Tracing the Spatiotemporal Process of Information Diffusion in Real Time	10.1109/TVCG.2012.291	http://dx.doi.org/10.1109/TVCG.2012.291	2649	2658	6327271	data mining;data visualisation;social networking (online)	Twitter;Whisper;flux line-drawing algorithm;focused diffusion series;information diffusion;social media;social-spatial extent;spatial hierarchical layout;spatial pattern;spatiotemporal process;sunflower metaphor;temporal pattern;temporal trend;visualization design	Diffusion processes;Media;Monitoring;Real-time systems;Social network services;Twitter	Information visualization;Social media;contagion;information diffusion;microblogging;spatiotemporal patterns	When and where is an idea dispersed? Social media, like Twitter, has been increasingly used for exchanging information, opinions and emotions about events that are happening across the world. Here we propose a novel visualization design, “Whisper”, for tracing the process of information diffusion in social media in real time. Our design highlights three major characteristics of diffusion processes in social media: the temporal trend, social-spatial extent, and community response of a topic of interest. Such social, spatiotemporal processes are conveyed based on a sunflower metaphor whose seeds are often dispersed far away. In Whisper, we summarize the collective responses of communities on a given topic based on how tweets were retweeted by groups of users, through representing the sentiments extracted from the tweets, and tracing the pathways of retweets on a spatial hierarchical layout. We use an efficient flux line-drawing algorithm to trace multiple pathways so the temporal and spatial patterns can be identified even for a bursty event. A focused diffusion series highlights key roles such as opinion leaders in the diffusion process. We demonstrate how our design facilitates the understanding of when and where a piece of information is dispersed and what are the social responses of the crowd, for large-scale events including political campaigns and natural disasters. Initial feedback from domain experts suggests promising use for today's information consumption and dispersion in the wild.	Nan Cao;Yu-Ru Lin;Xiaohua Sun;Lazer, D.;Shixia Liu;Huamin Qu	Hong Kong Univ. of Sci. & Technol., Hong Kong, China|c|;;;;;	37604309600;37598450200;38488930200;38489726300;37406039100;37272637300
	SciVis+InfoVis+some VAST	Dec. 2012	Exploring Flow, Factors, and Outcomes of Temporal Event Sequences with the Outflow Visualization	10.1109/TVCG.2012.225	http://dx.doi.org/10.1109/TVCG.2012.225	2659	2668	6327272	data visualisation	EMR;aggregate pathway;cardinality;electronic medical record;event progression pathway;event sequence data;event state;outflow visualization technique;pathway state transition;sports event;temporal event sequence	Data visualization;Image color analysis;Information analysis;Layout;Sequential analysis	Outflow;information visualization;state diagram;state transition;temporal event sequences	Event sequence data is common in many domains, ranging from electronic medical records (EMRs) to sports events. Moreover, such sequences often result in measurable outcomes (e.g., life or death, win or loss). Collections of event sequences can be aggregated together to form event progression pathways. These pathways can then be connected with outcomes to model how alternative chains of events may lead to different results. This paper describes the Outflow visualization technique, designed to (1) aggregate multiple event sequences, (2) display the aggregate pathways through different event states with timing and cardinality, (3) summarize the pathways' corresponding outcomes, and (4) allow users to explore external factors that correlate with specific pathway state transitions. Results from a user study with twelve participants show that users were able to learn how to use Outflow easily with limited training and perform a range of tasks both accurately and rapidly.	Wongsuphasawat, K.;Gotz, D.	Univ. of Maryland, College Park, MD, USA|c|;	37670523200;37601397400
	SciVis+InfoVis+some VAST	Dec. 2012	RankExplorer: Visualization of Ranking Changes in Large Time Series Data	10.1109/TVCG.2012.253	http://dx.doi.org/10.1109/TVCG.2012.253	2669	2678	6327273	data visualisation;search engines;time series	Bing;Google;RankExplorer;changing glyph;embedded color bars;extended ThemeRiver view;interactive exploration;ranking category;ranking change visualization;search engine;segmentation method;time series curve;time series data;user interaction	Data visualization;Encoding;Image color analysis;Market research;Time series analysis	Themeriver;Time-series data;interaction techniques;ranking change	For many applications involving time series data, people are often interested in the changes of item values over time as well as their ranking changes. For example, people search many words via search engines like Google and Bing every day. Analysts are interested in both the absolute searching number for each word as well as their relative rankings. Both sets of statistics may change over time. For very large time series data with thousands of items, how to visually present ranking changes is an interesting challenge. In this paper, we propose RankExplorer, a novel visualization method based on ThemeRiver to reveal the ranking changes. Our method consists of four major components: 1) a segmentation method which partitions a large set of time series curves into a manageable number of ranking categories; 2) an extended ThemeRiver view with embedded color bars and changing glyphs to show the evolution of aggregation values related to each ranking category over time as well as the content changes in each ranking category; 3) a trend curve to show the degree of ranking changes over time; 4) rich user interactions to support interactive exploration of ranking changes. We have applied our method to some real time series data and the case studies demonstrate that our method can reveal the underlying patterns related to ranking changes which might otherwise be obscured in traditional visualizations.	Conglei Shi;Weiwei Cui;Shixia Liu;Panpan Xu;Wei Chen;Huamin Qu	Hong Kong Univ. of Sci. & Technol., Hong Kong, China|c|;;;;;	38019494400;37391623900;37406039100;37999118700;37279188600;37272637300
	SciVis+InfoVis+some VAST	Dec. 2012	Design Considerations for Optimizing Storyline Visualizations	10.1109/TVCG.2012.212	http://dx.doi.org/10.1109/TVCG.2012.212	2679	2688	6327274	data visualisation;evolutionary computation;humanities;interactive systems;optimisation	XKCD;design considerations;evolutionary computation;global trends;hand-drawn illustration;local interactions;movie narrative charts;social interactions;storyline visualization automation;storyline visualization optimization	Data visualization;Design methodology;Genomics;Layout;Motion pictures;White spaces	Layout algorithm;design study;storyline visualization;timeline visualization	Storyline visualization is a technique used to depict the temporal dynamics of social interactions. This visualization technique was first introduced as a hand-drawn illustration in XKCD's “Movie Narrative Charts” [21]. If properly constructed, the visualization can convey both global trends and local interactions in the data. However, previous methods for automating storyline visualizations are overly simple, failing to achieve some of the essential principles practiced by professional illustrators. This paper presents a set of design considerations for generating aesthetically pleasing and legible storyline visualizations. Our layout algorithm is based on evolutionary computation, allowing us to effectively incorporate multiple objective functions. We show that the resulting visualizations have significantly improved aesthetics and legibility compared to existing techniques.	Tanahashi, Y.;Kwan-Liu Ma	ViDi Res. Group, Univ. of California, Davis, CA, USA|c|;	38490419400;38490054000
	SciVis+InfoVis+some VAST	Dec. 2012	Beyond Mouse and Keyboard: Expanding Design Considerations for Information Visualization Interactions	10.1109/TVCG.2012.204	http://dx.doi.org/10.1109/TVCG.2012.204	2689	2698	6327275	data visualisation;graphical user interfaces;human computer interaction;interactive systems;keyboards;mouse controllers (computers)	HCI interaction model;InfoVis interaction classification;WIMP interface;cognition;design consideration;desktop;icons;information visualization interaction;interaction technology;interactivity;keyboard;menus;mouse;natural interaction;pointer;windows	Data visualization;Human computer interaction;Information analysis;Instruments;Taxonomy;User interfaces	Design considerations;NUI (Natural User Interface);interaction;post-WIMP	The importance of interaction to Information Visualization (InfoVis) and, in particular, of the interplay between interactivity and cognition is widely recognized [12, 15, 32, 55, 70]. This interplay, combined with the demands from increasingly large and complex datasets, is driving the increased significance of interaction in InfoVis. In parallel, there have been rapid advances in many facets of interaction technologies. However, InfoVis interactions have yet to take full advantage of these new possibilities in interaction technologies, as they largely still employ the traditional desktop, mouse, and keyboard setup of WIMP (Windows, Icons, Menus, and a Pointer) interfaces. In this paper, we reflect more broadly about the role of more “natural” interactions for InfoVis and provide opportunities for future research. We discuss and relate general HCI interaction models to existing InfoVis interaction classifications by looking at interactions from a novel angle, taking into account the entire spectrum of interactions. Our discussion of InfoVis-specific interaction design considerations helps us identify a series of underexplored attributes of interaction that can lead to new, more “natural,” interaction techniques for InfoVis.	Bongshin Lee;Isenberg, P.;Riche, N.H.;Carpendale, S.	;;;	37293389400;37591317800;37590950700;37285000100
	SciVis+InfoVis+some VAST	Dec. 2012	Intelligent Graph Layout Using Many Users&#39; Input	10.1109/TVCG.2012.236	http://dx.doi.org/10.1109/TVCG.2012.236	2699	2708	6327276	graph theory;interactive systems;social sciences computing	Laplacian constrained distance embedding;crowd sourcing manner;dynamic viewing;graph drawing;intelligent graph layout;light-weight interactive system;user input;user preference	Algorithm design and analysis;Crowdsourcing;Human factors;Laplace equations;Layout;Stress	Graph layout;Laplacian matrix;crowd sourcing;editing;force directed layout;merging;stress model	In this paper, we propose a new strategy for graph drawing utilizing layouts of many sub-graphs supplied by a large group of people in a crowd sourcing manner. We developed an algorithm based on Laplacian constrained distance embedding to merge subgraphs submitted by different users, while attempting to maintain the topological information of the individual input layouts. To facilitate collection of layouts from many people, a light-weight interactive system has been designed to enable convenient dynamic viewing, modification and traversing between layouts. Compared with other existing graph layout algorithms, our approach can achieve more aesthetic and meaningful layouts with high user preference.	Xiaoru Yuan;Limei Che;Yifan Hu;Xin Zhang	Key Lab. of Machine Perception (Minist. of Educ.), Peking Univ., Beijing, China|c|;;;	37403856700;38490517900;37403239700;38489293500
	SciVis+InfoVis+some VAST	Dec. 2012	PivotPaths: Strolling through Faceted Information Spaces	10.1109/TVCG.2012.252	http://dx.doi.org/10.1109/TVCG.2012.252	2709	2718	6327277	information filters;information resources;information retrieval	PivotPaths;academic publications;facet lists;faceted information resources;filter operations;interactive visualization;iterative design-and-evaluation process	Context;Facial animation;Information services;Layout;Motion pictures;Navigation;Visualization	Information visualization;animation;exploratory search;information seeking;interactivity;node-link diagrams	We present PivotPaths, an interactive visualization for exploring faceted information resources. During both work and leisure, we increasingly interact with information spaces that contain multiple facets and relations, such as authors, keywords, and citations of academic publications, or actors and genres of movies. To navigate these interlinked resources today, one typically selects items from facet lists resulting in abrupt changes from one subset of data to another. While filtering is useful to retrieve results matching specific criteria, it can be difficult to see how facets and items relate and to comprehend the effect of filter operations. In contrast, the PivotPaths interface exposes faceted relations as visual paths in arrangements that invite the viewer to `take a stroll' through an information space. PivotPaths supports pivot operations as lightweight interaction techniques that trigger gradual transitions between views. We designed the interface to allow for casual traversal of large collections in an aesthetically pleasing manner that encourages exploration and serendipitous discoveries. This paper shares the findings from our iterative design-and-evaluation process that included semi-structured interviews and a two-week deployment of PivotPaths applied to a large database of academic publications.	Dork, M.;Riche, N.H.;Ramos, G.;Dumais, S.	;;;	37590950400;37590950700;38030407700;37871035200
	SciVis+InfoVis+some VAST	Dec. 2012	Interaction Support for Visual Comparison Inspired by Natural Behavior	10.1109/TVCG.2012.237	http://dx.doi.org/10.1109/TVCG.2012.237	2719	2728	6327278	data analysis;data visualisation	complementary visual cues;integrated interaction technique;interaction concept;interaction support;interactive data analysis;interactive data exploration;natural behavior;visual comparison;visualization technique	Animation;Computers;Data visualization;Layout;Navigation;Shape;Visualization	Interaction techniques;human-computer interaction;natural interaction;visual comparison;visualization	Visual comparison is an intrinsic part of interactive data exploration and analysis. The literature provides a large body of existing solutions that help users accomplish comparison tasks. These solutions are mostly of visual nature and custom-made for specific data. We ask the question if a more general support is possible by focusing on the interaction aspect of comparison tasks. As an answer to this question, we propose a novel interaction concept that is inspired by real-world behavior of people comparing information printed on paper. In line with real-world interaction, our approach supports users (1) in interactively specifying pieces of graphical information to be compared, (2) in flexibly arranging these pieces on the screen, and (3) in performing the actual comparison of side-by-side and overlapping arrangements of the graphical information. Complementary visual cues and add-ons further assist users in carrying out comparison tasks. Our concept and the integrated interaction techniques are generally applicable and can be coupled with different visualization techniques. We implemented an interactive prototype and conducted a qualitative user study to assess the concept's usefulness in the context of three different visualization techniques. The obtained feedback indicates that our interaction techniques mimic the natural behavior quite well, can be learned quickly, and are easy to apply to visual comparison tasks.	Tominski, C.;Forsell, C.;Johansson, J.	Univ. of Rostock, Rostock, Germany|c|;;	37283236000;37546939800;37273045500
	SciVis+InfoVis+some VAST	Dec. 2012	RelEx: Visualization for Actively Changing Overlay Network Specifications	10.1109/TVCG.2012.255	http://dx.doi.org/10.1109/TVCG.2012.255	2729	2738	6327279	automotive electronics;data visualisation;electronic engineering computing;on-board communications;overlay networks;user centred design	automotive engineer;change management;data abstraction;in-car communication network;logical communication specification;network visualization design;overlay network specification;physical network;social network analysis;summative evaluation;traffic pattern;usability testing;user-centered design process;visual network analysis;visualization tool RelEx	Automotive engineering;Change detection algorithms;Collaboration;Data visualization;Network topology;Traffic control	Network visualization;automotive;change management;design study;traffic optimization;traffic routing	We present a network visualization design study focused on supporting automotive engineers who need to specify and optimize traffic patterns for in-car communication networks. The task and data abstractions that we derived support actively making changes to an overlay network, where logical communication specifications must be mapped to an underlying physical network. These abstractions are very different from the dominant use case in visual network analysis, namely identifying clusters and central nodes, that stems from the domain of social network analysis. Our visualization tool RelEx was created and iteratively refined through a full user-centered design process that included a full problem characterization phase before tool design began, paper prototyping, iterative refinement in close collaboration with expert users for formative evaluation, deployment in the field with real analysts using their own data, usability testing with non-expert users, and summative evaluation at the end of the deployment. In the summative post-deployment study, which entailed domain experts using the tool over several weeks in their daily practice, we documented many examples where the use of RelEx simplified or sped up their work compared to previous practices.	Sedlmair, M.;Frank, A.;Munzner, T.;Butz, A.	Univ. of British Columbia, Vancouver, BC, Canada|c|;;;	37590945600;38490124000;37349490300;37299644000
	SciVis+InfoVis+some VAST	Dec. 2012	Evaluating the Effect of Style in Information Visualization	10.1109/TVCG.2012.221	http://dx.doi.org/10.1109/TVCG.2012.221	2739	2748	6327280	data visualisation	expert-rated depth;identical scatterplot technique;information visualization demonstrators;insight analysis;interactive embellishment;online evaluation study;self-reported depth;specific interaction operations;style demonstrators;visual embellishment	Abstracts;Benchmark testing;Data visualization;Electronic mail;Subspace constraints;Usability;Visualization	Visualization;aesthetics;design;evaluation;online study;style;user experience	This paper reports on a between-subject, comparative online study of three information visualization demonstrators that each displayed the same dataset by way of an identical scatterplot technique, yet were different in style in terms of visual and interactive embellishment. We validated stylistic adherence and integrity through a separate experiment in which a small cohort of participants assigned our three demonstrators to predefined groups of stylistic examples, after which they described the styles with their own words. From the online study, we discovered significant differences in how participants execute specific interaction operations, and the types of insights that followed from them. However, in spite of significant differences in apparent usability, enjoyability and usefulness between the style demonstrators, no variation was found on the self-reported depth, expert-rated depth, confidence or difficulty of the resulting insights. Three different methods of insight analysis have been applied, revealing how style impacts the creation of insights, ranging from higher-level pattern seeking to a more reflective and interpretative engagement with content, which is what underlies the patterns. As this study only forms the first step in determining how the impact of style in information visualization could be best evaluated, we propose several guidelines and tips on how to gather, compare and categorize insights through an online evaluation study, particularly in terms of analyzing the concise, yet wide variety of insights and observations in a trustworthy and reproducable manner.	Vande Moere, A.;Tomitsch, M.;Wimmer, C.;Grechenig, T.	;;;	37567251200;37946592500;38489562800;37266011000
	SciVis+InfoVis+some VAST	Dec. 2012	Sketchy Rendering for Information Visualization	10.1109/TVCG.2012.262	http://dx.doi.org/10.1109/TVCG.2012.262	2749	2758	6327281	charts;computational geometry;data visualisation;diagrams;mathematics computing;rendering (computer graphics);statistical analysis;trees (mathematics)	aesthetic quality;bar chart;data graphics;design effort;ellipse rendering;graphical features;line chart;line rendering;narrative quality;node-link diagram;polygon rendering;processing graphics environment;programming modification;shape rendering;sketchiness;sketchy rendering;sketchy style information visualization;spatial imprecision;statistical graphics;stimulus-response test;treemap;user perception;visualization annotation;visualization design	Data visualization;Rendering (computer graphics);Shape analysis	NPR;hand-drawn;non-photorealistic rendering;sketch;uncertainty;visualization	We present and evaluate a framework for constructing sketchy style information visualizations that mimic data graphics drawn by hand. We provide an alternative renderer for the Processing graphics environment that redefines core drawing primitives including line, polygon and ellipse rendering. These primitives allow higher-level graphical features such as bar charts, line charts, treemaps and node-link diagrams to be drawn in a sketchy style with a specified degree of sketchiness. The framework is designed to be easily integrated into existing visualization implementations with minimal programming modification or design effort. We show examples of use for statistical graphics, conveying spatial imprecision and for enhancing aesthetic and narrative qualities of visualization. We evaluate user perception of sketchiness of areal features through a series of stimulus-response tests in order to assess users' ability to place sketchiness on a ratio scale, and to estimate area. Results suggest relative area judgment is compromised by sketchy rendering and that its influence is dependent on the shape being rendered. They show that degree of sketchiness may be judged on an ordinal scale but that its judgement varies strongly between individuals. We evaluate higher-level impacts of sketchiness through user testing of scenarios that encourage user engagement with data visualization and willingness to critique visualization design. Results suggest that where a visualization is clearly sketchy, engagement may be increased and that attitudes to participating in visualization annotation are more positive. The results of our work have implications for effective information visualization design that go beyond the traditional role of sketching as a tool for prototyping or its use for an indication of general uncertainty.	Wood, J.;Isenberg, P.;Isenberg, T.;Dykes, J.;Boukhelifa, N.;Slingsby, A.	giCentre, City Univ. London, London, UK|c|;;;;;	37399045100;37591317800;37297057400;37605079900;37325862800;37590960700
	SciVis+InfoVis+some VAST	Dec. 2012	An Empirical Study on Using Visual Embellishments in Visualization	10.1109/TVCG.2012.197	http://dx.doi.org/10.1109/TVCG.2012.197	2759	2768	6327282	data visualisation	concept comprehension;divided attention;dual-task methodology;memorization;memory-based trials;rhetorical illustrations;speech figure;visual embellishments;visual search;visualization	Complexity theory;Data visualization;Grasping;Humans;Memory management;Speech;Visualization	Visual embellishments;cognition;evaluation;icons;long-term memory;metaphors;visual search;working memory	In written and spoken communications, figures of speech (e.g., metaphors and synecdoche) are often used as an aid to help convey abstract or less tangible concepts. However, the benefits of using rhetorical illustrations or embellishments in visualization have so far been inconclusive. In this work, we report an empirical study to evaluate hypotheses that visual embellishments may aid memorization, visual search and concept comprehension. One major departure from related experiments in the literature is that we make use of a dual-task methodology in our experiment. This design offers an abstraction of typical situations where viewers do not have their full attention focused on visualization (e.g., in meetings and lectures). The secondary task introduces “divided attention”, and makes the effects of visual embellishments more observable. In addition, it also serves as additional masking in memory-based trials. The results of this study show that visual embellishments can help participants better remember the information depicted in visualization. On the other hand, visual embellishments can have a negative impact on the speed of visual search. The results show a complex pattern as to the benefits of visual embellishments in helping participants grasp key concepts from visualization.	Borgo, R.;Abdul-Rahman, A.;Mohamed, F.;Grant, P.W.;Reppa, I.;Floridi, L.;Min Chen	Comput. Sci., Swansea Univ., Swansea, UK|c|;;;;;;	37591074800;38489317800;38489907300;37267321300;38489814200;38490006300;38273288300
	SciVis+InfoVis+some VAST	Dec. 2012	Evaluating Sketchiness as a Visual Variable for the Depiction of Qualitative Uncertainty	10.1109/TVCG.2012.220	http://dx.doi.org/10.1109/TVCG.2012.220	2769	2778	6327283	data visualisation;rendering (computer graphics)	adult strokes;child strokes;data quality;graphical primitives;hand-drawn lines;line trajectories;nonphotorealistic rendering;qualitative uncertainty depiction;sketchiness evaluation;visual impreciseness;visual variable;visualization uncertainty	Data visualization;Gray-scale;Image color analysis;Rendering (computer graphics);Shape analysis;Uncertainty	Uncertainty visualization;perception;qualitative evaluation;quantitative evaluation	We report on results of a series of user studies on the perception of four visual variables that are commonly used in the literature to depict uncertainty. To the best of our knowledge, we provide the first formal evaluation of the use of these variables to facilitate an easier reading of uncertainty in visualizations that rely on line graphical primitives. In addition to blur, dashing and grayscale, we investigate the use of `sketchiness' as a visual variable because it conveys visual impreciseness that may be associated with data quality. Inspired by work in non-photorealistic rendering and by the features of hand-drawn lines, we generate line trajectories that resemble hand-drawn strokes of various levels of proficiency-ranging from child to adult strokes-where the amount of perturbations in the line corresponds to the level of uncertainty in the data. Our results show that sketchiness is a viable alternative for the visualization of uncertainty in lines and is as intuitive as blur; although people subjectively prefer dashing style over blur, grayscale and sketchiness. We discuss advantages and limitations of each technique and conclude with design considerations on how to deploy these visual variables to effectively depict various levels of uncertainty for line marks.	Boukhelifa, N.	INRIA, Sophia Antipolis, France|c|	
	SciVis+InfoVis+some VAST	Dec. 2012	Understanding Pen and Touch Interaction for Data Exploration on Interactive Whiteboards	10.1109/TVCG.2012.275	http://dx.doi.org/10.1109/TVCG.2012.275	2779	2788	6327284	data visualisation;interactive systems;user interfaces	WIMP interface paradigm;Windows-icons-menus-and-pointer;Wizard of Oz study;bar graph;cascading menus;control panel;data exploration;dialog boxes;information visualization;interactive visualization interface;interactive whiteboard;line graph;pen-and-touch interaction;scatterplot	Context awareness;Data visualization;Sociology;Statistical analysis;Writing	Pen and touch;Wizard of Oz;data exploration;interaction;whiteboard	Current interfaces for common information visualizations such as bar graphs, line graphs, and scatterplots usually make use of the WIMP (Windows, Icons, Menus and a Pointer) interface paradigm with its frequently discussed problems of multiple levels of indirection via cascading menus, dialog boxes, and control panels. Recent advances in interface capabilities such as the availability of pen and touch interaction challenge us to re-think this and investigate more direct access to both the visualizations and the data they portray. We conducted a Wizard of Oz study to explore applying pen and touch interaction to the creation of information visualization interfaces on interactive whiteboards without implementing a plethora of recognizers. Our wizard acted as a robust and flexible pen and touch recognizer, giving participants maximum freedom in how they interacted with the system. Based on our qualitative analysis of the interactions our participants used, we discuss our insights about pen and touch interactions in the context of learnability and the interplay between pen and touch gestures. We conclude with suggestions for designing pen and touch enabled interactive visualization interfaces.	Walny, J.;Bongshin Lee;Johns, P.;Riche, N.H.;Carpendale, S.	;;;;	38016653500;37293389400;38489632600;37590950700;37285000100
	SciVis+InfoVis+some VAST	Dec. 2012	The DeepTree Exhibit: Visualizing the Tree of Life to Facilitate Informal Learning	10.1109/TVCG.2012.272	http://dx.doi.org/10.1109/TVCG.2012.272	2789	2798	6327285	biology computing;computer aided instruction;data visualisation;groupware;iterative methods;rendering (computer graphics);user interfaces	DeepTree exhibit;Tree of Life visualization;active learning;collaborative exploration;collaborative learning;custom rendering;evolutionary concept;fractal-based tree layout;informal learning;iterative process;multitouch interactive visualization;multiuser interactive visualization;multiuser interface;navigation engine;visual complexity reduction	Collaboration;Data visualization;Information science;Layout;Navigation;Phylogeny;Rendering (computer graphics)	Informal science education;collaborative learning;large tree visualizations;multi-touch interaction	In this paper, we present the DeepTree exhibit, a multi-user, multi-touch interactive visualization of the Tree of Life. We developed DeepTree to facilitate collaborative learning of evolutionary concepts. We will describe an iterative process in which a team of computer scientists, learning scientists, biologists, and museum curators worked together throughout design, development, and evaluation. We present the importance of designing the interactions and the visualization hand-in-hand in order to facilitate active learning. The outcome of this process is a fractal-based tree layout that reduces visual complexity while being able to capture all life on earth; a custom rendering and navigation engine that prioritizes visual appeal and smooth fly-through; and a multi-user interface that encourages collaborative exploration while offering guided discovery. We present an evaluation showing that the large dataset encouraged free exploration, triggers emotional responses, and facilitates visitor engagement and informal learning.	Block, F.;Horn, M.S.;Phillips, B.C.;Diamond, J.;Evans, E.M.;Chia Shen	;;;;;	37884160900;38490270400;38490599200;38489620500;38490437100;38490180100
	SciVis+InfoVis+some VAST	Dec. 2012	Living Liquid: Design and Evaluation of an Exploratory Visualization Tool for Museum Visitors	10.1109/TVCG.2012.244	http://dx.doi.org/10.1109/TVCG.2012.244	2799	2808	6327286	data visualisation;interactive systems;museums;natural sciences computing;touch sensitive screens	Living Liquid;exploratory visualization tool;informal learning environments;interactive visualizations;iterative development;science museum visitors;simulated marine microbes;think-aloud protocols;time-varying global distribution;touchscreen interface	Data visualization;Image color analysis;Information analysis;Learning systems;Motion pictures;Performance evaluation;Prototypes	Information visualization;evaluation;informal learning environments;science museums;user interaction;user studies	Interactive visualizations can allow science museum visitors to explore new worlds by seeing and interacting with scientific data. However, designing interactive visualizations for informal learning environments, such as museums, presents several challenges. First, visualizations must engage visitors on a personal level. Second, visitors often lack the background to interpret visualizations of scientific data. Third, visitors have very limited time at individual exhibits in museums. This paper examines these design considerations through the iterative development and evaluation of an interactive exhibit as a visualization tool that gives museumgoers access to scientific data generated and used by researchers. The exhibit prototype, Living Liquid, encourages visitors to ask and answer their own questions while exploring the time-varying global distribution of simulated marine microbes using a touchscreen interface. Iterative development proceeded through three rounds of formative evaluations using think-aloud protocols and interviews, each round informing a key visualization design decision: (1) what to visualize to initiate inquiry, (2) how to link data at the microscopic scale to global patterns, and (3) how to include additional data that allows visitors to pursue their own questions. Data from visitor evaluations suggests that, when designing visualizations for public audiences, one should (1) avoid distracting visitors from data that they should explore, (2) incorporate background information into the visualization, (3) favor understandability over scientific accuracy, and (4) layer data accessibility to structure inquiry. Lessons learned from this case study add to our growing understanding of how to use visualizations to actively engage learners with scientific data.	Ma, J.;Liao, I.;Kwan-Liu Ma;Frazier, J.	Exploratorium, San Francisco, CA, USA|c|;;;	38489269900;38241310900;38490053400;38233965200
	SciVis+InfoVis+some VAST	Dec. 2012	Visualizing Student Histories Using Clustering and Composition	10.1109/TVCG.2012.288	http://dx.doi.org/10.1109/TVCG.2012.288	2809	2818	6327287	data visualisation;history;pattern clustering;student experiments;time series	analytic strategies;clustering algorithms;intuitive time-series visualizations;student course history data;student histories visualization	Data visualization;History;Image color analysis;Market research;Trajectory	Clustering;aggregate visualization;student performance analysis;visualization composition	While intuitive time-series visualizations exist for common datasets, student course history data is difficult to represent using traditional visualization techniques due its concurrent nature. A visual composition process is developed and applied to reveal trends across various groupings. By working closely with educators, analytic strategies and techniques are developed to leverage the visualization composition to reveal unknown trends in the data. Furthermore, clustering algorithms are developed to group common course-grade histories for further analysis. Lastly, variations of the composition process are implemented to reveal subtle differences in the underlying data. These analytic tools and techniques enabled educators to confirm expected trends and to discover new ones.	Trimm, D.;Rheingans, P.;desJardins, M.	Univ. of Maryland, Baltimore County (UMBC), Baltimore, MD, USA|c|;;	38490236900;37282292000;37552291200
	SciVis+InfoVis+some VAST	Dec. 2012	SnapShot: Visualization to Propel Ice Hockey Analytics	10.1109/TVCG.2012.263	http://dx.doi.org/10.1109/TVCG.2012.263	2819	2828	6327288	data analysis;data mining;data visualisation;sport;statistical analysis	R software;SAS software;SnapShot;Stata software;analytical workflow;dataspace exploration;dynamic games;game data;hockey intelligence gathering;hockey statistics;hypothesis sharing;ice hockey analytics;independent consultants;information visualization;internal colleagues;knowledge discovery;professional ice hockey analyst;professional team personnel;radial heat map;shot data display;shot length;sports analysis;stakeholders;statistical software package	Data visualization;Games;Human computer interaction;Knowledge discovery;Sports equipment	Visual knowledge discovery;human computer interaction;hypothesis testing;visual evidence;visual knowledge representation	Sports analysts live in a world of dynamic games flattened into tables of numbers, divorced from the rinks, pitches, and courts where they were generated. Currently, these professional analysts use R, Stata, SAS, and other statistical software packages for uncovering insights from game data. Quantitative sports consultants seek a competitive advantage both for their clients and for themselves as analytics becomes increasingly valued by teams, clubs, and squads. In order for the information visualization community to support the members of this blossoming industry, it must recognize where and how visualization can enhance the existing analytical workflow. In this paper, we identify three primary stages of today's sports analyst's routine where visualization can be beneficially integrated: 1) exploring a dataspace; 2) sharing hypotheses with internal colleagues; and 3) communicating findings to stakeholders.Working closely with professional ice hockey analysts, we designed and built SnapShot, a system to integrate visualization into the hockey intelligence gathering process. SnapShot employs a variety of information visualization techniques to display shot data, yet given the importance of a specific hockey statistic, shot length, we introduce a technique, the radial heat map. Through a user study, we received encouraging feedback from several professional analysts, both independent consultants and professional team personnel.	Pileggi, H.;Stolper, C.D.;Boyle, J.M.;Stasko, J.T.	;;;	38488862800;38490190000;38490191800;37267736900
	SciVis+InfoVis+some VAST	Dec. 2012	Scatter/Gather Clustering: Flexibly Incorporating User Feedback to Steer Clustering Results	10.1109/TVCG.2012.258	http://dx.doi.org/10.1109/TVCG.2012.258	2829	2838	6327289	bioacoustics;data analysis;data visualisation;nonlinear programming;pattern clustering;sonar;zoology	baffle shapes;bat biosonar system;cluster locality;clustering algorithms;clustering gathering;clustering result steering;clustering scattering;data dynamic restructurings;ears;interaction style;nonlinear optimization framework;nose;user feedback;user-supplied constraints;visual analytic application	Algorithm design and analysis;Clustering algorithms;Computer science;Linear programming;Optimization;Visual analytics	Scatter/gather clustering;alternative clustering;constrained clustering	Significant effort has been devoted to designing clustering algorithms that are responsive to user feedback or that incorporate prior domain knowledge in the form of constraints. However, users desire more expressive forms of interaction to influence clustering outcomes. In our experiences working with diverse application scientists, we have identified an interaction style scatter/gather clustering that helps users iteratively restructure clustering results to meet their expectations. As the names indicate, scatter and gather are dual primitives that describe whether clusters in a current segmentation should be broken up further or, alternatively, brought back together. By combining scatter and gather operations in a single step, we support very expressive dynamic restructurings of data. Scatter/gather clustering is implemented using a nonlinear optimization framework that achieves both locality of clusters and satisfaction of user-supplied constraints. We illustrate the use of our scatter/gather clustering approach in a visual analytic application to study baffle shapes in the bat biosonar (ears and nose) system. We demonstrate how domain experts are adept at supplying scatter/gather constraints, and how our framework incorporates these constraints effectively without requiring numerous instance-level constraints.	Hossain, M.S.;Ojili, P.K.R.;Grimm, C.;Muller, R.;Watson, L.T.;Ramakrishnan, N.	Dept. of Comput. Sci., Virginia Tech, Blacksburg, VA, USA|c|;;;;;	38242697300;38488976900;37323515100;38489368300;37273212300;37273204200
	SciVis+InfoVis+some VAST	Dec. 2012	Visual Classifier Training for Text Document Retrieval	10.1109/TVCG.2012.277	http://dx.doi.org/10.1109/TVCG.2012.277	2839	2848	6327290	data visualisation;interactive systems;iterative methods;learning (artificial intelligence);pattern classification;query processing;text analysis	classification methods;filter criteria;interactive classifier training;interactive visualization;iterative feedback loops;labeled documents;machine learning;search queries;text document retrieval;text search;user controlled classification methods;visual classifier training	Classification;Human computer interaction;Information retrieval;Learning systems;Performance evaluation;Training data;Visual analytics	Visual analytics;active learning;classification;human computer interaction;information retrieval;user evaluation	Performing exhaustive searches over a large number of text documents can be tedious, since it is very hard to formulate search queries or define filter criteria that capture an analyst's information need adequately. Classification through machine learning has the potential to improve search and filter tasks encompassing either complex or very specific information needs, individually. Unfortunately, analysts who are knowledgeable in their field are typically not machine learning specialists. Most classification methods, however, require a certain expertise regarding their parametrization to achieve good results. Supervised machine learning algorithms, in contrast, rely on labeled data, which can be provided by analysts. However, the effort for labeling can be very high, which shifts the problem from composing complex queries or defining accurate filters to another laborious task, in addition to the need for judging the trained classifier's quality. We therefore compare three approaches for interactive classifier training in a user study. All of the approaches are potential candidates for the integration into a larger retrieval system. They incorporate active learning to various degrees in order to reduce the labeling effort as well as to increase effectiveness. Two of them encompass interactive visualization for letting users explore the status of the classifier in context of the labeled documents, as well as for judging the quality of the classifier in iterative feedback loops. We see our work as a step towards introducing user controlled classification methods in addition to text search and filtering for increasing recall in analytics scenarios involving large corpora.	Heimerl, F.;Koch, S.;Bosch, H.;Ertl, T.	Inst. for Visualization & Interactive Syst., Univ. Stuttgart, Stuttgart, Germany|c|;;;	38490630000;37593029700;37683989300;37268023800
	SciVis+InfoVis+some VAST	Dec. 2012	Reinventing the Contingency Wheel: Scalable Visual Analytics of Large Categorical Data	10.1109/TVCG.2012.254	http://dx.doi.org/10.1109/TVCG.2012.254	2849	2858	6327291	data visualisation	Pearson residuals;automated methods;business domains;contingency wheel++;coordinated views;frequency-based abstraction;interactive exploration environment;large categorical data;multilevel overview+detail interface;negative associations;nontrivial patterns;positive associations;scalable visual analytics;scientific domains;two-way contingency tables;visual elements;visualization methods	Data visualization;Frequency measurement;Histograms;Motion pictures;Visual analytics	Large categorical data;contingency table analysis;information interfaces and representation;visual analytics	Contingency tables summarize the relations between categorical variables and arise in both scientific and business domains. Asymmetrically large two-way contingency tables pose a problem for common visualization methods. The Contingency Wheel has been recently proposed as an interactive visual method to explore and analyze such tables. However, the scalability and readability of this method are limited when dealing with large and dense tables. In this paper we present Contingency Wheel++, new visual analytics methods that overcome these major shortcomings: (1) regarding automated methods, a measure of association based on Pearson's residuals alleviates the bias of the raw residuals originally used, (2) regarding visualization methods, a frequency-based abstraction of the visual elements eliminates overlapping and makes analyzing both positive and negative associations possible, and (3) regarding the interactive exploration environment, a multi-level overview+detail interface enables exploring individual data items that are aggregated in the visualization or in the table using coordinated views. We illustrate the applicability of these new methods with a use case and show how they enable discovering and analyzing nontrivial patterns and associations in large categorical data.	Alsallakh, B.;Aigner, W.;Miksch, S.;Groller, M.E.	;;;	38242873200;37267339500;37282584500;37282552200
	VAST	Dec. 2012	An Affordance-Based Framework for Human Computation and Human-Computer Collaboration	10.1109/TVCG.2012.195	http://dx.doi.org/10.1109/TVCG.2012.195	2859	2868	6327292	data visualisation;groupware;human computer interaction	affordance-based framework;analytical reasoning;human computation;human-computer collaboration;visual analytics;visual interactive interface	Cognition;Computation theory;Human factors;Resource management;Visual analytics	Human computation;framework;human complexity;theory	Visual Analytics is “the science of analytical reasoning facilitated by visual interactive interfaces” [70]. The goal of this field is to develop tools and methodologies for approaching problems whose size and complexity render them intractable without the close coupling of both human and machine analysis. Researchers have explored this coupling in many venues: VAST, Vis, InfoVis, CHI, KDD, IUI, and more. While there have been myriad promising examples of human-computer collaboration, there exists no common language for comparing systems or describing the benefits afforded by designing for such collaboration. We argue that this area would benefit significantly from consensus about the design attributes that define and distinguish existing techniques. In this work, we have reviewed 1,271 papers from many of the top-ranking conferences in visual analytics, human-computer interaction, and visualization. From these, we have identified 49 papers that are representative of the study of human-computer collaborative problem-solving, and provide a thorough overview of the current state-of-the-art. Our analysis has uncovered key patterns of design hinging on humanand machine-intelligence affordances, and also indicates unexplored avenues in the study of this area. The results of this analysis provide a common framework for understanding these seemingly disparate branches of inquiry, which we hope will motivate future work in the field.	Crouser, R.J.;Chang, R.	;	38229914800;37592409400
	VAST	Dec. 2012	Examining the Use of a Visual Analytics System for Sensemaking Tasks: Case Studies with Domain Experts	10.1109/TVCG.2012.224	http://dx.doi.org/10.1109/TVCG.2012.224	2869	2878	6327293	data visualisation;inference mechanisms	domain expert;formal evaluation;sensemaking task;visual analytics system	Data visualization;Electronic mail;Market research;Qualitative analysis;Visual analytics	Visual analytics;case study;qualitative evaluation	While the formal evaluation of systems in visual analytics is still relatively uncommon, particularly rare are case studies of prolonged system use by domain analysts working with their own data. Conducting case studies can be challenging, but it can be a particularly effective way to examine whether visual analytics systems are truly helping expert users to accomplish their goals. We studied the use of a visual analytics system for sensemaking tasks on documents by six analysts from a variety of domains. We describe their application of the system along with the benefits, issues, and problems that we uncovered. Findings from the studies identify features that visual analytics systems should emphasize as well as missing capabilities that should be addressed. These findings inform design implications for future systems.			
	VAST	Dec. 2012	Semantic Interaction for Sensemaking: Inferring Analytical Reasoning for Model Steering	10.1109/TVCG.2012.260	http://dx.doi.org/10.1109/TVCG.2012.260	2879	2888	6327294	data visualisation;inference mechanisms	ForceSPIRE;analytical reasoning;clustering model;cognitively demanding task;expressive interactions;human intuition;keyword weighting;mathematical models;model steering;parametric modifications;semantic interaction;sensemaking;spatially clustering data;visual analytic prototype;visual analytic tools;visualization	Analytical models;Mathematical model;Semantics;User interfaces;Visual analytics	User Interaction;analytic reasoning;sensemaking;visual analytics;visualization	Visual analytic tools aim to support the cognitively demanding task of sensemaking. Their success often depends on the ability to leverage capabilities of mathematical models, visualization, and human intuition through flexible, usable, and expressive interactions. Spatially clustering data is one effective metaphor for users to explore similarity and relationships between information, adjusting the weighting of dimensions or characteristics of the dataset to observe the change in the spatial layout. Semantic interaction is an approach to user interaction in such spatializations that couples these parametric modifications of the clustering model with users' analytic operations on the data (e.g., direct document movement in the spatialization, highlighting text, search, etc.). In this paper, we present results of a user study exploring the ability of semantic interaction in a visual analytic prototype, ForceSPIRE, to support sensemaking. We found that semantic interaction captures the analytical reasoning of the user through keyword weighting, and aids the user in co-creating a spatialization based on the user's reasoning and intuition.	Endert, A.;Fiaux, P.;North, C.	;;	37681759500;38489240900;37419565900
	VAST	Dec. 2012	Visual Analytics Methodology for Eye Movement Studies	10.1109/TVCG.2012.276	http://dx.doi.org/10.1109/TVCG.2012.276	2889	2898	6327295	data visualisation;eye;knowledge acquisition;user interfaces	empirical evaluation;eye movement analysis;eye movement study;eye tracking data;geographic movement data;knowledge extraction;scanpaths;task analysis;viewing behaviors;visual analytics methodology;visual displays evaluation;visual interfaces	Data visualization;Eye;Standards;Tracking;Trajectory;Visual analytics	Visual analytics;eye tracking;movement data;trajectory analysis	Eye movement analysis is gaining popularity as a tool for evaluation of visual displays and interfaces. However, the existing methods and tools for analyzing eye movements and scanpaths are limited in terms of the tasks they can support and effectiveness for large data and data with high variation. We have performed an extensive empirical evaluation of a broad range of visual analytics methods used in analysis of geographic movement data. The methods have been tested for the applicability to eye tracking data and the capability to extract useful knowledge about users' viewing behaviors. This allowed us to select the suitable methods and match them to possible analysis tasks they can support. The paper describes how the methods work in application to eye tracking data and provides guidelines for method selection depending on the analysis tasks.	Andrienko, G.;Andrienko, N.;Burch, M.;Weiskopf, D.	;;;	37283047100;37283047700;37586953400;37268045000
	VAST	Dec. 2012	A Visual Analytics Approach to Multiscale Exploration of Environmental Time Series	10.1109/TVCG.2012.191	http://dx.doi.org/10.1109/TVCG.2012.191	2899	2907	6327296	data visualisation;environmental science computing;interactive systems;matrix algebra;pattern recognition;statistical analysis;time series	environmental science;environmental system dynamics;environmental time series;interactive exploration;interesting temporal pattern detection;matrix visualization;multiscale exploration;numerical time series;statistical value;visual analytics approach;visual identification	Data visualization;Earth;Entropy;Meteorology;Time series analysis;Visual analytics	Time series analysis;multiscale visualization;visual analytics	We present a Visual Analytics approach that addresses the detection of interesting patterns in numerical time series, specifically from environmental sciences. Crucial for the detection of interesting temporal patterns are the time scale and the starting points one is looking at. Our approach makes no assumption about time scale and starting position of temporal patterns and consists of three main steps: an algorithm to compute statistical values for all possible time scales and starting positions of intervals, visual identification of potentially interesting patterns in a matrix visualization, and interactive exploration of detected patterns. We demonstrate the utility of this approach in two scientific scenarios and explain how it allowed scientists to gain new insight into the dynamics of environmental systems.	Sips, M.;Kothur, P.;Unger, A.;Hege, H.-C.;Dransch, D.	German Res. Center for Geosci. GFZ, Germany|c|;;;;	37827860500;38490165500;37628385900;37282272000;38490186800
	VAST	Dec. 2012	The User Puzzle&amp;#8212;Explaining the Interaction with Visual Analytics Systems	10.1109/TVCG.2012.273	http://dx.doi.org/10.1109/TVCG.2012.273	2908	2916	6327297	cognition;data visualisation;human computer interaction;psychology	HCI;analytical procedures;human reasoning;psychology;user puzzle;visual analytics systems	Cognition;Human factors;Problem-solving;Psychology;Visual analytics	Cognitive theory;interaction design;problem solving;reasoning;visual knowledge discovery	Visual analytics emphasizes the interplay between visualization, analytical procedures performed by computers and human perceptual and cognitive activities. Human reasoning is an important element in this context. There are several theories in psychology and HCI explaining open-ended and exploratory reasoning. Five of these theories (sensemaking theories, gestalt theories, distributed cognition, graph comprehension theories and skill-rule-knowledge models) are described in this paper. We discuss their relevance for visual analytics. In order to do this more systematically, we developed a schema of categories relevant for visual analytics research and evaluation. All these theories have strengths but also weaknesses in explaining interaction with visual analytics systems. A possibility to overcome the weaknesses would be to combine two or more of these theories.	Pohl, M.;Smuc, M.;Mayr, E.	Vienna Univ. of Technol., Vienna, Austria|c|;;	37560816000;37646291700;37624360500
	VAST	Dec. 2012	Enterprise Data Analysis and Visualization: An Interview Study	10.1109/TVCG.2012.219	http://dx.doi.org/10.1109/TVCG.2012.219	2917	2926	6327298	business data processing;data analysis;data visualisation;decision making;organisational aspects	business decision;customer engagement;data analysts;data visualization;enterprise data analysis;finance;fraud combatting;healthcare;industrial data analysis;marketing;operation streamlining;organizational context;organizational feature;outstanding challenge;production improvement;recurring pain point;retail;semistructured interview;social context;visual analysis research;visual analytic tool adoption	Collaboration;Computer hacking;Data visualization;Distributed databases;Organizations	Data;analysis;enterprise;visualization	Organizations rely on data analysts to model customer engagement, streamline operations, improve production, inform business decisions, and combat fraud. Though numerous analysis and visualization tools have been built to improve the scale and efficiency at which analysts can work, there has been little research on how analysis takes place within the social and organizational context of companies. To better understand the enterprise analysts' ecosystem, we conducted semi-structured interviews with 35 data analysts from 25 organizations across a variety of sectors, including healthcare, retail, marketing and finance. Based on our interview data, we characterize the process of industrial data analysis and document how organizational features of an enterprise impact it. We describe recurring pain points, outstanding challenges, and barriers to adoption for visual analytic tools. Finally, we discuss design implications and opportunities for visual analysis research.	Kandel, S.;Paepcke, A.;Hellerstein, J.M.;Heer, J.	Stanford Univ., Stanford, CA, USA|c|;;;	38490119500;37282105600;38183421000;37550791300
	VAST	14-19 Oct. 2012	Visual cluster exploration of web clickstream data	10.1109/VAST.2012.6400494	http://dx.doi.org/10.1109/VAST.2012.6400494	3	12	6400494	Internet;Markov processes;data analysis;data visualisation;electronic commerce;pattern clustering;self-organising feature maps;user interfaces	Markov chain models;Web clickstream data;clickstream clusters;e-commerce companies;eBay;intuitive user interface;self-organizing map;user behavior patterns;visual analytics system;visual cluster exploration	Data models;Data visualization;Hidden Markov models;Layout;Markov processes;Prototypes;Visualization		Web clickstream data are routinely collected to study how users browse the web or use a service. It is clear that the ability to recognize and summarize user behavior patterns from such data is valuable to e-commerce companies. In this paper, we introduce a visual analytics system to explore the various user behavior patterns reflected by distinct clickstream clusters. In a practical analysis scenario, the system first presents an overview of clickstream clusters using a Self-Organizing Map with Markov chain models. Then the analyst can interactively explore the clusters through an intuitive user interface. He can either obtain summarization of a selected group of data or further refine the clustering result. We evaluated our system using two different datasets from eBay. Analysts who were working on the same data have confirmed the system's effectiveness in extracting user behavior patterns from complex datasets and enhancing their ability to reason.			
	VAST	14-19 Oct. 2012	An adaptive parameter space-filling algorithm for highly interactive cluster exploration	10.1109/VAST.2012.6400493	http://dx.doi.org/10.1109/VAST.2012.6400493	13	22	6400493	cache storage;data visualisation;human computer interaction;pattern clustering;query processing;storage management	K means clustering;LRU caching;Mesonet meteorological data;adaptive parameter space-filling algorithm;best-effort clustering;continuous interactive response time;dynamic query visualization;exact-range clustering;heuristic prefetching technique;interactive cluster exploration;motion parallax;multidimensional clustering;multidimensional data;nearest neighbor calculation;one-dimensional filtering lens;partial query;query computation;time series data;two-dimensional parameter space;user interaction pattern;visual benefit;visualization system;visualization tool	Algorithm design and analysis;Clustering algorithms;Data visualization;Lenses;Prefetching;Time series analysis;Visualization	D.2.2 [Software Engineering]: Design Tools and Techniques — [User Interfaces];H.2.3 [Information Systems]: Database Management — [Languages];H.5.2 [Information Systems]: Information Interfaces and Presentation — [User Interfaces]	For a user to perceive continuous interactive response time in a visualization tool, the rule of thumb is that it must process, deliver, and display rendered results for any given interaction in under 100 milliseconds. In many visualization systems, successive interactions trigger independent queries and caching of results. Consequently, computationally expensive queries like multidimensional clustering cannot keep up with rapid sequences of interactions, precluding visual benefits such as motion parallax. In this paper, we describe a heuristic prefetching technique to improve the interactive response time of KMeans clustering in dynamic query visualizations of multidimensional data. We address the tradeoff between high interaction and intense query computation by observing how related interactions on overlapping data subsets produce similar clustering results, and characterizing these similarities within a parameter space of interaction. We focus on the two-dimensional parameter space defined by the minimum and maximum values of a time range manipulated by dragging and stretching a one-dimensional filtering lens over a plot of time series data. Using calculation of nearest neighbors of interaction points in parameter space, we reuse partial query results from prior interaction sequences to calculate both an immediate best-effort clustering result and to schedule calculation of an exact result. The method adapts to user interaction patterns in the parameter space by reprioritizing the interaction neighbors of visited points in the parameter space. A performance study on Mesonet meteorological data demonstrates that the method is a significant improvement over the baseline scheme in which interaction triggers on-demand, exact-range clustering with LRU caching. We also present initial evidence that approximate, temporary clustering results are sufficiently accurate (compared to exact results) to convey useful cluster structure during rapid and protracted interaction.	Ahmed, Z.	Sch. of Comput. Sci. &amp; Center for Spatial Anal., Univ. of Oklahoma, Norman, OK, USA|c|	
	VAST	14-19 Oct. 2012	Inter-active learning of ad-hoc classifiers for video visual analytics	10.1109/VAST.2012.6400492	http://dx.doi.org/10.1109/VAST.2012.6400492	23	32	6400492	cognition;data visualisation;inference mechanisms;information filtering;information filters;learning (artificial intelligence);pattern classification;training;video retrieval	ad-hoc classifier training process visual feedback;analytical reasoning process;classifier learning;data annotation quality;data labeling queries;data visualization;human expert background knowledge;information filters;inter-active learning;time constraints;user expertise;user mental model;video visual analytics	Analytical models;Data models;Data visualization;Humans;Labeling;Training;Visual analytics	H.3.3 [Information Systems]: Information Storage and Retrieval — Information Search and Retrieval;I.2.6 [Computing Methodologies]: Artificial Intelligence — Learning	Learning of classifiers to be used as filters within the analytical reasoning process leads to new and aggravates existing challenges. Such classifiers are typically trained ad-hoc, with tight time constraints that affect the amount and the quality of annotation data and, thus, also the users' trust in the classifier trained. We approach the challenges of ad-hoc training by inter-active learning, which extends active learning by integrating human experts' background knowledge to greater extent. In contrast to active learning, not only does inter-active learning include the users' expertise by posing queries of data instances for labeling, but it also supports the users in comprehending the classifier model by visualization. Besides the annotation of manually or automatically selected data instances, users are empowered to directly adjust complex classifier models. Therefore, our model visualization facilitates the detection and correction of inconsistencies between the classifier model trained by examples and the user's mental model of the class definition. Visual feedback of the training process helps the users assess the performance of the classifier and, thus, build up trust in the filter created. We demonstrate the capabilities of inter-active learning in the domain of video visual analytics and compare its performance with the results of random sampling and uncertainty sampling of training sets.			
	VAST	14-19 Oct. 2012	A correlative analysis process in a visual analytics environment	10.1109/VAST.2012.6400491	http://dx.doi.org/10.1109/VAST.2012.6400491	33	42	6400491	data analysis;data visualisation;statistical analysis	Pearson product-moment correlation coefficient;correlative analysis process;multivariate datasets;spatiotemporal aggregation levels;spatiotemporal correlations;statistics;visual analytics environment	Correlation;Data visualization;Educational institutions;Market research;Spatiotemporal phenomena;Time series analysis;Visual analytics	Visual analytics;correlative analysis	Finding patterns and trends in spatial and temporal datasets has been a long studied problem in statistics and different domains of science. This paper presents a visual analytics approach for the interactive exploration and analysis of spatiotemporal correlations among multivariate datasets. Our approach enables users to discover correlations and explore potentially causal or predictive links at different spatiotemporal aggregation levels among the datasets, and allows them to understand the underlying statistical foundations that precede the analysis. Our technique utilizes the Pearson's product-moment correlation coefficient and factors in the lead or lag between different datasets to detect trends and periodic patterns amongst them.	Malik, A.	Purdue Univ., West Lafayette, IN, USA|c|	
	VAST	14-19 Oct. 2012	Visual pattern discovery using random projections	10.1109/VAST.2012.6400490	http://dx.doi.org/10.1109/VAST.2012.6400490	43	52	6400490	data mining;data visualisation;random processes	analytic multivariate visualization platform;binning projections;data analysis;feature subspaces;high-dimensional data spaces;low-dimensional projections;multidimensional spaces;projection pursuit index;random projections;score function optimization;visual pattern characterization;visual pattern discovery	Data mining;Data visualization;Indexes;Manifolds;Vectors;Visual analytics	High-dimensional Data;Random Projections	An essential element of exploratory data analysis is the use of revealing low-dimensional projections of high-dimensional data. Projection Pursuit has been an effective method for finding interesting low-dimensional projections of multidimensional spaces by optimizing a score function called a projection pursuit index. However, the technique is not scalable to high-dimensional spaces. Here, we introduce a novel method for discovering noteworthy views of high-dimensional data spaces by using binning and random projections. We define score functions, akin to projection pursuit indices, that characterize visual patterns of the low-dimensional projections that constitute feature subspaces. We also describe an analytic, multivariate visualization platform based on this algorithm that is scalable to extremely large problems.	Anand, A.	Dept. of Comput. Sci., Univ. of Illinois at Chicago, Chicago, IL, USA|c|	
	VAST	14-19 Oct. 2012	iLAMP: Exploring high-dimensional spacing through backward multidimensional projection	10.1109/VAST.2012.6400489	http://dx.doi.org/10.1109/VAST.2012.6400489	53	62	6400489	data visualisation	backward multidimensional projection;biology computing;coined iLAMP;computing power improvement;data complexity;data dimensionality;engineering computing;extrapolated data;flexible mechanisms;greatly augmenting data collection;high-dimensional spaces;high-dimensional spacing;interactive exploration technique;inverse linear affine multidimensional projection;mapping low-dimensional information;multidimensional datasets understanding;physics computing;planar domain;projection methods;scientific computing;scientific observation;technological advances	Data visualization;Measurement;Optimization;Robustness;Space exploration;Vectors;Visualization		Ever improving computing power and technological advances are greatly augmenting data collection and scientific observation. This has directly contributed to increased data complexity and dimensionality, motivating research of exploration techniques for multidimensional data. Consequently, a recent influx of work dedicated to techniques and tools that aid in understanding multidimensional datasets can be observed in many research fields, including biology, engineering, physics and scientific computing. While the effectiveness of existing techniques to analyze the structure and relationships of multidimensional data varies greatly, few techniques provide flexible mechanisms to simultaneously visualize and actively explore high-dimensional spaces. In this paper, we present an inverse linear affine multidimensional projection, coined iLAMP, that enables a novel interactive exploration technique for multidimensional data. iLAMP operates in reverse to traditional projection methods by mapping low-dimensional information into a high-dimensional space. This allows users to extrapolate instances of a multidimensional dataset while exploring a projection of the data to the planar domain. We present experimental results that validate iLAMP, measuring the quality and coherence of the extrapolated data; as well as demonstrate the utility of iLAMP to hypothesize the unexplored regions of a high-dimensional space.			
	VAST	14-19 Oct. 2012	Subspace search and visualization to make sense of alternative clusterings in high-dimensional data	10.1109/VAST.2012.6400488	http://dx.doi.org/10.1109/VAST.2012.6400488	63	72	6400488	data analysis;data reduction;data visualisation;pattern clustering;search problems	HD data space;HD input data space;cluster analysis;dimensionality reduction;explorative data analysis;high-dimensional data clusterings;high-dimensional data space;interestingness-guided subspace search algorithm;navigation facilities;subspace search;subspace similarity functions;subspace visualization;visual analysis;visual mappings;visual-interactive methods	Algorithm design and analysis;Clustering algorithms;Data visualization;Educational institutions;High definition video;Topology;Visualization	Display algorithms;H.2.8 [Database Applications]: Data mining;H.3.3 [Information Search and Retrieval]: Selection process;I.3.3 [Picture/Image Generation]	In explorative data analysis, the data under consideration often resides in a high-dimensional (HD) data space. Currently many methods are available to analyze this type of data. So far, proposed automatic approaches include dimensionality reduction and cluster analysis, whereby visual-interactive methods aim to provide effective visual mappings to show, relate, and navigate HD data. Furthermore, almost all of these methods conduct the analysis from a singular perspective, meaning that they consider the data in either the original HD data space, or a reduced version thereof. Additionally, HD data spaces often consist of combined features that measure different properties, in which case the particular relationships between the various properties may not be clear to the analysts a priori since it can only be revealed if appropriate feature combinations (subspaces) of the data are taken into consideration. Considering just a single subspace is, however, often not sufficient since different subspaces may show complementary, conjointly, or contradicting relations between data items. Useful information may consequently remain embedded in sets of subspaces of a given HD input data space. Relying on the notion of subspaces, we propose a novel method for the visual analysis of HD data in which we employ an interestingness-guided subspace search algorithm to detect a candidate set of subspaces. Based on appropriately defined subspace similarity functions, we visualize the subspaces and provide navigation facilities to interactively explore large sets of subspaces. Our approach allows users to effectively compare and relate subspaces with respect to involved dimensions and clusters of objects. We apply our approach to synthetic and real data sets. We thereby demonstrate its support for understanding HD data from different perspectives, effectively yielding a more complete view on HD data.	Tatu, A.	Univ. of Konstanz, Konstanz, Germany|c|	
	VAST	14-19 Oct. 2012	Just-in-time annotation of clusters, outliers, and trends in point-based data visualizations	10.1109/VAST.2012.6400487	http://dx.doi.org/10.1109/VAST.2012.6400487	73	82	6400487	data visualisation;information retrieval;pattern clustering;statistical analysis	computational techniques;just-in-time cluster annotation;just-in-time descriptive analytics;just-in-time outlier annotation;just-in-time trend annotation;point-based multidimensional data visualization technique;qualitative mental models;runtime performance characteristics;statistical techniques;user interaction;visual feature semantics	Clustering algorithms;Data mining;Data visualization;Feature extraction;Market research;Semantics;Visualization	Just-in-time descriptive analytics;feature identification and characterization;point-based visualizations	We introduce the concept of just-in-time descriptive analytics as a novel application of computational and statistical techniques performed at interaction-time to help users easily understand the structure of data as seen in visualizations. Fundamental to just-intime descriptive analytics is (a) identifying visual features, such as clusters, outliers, and trends, user might observe in visualizations automatically, (b) determining the semantics of such features by performing statistical analysis as the user is interacting, and (c) enriching visualizations with annotations that not only describe semantics of visual features but also facilitate interaction to support high-level understanding of data. In this paper, we demonstrate just-in-time descriptive analytics applied to a point-based multi-dimensional visualization technique to identify and describe clusters, outliers, and trends. We argue that it provides a novel user experience of computational techniques working alongside of users allowing them to build faster qualitative mental models of data by demonstrating its application on a few use-cases. Techniques used to facilitate just-in-time descriptive analytics are described in detail along with their runtime performance characteristics. We believe this is just a starting point and much remains to be researched, as we discuss open issues and opportunities in improving accessibility and collaboration.			
	VAST	14-19 Oct. 2012	Dis-function: Learning distance functions interactively	10.1109/VAST.2012.6400486	http://dx.doi.org/10.1109/VAST.2012.6400486	83	92	6400486	data analysis;data visualisation;iterative methods;learning (artificial intelligence);pattern classification	classification;data visual representation;dis-function;iterative approach;machine learning;optimization;two-dimensional scatterplot view;uniformly weighted Euclidean distance function	Bars;Data visualization;Euclidean distance;Machine learning;Vectors;Yttrium		The world's corpora of data grow in size and complexity every day, making it increasingly difficult for experts to make sense out of their data. Although machine learning offers algorithms for finding patterns in data automatically, they often require algorithm-specific parameters, such as an appropriate distance function, which are outside the purview of a domain expert. We present a system that allows an expert to interact directly with a visual representation of the data to define an appropriate distance function, thus avoiding direct manipulation of obtuse model parameters. Adopting an iterative approach, our system first assumes a uniformly weighted Euclidean distance function and projects the data into a two-dimensional scatterplot view. The user can then move incorrectly-positioned data points to locations that reflect his or her understanding of the similarity of those data points relative to the other data points. Based on this input, the system performs an optimization to learn a new distance function and then re-projects the data to redraw the scatter-plot. We illustrate empirically that with only a few iterations of interaction and optimization, a user can achieve a scatterplot view and its corresponding distance function that reflect the user's knowledge of the data. In addition, we evaluate our system to assess scalability in data size and data dimension, and show that our system is computationally efficient and can provide an interactive or near-interactive user experience.	Brown, E.T.	Dept. of Comput. Sci., Tufts Univ., Medford, MA, USA|c|	
	VAST	14-19 Oct. 2012	LeadLine: Interactive visual analysis of text data through event identification and exploration	10.1109/VAST.2012.6400485	http://dx.doi.org/10.1109/VAST.2012.6400485	93	102	6400485	information retrieval;social networking (online);text analysis	LeadLine;automatically identify meaningful events;entity recognition techniques;event exploration;event identification;information extraction;interactive visual analysis;interactive visual analytics system;large-scale text corpora;microblogs;news data;online news;social media data;support exploration;text data;visual text analysis systems	Crawlers;Data mining;Event detection;Lead;Time series analysis;Twitter;Visualization		Text data such as online news and microblogs bear valuable insights regarding important events and responses to such events. Events are inherently temporal, evolving over time. Existing visual text analysis systems have provided temporal views of changes based on topical themes extracted from text data. But few have associated topical themes with events that cause the changes. In this paper, we propose an interactive visual analytics system, LeadLine, to automatically identify meaningful events in news and social media data and support exploration of the events. To characterize events, LeadLine integrates topic modeling, event detection, and named entity recognition techniques to automatically extract information regarding the investigative 4 Ws: who, what, when, and where for each event. To further support analysis of the text corpora through events, LeadLine allows users to interactively examine meaningful events using the 4 Ws to develop an understanding of how and why. Through representing large-scale text corpora in the form of meaningful events, LeadLine provides a concise summary of the corpora. LeadLine also supports the construction of simple narratives through the exploration of events. To demonstrate the efficacy of LeadLine in identifying events and supporting exploration, two case studies were conducted using news and social media data.	Wenwen Dou	Univ. of North Carolina at Charlotte, Charlotte, NC, USA|c|	
	VAST	14-19 Oct. 2012	Relative N-gram signatures: Document visualization at the level of character N-grams	10.1109/VAST.2012.6400484	http://dx.doi.org/10.1109/VAST.2012.6400484	103	112	6400484	data visualisation;pattern classification;text analysis	CNG classifier;character n-grams;common n-grams classifier;document visualization;relative n-gram signatures;text analytic visualization system;text classification algorithm	Color;Context;Data visualization;Electronic mail;Frequency measurement;Visual analytics	Visual analytics;text classification;visual text analysis	The Common N-Gram (CNG) classifier is a text classification algorithm based on the comparison of frequencies of character n-grams (strings of characters of length n) that are the most common in the considered documents and classes of documents. We present a text analytic visualization system that employs the CNG approach for text classification and uses the differences in frequency values of common n-grams in order to visually compare documents at the sub-word level. The visualization method provides both an insight into n-gram characteristics of documents or classes of documents and a visual interpretation of the workings of the CNG classifier.			
	VAST	14-19 Oct. 2012	The Deshredder: A visual analytic approach to reconstructing shredded documents	10.1109/VAST.2012.6400560	http://dx.doi.org/10.1109/VAST.2012.6400560	113	122	6400560	data analysis;data visualisation;document image processing	DARPA shredder challenge;Deshredder;document reconstruction system;functional task taxonomy leading;near automatic approach;nearest neighbor matching techniques;shredded document reconstruction;shredded piece contours;shredded piece representation;time series;visual analytic approach	Assembly;Humans;Image reconstruction;Shape;Time series analysis;Visual analytics	H.5 [Information Interfaces and Presentation]: User Interfaces — Graphical;I.4.7 [Image Processing and Computer Vistion]: Feature Representation —;I.7.5 [Document and Text Processing]: Document Capture — Graphics recognition and interpretation	Reconstruction of shredded documents remains a significant challenge. Creating a better document reconstruction system enables not just recovery of information accidentally lost but also understanding our limitations against adversaries' attempts to gain access to information. Existing approaches to reconstructing shredded documents adopt either a predominantly manual (e.g., crowd-sourcing) or a near automatic approach. We describe Deshredder, a visual analytic approach that scales well and effectively incorporates user input to direct the reconstruction process. Deshredder represents shredded pieces as time series and uses nearest neighbor matching techniques that enable matching both the contours of shredded pieces as well as the content of shreds themselves. More importantly, Deshred-der's interface support visual analytics through user interaction with similarity matrices as well as higher level assembly through more complex stitching functions. We identify a functional task taxonomy leading to design considerations for constructing deshredding solutions, and describe how Deshredder applies to problems from the DARPA Shredder Challenge through expert evaluations.	Butler, P.	Dept. of Comput. Sci., Virginia Tech, Blacksburg, VA, USA|c|	
	VAST	14-19 Oct. 2012	Analyst&#39;s Workspace: An embodied sensemaking environment for large, high-resolution displays	10.1109/VAST.2012.6400559	http://dx.doi.org/10.1109/VAST.2012.6400559	123	131	6400559	computer displays;data structures;data visualisation	analyst workspace;distributed cognition;document spatial layout;embodied sensemaking environment;entity-centric approach;explorative investigative approach;high-resolution display;visual data representation	Cognition;Humans;Keyboards;Monitoring;Navigation;Organizations;Visualization	Embodiment;distributed cognition;highresolution display;large;sensemaking;space	Distributed cognition and embodiment provide compelling models for how humans think and interact with the environment. Our examination of the use of large, high-resolution displays from an embodied perspective has lead directly to the development of a new sensemaking environment called Analyst's Workspace (AW). AW leverages the embodied resources made more accessible through the physical nature of the display to create a spatial workspace. By combining spatial layout of documents and other artifacts with an entity-centric, explorative investigative approach, AW aims to allow the analyst to externalize elements of the sensemaking process as a part of the investigation, integrated into the visual representations of the data itself. In this paper, we describe the various capabilities of AW and discuss the key principles and concepts underlying its design, emphasizing unique design principles for designing visual analytic tools for large, high-resolution displays.	Andrews, C.	Virginia Tech, Blacksburg, VA, USA|c|	
	VAST	14-19 Oct. 2012	SocialNetSense: Supporting sensemaking of social and structural features in networks with interactive visualization	10.1109/VAST.2012.6400558	http://dx.doi.org/10.1109/VAST.2012.6400558	133	142	6400558	data analysis;data visualisation;feature extraction;social networking (online)	SocialNetSense;bottom-up process;information collection;information gathering;information organization;information synthesis;interactive visualization;network structure;process awareness;scholar collaboration network;sensemaking tool;social attribute information;social feature relationship;social network analysis;social network dataset;social-network visual analytics;structural feature;top-down process	Collaboration;Data visualization;Educational institutions;History;Social network services;Visual analytics	Social network;SocialNetSense;sensemaking;visual analytics;visualization	Increasingly, social network datasets contain social attribute information about actors and their relationship. Analyzing such network with social attributes requires making sense of not only its structural features, but also the relationship between social features in attributes and network structures. Existing social network analysis tools are usually weak in supporting complex analytical tasks involving both structural and social features, and often overlook users' needs for sensemaking tools that help to gather, synthesize, and organize information of these features. To address these challenges, we propose a sensemaking framework of social-network visual analytics in this paper. This framework considers both bottom-up processes, which are about constructing new understandings based on collected information, and top-down processes, which concern using prior knowledge to guide information collection, in analyzing social networks from both social and structural perspectives. The framework also emphasizes the externalization of sensemaking processes through interactive visualization. Guided by the framework, we develop a system, SocialNetSense, to support the sensemaking in visual analytics of social networks with social attributes. The example of using our system to analyze a scholar collaboration network shows that our approach can help users gain insight into social networks both structurally and socially, and enhance their process awareness in visual analytics.	Liang Gou	Pennsylvania State Univ., University Park, PA, USA|c|	
	VAST	14-19 Oct. 2012	Spatiotemporal social media analytics for abnormal event detection and examination using seasonal-trend decomposition	10.1109/VAST.2012.6400557	http://dx.doi.org/10.1109/VAST.2012.6400557	143	152	6400557	Internet;data visualisation;graphical user interfaces;interactive systems;social networking (online);time series	Flickr;Internet user;Twitter;YouTube;abnormal event detection;abnormal topics examination;abnormal topics exploration;control chart method;interactive social media data analysis;interactive visual analysis process;latent Dirichlet allocation;local event;seasonal trend decomposition;seasonal-trend decomposition;situational awareness;social media service;space-time indexed data;spatiotemporal data;spatiotemporal social media analytics;time-stamped geo-located data;topic time series;visual analytics approach;visualization;volume-based importance	Data mining;Earthquakes;Educational institutions;Media;Spatiotemporal phenomena;Time series analysis;Twitter	H.3.3 [Information Storage and Retrieval];H.5.2 [Information Interfaces and Presentation]: User Interfaces — GUI;Information Search and Retrieval — Information filtering;relevance feedback	Recent advances in technology have enabled social media services to support space-time indexed data, and internet users from all over the world have created a large volume of time-stamped, geo-located data. Such spatiotemporal data has immense value for increasing situational awareness of local events, providing insights for investigations and understanding the extent of incidents, their severity, and consequences, as well as their time-evolving nature. In analyzing social media data, researchers have mainly focused on finding temporal trends according to volume-based importance. Hence, a relatively small volume of relevant messages may easily be obscured by a huge data set indicating normal situations. In this paper, we present a visual analytics approach that provides users with scalable and interactive social media data analysis and visualization including the exploration and examination of abnormal topics and events within various social media data sources, such as Twitter, Flickr and YouTube. In order to find and understand abnormal events, the analyst can first extract major topics from a set of selected messages and rank them probabilistically using Latent Dirichlet Allocation. He can then apply seasonal trend decomposition together with traditional control chart methods to find unusual peaks and outliers within topic time series. Our case studies show that situational awareness can be improved by incorporating the anomaly and trend examination techniques into a highly interactive visual analysis process.			
	VAST	14-19 Oct. 2012	AlVis: Situation awareness in the surveillance of road tunnels	10.1109/VAST.2012.6400556	http://dx.doi.org/10.1109/VAST.2012.6400556	153	162	6400556	data visualisation;disasters;road accidents;spatiotemporal phenomena;traffic information systems;tunnels;video surveillance	AlVis design;alert visualization;complex disaster scenarios;historic video;human operators;live video;road tunnel surveillance;situation awareness;situation-sensitive information prioritization;spatio-temporal development;tunnel experts;tunnel model;tunnel operators;video data	Cameras;Context;Data visualization;Roads;Surveillance;Vehicles;Visualization	K.6.1 [Management of Computing and Information Systems];Project and People Management—Life Cycle	In the surveillance of road tunnels, video data plays an important role for a detailed inspection and as an input to systems for an automated detection of incidents. In disaster scenarios like major accidents, however, the increased amount of detected incidents may lead to situations where human operators lose a sense of the overall meaning of that data, a problem commonly known as a lack of situation awareness. The primary contribution of this paper is a design study of AlVis, a system designed to increase situation awareness in the surveillance of road tunnels. The design of AlVis is based on a simplified tunnel model which enables an overview of the spatiotemporal development of scenarios in real-time. The visualization explicitly represents the present state, the history, and predictions of potential future developments. Concepts for situation-sensitive prioritization of information ensure scalability from normal operation to major disaster scenarios. The visualization enables an intuitive access to live and historic video for any point in time and space. We illustrate AlVis by means of a scenario and report qualitative feedback by tunnel experts and operators. This feedback suggests that AlVis is suitable to save time in recognizing dangerous situations and helps to maintain an overview in complex disaster scenarios.			
	VAST	14-19 Oct. 2012	Smart super views &#x2014; A knowledge-assisted interface for medical visualization	10.1109/VAST.2012.6400555	http://dx.doi.org/10.1109/VAST.2012.6400555	163	172	6400555	data acquisition;data visualisation;knowledge based systems;medical computing	data acquisition;information acquisition;knowledge-assisted interface;medical visualization;qualitative feedback;smart super views	Biomedical imaging;Bones;Data visualization;Electronic mail;Fuzzy logic;Pragmatics;Semantics	Fuzzy Logic;Interaction;Visualization	Due to the ever growing volume of acquired data and information, users have to be constantly aware of the methods for their exploration and for interaction. Of these, not each might be applicable to the data at hand or might reveal the desired result. Owing to this, innovations may be used inappropriately and users may become skeptical. In this paper we propose a knowledge-assisted interface for medical visualization, which reduces the necessary effort to use new visualization methods, by providing only the most relevant ones in a smart way. Consequently, we are able to expand such a system with innovations without the users to worry about when, where, and especially how they may or should use them. We present an application of our system in the medical domain and give qualitative feedback from domain experts.	Mistelbauer, G.	Vienna Univ. of Technol., Vienna, Austria|c|	
	VAST	14-19 Oct. 2012	Visual analytics for the big data era &#x2014; A comparative review of state-of-the-art commercial systems	10.1109/VAST.2012.6400554	http://dx.doi.org/10.1109/VAST.2012.6400554	173	182	6400554	data visualisation;public domain software	IBM;SAP;open source toolkit;software company;software vendor;visual analytics system development;visualization technique	Analytical models;Bismuth;Data handling;Data models;Data visualization;Software;Visual analytics	H.4 [Information Systems]: INFORMATION SYSTEMS APPLICATIONS;K.1 [Computing Milieux];THE COMPUTER INDUSTRY — Markets	Visual analytics (VA) system development started in academic research institutions where novel visualization techniques and open source toolkits were developed. Simultaneously, small software companies, sometimes spin-offs from academic research institutions, built solutions for specific application domains. In recent years we observed the following trend: some small VA companies grew exponentially; at the same time some big software vendors such as IBM and SAP started to acquire successful VA companies and integrated the acquired VA components into their existing frameworks. Generally the application domains of VA systems have broadened substantially. This phenomenon is driven by the generation of more and more data of high volume and complexity, which leads to an increasing demand for VA solutions from many application domains. In this paper we survey a selection of state-of-the-art commercial VA frameworks, complementary to an existing survey on open source VA tools. From the survey results we identify several improvement opportunities as future research directions.	Leishi Zhang	Univ. of Konstanz, Konstanz, Germany|c|	
	VAST	14-19 Oct. 2012	Visual analytics methods for categoric spatio-temporal data	10.1109/VAST.2012.6400553	http://dx.doi.org/10.1109/VAST.2012.6400553	183	192	6400553	data visualisation;geographic information systems;interactive systems;meteorology;spatiotemporal phenomena	categorical change visualization;computational techniques;location analysis;meteorologic areas;movement tracking;space-referenced categorical data analysis;spatial data;spatial geographical locations;spatial geographical objects;spatiotemporal data categoric;task-oriented selection;time analysis;time-referenced categorical data analysis;visual analytics methods	Algorithm design and analysis;Data analysis;Data visualization;Electronic mail;Image color analysis;Time series analysis;Visualization		We focus on visual analysis of space- and time-referenced categorical data, which describe possible states of spatial (geographical) objects or locations and their changes over time. The analysis of these data is difficult as there are only limited possibilities to analyze the three aspects (location, time and category) simultaneously. We present a new approach which interactively combines (a) visualization of categorical changes over time; (b) various spatial data displays; (c) computational techniques for task-oriented selection of time steps. They provide an expressive visualization with regard to either the overall evolution over time or unusual changes. We apply our approach on two use cases demonstrating its usefulness for a wide variety of tasks. We analyze data from movement tracking and meteorologic areas. Using our approach, expected events could be detected and new insights were gained.	von Landesberger, T.	Tech. Univ. Darmstadt, Darmstadt, Germany|c|	
	VAST	14-19 Oct. 2012	Watch this: A taxonomy for dynamic data visualization	10.1109/VAST.2012.6400552	http://dx.doi.org/10.1109/VAST.2012.6400552	193	202	6400552	computer animation;data analysis;data structures;data visualisation;interactive systems	animation;conceptual framework;data access;data change;data transformation;design choice;dynamic data visualization technique;dynamic visualization;static visualization;streaming data;task suitability;taxonomy;visual representation interpretability;visualization process	Context;Data visualization;Encoding;Image color analysis;Retina;Taxonomy;Visualization	Dynamic Data;Interpretation	Visualizations embody design choices about data access, data transformation, visual representation, and interaction. To interpret a static visualization, a person must identify the correspondences between the visual representation and the underlying data. These correspondences become moving targets when a visualization is dynamic. Dynamics may be introduced in a visualization at any point in the analysis and visualization process. For example, the data itself may be streaming, shifting subsets may be selected, visual representations may be animated, and interaction may modify presentation. In this paper, we focus on the impact of dynamic data. We present a taxonomy and conceptual framework for understanding how data changes influence the interpretability of visual representations. Visualization techniques are organized into categories at various levels of abstraction. The salient characteristics of each category and task suitability are discussed through examples from the scientific literature and popular practices. Examining the implications of dynamically updating visualizations warrants attention because it directly impacts the interpretability (and thus utility) of visualizations. The taxonomy presented provides a reference point for further exploration of dynamic data visualization techniques.	Cottam, J.A.	Indiana Univ., Bloomington, IN, USA|c|	
	VAST	14-19 Oct. 2012	Information retrieval failure analysis: Visual analytics as a support for interactive &#x201C;what-if&#x201D; investigation	10.1109/VAST.2012.6400551	http://dx.doi.org/10.1109/VAST.2012.6400551	204	206	6400551	data analysis;data visualisation;information retrieval;learning (artificial intelligence)	IR systems;analytical model;discounted cumulative gain metric family;information retrieval failure analysis;interactive what-if investigation;machine learning approach;ranking model;visual analytics;what-if analysis	Analytical models;Educational institutions;Failure analysis;Image color analysis;Information retrieval;Prototypes;Visual analytics		This poster provides an analytical model for examining performances of IR systems, based on the discounted cumulative gain family of metrics, and visualization for interacting and exploring the performances of the system under examination. Moreover, we propose machine learning approach to learn the ranking model of the examined system in order to be able to conduct a “what-if” analysis and visually explore what can happen if you adopt a given solution before having to actually implement it.	Angelini, M.	Sapienza Univ. of Roma, Rome, Italy|c|	
	VAST	14-19 Oct. 2012	A visual analytics approach to understanding cycling behaviour	10.1109/VAST.2012.6400550	http://dx.doi.org/10.1109/VAST.2012.6400550	207	208	6400550	behavioural sciences computing;data mining;data visualisation;pattern classification;sport;travel industry	London's bike share scheme;customer journeys;customer level classifications;customer usage over time;data mining;ethnographic study;information visualization;journey length;public attitude surveys;sustainable travel policy;transactional dataset;travel times;understanding cycling behaviour;user classifications;visual analytics approach;visual analytics techniques	Cities and towns;Computer interfaces;Context;Educational institutions;Prototypes;Radio frequency;Visual analytics			Beecham, R.	City Univ. London, London, UK|c|	
	VAST	14-19 Oct. 2012	Matrix-based visual correlation analysis on large timeseries data	10.1109/VAST.2012.6400549	http://dx.doi.org/10.1109/VAST.2012.6400549	209	210	6400549	data visualisation;matrix algebra;time series	local weather conditions;matrix-based visual correlation analysis;photovoltaic power plant efficiency;time series data generation quantity;time series matrix data visualization;visual analysis techniques;visual comparison analysis;visual correlation analysis	Correlation;Meteorology;Power generation;Substations;Temperature measurement;Time series analysis;Visualization	H.3.3 [Information Search and Retrieval Design Tools and Techniques]: Information filtering —	In recent years, the quantity of time series data generated in a wide variety of domains grown consistently. Thus, it is difficult for analysts to process and understand this overwhelming amount of data. In the specific case of time series data another problem arises: time series can be highly interrelated. This problem becomes even more challenging when a set of parameters influences the progression of a time series. However, while most visual analysis techniques support the analysis of short time periods, e.g. one day or one week, they fail to visualize large-scale time series, ranging over one year or more. In our approach we present a time series matrix visualization that tackles this problem. Its primary advantages are that it scales to a large number of time series with different start and end points and allows for the visual comparison / correlation analysis of a set of influencing factors. To evaluate our approach, we applied our technique to a real-world data set, showing the impact of local weather conditions on the efficiency of photovoltaic power plants.			
	VAST	14-19 Oct. 2012	Feature-similarity visualization of MRI cortical surface data	10.1109/VAST.2012.6400548	http://dx.doi.org/10.1109/VAST.2012.6400548	211	212	6400548	biomedical MRI;data reduction;data visualisation;feature extraction;image retrieval;medical image processing;neurophysiology	INVIZIAN;Jensen-Shannon divergence metric;MRI cortical surface data;analytics-based framework;clinical neuroimaging studies;coordinate systems;cortical surfaces characteristic visualization;dimension reduction methods;feature data;feature relatedness visualization;feature-similarity visualization;informatics visualization-for-neuroimaging;neuroanatomical similarity;query-based framework;subject patient attribute values;surface data collection visualization	Alzheimer's disease;Brain;Data visualization;Magnetic resonance imaging;Market research;Neuroimaging	I.3 [Computer Graphics];I.3.3 [Viewing Algorithms];I.3.8 [Applications];Three-Dimensional Graphics and Realism	We present an analytics-based framework for simultaneous visualization of large surface data collections arising in clinical neuroimaging studies. Termed Informatics Visualization for Neuroimaging (INVIZIAN), this framework allows the visualization of both cortical surfaces characteristics and feature relatedness in unison. It also uses dimension reduction methods to derive new coordinate systems using a Jensen-Shannon divergence metric for positioning cortical surfaces in a metric space such that the proximity in location is proportional to neuroanatomical similarity. Feature data such as thickness and volume are colored on the cortical surfaces and used to display both subject-specific feature values and global trends within the population. Additionally, a query-based framework allows the neuroscience researcher to investigate probable correlations between neuroanatomical and subject patient attribute values such as age and diagnosis.	Bowman, I.	Sch. of Med., Lab. of Neuro Imaging, UCLA, Los Angeles, CA, USA|c|	
	VAST	14-19 Oct. 2012	Augmenting visual representation of affectively charged information using sound graphs	10.1109/VAST.2012.6400547	http://dx.doi.org/10.1109/VAST.2012.6400547	213	214	6400547	audio-visual systems;data analysis;data visualisation;graphs;interactive systems;social networking (online)	VA;Website;affective valence trend;affectively charged information;analytical reasoning process;audio-visual representation;auditory graphs;augmented information representation;interaction techniques;multimodal information representation;news articles;pilot experiment;sentiment analysis tool;short texts;social comments;sonifications;sound graphs;visual analytic research;visual representation augmentation	Accuracy;Data mining;Data visualization;Guidelines;Market research;Visual analytics	H.5.2 [Information Interfaces and Presentation]: User Interfaces(D.2.2, H.1.2, I.3.6) —;K.4.1 [Computers and Society]: Public Policy Issues — Human Safety	Within the Visual Analytics research agenda there is an interest on studying the applicability of multimodal information representation and interaction techniques for the analytical reasoning process. The present study summarizes a pilot experiment conducted to understand the effects of augmenting visualizations of affectively-charged information using auditory graphs. We designed an audiovisual representation of social comments made to different news posted on a popular website, and their affective dimension using a sentiment analysis tool for short texts. Participants of the study were asked to create an assessment of the affective valence trend (positive or negative) of the news articles using for it, the visualizations and sonifications. The conditions were tested looking for speed/accuracy trade off comparing the visual representation with an audiovisual one. We discuss our preliminary findings regarding the design of augmented information-representation.			
	VAST	14-19 Oct. 2012	Time-oriented visualization and anticipation	10.1109/VAST.2012.6400546	http://dx.doi.org/10.1109/VAST.2012.6400546	215	216	6400546	cognition;command and control systems;data visualisation;decision making;decision support systems;human computer interaction;human factors	DSS;anticipation;cognitive function;command and control situation;decision support system;display format;emergency response management;functional simulation;naval antiair warfare;operator temporal awareness;real-time dynamic decision making;safety-critical environment;temporal display;time-critical decision;time-oriented visualization	Command and control systems;Decision making;Decision support systems;Electronic mail;Fires;Marine vehicles;Real-time systems	Temporal awareness;decision support;human factors;time management	Temporal awareness is pivotal to successful real-time dynamic decision making in a wide range of command and control situations; particularly in safety-critical environments. However, little explicit support for operators' temporal awareness is provided by decision support systems (DSS) for time-critical decisions. In the context of functional simulations of naval anti-air warfare and emergency response management, the present study compares operator support provided by two display formats. In both environments, we contrast a baseline condition to a condition in which a temporal display was integrated to the original interface to support operators' temporal awareness. We also wish to establish whether the implementation of time-based DSSs may also come with drawbacks on cognitive functioning and performance.	Chamberland, C.	Univ. Laval, Quebec City, QC, Canada|c|	
	VAST	14-19 Oct. 2012	Visualising variations in household energy consumption	10.1109/VAST.2012.6400545	http://dx.doi.org/10.1109/VAST.2012.6400545	217	218	6400545	computer graphics;demography;electricity supply industry;interactive systems;local government;pattern classification;power consumption;power engineering computing;sustainable development	UK energy user type;computer graphics;consumption awareness;demographic characteristics;domestic energy consumption habit;energy industry;energy saving;energy-based data classification;geodemographics;household energy consumption;household energy use;interactive interface;local government;sustainable energy consumption	Carbon dioxide;Data visualization;Electricity;Energy consumption;Sociology;Statistics;Visualization	H.2.8 [Information Systems]: Database Management — Database Applications;H.5.2 [Information Systems]: Information Interfaces and Presentation — User Interfaces;I.3.8 [Computing Methodologies]: Computer Graphics — Applications	There is limited understanding of the relationship between neighbourhoods, demographic characteristics and domestic energy consumption habits. We report upon research that combines datasets relating to household energy use with geodemographics to enable better understanding of UK energy user types. A novel interactive interface is planned to evaluate the performance of specifically created energy-based data classifications. The research aims to help local governments and the energy industry in targeting households and populations for new energy saving schemes and in improving efforts to promote sustainable energy consumption. The new classifications may also stimulate consumption awareness amongst domestic users. This poster reports on initial visual findings and describes the research methodology, data sources and future visualisation requirements.	Goodwin, S.	giCentre, City Univ. London, London, UK|c|	
	VAST	14-19 Oct. 2012	Optimizing an SPT-tree for visual analytics	10.1109/VAST.2012.6400544	http://dx.doi.org/10.1109/VAST.2012.6400544	219	220	6400544	data visualisation;rendering (computer graphics);tree data structures;trees (mathematics)	SPT-tree optimization;bitwise comparisons;direct-volume rendering;geospatial temporal visual analytics software;geospatial-temporal visualizations;information visualization;locational codes;scientific visualization community;space-partioning time tree;spatial data structures;traversal speed improvement	Communities;Data visualization;Geospatial analysis;Indexing;Octrees;Optimization;Visual analytics		Despite the extensive work done in the scientific visualization community on the creation and optimization of spatial data structures, there has been little adaptation of these structures in visual analytics and information visualization. In this work we present how we modify a space-partioning time (SPT) tree - a structure normally used in direct-volume rendering - for geospatial-temporal visualizations. We also present optimization techniques to improve the traversal speed of our structure through locational codes and bitwise comparisons. Finally, we present the results of an experiment that quantitatively evaluates our modified SPT tree with and without our optimizations. Our results indicate that retrieval was nearly three times faster when using our optimizations, and are consistent across multiple trials. Our finding could have implications for performance in using our modified SPT tree in large-scale geospatial temporal visual analytics software.			
	VAST	14-19 Oct. 2012	Using translational science in visual analytics	10.1109/VAST.2012.6400543	http://dx.doi.org/10.1109/VAST.2012.6400543	221	222	6400543	cognition;data visualisation;medicine;user interfaces	dissemination;documentation;medicine;translational science;visual analytics interfaces	Best practices;Cognition;Educational institutions;Laboratories;Protocols;Visual analytics	cognition;evaluation;experimental studies	We introduce translational science, a research discipline from medicine, and show how adapting it for visual analytics can improve the design and evaluation of visual analytics interfaces. Translational science “translates” knowledge from the lab to the real-world to “ground truth” by incorporating a 3 phase program of research. Phase 1 & 2 include protocols for research in the lab and field and Phase 3 focuses on dissemination and documentation. We discuss these phases and how they may be applied to visual analytics research.	Green, T.M.	Sch. of Interactive Arts + Sci., Simon Fraser Univ., Surrey, BC, Canada|c|	
	VAST	14-19 Oct. 2012	Incorporating GOMS analysis into the design of an EEG data visual analysis tool	10.1109/VAST.2012.6400542	http://dx.doi.org/10.1109/VAST.2012.6400542	223	224	6400542	cognition;data analysis;data visualisation;electroencephalography;medical signal processing	EEG data visual analysis tool design;GOMS analysis;analysis quality;data analysis strategy;electroencephalography analyst;goal-operator-method-selector task analysis;subjective cognitive load	Brain modeling;Data visualization;Electrodes;Electroencephalography;Mathematical model;Standards;Visualization	Human factors;user-centered design	In this paper, we present a case study where we incorporate GOMS (Goals, Operators, Methods, and Selectors) [2] task analysis into the design process of a visual analysis tool. We performed GOMS analysis on an Electroencephalography (EEG) analyst's current data analysis strategy to identify important user tasks and unnecessary user actions in his current workflow. We then designed an EEG data visual analysis tool based on the GOMS analysis result. Evaluation results show that the tool we have developed, EEGVis, allows the user to analyze EEG data with reduced subjective cognitive load, faster speed and increased confidence in the analysis quality. The positive evaluation results suggest that our design process demonstrates an effective application of GOMS analysis to discover opportunities for designing better tools to support the user's visual analysis process.	Hua Guo	Dept. of Comput. Sci., Brown Univ., Providence, RI, USA|c|	
	VAST	14-19 Oct. 2012	Exploring cyber physical data streams using Radial Pixel Visualizations	10.1109/VAST.2012.6400541	http://dx.doi.org/10.1109/VAST.2012.6400541	225	226	6400541	computer centres;correlation methods;cybernetics;data visualisation	CPS data;RPV;automated analysis;correlation analysis;cyber physical data streams;cyber physical system;data center energy consumption;data value;multivariate data stream visual representation;peak point detection;periodic thermal hot spots;radial pixel visualizations;richly instrumented systems;smart buildings	Buildings;Correlation;Data visualization;Energy consumption;Servers;Temperature measurement;Temperature sensors	Radial pixel visualization;correlations;cyber physical system;peaks;time-series data	Cyber physical systems (CPS), such as smart buildings and data centers, are richly instrumented systems composed of tightly coupled computational and physical elements that generate large amounts of data. To explore CPS data and obtain actionable insights, we construct a Radial Pixel Visualization (RPV) system, which uses multiple concentric rings to show the data in a compact circular layout of small polygons (pixel cells), each of which represents an individual data value. RPV provides an effective visual representation of locality and periodicity of the high volume, multivariate data streams, and seamlessly combines them with the results of an automated analysis. In the outermost ring the results of correlation analysis and peak point detection are highlighted. Our explorations demonstrates how RPV can help administrators to identify periodic thermal hot spots, understand data center energy consumption, and optimize IT workload.	Hao, M.	Hewlett-Packard Labs., Palo Alto, CA, USA|c|	
	VAST	14-19 Oct. 2012	Exploring the impact of emotion on visual judgement	10.1109/VAST.2012.6400540	http://dx.doi.org/10.1109/VAST.2012.6400540	227	228	6400540	cognition;data visualisation;emotion recognition;psychology	affective-priming technique;cognitive task;crowdsourced user study;emotion impact;graphical perception experiment;locus of control;personality difference;psychology;stable traits;visual judgement task;visualization	Accuracy;Atmospheric measurements;Human factors;Humans;Psychology;Visual analytics		Existing research suggests that individual personality differences can influence performance with visualizations. In addition to stable traits such as locus of control, research in psychology has found that temporary changes in affect (emotion) can significantly impact individual performance on cognitive tasks. We examine the relationship between fundamental visual judgement tasks and affect through a crowdsourced user study that combines affective-priming techniques from psychology with longstanding graphical perception experiments. Our results suggest that affective-priming can significantly influence accuracy in visual judgements, and that some chart types may be more affected than others.	Harrison, L.	UNC-Charlotte, Charlotte, NC, USA|c|	
	VAST	14-19 Oct. 2012	Visualizing flows of images in social media	10.1109/VAST.2012.6400539	http://dx.doi.org/10.1109/VAST.2012.6400539	229	230	6400539	data visualisation;image retrieval;information retrieval systems;social networking (online)	3D visualization system;blog archive;change recognition;commercial picture design;image bricks;image cluster extraction;image flow visualization;mass media;product design;social media	Blogs;Feature extraction;Histograms;Market research;Media;Timing;Visualization		Mass and social media provide flows of images for real world events. It is sometimes difficult to represent realities and impressions of events using only text. However, even a single photo might remind us complex events. Along with events in the real world, there are representative images, such as design of products and commercial pictures. We can therefore recognize changes in trends of people's ideas, experiences, and interests through observing the flows of such representative images. This paper presents a novel 3D visualization system to explore temporal changes in trends using images associating with different topics, called Image Bricks. We show case studies using images extracted from our six-year blog archive. We first extract clusters of images as topics related to given keywords. We then visualize them on multiple timelines in a 3D space. Users can visually read stories of topics through exploring visualized images.			
	VAST	14-19 Oct. 2012	Using visual analytics to detect problems in datasets collected from photo-sharing services	10.1109/VAST.2012.6400538	http://dx.doi.org/10.1109/VAST.2012.6400538	231	232	6400538	data analysis;data visualisation;pedestrians;social networking (online);traffic engineering computing	Flickr;Geograph;Panoramio;PhD project;Picasa Web albums;doctoral research;pedestrian-routing system;photo-sharing services;spatial crawled photographic data distribution;user-generated photographic content;visual analytics	Cities and towns;Educational institutions;Electronic mail;Semantics;USA Councils;User-generated content;Visual analytics		Datasets that are collected for research often contain millions of records and may carry hidden pitfalls that are hard to detect. This work demonstrates how visual analytics can be used for identifying problems in the spatial distribution of crawled photographic data in different datasets: Picasa Web Albums, Panoramio, Flickr and Geograph, chosen to be potential data sources for ongoing doctoral research. This poster summary describes a number of problems found in the datasets using visual analytics and suggests that greater attention should be paid to assessing the quality of data gathered from user-generated photographic content. This work is the first part of a three-year PhD project aimed at producing a pedestrian-routing system that can suggest attractive pathways extracted from user-generated photographic content.	Kachkaev, A.	giCentre, City Univ. London, London, UK|c|	
	VAST	14-19 Oct. 2012	A generic model for the integration of interactive visualization and statistical computing using R	10.1109/VAST.2012.6400537	http://dx.doi.org/10.1109/VAST.2012.6400537	233	234	6400537	data visualisation;interactive systems;statistical analysis	R console;R object browser;coordinated multiple views framework;cyclic analysis workflow;data columns dynamic update;interactive modeling process;interactive selection;interactive visualization;statistical computation package R	Analytical models;Browsers;Computational modeling;Data models;Data visualization;Visualization		This poster describes general concepts of integrating the statistical computation package R into a coordinated multiple views framework. The integration is based on a cyclic analysis workflow. In this model, interactive selections are a key aspect to trigger and control computations in R. Dynamic updates of data columns are a generic mechanism to transfer computational results back to the interactive visualization. Further aspects include the integration of the R console and an R object browser as views in our system. We illustrate our approach by means of an interactive modeling process.	Kehrer, J.	VRVis Res. Center, Vienna, Austria|c|	
	VAST	14-19 Oct. 2012	The spatiotemporal multivariate hypercube for discovery of patterns in event data	10.1109/VAST.2012.6400536	http://dx.doi.org/10.1109/VAST.2012.6400536	235	236	6400536	data mining;data visualisation;decision making;interactive systems;spatiotemporal phenomena	decision making;event interaction framework;event transition framework;multidimensional spatiotemporal hypercube multivariate pattern discovery;pattern detection;spatiomultivariate data dimension;spatiotemporal multivariate event data space;visual analytics tools	Abstracts;Data visualization;Educational institutions;Hypercubes;Intelligent systems;Spatiotemporal phenomena;Visual analytics	Coordinated and multiple views;Field studies;Multidimensional data;Visual analytics;Visual knowledge discovery	Event data can hold valuable decision making information, yet detecting interesting patterns in this type of data is not an easy task because the data is usually rich and contains spatial, temporal as well as multivariate dimensions. Research into visual analytics tools to support the discovery of patterns in event data often focuses on the spatiotemporal or spatiomultivariate dimension of the data only. Few research efforts focus on all three dimensions in one framework. An integral view on all three dimensions is, however, required to unlock the full potential of event datasets. In this poster, we present an event visualization, transition, and interaction framework that enables an integral view on all dimensions of spatiotemporal multivariate event data. The framework is built around the notion that the event data space can be considered a spatiotemporal multivariate hypercube. Results of a case study we performed suggest that a visual analytics tool based on the proposed framework is indeed capable to support users in the discovery of multidimensional spatiotemporal multivariate patterns in event data.	Olislagers, F.	Intell. Syst. Lab. Amsterdam, Univ. of Amsterdam, Amsterdam, Netherlands|c|	
	VAST	14-19 Oct. 2012	Priming Locus of Control to affect performance	10.1109/VAST.2012.6400535	http://dx.doi.org/10.1109/VAST.2012.6400535	237	238	6400535	data visualisation;psychology	Amazon Mechanical Turk;LOC measure;LOC scale;locus of control;performance predictor;personality psychology;user accuracy;user speed;visual metaphor;visualization design;visualization tool	Educational institutions;Electronic mail;Layout;Marine vehicles;Psychology;Visual analytics		Recent research suggests that the personality trait Locus of Control (LOC) can be a reliable predictor of performance when learning a new visualization tool. While these results are compelling and have direct implications to visualization design, the relationship between a user's LOC measure and their performance is not well understood. We hypothesize that there is a dependent relationship between LOC and performance; specifically, a person's orientation on the LOC scale directly influences their performance when learning new visualizations. To test this hypothesis, we conduct an experiment with 300 subjects using Amazon's Mechanical Turk. We adapt techniques from personality psychology to manipulate a user's LOC so that users are either primed to be more internally or externally oriented on the LOC scale. Replicating previous studies investigating the effect of LOC on performance, we measure users' speed and accuracy as they use visualizations with varying visual metaphors. Our findings demonstrate that changing a user's LOC impacts their performance. We find that a change in users' LOC results in performance changes.			
	VAST	14-19 Oct. 2012	Visual exploration of local interest points in sets of time series	10.1109/VAST.2012.6400534	http://dx.doi.org/10.1109/VAST.2012.6400534	239	240	6400534	data analysis;data visualisation;interactive systems;time series	data analysis;interactive specification;interest point detection;local interest points;time series;visual analysis;visual exploration	Data visualization;Detectors;Educational institutions;Shape;Sorting;Time series analysis;Visualization		Visual analysis of time series data is an important, yet challenging task with many application examples in fields such as financial or news stream data analysis. Many visual time series analysis approaches consider a global perspective on the time series. Fewer approaches consider visual analysis of local patterns in time series, and often rely on interactive specification of the local area of interest. We present initial results of an approach that is based on automatic detection of local interest points. We follow an overview-first approach to find useful parameters for the interest point detection, and details-on-demand to relate the found patterns. We present initial results and detail possible extensions of the approach.	Schreck, T.	Univ. of Konstanz, Konstanz, Germany|c|	
	VAST	14-19 Oct. 2012	Infographics at the Congressional Budget Office	10.1109/VAST.2012.6400533	http://dx.doi.org/10.1109/VAST.2012.6400533	241	242	6400533	budgeting;computer graphics;government data processing	CBO;Congressional Budget Office;employee;federal government;static infographics	Data visualization;Economics;Government;Legislation;Presses;Software	data visualization;federal budget;federal government;infographics	The Congressional Budget Office (CBO) is an agency of the federal government with about 240 employees that provides the U.S. Congress with timely, nonpartisan analysis of important budgetary and economic issues. Recently, CBO began producing static infographics to present its headline stories and to provide information to the Congress in different ways.			
	VAST	14-19 Oct. 2012	A case study: Tracking and visualizing the evolution of dark matter halos and groups of satellite halos in cosmology simulations	10.1109/VAST.2012.6400532	http://dx.doi.org/10.1109/VAST.2012.6400532	243	244	6400532	astronomy computing;cosmology;dark matter	cosmic structure evolution;cosmological simulation;dark matter halo evolution;dark matter tracer particle tracking;host halos;host structures;merger trees;multilevel tracking model;satellite halos	Computational modeling;Corporate acquisitions;Merging;Physics;Satellites;Silver;Visualization	Group tracking;merger tree;multi-level tracking	In this poster, we track the evolution of cosmic structures and higher level host structures in cosmological simulation as they interact with each other. The structures found in these simulations are made up of groups of dark matter tracer particles called satellite halos and groups of satellite halos called host halos. We implement a multilevel tracking model to track dark matter tracer particles, satellite halos and host halos to understand their behaviour and show how the different structures are formed over time. We also represent the evolution of halos in the form of merger trees for detailed analysis by cosmologists.	Takle, J.	Dept. of Electr. &amp; Comput. Eng., Rutgers Univ., Piscataway, NJ, USA|c|	
	VAST	14-19 Oct. 2012	VDQAM: A toolkit for database quality evaluation based on visual morphology	10.1109/VAST.2012.6400531	http://dx.doi.org/10.1109/VAST.2012.6400531	245	246	6400531	data mining;data visualisation;database management systems	VDQAM;cognitive ability;data analysis;data mining;data quality evaluation;database quality evaluation;extracting process;loading process;transforming process;visual analysis method;visual morphology	Data analysis;Data mining;Data visualization;Morphology;Visual databases;Visualization	Database Quality;Interactive Visualization;Visual Analysis	Data quality evaluation is one of the most critical steps during the data mining processes. Data with poor quality often leads to poor performance in data mining, low efficiency in data analysis, wrong decision which bring great economic loss to users and organizations further. Although many researches have been carried out from various aspects of the extracting, transforming, and loading processes in data mining, most researches pay more attention to analysis automation than to data quality evaluation. To address the data quality evaluation issues, we propose an approach to combine human beings' powerful cognitive abilities in data quality evaluation with the high efficiency ability of computer, and develop a visual analysis method for data quality evaluation based on visual morphology.	Dongxing Teng	Inst. of Software, Beijing, China|c|	
	VAST	14-19 Oct. 2012	LensingWikipedia: Parsing text for the interactive visualization of human history	10.1109/VAST.2012.6400530	http://dx.doi.org/10.1109/VAST.2012.6400530	247	248	6400530	Web sites;data visualisation;grammars;information retrieval;natural language processing;reviews;text analysis;word processing	LensingWikipedia;NLP tools;Web-based interactive visual browser;bag of words;human history articles;information extraction;interactive human history visualization;state-of-the-art natural language processing tools;text parsing;textual information visualization approach;valuable linguistic information;word clusters	Data mining;Electronic publishing;Encyclopedias;Internet;Pragmatics;Semantics		Extracting information from text is challenging. Most current practices treat text as a bag of words or word clusters, ignoring valuable linguistic information. Leveraging this linguistic information, we propose a novel approach to visualize textual information. The novelty lies in using state-of-the-art Natural Language Processing (NLP) tools to automatically annotate text which provides a basis for new and powerful interactive visualizations. Using NLP tools, we built a web-based interactive visual browser for human history articles from Wikipedia.			
	VAST	14-19 Oct. 2012	VAST Challenge 2012: Visual analytics for big data	10.1109/VAST.2012.6400529	http://dx.doi.org/10.1109/VAST.2012.6400529	251	255	6400529				Visual analytics;contest;evaluation;human information interaction;metrics;sense making	The 2012 Visual Analytics Science and Technology (VAST) Challenge posed two challenge problems for participants to solve using a combination of visual analytics software and their own analytic reasoning abilities. Challenge 1 (C1) involved visualizing the network health of the fictitious Bank of Money to provide situation awareness and identify emerging trends that could signify network issues. Challenge 2 (C2) involved identifying the issues of concern within a region of the Bank of Money network experiencing operational difficulties utilizing the provided network logs. Participants were asked to analyze the data and provide solutions and explanations for both challenges. The data sets were downloaded by nearly 1100 people by the close of submissions. The VAST Challenge received 40 submissions with participants from 12 different countries, and 14 awards were given.	Cook, Kristin	Pacific Northwest National Laboratory|c|	
	VAST	14-19 Oct. 2012	NetSecRadar: A real-time visualization system for network security: VAST 2012 Mini Challenge. Award: Honorable mention for interesting use of radial visualization technique	10.1109/VAST.2012.6400516	http://dx.doi.org/10.1109/VAST.2012.6400516	281	282	6400516				Intrusion detection;real-time monitoring;security visualization;visual interfaces	NetSecRadar is a visual analytics system to aid in monitoring the network security in real time and perceiving the overall view of the security situation using radial graph. At present, we use this tool mainly for IDS alerts to analyze the irregular behavioral patterns, and synthesize interactions, filtering and drill-down to detect the potential intrusions. In conclusion, we describe how this system was used to analyze the mini-challenges of the 2012 VAST challenge.	Zhao, Ying	Central South University, Changsha, China|c|	
	VAST	14-19 Oct. 2012	Big data exploration through visual analytics	10.1109/VAST.2012.6400514	http://dx.doi.org/10.1109/VAST.2012.6400514	285	286	6400514				Visual analytics;big data;data visualization;exploratory data analysis	SAS® Visual Analytics Explorer is an advanced data visualization and exploratory data analysis application that is a component of the SAS Visual Analytics solution. It excels at handling big data problems like the VAST challenge. With a wide range of visual analytics features and the ability to scale to massive datasets, SAS Visual Analytics Explorer enables analysts to find patt er n s and relationships quickly and easily, no matter the size of their data. In this summary paper, we explain how we used SAS Visual Analytics Explorer to solve the VAST Challenge 2012 minichallenge 1.	Abousalh-Neto, Nascif A.	SAS Institute|c|	
	VAST	14-19 Oct. 2012	VAST 2012 Mini-Challenge 2: Chart- and Matrix-based approach to network operations forensics	10.1109/VAST.2012.6400513	http://dx.doi.org/10.1109/VAST.2012.6400513	287	288	6400513					We report the approach and results on the VAST 2012 MiniChallenge 2: Bank of Money Regional Office Network Operations Forensics. Using commercial data mining, visualization and database software such as KNIME, Tableau and MySQL as well as a custom-written source vs. destination IP pixel matrix, our team of students identified suspicious IRC traffic, an attack on the firewall, a drop in the firewall connections, an attempt for sensitive information exchange and a possible Distributed Denial-of-Service attack executed partly from a host within the bank network.	Hildenbrand, Jan	University of Konstanz, Germany|c|	
	VAST	14-19 Oct. 2012	Pixel-oriented Treemap for multiple displays	10.1109/VAST.2012.6400512	http://dx.doi.org/10.1109/VAST.2012.6400512	289	290	6400512				Large display;multiple displays;physical navigation;pixel-oriented visualization;treemap	We have developed a Pixel-oriented Treemap visualization intended for use on multiple displays with collaborating users. It visualizes the health and status of about a million devices with a Treemap layout. In this paper we describe how we found useful pieces of the VAST 2012 Challenge MC1dataset and discuss how users interacted with this visualization during the analysis.	Chung, Haeyong	Virginia Tech|c|	
	VAST	14-19 Oct. 2012	3D anomaly bar visualization for large-scale network: VAST 2012 Mini Challenge #1	10.1109/VAST.2012.6400511	http://dx.doi.org/10.1109/VAST.2012.6400511	291	292	6400511					In this VAST challenge, log data coming from locations all over the Bank of Money facilities that contains close one million IP addresses. Since the geography of the data plays an important role for potential anomalies detection, we present a particular visualization solution based on Google Earth that can provide measure and deal with geo-spatial data. By mapping three important attributes, i.e., number of connections, policy status and activity flag, into 3D bars on top of physical locations (coordinates), anomaly distribution and trends can be efficiently visualized and analyzed. The general KML file generator can be extended for further analysis on other GIS systems.	Zhang, Tao	Central Michigan University|c|	
	VAST	14-19 Oct. 2012	Network infrastructure visualisation using high-dimensional node-attribute data	10.1109/VAST.2012.6400510	http://dx.doi.org/10.1109/VAST.2012.6400510	293	294	6400510					We present an extended version of targeted projection pursuit, a high dimensional data exploration tool adapted for producing graph layouts using node-attributes. Attributes are generated based on detected events in the intrusion detection system and firewall logs and how often they occur for each IP address. Edges are the directed links between source and destination IPs. The layout is interactive and users can manipulate the points in order to find interesting layouts and then further analyse how these layouts are related to the events in the logs. Thus, they first allow the user to detect anomalies and then gives them a platform to investigate why they occur.	Gibson, Helen	Northumbria University|c|	
	VAST	14-19 Oct. 2012	VAST Challenge 2012: Interactively finding anomalies in geo-temporal multivariate data	10.1109/VAST.2012.6400508	http://dx.doi.org/10.1109/VAST.2012.6400508	297	298	6400508				human information interaction;intelligence analysis;interactive visualization;visual analytics	In this paper we describe the tool we developed to solve the VAST 2012 Challenge. We present how an expert can online analyze huge amounts of multivariate data in space and time. The visual components and the interaction between the system and the user are described, as well as the setup to allow on the fly processing and retrieval of huge amounts of data. We summarize how our tool helped us to solve the challenge.	Migut, Gosia	University of Amsterdam The Netherlands|c|	
	VAST	14-19 Oct. 2012	Agile visual analytics for banking cyber &#x201C;big data&#x201D;	10.1109/VAST.2012.6400507	http://dx.doi.org/10.1109/VAST.2012.6400507	299	300	6400507				agile;big data;framework;human information interaction;toolkit;visual analytics;visualization	This paper describes the rapid development of a tailored cyber situational awareness and analysis application for the 2012 IEEE VAST Mini-Challenge 1 (MC1) — Cyber Situation Awareness. The novel aspect of this project was in the process of developing the tailored solution for a “big data” application. Aperture is an open, adaptable, and extensible Web 2.0 visualization framework, designed to produce visualizations for analysts and decision makers in any common web browser. Aperture utilizes a novel layer-based approach to visualization assembly, and a data mapping API that simplifies the process of transformation of data or analytic results into visual forms and properties.	Jonker, David	Oculus Info Inc.|c|	
	VAST	14-19 Oct. 2012	Visual analytics for network security	10.1109/VAST.2012.6400506	http://dx.doi.org/10.1109/VAST.2012.6400506	301	302	6400506				H.5.2 [Information Interfaces and Presentation];User Interfaces — Interaction Systems	To visualize the VAST 2012 Mini Challenge 2 datasets, we use the InfoVis Toolkit (IVTK). Custom visualizations as well as extra interaction capabilities have been added to the toolkit. Custom-made Python scripts are used for data preprocessing purposes. In this work, we show how visualization tools may be combined to leverage network forensic analysis tasks.	Shurkhovetskyy, Georgiy	Modern Sciences and Arts University|c|	
