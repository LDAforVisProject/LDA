the authors discuss effective techniques for representing scalar and vector valued functions that interpolate to irregularly located data special attention is given to the situations where the sampling domain is a two-dimensional plane 3-d volume or a closed 3-d surface the authors first discuss the multiquadric and thin-plate spline methods for interpolating scalar_data sampled at arbitrary locations in a plane straightforward generalizations are then made to data sampled in 3-d volumetric regions as well as in higher dimensional spaces the globally defined interpolants can be evaluated on a fine regular grid and they can then be visualized using conventional techniques triangular and tetrahedral based visualization_techniques are also presented
the authors discuss the special properties of volumetric cell data eg noise discontinuity raggedness and the particular difficulties encountered when trying to visualize them in three dimensions the authors describe some of the solutions adopted specifically in surface discrimination and shading nerve cells neuroblastoma grown in tissue culture were selected as the biological preparation because these cells possess very rich actin structures the cells were stained with a fluorescent probe specific for actin rhodamine-phalloidin and were viewed and optically sectioned using the bio-rad mrc 600 confocal fluorescence microscope the slice dataset was then reconstructed and processed in the biocube environment a comprehensive system developed for volume_visualization of cellular structures the actin cytoskeleton of single cells was visualized and manipulated using this system
the authors describe the architecture of an end-user visualization system that supports interactive analysis of three-dimensional scalar and vector data in a heterogeneous hardware environment the system supports a variety of visualization methods with applicability in disciplines such as computational fluid dynamics earth and space sciences and finite-element analysis the authors discuss how design goals and hardware constraints lead to a simple cohesive paradigm for implementing a powerful flexible and portable visualization system to assure efficient operation across a broad range of hardware platforms the tools were implemented so that their interactivity is largely independent of data complexity to gain portability the system was built on a platform-independent graphics layer and user interface management system the authors outline general concerns with current visualization methods and show how the approach simplifies the visualization process
the authors present a flexible and efficient method to simulate the doppler shift in this new method the spectral curves of surface properties and light composition are represented by spline functions of wavelength these functions can cover the entire electromagnetic em waves bandwidth and incorporate the thermal radiation of objects into the surface property description in particular a temperature-dependent emission spectral distribution can be assigned to each object for imaging the nonvisible thermal spectra which may become visible due to blue shift the doppler shift and shading operations are performed through the manipulation of spline coefficients the evaluation of the spline functions which is computationally expensive is only carried out once-at the end of each shading loop for generating the display rgb values
progress in scientific_visualization could be accelerated if workers could more readily find visualization_techniques relevant to a given problem the authors describe an approach to this problem based on a classification of visualization_techniques that is independent of particular application domains a user breaks up a problem into subproblems describes these subproblems in terms of the objects to be represented and the operations to be supported by a representation locates applicable visualization_techniques in a catalog and combines these representations into a composite representation for the original problem the catalog and its underlying classification provide a way for workers in different application disciplines to share methods
a hierarchical triangulation built from a digital elevation model in grid form is described the authors present an algorithm that produces a hierarchy of triangulations in which each level of the hierarchy corresponds to a guaranteed level ofaccuracy the number of very thin triangles slivers is significantly reduced such triangles produced undesirable effects in animation in addition the number of levels of the triangulated irregular network tin tree is reduced this speeds up searching within the data structure tests on data with digital elevation input have confirmed the theoretical expectations on eight such sets the average sliveriness with the method was between 1/5 and 1/10 of old triangulations and number of levels was about one third there was an increase in the number of descendants at each level but the total number of triangles was also lower
summary form only given the basic parameters of current tv the origins of hdtv and the various types of tv systems being proposed in japan america and europe are reviewed available hdtv hardware new applications that this hardware enables and the economics involved are discussed how hdtv fits into the film and television industries from the perspectives of production distribution and creativity hdtv's demands upon telecommunications and why data_compression plays a critical role have been examined the evolution of the present workstation from many analytical perspectives leading up to the most recent product introductions of all the major vendors developments in accelerator boards and interactive_graphics peripherals and the evolution of the man/machine interface are discussed
it is shown that by a simple one-way mapping from quaternions to complex numbers the problem of generating a four-dimensional mandelbrot set by iteration of a quadratic function in quaternions can be reduced to iteration of the same function in the complex domain and thus the function values in 4-d can be obtained by a simple table lookup the computations are cut down by an order simple ways of displaying the fractal without shading and ways of fast ray tracing such a fractal using the table so generated are discussed further speedup in ray tracing can be achieved by estimates of a distance of a point from the mandelbrot set animation is a key factor in visualizing 4-d objects three types of animation are attempted translation in 4-d rotation in 4-d and fly-through in 3-d
the author applied image_processing and volume_rendering algorithms together with considerations on the physiology of the human_visual_system to improve the quality of perception of the information contained in positron emission tomography pet brain images and to highlight the existing anatomical information the psychophysical considerations for selecting color and brightness level are used to visualize functional and anatomical structures in three dimensions one is able to perceive in the images the levels of rates of glucose metabolism of regions in the brain and their relative locations in addition some of the anatomic structures such as the interhemispheric fissure the caudate nucleus and the thalamus are apparent
visualizing the third dimension while designing three-dimensional 3-d objects is an awkward process in mechanical computer-aided-design cad systems given the current state of the art the authors describe a computer system that automatically constructs the shape of a 3-d object from a single 2-d sketch the method makes it convenient to create and manipulate 3-d objects and is thus seen as an intelligent user interface for cad and 3-d graphics applications the proposed technique is built on well-known results in image_analysis these results are applied in conjunction with some perceptual rules to determine 3-d structure from a rough line drawing the principles are illustrated by a computerimplementation that works in a nontrivial object domain
the problem of presenting and gaining deeper understanding of a multidimensional system a mathematical model predicting 20-90 s oscillations in breathing is presented the authors utilized custom software for interactive analysis of a three-dimensional model plus wavefront software to render translucent images of the 3d surfaces the results show that under conditions of no peripheral chemosensor sensitivity periodic breathing is predicted to occur with 1 an increase in circulatory transit time between the lungs and brain 2 the presence of marked steady state hypoventilation and/or 3 an increase in brain blood flow rate it is concluded that the peripheral chemosensors carotid bodies are not essential for the development of periodic breathing
visual_data_analysis vda is a visualization approach that combines vector and raster graphics to provide insights into various aspects of multidimensional datasets vda methods have found application in aerospace engineering research vda is being used to develop nondestructive evaluation testing techniques for graphite epoxy composites by providing insights into stress waves propagating through them visual_data_analysis was used to analyze stress wave propagation determine the origin of an unexplained wave distortion and create a theoretical model to eliminate the distortion utilizing mathematical modeling
an algorithm for rendering scalar field data that reduces rendering times by as much as two orders of magnitude over traditional full resolution images is presented less than full-resolution sampling of the scalar field is performed using a fast ray tracing method the sampling grid points are output as a set of screen-based gouraud shaded polygons which are rendered in hardware by a graphics workstation a gradient-based variable resolution algorithm that further improves rendering speed is presented several examples are presented
the author presents a simple and flexible method of sharp coding for higher dimensional data sets that allows the database operator or the scientist quick access to promising patterns within and among records or samples the example used is a 13-parameter set of solar wind magnetosphere and ground observation data collected hourly for 21 days in 1976 the software system is a prototype developed to demonstrate the glyph approach to depicting higher-dimensional data sets the experiment was to depict all parameters simultaneously to see if any global or local patterns emerged this experiment proves that much more complex data can be presented for visual pattern_extraction than standard methods allow
computer_graphics has long been concerned with representing and displaying surfaces in three-dimensional space the author addresses the questions of representation and display in a higher dimensional setting specifically that of 3-manifolds immersed in four-dimensional space the author describes techniques for visualizing the cross-section surfaces of a 3-manifold formed by a cutting hyperplane the manifold is first triangulated so that the cross-section may be computed on a per tetrahedron basis the triangulated manifold is stored in a data structure which efficiently supports calculation of curvature these techniques have been implemented on personal iris
an exploratory effort to classify visual representations into homogeneous clusters is discussed the authors collected hierarchical sorting data from twelve subjects five principal groups of visual representations emerged from a cluster analysis of sorting data graphs and tables maps diagrams networks and icons two dimensions appear to distinguish these clusters the amount of spatial information and cognitive processing effort the authors discuss visual information processing issues relevant to the research methodology and data analyses used to develop the classification system results of the empirical_study and possible directions for future research
the vis-5d system provides highly interactive visual access to five-dimensional data sets containing up to 50 million data points vis-5d runs on the stardent st-1000 and st-2000 workstations and generates animated three-dimensional graphics from gridded data sets in real time it provides a widget-based user interface and fast visual response which allows scientists to interactively explore their data sets vis-5d generates literal and intuitive depictions of data has user controls that are data oriented rather than graphics oriented and provides a wysiwyg what-you-see-is-what-you-get response the result is a system that enables scientists to produce and direct their own animations
the animation of two-dimensional objects in a 2-d planar environment is discussed the use of chain codes as a boundary representation for 2-d objects undergoing animation is shown to be practical for several typical transformations various methods for implementing the transformations are described quantized methods transform groups of chain code elements into other groups while incremental methods construct the transformed chain code element by element the low cost of quantized methods which rely on table lookup and minimal arithmetic are weighed against the increasedaccuracy offered by incremental methods which maintain error indicators to ensure minimal differences between ideal and generated chain codes methods for scaling rotation and elastic deformation of objects based solely on chain code elements are discussed
the author presents a simple procedural interface for volume_rendering the interface is built on three types of objects volumes which contain the data to be visualized environments which set up viewing and lighting and image objects which convert results to a user-definable format a volume is rendered against a particular environment with the results sent to an image object for conversion by defining volume qualities such as color opacity and gradient in terms of user-definable transfer_functions the rendering process is made independent of the data set's underlying representation
mediaview is a computer program that provides a generic infrastructure for authoring and interacting with multimedia documents among its applications is the ability to furnish a user with a comprehensive environment for analysis and visualization with this program the user can produce a document that contains mathematics datasets and associated visualizations from the dataset or embedded mathematics animated sequences can be produced in situ equations that appear in a document have a backing format that is compatible with the mathematica language thus by clicking on an equation its semantics are conveyed to mathematica where the user can perform a variety of symbolic and numerical operations since the document is all digital it can be shared on a local network or mailed electronically to a distant site animations and any other substructures of the document persist through the mailing process and can be awakened at the destination by the recipient
a forecasting system for the great lakes in which the data generated by a three-dimensional numerical model is visualized by a 3-d/stereoscopic display module is discussed the module consists of a control panel and a display window with the capability of interactively rendering the results the event scheduling for scenario testing to steer the 3-d numerical model is achieved by a similar panel these panels set up the simulation and control the data flow between the graphics workstation and supercomputer rendering methods stereo imagery and animation are incorporated to display the results interaction between the user the workstation and the supercomputer allows steering of the simulation and tracing of the simulation output distributed software for postprocessing and volume_rendering are used to enhance the representation
the authors describe some simple visualization_techniques that may be used to explore dynamic three-dimensional scalar_fields in an interactive way scalar_data are assumed to have been already computed and graphic manipulations are done afterwards on a graphics workstation structured grids finite-difference grids are used leading to an easy and fast exploration of the interior of a volume smooth animation and simultaneous visualization of two or three scalar_fields is described these methods were tested on various types of data from different fields of petroleum engineering ie oil reservoir simulation geophysics geology and combustion engine_simulations
the authors describe an innovative personal visualization system and its application to several research and engineering problems the system bridges both hardware and software components to permit a user to graphically describe a visualization problem to the computer thereby reducing program development time to a few hours low-cost visualization is achieved using pc-based software that can either be executed on a pc or drive graphic workstations for high-resolution displays in either case supercomputer computation rates are available for the visualization process on pcs this is done with one or more pip plug in cards each of which is capable of 100 million floating point operations per second on workstations this is done with the quen array processor applications mentioned include ocean wave imaging characterizing superconductors and solar sail visualization
the authors describe several dynamic graphics tools for visualizing network data involving statistics associated with the nodes or links in a network the authors suggest a number of ideas for the static display of network data while motivating the need for interaction through dynamic graphics a brief discussion of dynamic graphics in general is presented the authors specialize this to the case of network data an example is presented
a general method for rendering isosurfaces of multivariate rational and polynomial tensor products is described the method is robust up to degree 15 handling singularities without introducing spurious rendering artifacts the approach does not solve the problem of singularities in general but it removes the problem from the rendering domain to the interpolation/approximation domain it is based on finding real roots of a polynomial in bernstein form this makes it particularly suitable for parallel and pipelined processing it is envisioned that the tensor products will be used as approximants or interpolants for empirical data or scalar_fields an interpolation scheme is given as an example
the authors propose a methodology that will extract a topologically closed geometric model from a two-dimensional image this is accomplished by starting with a simple model that is already topologically closed and deforming the model based on a set of constraints so that the model grows shrinks to fit the feature within the image while maintaining its closed and locally simple nature the initial model is a non-self-intersecting polygon that is either embedded in the feature or surrounds the feature there is a cost function associated with every vertex that quantifies its deformation the properties of simple polygons and the relationship between noise and feature the constraints embody local properties of simple polygons and the nature of the relationship between noise and the features in the image
the first half of a two-step quaternion julia set_visualization system is described this step uses a quarternion square root function to adapt the classic inverse iteration algorithm to the quaternions the augmented version produces a 3-d julia set defined by a point cloud that can be interactively manipulated on a graphics workstation several cues are assigned to the point cloud to increase depth perception finally a short theorem is proven that extends the domain of the inverse iteration method to a rotational family of quadratic quaternion julia sets
some ideas and techniques for visualizing volumetric_data are introduced the methods presented are different from both the volume_rendering techniques and surface contour methods volumetric_data is data with a domain of three independent variables the independent variables do not have to indicate a position in space and can be abstract in the sense that they can represent any quantity the authors cover only the case where the dependent data is a single scalar the authors describe a collection of techniques and ideas for graphing cuberille grid data all of these techniques are quite simple and rather easy to implement during the development of these techniques the authors were particularly concerned with allowing the user to interact with the system in order to interrogate and analyze the relationships indicated by the volumetric_data
a package that can bridge the connection between scattered data sets and the highly structured sets required by graphics algorithms is described although export of evaluation data is a necessary capability it is very important that this package has a fully featured three-dimensional graphics subsystem to interactively guide the researcher toward the final visualization results at that point the option exists of using more sophisticated and more powerful graphics tools to achieve the desired presentation the application presented has been designed to effectively meet these needs and to promote the awareness of the value of interpolation tools in visualization full details of this design are presented
the use of critical point analysis to generate representations of the vector_field_topology of numerical flow data sets is discussed critical_points are located and characterized in a two-dimensional domain which may be either a two-dimensional flow_field or the tangential velocity field near a three-dimensional body tangent curves are then integrated out along the principal directions of certain classes of critical_points the points and curves are linked to form a skeleton representing the two-dimensional vector_field_topology when generated from the tangential velocity field near a body in a three-dimensional flow the skeleton includes the critical_points and curves which provide a basis for analyzing the three-dimensional structure of the flow separation
an approach that uses advanced computer_graphics workstations and volume_rendering algorithms for accurate reconstruction of volumetric microscopy data is described it has been found that excellent reconstructions can be made from serial sections acquired using a charge-coupled device and a conventional light microscope both confocal and nonconfocal reconstructions are examined the effects of differing light sources are considered 3d image_processing results are presented
the sphere quadtree sqt which is based on the recursive subdivision of spherical triangles obtained by projecting the faces of an icosahedron onto a sphere is discussed most databases for spherically distributed data are not structured in a manner consistent with their geometry as a result such databases possess undesirable artifacts including the introduction of tears in the data when they are mapped onto a flat file system furthermore it is difficult to make queries about the topological relationship among the data components without performing real arithmetic the sqt eliminates some of these problems the sqt allows the representation of data at multiple levels and arbitrary resolution efficient search strategies can be implemented for the selection of data to be rendered or analyzed by a specific technique geometric and topological consistency with the data are maintained
the authors discuss various visualization_techniques that have the goal of identifying unwanted curvature regions interactively on screen the authors give a critical survey of surface interrogation methods several isoline and contouring techniques are presented and the reflection line method which simulates the so-called light cage by computer_graphics is presented the isophote method analyzes surfaces by determining lines of equal light intensity silhouettes are special isophotes a different approach to these problems is the mapping-technique the mapping methods recognize unwanted curvature regions by detecting singularities of a special mapping of the curve or surface investigated curvature plots are a practical means of analyzing free-form surfaces all these methods are effective but generally need a lot of computational effort the free-form surface visualization by ray tracing is discussed
a methodology for guiding the choice of visual representations of data is presented the methodology provides objective and directed display design facilities such facilities can guide interactive_visualization design generate standard visualizations automatically and assess the extent to which chosen representations can convey the required information to data_analysis the methodology is based on objectively distinguishing the types of information conveyed by various visual representations and matching these to the intrinsic characteristics of data and to aims for its interpretation this approach is directed toward developing a stronger theoretical basis for visualization in scientific computation the methodology is developed using a natural scene paradigm in which data variables are represented by identifiable properties of realistic scenes
the author describes a visualization model for three-dimensional scalar_data fields based on linear transport theory the concept of virtual particles for the extraction of information from data fields in introduced the role of different types of interaction of the data field with those particles such as absorption scattering source and color shift are discussed and demonstrated special attention is given to possible tools for the enhancement of interesting data features random texturing can provide visual insights as to the magnitude and distribution of deviations of related data fields eg originating from analytic models and measurements or in the noise content of a given data field hidden symmetries of a data set can often be identified visually by allowing it to interact with a preselected beam of physical particles with the attendant appearance of characteristic structural effects such as channeling
fieldview a visual_analysis tool designed to facilitate the interactive investigation of fluid mechanics data sets by providing an easy-to-use interface to the flow_field data is presented operating on nasa plot three-dimensional format data fieldview computes scalar and vector flow quantities and displays them using a variety of representations including animation an interactive viewing interface allows free motion around the data under study to allow the researcher to locate and study the interesting flow features of three-dimensional fluid dynamic_data
algorithms for rendering complex and shaded animation sequences are described the target display device for these image rendering algorithms is a multichannel display based on the superposing technique realized in hardware an animation sequence is displayed by superposing a dynamic foreground on a static background the static background can be a very complex scene and the dynamic foreground can be an image with a simple to medium complexity these two algorithms were developed based on raytracing
an algorithm that creates planar and arbitrarily curved_sections of free-form volumes is presented the definition of free-form volumes generalizes techniques from free-form curves and surfaces to trivariate representation the definition is given for volumes in the bernstein-bezier representation the author illustrates an intersection algorithm that can be used to perform intersection operations on free-form volumes some calculated examples are given the algorithm can be used as a subroutine for algorithms which are able to perform more general intersections of free-form volumes eg boolean operations on two free-form volumes
alternative models that use b-spline curves and surfaces for generating color sequences for univariate bivariate and trivariate mapping are introduced the main aim is to break away from simple geometric representation in order to provide more flexibility and control over color selection this facilitates the task of constructing a customized color scheme for a particular map the author gives a brief description of existing color schemes and their characteristics and provides some background for b-spline curves and surfaces
the use of qualitative shape synthesis for the display of 3-d binary objects is presented the proposed approach is applicable to multi-object scenes and to outdoor scenery as well it makes use of a new method the diffusion process that simulates diffusion of particles within the interior of a 3-d discrete object starting with initial concentrations of particles at the boundary-voxels the diffusion procedure simulates the propagation of these particles inwards boundary voxels of the object are colored according to the concentration of particles obtained by suspending the diffusion process this method assists shape characterization by providing a qualitative measure of boundary curvature and was used in achieving display of a variety of voxel-based objects examples of the use of this approach on synthetic terrain and range data are provided
the authors present techniques for automating the illustration of geometric models based on traditional hand illustration methods a system based on the techniques of traditional illustrators for automatically generating illustrations of complex three-dimensional models is described the system relies on a richer set of display primitives which are also outlined algorithmic details for emphasizing significant model components are discussed and some preliminary results are presented
this case_study describes how visualization tools were used in a nonlinear finite-element method fem analysis of rivet deformation after summarizing the problem at hand it is concluded that threefactors that aided the visualization process in this case can be extracted as general principles first focus the viewer on the area of interest second do not confuse the viewer with strange color scales and finally do not try to convey too much information in one image images should convey a maximum amount of information with a minimum of confusion in this particular case the most useful techniques proved to be animations of color-shaded contours where the viewer could zoom in on any area of particular interest animation was used for each of the seven different data types produced by the analysis package
the authors describe a conceptual_model the memory hierarchy framework and a visual language for using the model the model is more faithful to the structure of computers than the von neumann and turing models it addresses the issues of data movement and exposes and unifies storage mechanisms such as cache translation lookaside buffers main memory and disks the visual language presents the details of a computer's memory hierarchy in a concise drawing composed of rectangles and connecting segments using this framework the authors improved the performance of a matrix multiplication algorithm by more than an order of magnitude the framework gives insight into computer architecture and performance bottlenecks by making effective use of human visual abilities
methods of rendering reflections in curved surfaces are examined a numerical algorithm to derive spherical reflections is presented this algorithm has many attractive qualities such as low computation costs object space coherence device and resolution independence and generation of maximum information about reflections in curved surfaces the authors demonstrate that rendering reflections is a difficult problem as it defies analytic solutions the authors indicate several alternatives for generalizing this method to a broader domain
the authors describe some mathematical approaches and computer_graphics techniques for illustrating concepts related to fermat's last theorem they present a selection of visualization methods and describe observations made in the process of creating a three-minute computer animated videotape dealing with some elementary aspects of fermat's last theorem a problem in number theory the approach to the representation of the different concepts presented in the video was influenced by manyfactors the available hardware real and perceived constraints of the available software constraints imposed by the video medium and a number of peculiarities and features of the mathematical domain itself the authors describe the experiences with the software systems that played a part in these efforts some specific successful visualization_techniques and some unexpected mathematical insights
the authors discuss fast flow analysis software toolkit animplementation of a software system for fluid mechanics analysis visualization of computational aerodynamics requires flexible extensible and adaptable software tools for performing analysis tasks an overview of fast is given and its architecture is discussed interactive_visualization control is addressed the advantages and disadvantages of fast are discussed
the authors describe the real-time acoustic display capabilities developed for the virtual environment workstation view project the acoustic display is capable of generating localized acoustic cues in real time over headphones an auditor symbology a related collection of representational auditory objects or icons can be designed using the auditory cue editor which links both discrete and continuously varying acoustic parameters with information or events in the display during a given display scenario the symbology can be dynamically coordinated in real time with three-dimensional visual objects speech and gestural displays the types of displays feasible with the system range from simple warnings and alarms to the acoustic representation of multidimensional data or events
the idea of independently moving interacting graphical objects is introduced as a method for the visualization of continuous fields bird-oid objects or boids are discussed these boids derive from 1 icons which are geometric objects whose shape and appearance are related to the field variables 2 three-dimensional cursors by which a user interactively picks a point in space 3 particle traces which are numerically integrated trajectories in space 4 moving frames of vectors along space curves and 5 actors which are programming objects that can create and destroy instances of themselves act according to internal logic and communicate with each other and with a user a software prototype in the c++ language has been developed which demonstrates some of the capabilities of these objects for the visualization of scalar vector and tensor_fields defined over finite_elements or finite volumes
experimental investigations into the application of intelligent robot control technology to the problem of removing waste stored in tanks is discussed the authors describe the experimental environment used with particular attention to the hardware and software control environment and the graphical interface intelligent system control is achieved through the integration of extensive geometric and kinematic world models with real-time sensor-based control all operator interactions with the system are through fully animated graphical representations which validate all operator commands before execution to provide for safe operation sensing is used to add information to the robot system's world model and to allow sensor-based servo control during selected operations the results of an initial critical features test are reported and the potential to apply advanced intelligent control concepts to the removal of waste in storage tanks is discussed
a survey of 2d and 3d flow_visualization_techniques is provided the approach is based on applying volume_rendering to flow-visualization data linear_interpolation and b-spline approximation are used and several views are given for both suggestions for efficient volume_rendering are provided
different standard rendering methods applied to 4-d medical ultrasound data are discussed in particular maximum value projection sum of values projection transparent gray level gradient shading and surface shading have been tested due to the fact that ultrasound data suffer from a low signal to noise ratio image_processing and image_analysis are used to enhance and classify the volumetric_data set
blood movement investigated by magnetic resonance mr velocity mapping is generally presented in the form of velocity components in one or more chosen velocity encoding directions by viewing these components separately it is difficult for mr practitioners to conceptualize and comprehend the underlying flow structures especially when the image data have strong background noise a flow_visualization_technique that adapts the idea of particle_tracing used in classical fluid dynamics for visualizing flow is presented the flow image_processing relies on the strong correlation between the principal flow direction estimated from the distribution of the modulus of the velocity field and the direction derived from the raw image data by correlation calculation severe background noise can be eliminated flow pattern rendering and animation provide an efficient way for representing internal flow structures
effective methods for visualizing several sets of volumetric_data simultaneously are presented the methods involve the composition of multiple volumetric rendering techniques these techniques include contour curves color-blended contour regions projection graphs on surfaces isovalue surface construction and hypersurface projection graphs
the authors maintain that of particular importance for visualization excellence is an understanding of effective deictic facilities especially new techniques made possible by computation they explain what deixis is and why it is fundamental to visualization and they analyze some of the requirements for effective deixis in the context of emergent visualization technology
a set of volume_visualization tools that are based on the use of recursive ray tracing as the primary vehicle for realistic volume imaging is presented the tools include shadows mirrors specularity and constructive_solid_geometry the underlying representation for the ray tracer is a 3-d raster of voxels that holds the discrete form of the scene unlike traditional volume_rendering techniques the discrete recursive ray tracer models many illumination phenomena by traversing discrete rays in voxel space the approach provides true ray tracing of sampled or computed datasets as well as ray tracing of hybrid scenes where sampled or computed data are intermixed with geometric models and enhances the understanding of complex biomedical datasets
techniques for visualizing mathematical objects in four-dimensional 4d space that exploit four-dimensional lighting effects are explored the geometry of image production stereography and shadows in 4d is analyzed alternatives for smooth and specular shaded rendering of curves surfaces and solids in 4d are examined and a new approach that systematically converts curves or surfaces into uniquely renderable solids in 4d space by attaching spheres or circles to each point is proposed analogs of 3d shading methods are used to produce volume_renderings that distinguish objects whose 3d projections from 4d are identical analyzing the procedures needed to justify and evaluate a system as this for teaching humans to `see' in four dimensions leads to the proposal of a generally applicable four-step visualization paradigm
an experiment in exploratory data visualization using a massively parallel processor is described in exploratory data visualization it is typically not known what is being looked for instead the data are explored with a variety of visualization_techniques that can illuminate its nature by demonstrating patterns in it with this approach the authors were able to find new features in some of their oldest datasets and to create more vivid presentations of familiar features in these datasets their experience has also led to a better understanding of the nature of the exploratory_visualization and has resulted in some formal representations of the interaction process in this environment
a method of visual_comparison is described that provides the scientist with a unique tool to study the qualitative relationships between three sequences of numbers or symbols the program displays a 3d shape containing the sequence similarities and differences which manifest themselves as simple geometric shapes and colors that a human observer can easily detect and classify the method presents all possible correlations to the user giving it a considerable advantage over existing sequence comparison tools that only search for a programmed subset of all possible correlations thus using this technique researchers may detect sequence similarities that other analytic methods might completely overlook the program can also filter out undesirable or insignificant correlations the technique is easily adapted to a wide range of applications
this paper looks at the issues involved in using a visualization software package to extend the scope of an existing suite of semiconductor modeling software the visualization software and its hardware platform represent the state of the art in powerful interactive workstation visualization_systems a range of important issues to be considered when applying off-the-shelf visualization software to a real-world scientific problem is identified
software developed to deal with differing image file formats mismatched byte order and word sizes and confusing hardcopy device interfaces is described the sdsc image tool suite provides a simple extensible and portable mechanism for the support of a variety of common image formats so that tool-writers can concentrate on the task in hand rather than on the quirks of a particular image file format users of such tools are able to work with images generated from a variety of sources without being restricted to an arbitrary standard format the sdsc visualization printing suite creates a unified view of hardcopy devices
addresses 3d_visualization_techniques now being developed that are specific to coarse irregular grid fields such as finite-element models these include direct-generation of isovalues from finite_elements display of 3d gradient and tensor quantities and the display of multiple states of behavior items common to general 3d_visualization but with specific algorithmic andimplementation issues in finite_element analysis
an improvement to visualization_systems that provides a graphics window into an application displaying program data at run-time through an easy-to-use graphical interface is discussed with little or no instrumentation of the application the user will be able to dynamically select data for graphical display as the program executes on a remote computer system the data to be displayed and the type of display to be used are chosen interactively while the application is executing any data display can be enabled and disabled at any time it is not necessary to specify the data or graphics technique before compilation as with conventional graphics tools an architecture for such a remote_visualization system is proposed and animplementation called vista is described designed primarily for scientific_visualization vista or offers an environment for more effective debugging and program development
a mathematical data model for scientific_visualization that is based on the mathematics of fiber bundles is presented previous results are extended to the case of piecewise field representations associated with grid-based data representations and a general mathematical model for piecewise representations of fields on irregular grids is presented the various types of regularity that can be found in computational grids and techniques for compact field representation based on each form of regularity are discussed these techniques can be combined to obtain efficient methods for representing fields on grids with various regular or partially regular structures
reconstruction of 3d scenes using data from an acoustic imaging sonar is addressed the acoustic lens is described and issues concerning underwater 3d scene reconstruction from the lens data are examined two methods for visualizing objects in an acoustic snapshot of the ocean are discussed mathematical morphology and a synthesis of 3d digital imaging with volume_rendering
the author explores some of the primary problems that face designers of hardware and software for visualization who are attempting to create tools that will be used and widely accepted he describes possible solutions to some of these challenges that have been incorporated into fieldview a commercial tool for increasing engineering productivity in computational_fluid_dynamics_(cfd)
a description is given of the computer_graphics aspects of two architectures designed for imaging and graphics the two systems use parallel and pipelined architectures for high-performance graphics operations uwgpsp3 uses only commercially available off-the-shelf chips and consists of a tm34020 graphics system processor and four tms34082 floating point coprocessors that can be configured into pipelined or simd modes depending on the algorithm uwgsp4 uses dedicated asic chips for higher performance and consists of two main computational parts a parallel vector processor with 16 vector processing units used mainly for image_processing and a graphics subsystem which utilizes a parallel pipelined architecture for image synthesis
the use of ray casting in volume_rendering and its uses and advantages over surface rendering algorithms are discussed various adaptive algorithms that attempt to overcome its problem of high computational cost by taking advantage of image coherency and the bandlimited nature of volume data are described a method of subdividing the image plane with isosceles triangles instead of quadrants as is usually done is proposed it results in fewer rays being fired without sacrificing image quality a brief theoretical analysis of the algorithm in comparison with other methods is given
a computer was used to help study the packing of equal spheres in dimension four and higher a candidate of the densest packing in 4-space is described the configuration of 24 spheres touching a central sphere in this packing is shown to be rigid unlike the analog in 3-space in which the spheres can slide past each other a system for interactively manipulating and visualizing such configurations is described the voronoi cell for a sphere is the set of points closer to its center than to any other sphere center in the packing the packing density is the ratio of a sphere's volume to the average of the volumes of the voronoi cells a method of constructing voronoi cells and computing their volumes that works in any dimension is presented examples of voronoi cell volumes are given
techniques for displaying 3d isovalues of scalar_fields such as stress within a solid finite-element model generally involve examining each element for values of interest an inexpensive straightforward method is discussed for reducing the number of elements searched for such isovalues it takes advantage of one traversal of the element data to yield a compact classification of the model by result values and ranges with no sorting required this data structure can then relate any scalar isovalue to a set of element groups which are closely inclusive of the isovalue this method is intended for applications requiring repeated access to the analysis data such as animation and interactive rendering of isosurfaces and scalar_fields while applicable to general volume_visualization problems it is particularly well suited to optimizing real-valued continuum field results such as those found in finite-element data
volumetric rendering is applied to the interpretation of atomic-scale data generated from quantum molecular_dynamics computations in particular for silicon computations it is found that volumetric visualization of the computed 3d electronic charge density is a valuable tool for identifying defect states in silicon lattices in which oxygen atoms occur as impurities rendering of several judiciously selected ranges of charge density in translucent colors provides an effective means of identifying broken or altered molecular bonds and induced charge excesses in the lattice the resulting 3d images reveal important features missed previously in 2d charge density contour maps stereoscopic `blink comparison' of image pairs is an extremely valuable way to study the structural differences among various configurations and animation provides significant insight into the molecular_dynamics
summary form only given as follows historically scientific_visualization has been carried out in two primary modes interactive on desktop computers and batch on high-performance computers the next decade will see a merging of these two approaches with the advent of high-speed networking the networking is hierarchical in speed from ethernet to fddi to hippi this network effectively unites desktop computers with higher-value remote resources into a single metacomputer to take advantage of this new hardware configuration distributed_visualization software is being developed which allows the flexibility of the local workstation to be coupled with the computing power of distant supercomputers examples are discussed for 2d raster graphics and 3d rendered surface and volumetric graphics these new capabilities are having a remarkable impact on computational science
the mcclellan air force base installation restoration program irp which is responsible for identifying and remedying environmental contamination from past operation and disposal practices is considered since 1979 the irp has generated over 200 volumes of technical reports regarding the degree and extent of contamination at the base the base is in the process of automating the storage retrieval and analysis of the technical data generated by the cleanup program the requirements for the irp technical information system are discussed the development approach taken is presented visualization results from the system prototype are illustrated and future plans for development of the system are outlined
the benefits of using a distributed scientific_visualization tool in the field of acoustic modeling are demonstrated a user-friendly interface was developed under sunview a remote procedure call was used for transparent data transfer between a cray x-mp/28 and sun 4 workstation pv-wave a high-level graphics package was used to visualize the results
a low-cost high-performance_visualization tool based on the ibm pc is described characteristics of scientific and engineering visualization and requirements for real time analysis are discussed application programming without coding by use of flowgraphs is also presented
an algorithm for rendering of orthographic views of volume data on data-parallel computer architectures is described in particular the problem or rotating the volume in regard to thecommunication overhead associated with finely distributed memory is analyzed an earlier technique shear decomposition is extended to 3d and it is shown how this can be mapped onto a data-parallel architecture using only gridcommunication during the resampling associated with the rotation the rendering uses efficient parallel computation constructs that allow one to use sophisticated shading models and still maintain high-speed throughout this algorithm has been implemented on the connection machine and is used in an interactive volume-rendering application with multiple frames-per-second performance
this paper looks at the role visualization and visual_data_analysis play in the technical community it focuses on the premise that the wide variety of applications_of_visualization mandate a need for a variety of visualization software packages
current topographical mapping methods and problems associated with mapping are reviewed and one approach for improving the spatial resolution of scalp recorded eegs is detailed in particular techniques for interpolating the potential distribution and estimating the surface laplacian from multichannel data are presented and applied to human evoked potential data although developed for electroencephalographic data these spline algorithms can be applied to a variety of fields where visualization of spatial information is desired
a method for visualizing hierarchically structured information is described the tree-map visualization_technique makes 100% use of the available display space mapping the full hierarchy onto a rectangular region in a space-filling manner this efficient use of space allows very large hierarchies to be displayed in their entirety and facilitates the presentation of semantic information tree-maps can depict both the structure and content of the hierarchy however the approach is best suited to hierarchies in which the content of the leaf nodes and the structure of the hierarchy are of primary importance and the content information associated with internal nodes is largely derived from their children
software tools are traditionally connected using human-readable files an approach that buys flexibility and understandability at some cost in performance relative to binary file formats the possibility of using shared-memory functions to retain most of the existing style while leapfrogging the speed of reading binary files at least in some environments and for some applications is explored results of a benchmarking experiment confirm the benefits of this alternative
a collaboration designed to demonstrate the possibilities of access to supercomputers via the high-speed wide-area networks in order to carry out sophisticated interactive_visualization on local workstations is described the test case was visualization of 3d magnetic_resonance_imaging data with a vray performing surface_reconstruction to generate a set of triangles the resulting geometric data was sent to a local workstation to be rendered with minor enhancements to current network protocols enabling effective utilization of the 45 mb bandwidth of a t3-based network
a recently completedimplementation of a virtual environment for exploring numerically generated three-dimensional unsteady_flowfields is described a boom-mounted six-degree-of-freedom head-position-sensitive stereo crt system is used for viewing a hand-position-sensitive glove controller is used for injecting various tracers eg smoke into the virtual flowfield a multiprocessor graphics workstation is used for computation and rendering the techniques for visualizing unsteady_flows are described and the computer requirements for a variety of visualization_techniques are discussed these techniques generalize to visualization of other 3d vector_fields
this paper examines the role of experimental design data acquisition equipment and system integration in the holistic solution picture issues include data formats distributed computing environments and the need for truly interactive even real-time systems a major theme is reaching beyond volume_visualization to `volume comprehension' through volume_segmentation mensuration and geometry extraction
partial automation of the task of designing graphical displays that effectively depict the data to be visualized through cooperative computer-aided design ccad is described this paradigm combines the strengths of manual and automated design by interspersing guiding design operations by the human user with the exploration of design alternatives by the computer the approach is demonstrated in the context of the ive design system a ccad environment for the design of scientific_visualizations using a set of design rules that combine primitive visualization components in different ways these alternatives are presented graphically to the user who can browse through them select the most promising visualization and refine it manually
chemical reactions occurring within complex domains such as fractals can display behavior which differs radically from the expectation of classical chemical kinetics rather than relaxing to a uniform distribution at the steady state these nonclassical systems display large-scale order on many scales such self-organization is difficult to measure using the usual statistical techniques but is visually apparent the authors discuss some of the problems of visualizing chemical kinetics in fractal domains and describe evolution of the visualization as the chemist and visualization scientist collaborated
visual 3 a highly interactive environment for the visualization of 3d volumetric scientific data is described the volume can be broken up in a structured or unstructured manner and the problem can be static or unsteady in time because the data are volumetric and all the information can be changing traditional cad techniques are not appropriate therefore visual3 was developed using intermediate mode-rendering methods a unique aspect of visual3 is the dimensional windowing approach coupled with cursor mapping which allows efficient pointing in 3d space visual3 is composed of a large number of visualization tools that can be generally classified into identification scanning and probing techniques
this paper emphasizes the need for and importance of remote_visualization the potential impact of remote_visualization on application algorithmscommunication protocols and underlying networks is assessed opportunities for research and development to support remote_visualization in the context of the national research and education network are outlined
an algorithm based on mathematical morphology image_processing and volume_rendering has been developed to enhance the visual perception of definite and abstract structures embedded in multidimensional data undergoing visualization this erosion procedure enhances the depth and shape perception of structures present in the data beyond the perception facilitated by shading and contrasting colors alone the utility of this algorithm is demonstrated for medical imaging positron emission tomography and climate sea surface temperature data the resulting information is displayed in stereo
methods for displaying scientific data using textures and raster operations rather than geometric techniques are described the flexibility and simplicity of raster operations allow a greater choice of visualization_techniques with only a small set of basic operations in addition texture_mapping techniques that allow the representation of several variables simultaneously without a high degree of clutter are shown the combination of traditional geometric techniques image composition techniques and image rendering techniques can be integrated into a single framework for the display of scientific data a system for generating and operating on textures and images for the purposes of scientific_visualization is presented to illustrate its advantage the development of bump maps for vector filters and contour lines is demonstrated
a method of visualizing equations in their explicit form using 3d fields is described equations are written algebraically interpreted by an equation parser and then expressed as scalar_fields fields are represented as isosurfaces making use of an algorithm similar to the method of marching_cubes theimplementation allows the real-time interaction of equation parameters isosurface rotations and coloring a variety of applications from mathematics and physics are given together with examples of construction of data probes using equations
this paper addresses the question of how the work of the scientist will change in the new multimedia environments scenarios for the process of simulating and analyzing data in such environments are constructed and some of the underlying models used in their construction are examined
an experimental volume_visualization system netv that distributes volume imaging tasks to appropriate network resources is described netv gives offsite scientists easy access to high-end volume imaging software and hardware the system allows a user to submit volume imaging jobs to an imaging spooler on a visualization-server remote high-power compute engines process rendering tasks while local workstations run the user-interface the time required to submit a job render the job on a mini-supercomputer-class machine and return the volume imaging to the offsite scientist is far less than the time it would take to create a similar image on a local workstation-class machine
television coverage of golf fails to bring the viewer an appreciation of the complex topography of a golf green and how that topography affects the putting of golf balls a computer_graphics simulation that enhances the viewer's perception of these features using shaded polygonal models of the actual golf green used in tournaments is presented mathematical modeling of the golf ball's trajectory on its way toward the hole further enhances viewer understanding a putting difficulty map assesses the relative difficulty of putting from each location on the green to a given pin position the object-oriented system is written in c and runs on a variety of 3d graphics workstations as an experiment the system was used at a professional golf tournament and correctly simulated all putts during the final round
addresses the issue of the use of color as compared to monochromatic displays in visualization the paper presents the advantages and disadvantages of color displays and those of monochromic displays identifies situations where color can improve the representation those where it will degrade it and suggest guidelines on how and when to use color
a method is presented for juxtaposing 4d space-time vector_fields of which one contains a source variable and the other the response field thresholding ellipsoid fitting and vortex line generation are used to reduce the amount of information and help analyze the relationship between two 3d vector variables evolving in time the technique helps to highlight the topological relationship between the two in an effort to understand the causal connection these concepts are applied to on-going research in evolving fluid dynamics problems
the problems and advantages of integrating scientific computations and visualization into one common program system are examined an important point is the direct feedback of information from the visualization into an ongoing simulation some strong and weak points of the varying approaches in different software packages are shown the visualization component of the authors' program system and the advantages of its integration into the overall system are explained the weak points in their system and the work remaining to deal with them are described
gray-scale diagrams which can present large amounts of quantitative information in a compact format are considered as a candidate for business charts hundreds of data points can easily be represented in one diagram using small gray-scale squares or tiles without visually overloading a viewer an experiment was done to compare the subjects' responses to questions from three types of charts traditional column and line_charts and gray-scale tile charts the results showed that questions were answered more correctly and more quickly using gray-scale tile charts than using traditional charts however subjects reported they experienced more strain using gray-scale charts
a technique that harnesses color and texture perception to create integrated displays of 2d image-like multiparameter distributions is presented the power of the technique is demonstrated by an example of a synthesized dataset and compared with several other proposed techniques the nature of studies that are required to measure objectively and accurately the effectiveness of such displays is discussed
the author describes the interaction between computer_graphics students from the computer science department at rochester institute of technology and faculty from various disciplines in their attempts to utilize state-of-the-art computer_graphics techniques for the visualization of physical systems the structure of a computer_graphics course designed to act as the vehicle for this interaction is also described
two basic principles for interactive_visualization of high-dimensional_data-focusing and linking-are discussed focusing techniques may involve selecting subsets dimension reduction or some more general manipulation of the layout information on the page or screen a consequent of focusing is that each view only conveys partial information about the data and needs to be linked so that the information contained in individual views can be integrated into a coherent image of the data as a whole examples are given of how graphical data_analysis methods based on focusing and linking are used in applications including linguistics geographic_information systems time_series analysis and the analysis of multi-channel images arising in radiology and remote sensing
the authors have achieved rates as high as 15 frames per second for interactive direct visualization of 3d data by trading some function for speed while volume_rendering with a full complement of ramp classification capabilities is performed at 14 frames per second these speeds have made the combination of region selection with volume_rendering practical for the first time semantic-driven selection rather than geometric clipping has proved to be a natural means of interacting with 3d data internal organs in medical data or other regions of interest can be built from preprocessed region primitives the resulting combined system has been applied to real 3d medical data with encouraging results
the technique of placing directed line segments at grid points known as hedgehogging which has been used for visualizing 2d vector_fields is considered a means of rapidly rendering a slice of a 3d field suitable for a bilevel display is provided shape and shadowing are used to disambiguate orientation liberal use of lookup tables makes the technique very fast
in order to visualize both clouds and wind in climate simulations clouds were rendered using a 3d texture which was advected by the wind flow the simulation is described rendering the advection of texture coordinates and haze effects are discussed results are presented
a three-dimensional finite-element hydrodynamic model was constructed to simulate tidal cycles in galveston bay over a one-year period in order to view changes in water velocities and salinity a project undertaken to visualize the simulation results is reported the project comprised analyzing model requirements and determining suitable visualization_techniques visualizing a preliminary smaller-scale model to verify the techniques and visualizing the full-scale model problems encountered and resolutions of problems at each stage are described validation as well as insights revealed about the model through the preliminary and final visualization are discussed current application of visualization_techniques to the model is reported
maintenance of a front of particles an efficient method of generating a set of sample points over a two-dimensional stream surface is described the particles are repeatedly advanced a short distance through the flow_field new polygons are appended to the downstream edge of the surface the spacing of the particles is adjusted to maintain an adequate sampling across the width of the growing surface curve and ribbon methods of vector_field_visualization are reviewed
a brief example of the visualization and quantification of a complex fluid interaction is presented in order to give one a feeling for the difficulty of dealing with geometrical and topological questions in three dimensions and time to obtain a quantitative understanding visiometric techniques including thresholding object isolation ellipsoid fitting abstraction vector field line generation and data juxtaposition were used
issues and difficulties involved in the practicalimplementation of flow_visualization_techniques based on a database generated in numerical simulations of unsteady square jets are addressed instantaneous visualizations provide basic information on the topological features of the flow while animation of these visualizations gives an insight into the detailed dynamics of formation development and interaction of the coherent_structures controlling the entrainment and mixing processes
discusses the ways in which the understanding of visual perception could help improve the scientific_visualization process it is argued that as long as there is a human interface link to computer visualization_systems understanding how humans perceive information visually could help improve the quality and the effectiveness of the visualization process the fields of visual physiology psychophysics and cognitive psychology can explain why human vision is so efficient how to create better images and how to determine the limitations of particular representations
a framework for the generation of atlases of the human body based on the linkage of volume data to a knowledge base is presented the model has a two layer structure the lower level is a volume model with a set of semantic attributes belonging to each voxel its spatial representation is derived from data sets of magnetic_resonance_imaging mri and computer tomography the semantic attributes are assigned by an anatomist using a volume editor the upper level is a set of relations between these attributes which are also specified by the expert interactive_visualization tools such as multiple surface display transparent rendering and cutting are provided it is shown that the combination of this object-oriented data structure with advanced volume_visualization tools provides the look and feel of a real dissection
techniques for visualizing a simulated air flow in a clean room are developed by using an efficient cell traverse of tetrahedral cells generated from irregular volumes the proposed techniques probing and stream line display are related to the measurement techniques used in actual clean rooms the efficient traverse makes it possible to move freely around a given irregular volume and to spawn off stream lines a successful application of these techniques to a problem in a clean room is also described
it is shown how data visualization fits into the broader process of scientific data_analysis scientists from several disciplines were observed while they analyzed their own data examination of the observations exposed process elements outside conventional image viewing for example analysts queried for quantitative information made a variety of comparisons applied math managed data and kept records the characterization of scientific data_analysis reveals activity beyond that traditionally supported by computer it offers an understanding which has the potential to be applied to many future designs and suggests specific recommendations for improving the support of this important aspect of scientific computing
the architecture of the data explorer a scientific_visualization system is described data explorer supports the visualization of a wide variety of data by means of a flexible set of visualization modules a single powerful data model common to all modules allows a wide range of data types to be imported and passed between modules there is integral support for parallelism affecting the data model and the execution model the visualization modules are highly interoperable due in part to the common data model and exemplified by the renderer an execution model facilitates parallelization of modules and incorporates optimizations such as caching the two-process client-server system structure consists of a user interface that communicates with an executive via a dataflow language
controlled experiments with novice treemap users and real data highlight the strengths of treemaps and provide direction for improvement issues discussed include experimental results layout_algorithms nesting offsets labeling animation and small multiple displays treemaps prove to be a potent tool for hierarchy display the principles discussed are applicable to many information_visualization situations
this case_study is a result of a six-week feasibility exercise the aim of which was to explore the extend to which existing visualization software can be used for visualizing isis neutron scattering data isis is an experimental facility devoted to the use of pulsed neutrons and muons to investigate the microscopic structure and dynamics of all classes of condensed matter the feasibility study demonstrated the benefits of using visualization in exploring material_science data and also proved that it is possible to satisfy most of the requirements isis researchers place on a software environment by using application visualization_systems avss and without writing any new code the problems encountered and possible solutions are discussed
methods are presented for the visualization of fuzzy data based on the sensitivity of the human_visual_system to motion and dynamic changes and the ease of which electronic display devices can change their display the methods include taking an otherwise static image and displaying in an animation loop either its segmented components or a series of blurred versions of the whole image this approach was applied to sea-surface temperature data and was found to be effective in showing fuzzy details embedded in the data and in drawing the viewer's attention this approach and these methods could play a significant role in the display of browse products for massive data and information systems
while scientific_visualization_systems share many requirements with other graphical applications they also have special requirements that make solutions based on standard rendering hardware or software not entirely satisfactory those requirements are illustrated by describing the renderer used in a production scientific_visualization system data explorer the requirements for a visualization renderer are discussedimplementation techniques used to meet the requirements of parallelism volume_rendering of irregular data clipping and integration of rendering modalities are described the renderer described is a software renderer but it is hoped that the requirements andimplementation presented might influence the design of future generations of rendering hardware
the generation of smooth_surfaces from a mesh of three-dimensional data points is an important problem in geometric modeling apart from the pure construction of these curves and surfaces the analysis of their quality is equally important in the design and manufacturing process generalized focal surfaces are presented as a new surface interrogation tool
the need for direct volume_visualization display devices is discussed as well as some specifics of the texas instruments omniview technology the topics discussed include the concept of operations the rotating surface the display volume the transport theory model the image quality in the display and applications the outlook for future volumetric displays is addressed
discusses the uses of visualization in the field of neuroscience is reported the applications discussed are image_analysis for basic neurobiological problems image_analysis from basic to applied neurobiological problems management of images and graphics from anatomical experiments and visualization and analysis of multivariate electrophysiological data sets
work in progress at the san diego supercomputer center sdsc involving theimplementation of network clients and servers to provide networkwide access to video devices is described applications anywhere on the net can manage record and playback operations change video signal routing or adjust scan converter parameters details of networkcommunications protocols and device-specific control quirks are invisible to the user making the video equipment a true network resource
discusses the breadth and the effectiveness of application visualization_systems avss the current and future research areas involving avss drawbacks and limitations of certain application areas possible improvements to avss and alternative analysis and visualization approaches are discussed
the visualization of 3-d second-order tensor_fields and matrix data is studied the general problem of visualizing unsymmetric real or complex hermitian second-order tensor_fields can be reduced to the simultaneous visualization of a real and symmetric second-order tensor_field and a real vector field the emphasis is on exploiting the mathematical properties of tensor_fields in order to facilitate their visualization and to produce a continuous representation of the data the focus is on interactively sensing and exploring real and symmetric second-order tensor data by generalizing the vector notion of streamline to the tensor concept of hyperstreamline the importance of a structural_analysis of the data field analogous to the techniques of vector_field_topology extraction in order to obtain a unique and objective representation of second-order tensor_fields is stressed
a visualization_technique that makes it possible to display and analyze line count profile data is described the technique is to make a reduced picture of code with the line execution counts identified with color hot spots are shown in red warm spots in orange and so on it is possible to identify nonexecuted code and nonexecutable code such as declarations and static tables
an information_retrieval frame work that promotes graphical displays and that will make documents in the computer visualizable to the searcher is described as examples of such graphical displays two simulation results of using a kohonen feature map to generate map displays for information_retrieval are presented and discussed the map displays are a mapping from a high-dimensional document space to a two-dimensional space they show document relationships by various visual_cues such as dots links clusters and areas as well as their measurement and spatial arrangement using the map displays as an interface for document retrieval systems the user is provided with richer visual information to support browsing and searching
animplementation of a virtual environment for visualizing the geometry of curved spacetime by the display of interactive geodesics is described this technique displays the paths of particles under the influence of gravity as described by the general theory of relativity and is useful in the investigation of solutions to the field equations of that theory a boom-mounted six-degree-of-freedom head-position-sensitive stereo crt system is used for display a hand-position-sensitive glove controller is used to control the initial positions and directions of geodesics in spacetime a multiprocessor graphics workstation is used for computation and rendering several techniques for visualizing the geometry of spacetime using geodesics are discussed although this work is described exclusively in the context of physical four-dimensional spacetimes it extends to arbitrary geometries in arbitrary dimensions while this work is intended for researchers it is also useful for the teaching of general_relativity
experiences during the investigation of parallel methods for faster isosurface generation on simd single instruction stream multiple data_stream machines are described a sequential version of a well-known isosurfacing algorithm is algorithmically enhanced for a particular type of simd architecture the simdimplementation takes full advantage of the data parallel nature of the algorithm and experiments have proven theimplementation to be highly scalable a parallel tool which can generate 170 k polygons/s gives scientists the means to explore large 3d scalar or vector_fields interactively
a project in the field of computational electrocardiography which requires visualization of complex three-dimensional geometry and electric potential and current fields is described starting from magnetic resonance images mris from a healthy subject a multisurfaced model of the human thorax was constructed and used as the basis for computational studies relating potential distributions measured from the surface of the heart to potentials and currents throughout the volume of the thorax a form of the forward problem in electrocardiography both interactive and batch-mode graphics programs were developed to view manipulate and interactively edit the model geometry results are presented
a universe mapping project at the harvard-smithsonian center for astrophysics cfa called the cfa redshift survey is described the line-of-sight recession velocities of galaxies are measured by identifying absorption and emission lines in their spectra with the two angular positions of a galaxy on the sky and a measurement of its red-shift each galaxy can be placed in a three-dimensional 3-d map of the universe it is shown that visualization_techniques are important for exploring and analyzing the data for comparing the data with models and for designing the future computer animation of the data is a way of bringing the maps before the public
in the automotive industry it is highly important that the exterior body panels be esthetically pleasing one aspect of creating esthetically pleasing surfaces is to require that they be fair a system that has proven useful for diagnosis of surface fairness problems is presented how to choose a set of colors with perceptually uniform spacing is described and the usefulness of a logarithmic scale for relating curvature to colors is shown
various techniques are described for achieving interactive direct_volume_rendering of nonrectilinear data sets using fast projection splatting_methods the use of graphics_hardware rendering approximations parallelization and reduced resolution meshes are discussed results from the use of these techniques are presented in the form of color photos and comparative timings
techniques that manipulate logical time in order to produce coherent animations of parallel program behavior despite the presence of asynchrony are presented the techniques interpret program behavior in light of user-defined abstractions and generate animations based on a logical rather than a physical view of time if this interpretation succeeds the resulting animation is easily understood if it fails the programmer can be assured that the failure was not an artifact of the visualization it is shown that these techniques can be generally applied to enhance visualizations of a variety of types of data as they are produced by parallel mimd multiple instruction stream multiple data_stream computations
visage a scientific_visualization system implemented in an object-oriented message passing environment is described the system includes over 500 classes ranging from visualization and graphics to xlib and motif user interface objects are created using compiled c and interact through an interpreted scripting language the result is a flexible yet efficient system that has found wide application the object architecture the major issues faced when designing the visualization classes and sample applications are also described
a prototypeimplementation of a splatting volume renderer svr on a commercially available distributed memory mimd multiple instruction stream multiple data_stream parallel processor the ncube2 is described some relatively good rendering times can be achieved with the ncube svr message-passing bottlenecks occur when large numbers of floating-point values have to be collected from every processor for every picture for large images this is a severe limitation an initialimplementation of a svr on a distributed memory parallel computer demonstrates the need for parallel computers with high-bandwidth connections between processors and also for new parallelizable volume_rendering algorithms
discusses efforts to develop virtual environment ve systems the applications discussed are medical telesurgery maintenance access presence simulators accounting visualizations topographic visualizations and tools to assist developers in determining the value added of potential ve-based solutions
the binary space partitioning tree is a method of converting a discrete space representation to a particular continuous space representation the conversion is accomplished using standard discrete space operators developed for edge detection followed by a hough transform to generate candidate hyperplanes that are used to construct the partitioning tree the result is a segmented and compressed image represented in continuous space suitable for elementary computer vision operations and improved image transmission/storage examples of 256×256 medical images for which the compression is estimated to range between 1 and 05 b/pixel are given
reports from five research centers involved with atmospheric and environmental visualization issues are presented in this case_study visualization with heterogeneous computer architectures is highlighted in the us epa scientific_visualization center discussion the nasa marshall space flight center effort to develop the multidimensional analysis of sensor systems mass environment is presented florida state university's building of a new scientific_visualization package sci an is reported this is followed by a discussion of the design andimplementation of vis-ad an experimental laboratory for developing scientific algorithms at the university of wisconsin-madison the visualization of global atmospheric data at ibm thomas j watson research center is highlighted
the relationship between gravity and topography to study subseafloor structures is discussed specifically analysis of the dynamics of seafloor spreading using satellite altimetry is described the visualization of satellite altimetry data and the limitations of such applications are presented
a new way of reconstructing human fossils from fragmentary fossil material is described unlike the traditional method of making physical models using clay this new approach is based on geometrical modeling and visualization of digitized fossil data it can provide anthropologists with both quantifiable computer-based geometric models and physical plastic reconstructions of fossils
a technique for defining graphical depictions for all the data types defined in an algorithm is presented the ability to display arbitrary combinations of an algorithm's data objects in a common frame of reference coupled with interactive control of algorithm execution provides a powerful way to understand algorithm behavior type definitions are constrained so that all primitive values occurring in data objects are assigned scalar types a graphical display including user_interaction with the display is modeled by a special data type mappings from the scalar types into the display model type provide a simple user interface for controlling how all data types are depicted without the need for type-specific graphics logic
a methodology has been developed for constructing streamlines and particle paths in numerically generated fluid velocity fields a graphical technique is used to convert the discretely defined flow within a cell into one represented by two three-dimensional stream functions streamlines are calculated by tracking constant values of each stream function a process which corresponds to finding the intersection of two stream_surfaces the tracking process is mass conservative and does not use a time stepping method for integration thus eliminating a computationally intensive part of traditional tracking algorithms the method can be applied generally to any three-dimensional compressible or incompressible steady flow results presented compare the performance of the new method to the most commonly used scheme and show that calculation times can be reduced by an order of magnitude
volume warping a technique for deforming sampled volumetric_data using b-splines that is related to image warping and to the free-form deformations of tw sederberg and sr parry 1986 and s coquillart 1990 is presented the process is accelerated to near-real-time speed and the compromises that are made to effect such speeds are explained this technique expands the repertoire of volumetric modeling techniques and can be applied to any form of volumetric_data
issues involved in operating a sophisticated scientific instrument as a computer peripheral accessible over a high-speed network are studied a custom interactive_visualization_application was constructed to support investigation using a unique computer-controlled high-voltage electron microscope the researcher's workstation forms the visible third of a triumvirate along with the instrument and the compute resource the software was designed to support not only image acquisition but also many of the tasks that microscope researchers perform in analyzing images the result of this case_study is the identification of some of the issues regarding interacting with scientific instrumentation over high-speed networks and the construction of custom applications to support many of the tasks within a laboratory's research methodology
an algorithm that attempts to improve a triangulation by shifting the vertices so that curvature within the triangles is nearly equal is presented unnecessary triangles are removed the method is an effective way of guaranteeing that the triangle vertices are points of higher curvature and that the triangle edges correspond to distinctive edges on the surfaces triangulations of surfaces with constant curvature-and hence no distinctive features-will gain nothing from this or any other optimization algorithm as demonstrated by the results the techinque of moving triangle vertices can improve some triangulation models greatest improvements occur with surfaces characterized by sharp edges such as the pyramid and ridge models less improvement occurs on models that already approximate the surface topology and/or have less distinctive features
discusses issues relating to the complexity of scientific_visualization software systemimplementation it is argued that the complexity of currentimplementations of such systems may limit the utility for users because the interfaces typically require significant knowledge of the data being studied and the applicable visualization algorithms as well as its infrastructure of graphics imaging and data handling technology the issues unknowns and possible solutions associated with building effective scientific_visualization software are discussed
a technique is given for computer visualization of simultaneous three-dimensional vector and scalar_fields such as velocity and temperature in reacting fluid flow_fields the technique which is called virtual smoke simulates the use of colored smoke for experimental gaseous fluid flow_visualization however it is noninvasive and can animate in particular the dynamic behaviors of steady-state or instantaneous flow_fields obtained from numerical simulations virtual smoke is based on volume seeds and volume seedlings which are direct volume_visualization methods previously developed for highly interactive scalar volume data_exploration data from combustion simulations are used to demonstrate the effectiveness of virtual smoke
a visual interface for a multimedia_database management system mdbms is described dbms query languages are linear in syntax although natural language interfaces have been found to be useful natural language is ambiguous and difficult to process for queries on standard relational_data these difficulties can be avoided with the use of a visual graphical interface to guide the user in specifying the query for image and other media data which are ambiguous in nature natural_language_processing combined with direct graphical access to the domain knowledge is used to interpret and evaluate the natural language query the system fully supports graphical and image input/output in different formats the combination of visual effect and natural language specification the support of media data and the allowance of incremental query specification simplify the process of query specification not only for image or multimedia_databases but also for all databases
theimplementation of truly interactive_volume_visualization and terrain_rendering algorithms on the princeton engine pe video supercomputer is described the pe is a single-instruction multiple-data simd computer since it was originally developed as a real-time digital television system simulator it possesses many of the attributes necessary for interactive_visualization high-resolution displays high-bandwidth i/o supercomputer class computational performance and a local memory array large enough to store multiple landsat scenes and data volumes it is shown that it is possible to generate truly interactive terrain_rendering and volume_visualization by computing images in real-time at multiple frames/second
it is suggested that many existing platforms over emphasize ease-of-use and do not adequately address issues of extensibility a visualization testbed called superglue which is particularly suited for the rapid development of new visualization methods was built an interpreter supports rapid development of new code and an extensive class hierarchy encourages code reuse by explicitly designing for ease of programming it was possible to produce a visualization system which is powerful easy to use and rapidly improving the motivation of the work the architecture of the system and plans for further development are reported
interactive_visualization_systems provide a powerful means to explore complex data especially when coupled with 3-d interaction and display devices to produce virtual_worlds while designing a quality static 2-d visualization is already a difficult task for most users designing an interactive 3-d one is even more challenging to address this problem auto visual a research system that designs interactive virtual_worlds for visualizing and exploring multivariate relations of arbitrary arity is being developed auto visual uses worlds within worlds an interactive_visualization_technique that exploits nested heterogeneous coordinate systems to map multiple variables onto each spatial dimension auto visual's designs are guided by user-specified visualization tasks and by a catalog of design principles encoded using a rule-based language
surface-particles are very small facets modeled as points with a normal they can be used to visualize flow in several ways by variation of the properties of the particle sources a method is presented for the rendering of surface-particles this method includes an improved shading model the use of gaussian filters for the prevention of spatial and temporal artifacts an efficient scan-conversion algorithm the handling of occlusion and the simultaneous rendering of geometric objects and surface-particles the synthesis of images with limited depth of field is described which literally allows the scientist to focus on areas of interest
data flow visual language systems are being used to provide sophisticated environments for the visualization of scientific data these systems are evolving rapidly and are beginning to encompass related technologies such as distributed computing and user interface development systems a hierarchical classification of the components and issues involved is presented giving an understanding of the design decisions and trade-offs that the developers of these systems are making the component categories can be used as a framework for discussing where interoperability of competing visual programming environments might occur and what the future holds for these systems
calico a dynamic tool for the creation and manipulation of color mappings for the exploration of multivariate quantitative data was used to study the effects of user control and smooth change on user preferenceaccuracy and confidence the results of the study as well as other user_experiences with calico support the hypothesis that dynamic manipulation of color mappings is a useful feature of systems for the exploration of quantitative data using color the main effect observed is a clear user preference for representations providing control over the mapping a small but significant increase inaccuracy and greater confidence in information gleaned from manipulable displays a smaller and less consistent effect showed greater user preference for an confidence in representations which provided smooth change between images
the volvis system has been developed to satisfy the diverse requirements of the volume_visualization community by comfortably housing numerous visualization algorithms and methods within a consistent and well organized framework the volvis system is supported by a generalized abstract model which provides for both geometric and volumetric constructs volvis contains several rendering algorithms that span the speed versusaccuracy continuum a fast volume_rendering algorithm has been developed which is capable of exploiting existing graphics_hardware without placing any viewing restrictions or compromisingaccuracy in addition volvis includes a volumetric navigation facility key-frame animation generator quantitative analysis tools and a generalized protocol for communicating with 3d input devices
a method is presented to obtain a unique shape description of an object by using wavelet transforms wavelet transform is a signal analysis technique which decomposes a signal using a family of functions having a local property in both time and frequency domains a multiresolution expression of 3d volume data was first obtained by applying 3dorthogonal wavelet transforms with the shape then being approximated with a relatively small number of 3dorthogonal functions using only the significant functions in addition the resolution of the approximation can be varied point by point using the local property of the wavelets the method is applied to real volume data ie facial range data and mr images of a human head and typical results are shown
a two-pass surface_extraction algorithm for adaptive finite-element meshes is presented in the context of a visualization study for a particle impact and a turbine-blade containment problem the direct use of finite-element data structures for the computation of external surfaces surface normals and derived physical qualities is discussed an overview of the in-betweening which accounts for rigid body dynamics effects is presented with a brief discussion of a direct-to-videodisk animation strategy
a technique is given allowing interactive_visualization of large scalar discrete volume fields as semitransparent clouds `on the fly' ie without preprocessing interactivity is not restricted to geometric transformations but includes all possible methods of processing the data the system flexibly trades-off quality for performance at any desirable level in particular by using a scanline based method and a dda-based traversing scheme instead of ray-tracing one achieves real-time processing during previewing by means of the `pyramidal volume' traversing technique one achieves high-quality constant-time_filtering independent of the data resolution several filters help to detect `fuzzy' obscured hot spots even within noisy data the visualization pipeline allows the application of filters at four different stages maximizing their flexibility four different illumination models have been implemented
a voxel-based forward projection algorithm with a pipeline architecture for real-time applications is presented the multisensor capabilities electrooptical or visual and infrared currently implemented in software have also been applied to non-real-time imaging applications on workstations and minicomputers most suited for terrain-based applications the system features haze imbedded targets moving objects smooth shading and specular reflections
experiments in visualizing implications of landscape planning and design decisions using a combination of gis cad and video animation technology are described simple grid-cell gis databases and site-scale polygonal models are used to provide visualizations of site planning design proposals and environmental impact with both static and animated images rather than pursuing photo-realistic simulations the focus is on how abstractions and representational conventions can be used to gauge visual and environmental effects of proposals for landscape change in a dynamic interactive computer-aided design environment
discusses issues relating to the state of the art in scientific_data_management management of scientific data sets or databases is reviewed the generic science requirements as well as a case example that drives the underlying data_management system architecture are explored showing current technology limitations a concept of intelligent information fusion with sufficient detail on how to integrate advanced technologies to enhance scientific production is presented emphasis is on user_interfaces spatial_data structure uses of neural_networks for extracting information from scientific imagery uses of object-oriented database management systems animation and visualization_techniques
an algorithm is presented which describes an application independent method for reducing the number of polygonal primitives required to faithfully represent an object reducing polygon count without a corresponding reduction in object detail is important for achieving interactive frame rates in scientific_visualization reducing mass storage requirements and facilitating the transmission of large multi-timestep geometric data sets this paper shows how coplanar and nearly coplanar polygons can be merged into larger complex polygons and re-triangulated into fewer simple polygons than originally required the notable contributions of this paper are 1 a method for quickly grouping polygons into nearly coplanar sets 2 a fast approach for merging coplanar polygon sets and 3 a simple robust triangulation method for polygons created by 1 and 2 the central idea of the algorithm is the notion of treating polygonal data as a collection of segments and removing redundant segments to quickly form polygon hulls which represent the merged coplanar sets
a supercomputing-visualization facility for science and engineering applications was used for processing and visualizing supercomputer-generated data this facility includes a vector-processing supercomputer a graphics workstation a general purpose workstation a high-resolution color printer a scanner a film recorder a video tape recorder and a video laser disc recorder the facility is using a network system to connect computers workstations and graphical input/output devices the supercomputer generates time-dependent multivariate data using a global climate simulation model visualization software systems are used for visualizing these model-produced data visualization_techniques including iso-contouring iso-surface generation vectors and streamlines generation are used
in rogowitz and treinish 1993 we introduced an architecture for incorporating perceptual rules into the visualization process in this architecture higher-level descriptors of the data metadata flow to perceptual rules which constrain visualization operations in this paper we develop a deeper analysis of the rules the prerequisite metadata and the system for enabling their operation
the features of greatest interest in ocean_modeling in the gulf of mexico and along the gulf stream are the fronts and eddies resolving modeling and tracking these eddies over time is of great importance for climatological studies and economic advancement in this paper we present a novel technique for automatically locating contouring and tracking oceanic features such as eddies and fronts the models and resultant visualizations exhibit excellent correlation with observed data
this work briefly describes our approach to visualize results of transient flow simulations in the application areas of groundwater flow and pollutant transport as well as compressible fluid flow in engine parts the simulations use finite_element data structures and can have geometries which change over time we designed a client-server model to handle the huge amount of data that can be obtained either directly from the simulation process or from files on disk as standard visualization packages are not able to cope with transient unstructured_data we implemented streamlines stream_surfaces and particle_systems as our main visualization methods our experiences and results with these techniques are discussed in this paper
visualization of events in high energy physics is an important tool to check hard- and software and to generate pictures for presentation purposes the radial pattern of all events suggests the use of predefined projections especially p/z and y/x the representation can be improved by a "fish-eye" transformation and by angular projections which produce straight track patterns and allow extensive magnifications three dimensional data of radial structure are best displayed in the 3d v-plot which has optimal track separation and presents all relevant information in a clear way
in this paper we describe a database query system that provides visual relevance feedback in querying large_databases the goal of our system is to support the query specification process by using each pixel of the display to represent one data item of the database by arranging and coloring the pixels according to their relevance for the query the user gets a visual impression of the resulting data set using sliders for each condition of the query the user may change the query dynamically and receives immediate feedback by the visual representation of the resulting data set by using multiple windows for different parts of a complex query the user gets visual feedback for each part of the query and therefore will easier understand the overall result the system may be used to query any database that contains tens of thousands to millions of data items but it is especially helpful to explore large_data sets with an unknown distribution of values and to find the interesting hot spots in huge amounts of data the direct feedback allows to visually display the influence of incremental query refinements and therefore allows a better easier and faster query specification
shading is an effective exploratory_visualization tool widely used in scientific_visualization interactive or close to interactive shading of images offers significant benefit but is generally too computationally expensive for graphics workstations a novel method for providing interactive diffuse and specular shading capability on low-cost graphics workstations is described application to digital elevation models iso-surfaces in volumetric images and color-coded aspect maps are illustrated and an analysis of artifacts and of ways of minimizing artifacts is given
the package described in this paper has been designed for analyzing the data collected in the lep experiment aleph its main graphical feature is a deep interplay between the description of the objects manipulated and their relationships and their graphical representation the easy access to information through navigation between objects and its display makes possible a thorough study of the events produced by the detector this has proved to be very powerful in numerous occasions for analyzing data and testing programs the package provides as well statistical_analysis tools and a graphic editor it is based on the phigs graphics standard it will develop towards a more elaborate usage of the data structure in particular in the geometrical representations and towards object oriented languages to overcome some heaviness linked to the use of fortran
a probe for the interactive_visualization of flow_fields is presented the probe can be used to visualize many characteristics of the flow in detail for a small region in the data set the velocity and the local change of velocity the velocity_gradient tensor are visualized by a set of geometric primitives to this end the velocity_gradient tensor is transformed to a local coordinate frame and decomposed into components parallel with and perpendicular to the flow these components are visualized as geometric objects with an intuitively meaningful interpretation animplementation is presented which shows that this probe is a useful tool for flow_visualization
fast techniques for direct_volume_rendering over curvilinear grids common to computational fluid dynamics and finite_element analysis are developed three new projection_methods that use polygon-rendering hardware for speed are presented and compared with each other and with previous methods for tetrahedral_grids and rectilinear grids a simplified algorithm for visibility ordering based on a combination of breadth-first and depth-first searches is described a new multi-pass blending method is described that reduces visual artifacts that are introduced by linear_interpolation in hardware where exponential interpolation is needed visualization tools that permit rapid data banding and cycling through transfer_functions as well as region restriction are described
a prototype visualization management system is described which merges the capabilities of a database management system with any number of existing visualization packages such as avs or idl the prototype uses the postgres database management system to store and access earth science data through a simple graphical browser data located in the database is visualized by automatically invoking a desired visualization package and downloading an appropriate script or program the central idea underlying the system is that information on how to visualize a data set is stored in the database with the data set itself
streamlines and stream_surfaces are well known techniques for the visualization of fluid flow for steady velocity fields a streamline is the trace of a particle and a stream surface is the trace of a curve here a new method is presented for the construction of stream_surfaces the central concept is the representation of a stream surface as an implicit surface f x = c after the initial calculation of f a family of stream_surfaces can be generated efficiently by varying c the shapes of the originating curves are defined by the value of f at the boundary two techniques are presented for the calculation of f one based on solving the convection equation the other on backward tracing of the trajectories of grid points the flow around objects is discussed separately with this method irregular topologies of the originating curves and of the stream_surfaces can be handled easily further it can also be used for other visualization_techniques such as time surfaces and stream volumes finally an effective method for the automatic placement of originating curves is presented
the process of visualizing a scientific data set requires an extensive knowledge of the domain in which the data set is created because an in-depth knowledge of all scientific domains is not available to the creator of visualization software a flexible and extensible visualization system is essential in providing a productive tool to the scientist this paper presents a shading_language based on the renderman shading_language that extends the shading model used to render volume data sets data shaders written in this shading_language give the users of a volume_rendering system a means of specifying how a volume data set is to be rendered this flexibility is useful both as a visualization tool in the scientific community and as a research tool in the visualization community
new display technologies have begun to provide more innovative and potentially powerful methods to present information to a viewer however many of these techniques struggle to deliver accurate full color in this paper we address this difficulty by employing the dichromatic theory of color reflection which implies that many objects can be rendered accurately using only two primaries complex display systems with two primaries can be produced with significantly less work than is required for the traditional three primaries we discuss methods for selecting objects that can be rendered accurately on two-color displays and we present our experiments with a two-color display using monochromatic primaries
three dimensional computer models of the anatomy generated from volume acquisitions of computed_tomography and magnetic_resonance_imaging are useful adjuncts to 2d images this paper describes a system that merges the computer generated 3d models with live video to enhance the surgeon's understanding of the anatomy beneath the surface the system can be used as a planning aid before the operation and provide additional information during an operation the application of the system to a brain operation is described
the issue of monitoring the execution of asynchronous distributed_algorithms on loosely-coupled parallel processor systems is important for the purposes of i detecting inconsistencies and flaws in the algorithm ii obtaining important performance parameters for the algorithm and iii developing a conceptual understanding of the algorithm's behavior for given input stimulus through visualization for a particular class of asynchronous distributed_algorithms that may be characterized by independent and concurrent entities that execute asynchronously on multiple processors and interact with one another through explicit messages the following reasoning applies information about the flow of messages and the activity of the processors may contribute significantly towards the conceptual understanding of the algorithm's behavior and the functional correctness of theimplementation the computation and subsequent display of important parameters based upon the execution of the algorithm is an important objective of divide for instance the mean and standard deviation values for the propagation delay of atm cells between any two given broadband-isdn bisdn nodes in a simulation of bisdn network under stochastic input stimulus as a function of time are important clues to the degree of congestion in the broadband-isdn network although the execution of the algorithm typically generates high resolution data often a coarse-level visual representation of the data may be useful in facilitating the conceptual understanding of the behavior of the algorithm divide permits a user to specify a resolution less than that of the data from the execution of the algorithm which is then utilized to coalesce the data appropriately given that this process requires significant computational power for efficiency divide distributes the overall task of visual display into a number of user specified workstations that are configured as a loosely-coupled parallel processor divide has been implemented on a heterogeneous network of sun sparc 1 +  sparc 2 and 3/60 workstations and performance measurements indicate significant improvement over that of a uniprocessor-based visual display
an algorithm for rapid computation of richards's smooth molecular surface is described the entire surface is computed analytically triangulated and displayed at interactive rates the faster speeds for our program have been achieved by algorithmic improvements paralleling the computations and by taking advantage of the special geometrical properties of such surfaces our algorithm is easily parallelable and it has a time complexity of o k log k over n processors where n is the number of atoms of the molecule and k is the average number of neighbors per atom
the device unified interface is a generalized and easily expandable protocol for thecommunication between applications and input devices the key idea is to unify various device data into the parameters of a so-called "virtual input device" the device information-base which includes device dependent information is also incorporated into the virtual input device using the device unified interface system builders are able to design their applications independent of the input devices as well as utilize the capabilities of several devices in the same application
designers implementers and marketers of data_analysis tools typically have different perspectives than end users consequently data analysts often find themselves using tools focused on graphics and programming concepts rather than concepts which reflect their own domain and the context of their work some user studies focus on usability tests late in development others observe work activity but fail to show how to apply that knowledge in design this paper describes a methodology for applying observations of data_analysis work activity in prototype tool design the approach can be used both in designing improved data_analysis tools and customizing visualization_environments to specific applications we present an example of user-centered_design for a prototype tool to cull large_data sets we revisit the typical graphical approach of animating a large_data set from the point of view of an analyst who is culling data field evaluations using the prototype tool not only revealed valuable usability information but initiated in-depth discussions about user's work tools technology and requirements
in this work a new method for visualization of three-dimensional turbulent flow using particle_motion animation is presented the method is based on reynolds decomposition of a turbulent flow_field into a convective and a turbulent motion at each step of particle path generation a stochastic perturbation is added resulting in random-walk motions of particles a physical relation is established between the perturbations and the eddy-diffusivity which is calculated in a turbulent flow simulation the flow data used is a mean velocity field and an eddy-diffusivity field the erratic particle_motions are more than just a visual effect but represent a real physical phenomenon animplementation of the method is described and an example of a turbulent channel flow is given which clearly shows the random particle_motions in their context of general fluid motion patterns
volume_rendering has been proposed as a useful tool for extracting information from large_datasets where non-visual_analysis alone may not be feasible the scale of these applications implies that data_management is an important issue that needs to be addressed most volume_rendering algorithms however process data in raw uncompressed form in previous work we introduced a compressed volume format that may be volume rendered directly with minimal impact on rendering time in this paper we extend these ideas to a new volume format that not only reduces storage space and transmission time but is designed for fast volume_rendering as well the volume dataset is represented as indices into a small codebook of representative blocks with the data structure volume_shading calculations need only be performed on the codebook and image generation is accelerated by reusing precomputed block projections
we present a 3-d antialiasing algorithm for voxel-based geometric models the technique band-limits the continuous object before sampling it at the desired 3-d raster resolution by precomputing tables of filter values for different types and sizes of geometric objects the algorithm is very efficient and has a complexity that is linear with the number of voxels generated the algorithm not only creates voxel models which are free from object space aliasing but it also incorporates the image space antialiasing information as part of the view independent voxel model the resulting alias-free voxel models have been used to model synthetic scenes for discrete ray tracing applications the discrete ray-traced image is superior in quality to the image generated with a conventional surface-based ray tracer since silhouettes of objects shadows and reflections appear smooth jaggy-less in addition the alias-free models are also suitable for intermixing with sampled datasets since they can be treated uniformly as one common data representation
flow volumes are the volumetric equivalent of stream lines they provide more information about the vector field being visualized than do stream lines or ribbons presented is an efficient method for producing flow volumes composed of transparently rendered tetrahedra for use in an interactive_system the problems of rendering subdivision sorting composing artifacts and user_interaction are dealt with efficiency comes from rendering only the volume of the smoke and using hardware texturing and compositing
we discuss a system which provides a single unified model of oil and gas reservoirs that is used across a range of disciplines from geologists to reservoir engineers it has to store manipulate and display reservoir phenomena which are observed over several orders of magnitude from 1 mm to 10 km we propose that the current capabilities of visualization over this range of scales can remove perception barriers that have existed between disciplines and provide clear insights into the problems of modeling reservoirs from geological and engineering perspectives
visualization has proved an efficient tool in the understanding of large_data sets in computational science and engineering there is growing interest today in the development of problem_solving_environments which integrate both visualization and the computational process which generates the data the grasparc project has looked at some of the issues involved in creating such an environment an architecture is proposed in which tools for computation and visualization can be embedded in a framework which assists in the management of the problem_solving process this framework has an integral data_management facility which allows an audit trail of the experiments to be recorded this design therefore allows not only steering but also backtracking and more complicated problem_solving strategies a number of demonstrator case studies have been implemented
the paper describes a highly interactive method for computer visualization of simultaneous three-dimensional vector and scalar flow_fields in convection-diffusion systems this method allows a computational fluid dynamics user to visualize the basic physical process of dispersion and mixing rather than just the vector and scalar values computed by the simulation it is based on transforming the vector field from a traditionally eulerian reference frame into a lagrangian reference frame fluid elements are traced through the vector field for the mean path as well as the statistical dispersion of the fluid elements about the mean position by using added scalar information about the root mean square value of the vector field and its lagrangian time scale in this way clouds of fluid elements are traced not just mean paths we have used this method to visualize the simulation of an industrial incinerator to help identify mechanisms for poor mixing
hyperslice is a new method for the visualization of scalar functions of many variables with this method the multi-dimensional function is presented in a simple and easy to understand way in which all dimensions are treated identically the central concept is the representation of a multi-dimensional function as a matrix oforthogonal two-dimensional slices these two-dimensional slices lend themselves very well to interaction via direct_manipulation due to a one to one relation between screen space and variable space several interaction techniques for navigation the location of maxima and the use of user-defined paths are presented
in this work we focus on one of the key problems of scientific_visualization the object recognition dilemma the necessity to pre-interpret application data in order to classify object surface voxels prior to rendering has prevented many visualization methods from becoming practical we propose the concept of vision by visualization which integrates computer vision methods into the visualization process based on this we present the vision camera a new tool allowing for interactive object recognition during volume data walkthroughs this camera_model is characterized by a flexible front-plane which under the control of user-specified parameters and image features elastically matches to object surfaces while shifted through a data volume thus objects are interactively carved out and can be visualized by standard volume_visualization methodsimplementation and application of the model are described our results suggest that by the integration of human and machine vision new perspectives for data_exploration are opened up
the set of possible orientations of a rigid three-dimensional object is a topological space with three degrees of freedom this paper investigates the suitability of various techniques of visualizing this space with a good technique the natural distance between orientations will be represented fairly accurately and distortion to the "shape" of a collection of orientations induced by the change of reference orientation will be minor the traditional euler-angle parameterization fails on both counts less well-known techniques exploit the fact that there is a rotation that takes the reference orientation to a given one the given orientation is represented as a point along the axis of this rotation the distance of this point from the origin is determined by some scaling function of the magnitude of that rotation free natural scaling functions are studied none is perfect but several are satisfactory
direct analysis of spacecraft observations of stratospheric ozone yields information about the morphology of annual austral depletion visual correlation of ozone with other atmospheric data illustrates the diurnal dynamics of the polar vortex and contributions from the upper troposphere including the formation and breakup of the depletion region each spring these data require care in their presentation to minimize the introduction of visualization artifacts that are erroneously interpreted as data features non-geographically registered data of differing mesh structures can be visually correlated via cartographic warping of underlying geometries without interpolation since this approach is independent of realization technique it provides a framework for experimenting with different visualization strategies this methodology preserves the fidelity of the original data sets in a coordinate system suitable for three-dimensional dynamic examination of upper atmospheric phenomena
recently researchers have started using texture for data visualization the rationale behind this is to exploit the sensitivity of the human_visual_system to texture in order to overcome the limitations inherent in the display of multidimensional data a fundamental issue that must be addressed is what textural features are important in texture perception and how they are used we designed an experiment to help identify the relevant higher order features of texture perceived by humans we used twenty subjects who were asked to rate 56 pictures from brodatz's album on 12 nine-point likert scales we applied the techniques of hierarchical cluster analysis non-parametric multidimensional scaling mds classification and regression tree analysis cart discriminant analysis and principal_component_analysis to data gathered from the subjects based on these techniques we identified threeorthogonal dimensions for texture to be repetitive vs non-repetitive high-contrast and non-directional vs low-contrast and directional granular coarse and low-complexity vs non-granular fine and high-complexity
we propose a new framework for doing scientific_visualization the basis for this framework is a combination of particle_systems and behavioral animation here particles are not only affected by the field that they are in but can also exhibit different programmed behaviors an intuitive delivery system based on virtual cans of spray paint is also described to introduce the smart particles into the data set hence the name spray rendering using this metaphor different types of spray paint are used to highlight different features in the data set spray rendering offers several advantages over existing methods 1 it generalizes the current techniques of surface volume and flow_visualization under one coherent framework 2 it works with regular and irregular grids as well as sparse and dense data sets 3 it allows selective progressive refinement 4 it is modular extensible and provides scientists with the flexibility for exploring relationships in their data sets in natural and artistic ways
we show how to create 3d models of maternal pelvis and fetal head from magnetic resonance images mri the models are used to simulate the progress of delivery in order to give a prognosis of successful labor
mriview is a software system that uses image_processing and visualization to provide neuroscience researchers with an integrated environment for combining functional and anatomical information key features of the software include semi-automated segmentation of volumetric head data and an interactive coordinate reconciliation method which utilizes surface visualization the current system is a precursor to a computational brain atlas we describe features this atlas will incorporate including methods under development for visualizing brain functional data obtained from several different research modalities
this paper is aimed at the exploratory_visualization of networks where there is a strength or weight associated with each link and makes use of any hierarchy present on the nodes to aid the investigation of large networks it describes a method of placing nodes on the plane that gives meaning to their relative positions the paper discusses how linking and interaction principles aid the user in the exploration two examples are given one of electronic mailcommunication over eight months within a department another concerned with changes to a large section of a computer program
the user of a parallel computer system would like to know the performance of a program in terms of how optimally it uses the system resources this task is increasingly performed by program performance visualization the limitations of conventional performance data_analysis techniques necessitate better visual_analysis methods that are scalable with the problem and system sizes and extensible they should represent some physical and logical structure of the parallel system and program the analysis techniques presented here have been motivated by the use of signal and two- and three-dimensional image_processing techniques being applied in some areas of scientific_visualization results of applying selected techniques are shown these techniques and tools have advantages and disadvantages when applied in this area
modular application builders mabs such as avs and iris explorer are increasingly being used in the visualization community such systems can already place compute intensive modules on supercomputers in order to utilize their power this paper details two major projects at epcc which attempted to fully integrate the mab concept with a distributed memory mimd dm-mimd environment the work presented was driven by two goals efficient use of the resource and case of use by programmer and end user we present a model of mabs and describe the major problems faced giving solutions to them through two case studies
human beings find it difficult to analyze local and global oligonucleotide patterns in the linear primary sequences of a genome in this paper we present a family of iterated function systems ifs that can be used to generate a set of visual models of a dna_sequence a new visualization function the w-curve that is derived from this ifs family is introduced using w-curves a user can readily compare subsequences within a long genomic sequence - or between genomic sequences - and can visually evaluate the effect of local variations mutations upon the global genomic information content
this paper introduces a novel representation called the infocrystal that can be used as a visualization tool as well as a visual_query_language to help users search for information the infocrystal visualizes all the possible relationships among n concepts users can assign relevance weights to the concepts and use thresholding to select relationships of interest the infocrystal allows users to specify boolean as well as vector-space queries graphically arbitrarily complex queries can be created by using the infocrystals as building blocks and organizing them in a hierarchical structure the infocrystal enables users to explore and filter information in a flexible dynamic and interactive way
making accurate computer_graphics representations of surfaces and volumes 2-manifolds and 3-manifolds embedded in four-dimensional space typically involves complex and time-consuming computations in order to make simulated worlds that help develop human intuition about the fourth dimensions we need techniques that permit real-time interactive_manipulation of the most sophisticated depictions available we propose the following new methods that bring us significantly closer to this goal an approach to high-speed 4d illuminated surface rendering incorporating 4d shading and occlusion coding a procedure for rapidly generating 2d screen images of tessellated 3-manifolds illuminated by 4d light these methods are orders of magnitude faster than previous approaches enabling the real-time manipulation of high-resolution 4d images on commercial graphics_hardware
we present the visualization and modeling techniques used in a case_study to build feature-based computational models from geophysical data visualization was used to inspect the quality of the interpretation of the geophysical data we describe the geophysical data graphical representation used to support rapid rendering and to enhance the perception differences between the interpretation of the data and the data itself in addition we present the modeling techniques used to convert the geophysical data into a feature-based computational model suitable for use by a numerical simulation package
this paper presents an environment for telecollaborative data_exploration it provides the following capabilities essential to data_exploration 1 users can probe the data defining regions of interest with arbitrary shapes 2 the selected data can be transformed and displayed in many different ways 3 linked cursors can be established between several windows showing data sets with arbitrary relationships 4 data can be displayed on any screen across a computer network allowing for telecollaboration arrangements with linked cursors around the world 5 our system is user-extensible allowing programmers to change any component of it while keeping the remaining functionality we demonstrate how the system can be used in several applications such as biomedical imaging robotics and wood classification
digital filtering is a crucial operation in volume_reconstruction and visualization lowpass filters are needed for subsampling and minification interpolation filters are needed for registration and magnification and to compensate for geometric distortions introduced by scanners interpolation filters are also needed in volume_rendering for ray-casting and slicing in this paper we describe a method for digital filter design of interpolation filters based on weighted chebyshev minimization theaccuracy of the resulting filters are compared with some commonly used filters defined by piecewise cubic polynomials a significant finding of this paper is that although piecewise cubic interpolation has some computational advantages and may yield visually satisfactory results for some data other data result in artifacts such as blurring furthermore piecewise cubic filters are inferior for operations such as registration better results are obtained by the filters derived in this papers at only small increases in computation
in this work we present a method for speeding the process of volume animation it exploits coherency between consecutive images to shorten the path rays take through the volume rays are provided with the information needed to leap over the empty space and commence volume traversal at the vicinity of meaningful data the algorithm starts by projecting the volume onto a c-buffer coordinates-buffer which stores the object-space coordinates of the first non-empty voxel visible from a pixel following a change in the viewing parameters the c-buffer is transformed accordingly next coordinates that possibly became hidden are discarded the remaining values serve as an estimate of the point where the new rays should start their volume traversal this method does not require 3-d preprocessing and does not suffer from any image degradation it can be combined with existing acceleration techniques and can support any ray traversal algorithm and material modeling scheme
3-dimensional data visualization from any input source involves the study and understanding of several steps these steps include data acquisition signal_processing image_processing and image generation using a forward-looking high frequency sonar system which focuses sound much like the eye focuses light standard and non-standard data processing algorithms and industry "standard"visualization algorithms this project produced accurate 3-dimensional representations of several underwater objects
in the field of computer applications to archaeology data visualization is one of the most recent and promising activity the visual reconstruction obtained from partially or totally ruined data is a problem that archaeologists often face with during their work the case we present here is the simulated reconstruction of a great egyptian tomb of the vii century bc excavated in the rocky cliff of the desert the visualization method is fundamental for testing the hypotheses made and as a strategic solution in the concrete reconstruction the hundreds of magnificent decorated blocks saved by museums will never be positioned again on its walls moreover in front of the stress and pollution caused to ancient monuments by a massive tourism the ever-growing improving of visualization and animation techniques like the ones presented in this paper makes of considerable interest the modeling and the exploration inside the virtual monuments through realistic tours
in the work we present a new architecture for visualization_systems that is based on data base management system dbms technology by building on the mechanisms present in a next-generation dbms rather than merely on the capabilities of a standard file manager we show that a simpler and more powerful visualization system can be constructed we retain the popular "boxes and arrows" programming notation for constructing visualization programs but add a "flight simulator" model of movement to navigate the output of such programs in addition we provide a means to specify a hierarchy of abstracts of data of different types and resolutions so that a "zoom" capability can be supported the underlying dbms support for this system tioga is briefly described as well as the current state of theimplementation
most of the current dataflow_visualization_systems are based on coarse-grain dataflow computing models in this paper we propose a fine-grain dataflow model that takes advantage of data locality properties of many visualization algorithms a fine-grain module works on small chunks of data one at a time by keeping a dynamically adjusted moving window on the input data_stream it is more memory efficient and has the potential of handling very large_data sets without taking up all the memory resources two popular visualization algorithms an iso-surface_extraction algorithm and a volume_rendering algorithm are implemented using the fine-grain model the performance measurements showed faster speed reduced memory usage and improved cpu utilization over a typical coarse-grain system
some techniques developed recently at dlr's institute of theoretical fluid mechanics in order to cope with the demands arising from today's work in aerodynamics are illustrated such new demands arise from new aerodynamical problems like the hypersonic flow_field around re-entry vehicles the study of unsteady phenomena which comes more and more within reach due to the increased availability of computing power and the tendency towards enhanced international cooperation especially within europe which calls for the use of co-operative systems on wide area networks
presently there are very few visualization_systems available for time-dependent flow_fields although existing visualization_systems for instantaneous flow_fields may be used to view time-dependent flow_fields at discrete points in time the time variable is usually not considered in the visualization_technique we present a simple and effective approach for visualizing time-dependent flow_fields using streaklines a system was developed to demonstrate this approach the system can process many time frames of flow_fields without requiring that all the data be in memory simultaneously and it also handles flow_fields with moving grids we have used the system to visualize streaklines from several large 3-d time-dependent flow_fields with moving grids the system was able to provide useful insights to the physical phenomena in the flow_fields
texture_mapping is normally used to convey geometric detail without adding geometric complexity this paper introduces boolean textures a texture_mapping technique that uses implicit functions to generate texture maps and texture coordinates these boolean textures perform clipping during a renderer's scan conversion step any implicit function is a candidate boolean texture clipper the paper describes how to use quadrics as clippers applications from engineering and medicine illustrate the effectiveness of texture as a clipping tool
volume_visualization is becoming an important tool for understanding large 3d data sets a popular technique for volume_rendering is known as splatting with new hardware architectures offering substantial improvements in the performance of rendering texture mapped objects we present textured splats an ideal reconstruction function for 3d signals is developed which can be used as a texture map for a splat extensions to the basic splatting technique are then developed to additionally represent vector_fields
some years ago it was established that the muon catalyzed fusion phenomenon could be used for the production of energy this fact has been causing a rebirth of interest in the universal methods of solving the quantum coulomb three-body problem the adiabatic hyperspherical ahs approach considered in this joint project has definite advantages in comparison with other methods the case_study proposed focuses on the study of the structure and behavior of the wave function of bound states of a quantum three-body system as well as of the basis_functions of the ahs approach adapted scientific_visualization tools such as surface rendering volume ray tracing and texturing will be used visualization allows to discover interesting features in the behavior of the basis_functions and to analyze the convergence of the ahs-expansion for the wave functions
progress towards interactive steering of the time-accurate unsteady finite-element simulation program dyna3d is reported rudimentary steering has been demonstrated in a distributed computational environment encompassing a supercomputer multiple graphics workstations and a single frame animation recorder the coroutine facility of avs application visualization system from avs inc and software produced in-house has been coordinated to prove the concept this work also applies to other large batch-oriented fortran simulations "dusty decks" presently in production use
we present two novel visualisation tools the influence explorer and the prosection matrix these were specifically created to support engineering artifact design and similar tasks in which a set of parameter values must be chosen to lead to acceptable artifact performance these tools combine two concepts one is the interactive and virtually immediate responsive display of data in a manner conducive to the acquisition of insight the other involving the precalculation of samples of artifact performance facilitates smooth exploration and optimisation leading to a design decision the anticipated benefits of these visualisation tools are illustrated by an example taken from electronic circuit design in which full account must be taken of the uncertainties in parameter values arising from inevitable variations in the manufacturing process
we describe the theoretical background for ave an automatic visualization engine for semantic_networks we have a functional notion of aesthetics and therefore understand meaningfulness as a central issue for information_visualization this implies that the diagrams should communicate the characteristics of the data as effectively as possible in this generative theory of diagram design we include data characterization systematic use of graphical means of expression and the combination of graphical means of expression after giving a brief introduction and an application scenario we discuss these aspects in detail finally a process model of an automatic visualization process is sketched and directions for further research are outlined
3d computer_graphics can be extremely expressive it is possible to display an entire securities market like the s&p 500 on a single screen with the correct approach to the visual_design of the layout these massive amounts of information can be quickly and easily comprehended by a human observer by using motion and animated interaction it is possible to use 3d as a reliable accurate and precise decision-support tool information animation applications are particularly suited to the securities industry because that is where we find huge amounts of data the value of which declines rapidly with time and where critical decisions are being made on this data in very short periods of time information animation technology is an important new tool for the securities industry where people need to be in the decision-making loop without suffering from information overload several examples are discussed including equity trading analytics fixed income trading analytics and fixed-income risk viewing
it is well known that graphical representations could be very helpful to browse in graph structured information but this promising approach requires the capability of an automatic layout system because the tedious and time consuming task of a manual layout leads to a rejection of this approach by the user in our approach we split the task of retrieving information into two phases that are getting the orientation within the network and reading currently visited information we present layout_algorithms for both phases which have the benefit of being flexible and adaptable to individual user requests and ensure the topological consistency ie the stability of the topology of the information layout during a sequence of display layouts the results show that especially the possibility of an animation of the layout process can assist the user essentially in maintaining the orientation in the information network
selective dynamic manipulation sdm is a paradigm for interacting with objects in visualizations its methods offer a high degree of selectivity in choosing object sets in the selection of interactive techniques and the properties they affect and in the degree to which a user action affects the visualization our goal is to provide a flexible set of techniques and feedback mechanisms that enable users to move objects and transform their appearance to perform a variety of information analysis tasks
visualizations which depict entireinformation_spaces provide context for navigation and browsing tasks however the limited size of the display screen makes creating effective global views difficult we have developed a technique for displaying and navigating largeinformation_spaces the key concept is the use of an information mural a two-dimensional reduced representation of an entireinformation_space that fits completely within a display window or screen information murals use grayscale shading and color along with anti-aliasing techniques to create a miniature version of the entire data set by incorporating navigational capabilities information murals become a tool that can be used as a global view along with more detailed informational displays information murals are utilized in our software_visualization research to help depict the execution of object-oriented programs and can also be used in more general information_visualization_applications
the paper describes an approach to iv that involves spatializing text content for enhanced visual browsing and analysis the application arena is large text document corpora such as digital libraries regulations and procedures archived reports etc the basic idea is that text content from these sources may be transformed to a spatial representation that preserves informational characteristics from the documents the spatial representation may then be visually browsed and analyzed in ways that avoid language processing and that reduce the analysts mental workload the result is an interaction with text that more nearly resembles perception and action with the natural world than with the abstractions of written language
virtual_reality can aid in designing large and complex structures such as ships skyscrapers factories and aircraft but before vr can realize this potential we need to solve a number of problems one of these problems the user's need to see and interact with non-geometric information is examined our vr environment realeyes can display large-scale and detailed geometry at reasonable frame rates >20 hz allowing a user to see and navigate within a design from a first person perspective however much if not most of the information associated with a particular design has no geometric representation this includes information such as schematics of electrical hydraulic and plumbing systems information describing materials or processes and descriptive textual information of other types many researchers have developed a wealth of techniques for presenting such data on flat-screen displays but until recently we have not had a means for naturally displaying such information within a vr environment to make non-geometric data more available we have implemented a version of mosaic that functions within a fully immersive vr system our system vrmosaic allows a user of vr to access and display most of the data available using flat screen mosaic moreover we have made it extensible to allow for the seamless integration of specialized forms of data and interaction this paper describes how we implemented vrmosaic using a vr-capable version of interviews it also describes some mosaic-like uses of that system and some "non-mosaic-like" extensions
the information_visualization and exploration environment nee is a system for automatic creation of dynamic queries applications ivee imports database relations and automatically creates environments holding visualizations and query devices ivee offers multiple visualizations such as maps and starfields and multiple query devices such as sliders alphasliders and toggles arbitrary graphical objects can be attached to database objects in visualizations multiple visualizations may be active simultaneously users can interactively lay out and change between types of query devices users may retrieve details-on-demand by clicking on visualization objects an html file may be provided along with the database specifying how details-on-demand information should be presented allowing for presentation of multimedia information in database objects finally multiple ivee clients running on separate workstations on a network can communicate by letting one user's actions affect the visualization in an another ivee client
the paper describes animplementation of a tool for visualizing and interacting with huge information hierarchies and some preliminary empirical_evaluation of the tool's efficacy existing systems for visualizing huge hierarchies using cone trees "break down" once the hierarchy to be displayed exceeds roughly 1000 nodes due to increasing visual_clutter the paper describes a system called fsviz which visualizes arbitrarily large hierarchies while retaining user control this is accomplished by augmenting cone trees with several graphical and interaction techniques usage-based filtering animated zooming hand-coupled rotation fish-eye zooming coalescing of distant nodes texturing effective use of colour for depth cueing and the applications of dynamic queries the fsviz system also improves upon earlier cone tree_visualization_systems through a more elaborate node layout_algorithm this algorithm enhances the usefulness of cone tree_visualization for large hierarchies by all but eliminating clutter
dataspace is a system for interactive 3-d visualization and analysis of large_databases dataspace utilizes the display space by placing panels of information possibly generated by different visualization_applications in a 3-d graph_layout and providing continuous navigation facilities selective rearrangements and transparency can be used to reduce occlusion or to compare or merge a set of images eg line graphs or scatter plots that are aligned and stacked in depth a prototype system supporting the basic 3-d graphic operations layout zoom rotation translation transparency has been implemented we provide several illustrative examples of dataspace displays taken from the current system we present the 3-d display paradigm describe the query layout and rendering steps required to create a display and discuss some performance issues
it is becoming increasingly important that support is provided for users who are dealing with complexinformation_spaces the need is driven by the growing number of domains where there is a requirement for users to understand navigate and manipulate large sets of computer based data by the increasing size and complexity of this information and by the pressures to use this information efficiently the paradigmatic example is the world_wide_web but other domains include software systems information systems and concurrent engineering one approach to providing this support is to provide sophisticated visualisation tools which lead the users to form an intuitive understanding of the structure and behaviour of their domain and which provide mechanisms which allow them to manipulate objects within their system the paper describes such a tool and a number of visualisation techniques that it implements
the explosive growth of information systems on the internet has clearly demonstrated the need to organise filter and present information in ways which allow users to cope with the sheer quantities of information available the scope for visualisation of gopher and www spaces is restricted by the limitations of their respective data models the far richer data model supported by the hyper-g internet information system is exploited by its harmony client to provide a number of tightly-coupled two- and three-dimensional visualisation and navigational facilities which help provide location feedback and alleviate user disorientation
as the internet continues to grow the amount of accessible information becomes increasingly vast search tools exist that allow users to find relevant information however a search can often produce such a large amount of data that it becomes hard to ferret out the most appropriate and highest quality information in addition some search tools lose valuable information when displaying the results to the user the paper describes a search visualization tool called fish for viewing hierarchically structured information and managing information overload fish forager for the information super highway allows users to visualize the results of search requests across large document spaces in a way that preserves the structure of theinformation_space fish displays the returned documents as rectangles using a combination of order indentation size and color to denote document hierarchy the score of the documents with respect to the search and other data attributes in addition the user can navigate through the document space for in-depth probing and refinement
the paper examines how to provide scientific_visualization capabilities to environmental scientists policy analysts and decision makers with personal computers pcs on their desktops an approach for using the world_wide_web www for disseminating knowledge on scientific_visualization and for intelligent access to visualization capabilities on high performance unix workstations is outlined
the use of thumbnails ie miniatures in the user-interface of image databases allows searching and selection of images without the need for naming policies treating parent images prior to reduction with edge-detecting smoothing lossy image compression or static codebook compression resulted in thumbnails where the distortion caused by reduction was lessened an experiment assessing these techniques found resulting thumbnails could be recognised more quickly and accurately than thumbnails of the same parent images that had been reduced without treatment this pretreatment in thumbnail creation is offered as an improvement
the goal is to improve the ability of people from all walks of life and interests to access search and use the information distributed in internet resources the process of interacting with information resources starts with browsing continues with digesting and assimilating pieces of information terminates with generation of new information and begins anew with analysis of pre-existing and new information our approach is user-centric-taking users needs into account by allowing them to interact with the information contained in large arrays of documents the visualization process is an integral part of the overall process we have covered three related categories in this methodology the first one is browsing through the world-wide web www hyperspace without becoming lost based on a visual representation of the hyperspace hierarchical structure hyperspace view the second category is overcoming the rigidity of the www by allowing the user to construct interactively and visually a personal hyperspace of information linking the documents according to the application or problem domain or to the user's own perception experience culture or way of thinking the third category includes discovery and analysis of new information and relationships in retrieved documents by aggregating relevant information and representing it visually
the explosive growth in world-widecommunications especially the internet has highlighted the need for techniques to visualize network traffic the traditional node and link network displays work well for small datasets but become visually cluttered and uninterpretable for large_datasets a natural 3d metaphor for displaying world-wide network data is to position the nodes on a globe and draw arcs between them coding the traffic this technique has several advantages of over the traditional 2d displays it naturally reduces line crossing clutter provides an intuitive model for navigation and indication of time and retains the geographic context coupling these strengths with some novel interaction techniques involving the globe surface translucency and arc heights illustrates the usefulness for this class of displays
presents an index of the authors whose papers are published in the conference
a recent study of a flow detail of an engine intake of future ground to orbit transport systems provided extremely complex data from numerical flow simulation and experimental flow_visualization the data posed a challenging problem to flow_visualization computational flow imaging cfi and the comparison of experimental imaging techniques versus computational imaging techniques some new visualization_techniques have been implemented to provide compact representations of the complex features in the data it turned out to be most useful to combine various specialized techniques for an icon-like representation of phenomena in a single image in order to study interaction of flow features some lessons were learned by simulating experimental visualization_techniques on the numerical data
as part of an inter-disciplinary effort we are visually exploring a current problem in philosophical logic related to information processing given a set of inconsistent sentences or inputs a processor cannot unambiguously infer any specific consequence traces represent subsets of possible consequences which can be inferred classically from partitions of the set of inputs we are interested in the relationship between a given set of boolean inputs and its respective traces we have developed a visualization paradigm which allows us to view and explore this relationship effectively
navigation in computer generatedinformation_spaces may be difficult resulting in users getting “lost in hyperspace” this work aims to build on research from the area of city planning to try to solve this problem we introduce the concepts of legibility and cognitive maps and the five features of urban landscape with which they are associated following this will be descriptions of techniques and algorithms which we have developed to allow these features to be introduced to three dimensional spaces for information visualisation next we describe a specific application of these techniques in the visualisation of the world_wide_web and conclude with a look at future development of the system
a new method of tracking free ranging marine mammals has been developed which employs a global positioning system gps receiver to accurately fix an animal's position when it surfaces and a tri axial magnetometer and velocity time depth recorder to track the animals underwater movements between surfacings in 3 dimensions concurrent with the development of the electronics of this movement and position tracking map tracking system has been the development of ways to analyze data from the map system spray rendering has been used to visualize the data and to combine it with environmental data allowing biologists view the animals activity in an environmental context considerable effort has been has been made to incorporate estimations of uncertainty and ways of minimizing it into our visualizations of the data
because of the nature of the die casting process the part geometry severely restricts the die geometry and hence affects the quality of the part however as is often the case in other manufacturing processes diecastings are currently designed purely based on their function the manufacturability of the diecastings is not considered until the design has been nearly completed and detailed this is due to the design support limitations of current cae tools we present a new volume-based approach to support diecastability evaluation especially in preliminary design our approach can be applied to arbitrarily shaped parts without pre-defined feature libraries the focus is on the identification of geometric characteristics eg heavy mass regions that could be responsible for thermal-related part defects a distance transform with city-block metric is used to extract this geometric property volume_visualization_techniques are also adopted to allow users to visualize the results in a clear and precise way
presents a method for constructing tensor product bezier surfaces from contour cross-section data minimal area triangulations are used to guide the surface construction and the final surface reflects the optimality of the triangulation the resulting surface differs from the initial triangulation in two important ways it is smooth as opposed to the piecewise planar triangulation and it is in tensor product form as opposed to the irregular triangular_mesh the surface_reconstruction is efficient because we do not require an exact minimal surface the triangulations are used as strong hints but no more than that the method requires the computation of both open and closed isoparametric curves of the surface using triangulations as a guide these isoparametric curves form a tensor product bezier surface we show how to control sampling density by filling and pruning isoparametric curves foraccuracy and economy a rectangular grid of points is produced that is compatible with the expected format for a tensor product surface interpolation so that a host of well-supported methods are available to generate and manipulate the surface
biological sequence similarity_analysis presents visualization challenges primarily because of the massive amounts of discrete multi dimensional data genomic data generated by molecular biologists is analyzed by algorithms that search for similarity to known sequences in large genomic databases the output from these algorithms can be several thousand pages of text and is difficult to analyze because of its length and complexity we developed and implemented a novel graphical representation for sequence similarity_search results which visually reveals features that are difficult to find in textual reports the method opens new possibilities in the interpretation of this discrete multidimensional data by enabling interactive investigation of the graphical representation
many practical problems in open channel hydraulics that were traditionally investigated in hydraulic model experiments are nowadays being solved by using computational fluid dynamics however in order to interpret computational results there is a clear preference among scientists and engineers for visualization in analogy with experimental techniques one such technique particle_tracing enables a dynamic lagrangian interpretation of a statically eulerian computed vector field however quite often the emphasis in particle_tracing is only on the mean flow properties while effects due to dispersion and mixing are often not accounted for hence turbulent flow characteristics have to be incorporated in a visualization system for practical hydraulic engineering problems the particle_tracing technique presented in this case_study has been specifically developed to combine both mean and fluctuating velocity vectors thus simulating stochastic perturbations around mean flow conditions a number of cases are presented that demonstrate the practical applicability of advanced visualization_techniques in realistic engineering studies
presents a simple robust and practical method for object simplification for applications where gradual elimination of high-frequency details is desired this is accomplished by sampling and low-pass filtering the object into multi-resolution volume buffers and applying the marching_cubes algorithm to generate a multi-resolution triangle-mesh hierarchy our method simplifies the genus of objects and can also help existing object simplification algorithms achieve better results at each level of detail a multi-layered mesh can be used for an optional and efficient antialiased rendering
outputs from a physiologically based toxicokinetic pb-tk model for fish were visualized by mapping time_series_data for specific tissues onto a three dimensional representation of a rainbow trout the trout representation was generated in stepwise fashion cross sectional images were obtained from an anesthetized fish using a magnetic_resonance_imaging mri system images were processed to classify tissue types images were stacked and processed to create a three dimensional representation of the fish encapsulating five volumes corresponding to the liver kidney muscle gastrointestinal tract and fat kinetic data for the disposition of pentachloroethane in trout were generated using a pb-tk model model outputs were mapped onto corresponding tissue volumes representing chemical concentration as color intensity the visualization was then animated to show the accumulation of pentachloroethane in each tissue during a continuous branchial gill exposure
this paper reports on the development of a strategy to generate databases used for real-time interactive landscape_visualization the database construction from real world data is intended to be as automated as possible the primary sources of information are remote sensing imagery recorded by landsat's thematic mapper tm and digital elevation models dem additional datasets traffic networks and buildings are added to extend the database in a first step the tm images are geocoded and then segmented into areas of different land coverage during the visual simulation highly detailed photo textures are applied onto the terrain based on the classification results to increase the apparent amount of detail the data processing and integration is carried out using custom image_processing and geographic_information systems gis software finally a sample visual simulation application is implemented emphasis is put on practicalimplementation to test the feasibility of the approach as a whole
an important goal of visualization technology is to support the exploration and analysis of very large amounts of data in this paper we propose a new visualization_technique called a `recursive pattern' which has been developed for visualizing large amounts of multidimensional data the technique is based on a generic recursive scheme which generalizes a wide range of pixel-oriented arrangements for displaying large_data sets by instantiating the technique with adequate data- and application-dependent parameters the user may greatly influence the structure of the resulting visualizations since the technique uses one pixel for presenting each data value the amount of data which can be displayed is only limited by the resolution of current display technology and by the limitations of human perceptibility beside describing the basic idea of the `recursive pattern' technique we provide several examples of useful parameter settings for the various recursion levels we further show that our `recursive pattern' technique is particularly advantageous for the large class of data sets which have a natural order according to one dimension eg time_series_data we demonstrate the usefulness of our technique by using a stock market application
there are many applications that can benefit from the simultaneous display of multiple layers of data the objective in these cases is to render the layered_surfaces in a such way that the outer structures can be seen and seen through at the same time the paper focuses on the particular application of radiation therapy treatment_planning in which physicians need to understand the three dimensional distribution of radiation dose in the context of patient anatomy we describe a promising technique for communicating the shape and position of the transparent skin surface while at the same time minimally occluding underlying isointensity dose surfaces and anatomical objects adding a sparse opaque texture comprised of a small set of carefully chosen lines we explain the perceptual motivation for explicitly drawing ridge and valley curves on a transparent surface describe straightforward mathematical techniques for detecting and rendering these lines and propose a small number of reasonably effective methods for selectively emphasizing the most perceptually relevant lines in the display
as part of a large effort evaluating the effect of the exxon valdez oil spill we are using the spatial selection features of an object relational_database management system to support the visualization of the ecological data the effort called the sound ecosystem assessment project sea is collecting and analyzing oceanographic and biological_data from prince william sound in alaska to support visualization of the sea data we are building a data_management system which includes a spatial index over a bounding polygon for all of the datasets which are collected in addition to other selection criteria the prototype provides several methods for selecting data within an arbitrary region this case_study presents the requirements and theimplementation for the application prototype which combines visualization and database technology the spatial indexing features of the illustra object relational_database management system are linked with the visualization capabilities of avs to create an interactive environment for analysis of sea data
the national library of medicine is creating a digital atlas of the human body this project called the visible human has already produced computed_tomography magnetic_resonance_imaging and physical cross-sections of a human male cadaver this paper describes a methodology and results for extracting surfaces from the visible male's ct data we use surface connectivity and isosurface_extraction techniques to create polygonal models of the skin bone muscle and bowels we also report early experiments with the physical cross-sections
presents an algorithm that accelerates the extraction of iso-surfaces from unstructured_grids by avoiding the traversal of the entire set of cells in the volume the algorithm consists of a sweep algorithm and a data decomposition scheme the sweep algorithm incrementally locates intersected elements and the data decomposition scheme restricts the algorithm's worst-case performance for data sets consisting of hundreds of thousands of elements our algorithm can reduce the cell traversal time by more than 90% over the naive iso-surface_extraction algorithm thus facilitating interactive probing of scalar_fields for large-scale problems on unstructured three-dimensional grids
the investigation of mechanisms responsible for the morphogenesis of complex biological organisms is an important area in biology p patens is an especially suitable plant for this research because it is a rather simple organism facilitating its observation yet it possesses developmental phenomena analogous to those which occur in higher plants allowing the extrapolation of hypotheses to more complex organisms the visualization consists of three components biological_data collection computer-modelling using l-systems and model verification the simulated developmental process is quite realistic and provides an excellent means for verifying the underlying hypotheses of morphogenesis
an important challenge in the visualization of three-dimensional volume data is the efficient processing and rendering of time-resolved sequences only the use of compression techniques which allow the reconstruction of the original domain from the compressed one locally makes it possible to evaluate these sequences in their entirety in this paper a new approach for the extraction and visualization of so-called time features from within time-resolved volume data is presented based on the asymptotic decay of multiscale representations of spatially localized time evolutions of the data singular points can be discriminated also the corresponding lipschitz exponents which describe the signals' local regularity can be determined and can be taken as a measure of the variation in time the compression ratio and the comprehension of the underlying signal is improved if we first restore the extracted regions which contain the most important information
we present an acceleration method for volumetric ray tracing which utilizes standard graphics_hardware without compromising imageaccuracy the graphics_hardware is employed to identify those segments of each ray that could possibly contribute to the final image a volumetric ray tracing algorithm is then used to compute the final image traversing only the identified segments of the rays this technique can be used to render volumetric isosurfaces as well as translucent volumes in addition this method can accelerate the traversal of shadow rays when performing recursive ray tracing
presents a conceptual framework and a process model for feature_extraction and iconic visualization feature_extraction is viewed as a process of data abstraction which can proceed in multiple stages and corresponding data abstraction levels the features are represented by attribute sets which play a key role in the visualization process icons are symbolic parametric objects designed as visual representations of features the attributes are mapped to the parameters or degrees of freedom of an icon we describe some generic techniques to generate attribute sets such as volume integrals and medial axis transforms a simple but powerful modeling language was developed to create icons and to link the attributes to the icon parameters we present illustrative examples of iconic visualization created with the techniques described showing the effectiveness of this approach
splatting is an object space direct_volume_rendering algorithm that produces images of high_quality but is computationally expensive like many other volume_rendering algorithms the paper presents a new technique that enhances the speed of splatting without trading off image quality this new method reduces rendering time by employing a simple indexing mechanism which allows to visit and splat only the voxels of interest it is shown that this algorithm is suitable for the dynamic situation in which viewing parameters and opacity transfer_functions change interactively we report experimental results on several test data sets of useful site and complexity and discuss the cost/benefit trade off of our method
invariant tori are examples of invariant_manifolds in dynamical_systems usual tools in dynamical_systems such as analysis and numerical simulations alone are often not sufficient to understand the complicated mechanisms that cause changes in these manifolds computer-graphical visualization is a natural and powerful addition to these tools used for the qualitative study of dynamical_systems especially for the study of invariant_manifolds the dynamics of two linearly coupled oscillators is the focus of this case_study with little or no coupling between the oscillators an invariant torus is present but it breaks down for strong coupling visualization has been employed to gain a qualitative understanding of this breakdown process the visualization has allowed key features of the tori to be recognized and it has proven to be indispensable in developing and testing hypotheses about the tori
computational_steering is the ultimate goal of interactive simulation researchers change parameters of their simulation and immediately receive feedback on the effect we present a general and flexible graphics tool that is part of an environment for computational_steering developed at cwi it enables the researcher to interactively develop his own interface with the simulation this interface is constructed with 3d parametrized geometric objects the properties of the objects are parametrized to output data and input parameters of the simulation the objects visualize the output of the simulation while the researcher can steer the simulation by direct_manipulation of the objects several applications of 3d computational_steering are presented
an efficient algorithm is presented for computing particle paths streak lines and time lines in time-dependent flows with moving curvilinear grids the integration velocity interpolation and step size control are all performed in physical space which avoids the need to transform the velocity field into computational space this leads to higheraccuracy because there are no jacobian matrix approximations and expensive matrix inversions are eliminated integrationaccuracy is maintained using an adaptive step size control scheme which is regulated by the path line curvature the problem of point location and interpolation in physical space is simplified by decomposing hexahedral cells into tetrahedral cells this enables the point location to be done analytically and substantially faster than with a newton-raphson iterative method results presented show this algorithm is up to six times faster than particle tracers which operate on hexahedral cells and produces almost identical traces
a software architecture is presented to integrate a database management system with data visualization one of its primary objectives the retention of user-data interactions is detailed by storing all queries over the data along with high-level descriptions of the query results and the associated visualization the processes by which a database is explored can be analyzed this approach can lead to important contributions in the development of user_models as “data explorers” metadata models for scientific databases intelligent assistants and data_exploration services we describe the underlying elements of this approach specifically the visual database exploration model and the metadata objects that support the model
the paper presents a splatting algorithm for volume_rendering of curvilinear grids a stochastic sampling technique called poisson sphere/ellipsoid sampling is employed to adaptively resample a curvilinear grid with a set of randomly distributed points whose energy support extents are well approximated by spheres and ellipsoids filter kernels corresponding to these spheres and ellipsoids are used to generate the volume rendered image of the curvilinear grid with a conventional footprint evaluation algorithm experimental results show that our approach can be regarded as an alternative to existing fast volume_rendering techniques of curvilinear grids
a standard method for visualizing vector_fields consists of drawing many small “glyphs” to represent the field this paper extends the technique from regular to curvilinear and unstructured_grids in order to achieve a uniform density of vector glyphs on nonuniformly spaced grids the paper describes two approaches to resampling the grid data one of the methods an element-based resampling can be used to visualize vector_fields at arbitrary surfaces within three-dimensional grids
the vast quantities of data which may be produced by modern radio telescopes have outstripped conventional visualisation techniques available to astronomers while research in other areas of visualisation finds some application in astronomy problems peculiar to the field require new techniques this paper presents a brief overview of some of the problems of visualisation for astronomy and compares different shading algorithms a more comprehensive overview may be found in norris 1994 and gooch 1995
wavelet transforms include data decompositions and reconstructions this paper is concerned with the authenticity issues of the data decomposition particularly for data visualization a total of six datasets are used to clarify the approximation characteristics of compactly supportedorthogonal wavelets we present an error tracking mechanism which uses the available wavelet resources to measure the quality of the wavelet approximations
proposes a new approach to the automatic generation of triangular irregular networks tins from dense terrain models we have developed and implemented an algorithm based on the greedy principle used to compute minimum-link paths in polygons our algorithm works by taking greedy cuts “bites” out of a simple closed polygon that bounds the yet-to-be triangulated region the algorithm starts with a large polygon bounding the whole extent of the terrain to be triangulated and works its way inward performing at each step one of three basic operations ear cutting greedy biting and edge splitting we give experimental evidence that our method is competitive with current algorithms and has the potential to be faster and to generate many fewer triangles also it is able to keep the structural terrain fidelity at almost no extra cost in running time and it requires very little memory beyond that for the input height array
to visualize the volume data acquired from computation or sampling it is necessary to estimate normals at the points corresponding to object surfaces volume data does not holds the geometric information for the surface comprising points so it is necessary to calculate normals using local information at each point the existing normal_estimation methods have some problems of estimating incorrect normals at discontinuous aliased or noisy points yagel et al 1992 solved some of these problems using their context-sensitive method however this method requires too much processing time and it loses some information on detailed parts of the object surfaces this paper proposes the surface-characteristic-sensitive normal_estimation method which applies different operators according to characteristics of each surface for the normal calculation this method has the same advantages of the context-sensitive method and also some other advantages such as less processing time and the reduction of the information loss on detailed parts
proposes an interactive method for exploring topological spaces based on the natural local geometry of the space examples of spaces appropriate for this visualization approach occur in abundance in mathematical_visualization surface and volume_visualization problems and scientific applications such as general_relativity our approach is based on using a controller to choose a direction in which to “walk” a manifold along a local geodesic path the method automatically generates orientation changes that produce a maximal viewable region with each step of the walk the proposed interaction framework has many natural properties to help the user develop a useful cognitive map of a space and is well-suited to haptic interfaces that may be incorporated into desktop virtual_reality systems
modular visualization_environments utilizing a data-flow execution model have become quite popular in recent years especially those that incorporate visual programming tools however simplisticimplementations of such an execution model are quite limited when applied to problems of realistic complexity which negate the intuitive advantage of data-flow systems this situation can be resolved by extending the execution model to incorporate a more complete and efficient programming infrastructure while still preserving the virtues of pure “data-flow” this approach has been used for theimplementation of a general-purpose software package ibm visualization data explorer
the paper presents an interactive approach for guiding the user's select of colormaps in visualization pravdacolor implemented as a module in the ibm visualization data explorer provides the user a selection of appropriate colormaps given the data type and spatial frequency the user's task and properties of the human perceptual system
reconstruction is used frequently in visualization of one two and three dimensional data data uncertainty is typically ignored and a deficiency of many interpolation schemes is smoothing which may indicate features or characteristics of the data that are not there the author investigates the use of iterated function systems ifs's for interpolation he shows new derivations for fractal interpolation in two and three dimensional scalar_data and new point and polytope rendering algorithms with tremendous speed advantages over ray tracing the interpolations may be used to give an indication of the uncertainty of the data statistically represent the data at a variety of scales allow tunability from the data and may allow more accurate data_analysis
this paper describes an approach for interactive_visualization of mixed scalar and vector_fields in which vector icons are generated from pre-voxelized icon templates and volume-rendered together with the volumetric scalar_data this approach displays simultaneously the global structure of the scalar field and the detailed features of the vector field interactive_visualization is achieved with incremental image update by re-rendering only a small portion of the image wherever and whenever a change occurs this technique supports a set of interactive_visualization tools including change of vector_field_visualization parameters real-time animation of vector icons advected within the scalar field a zooming lens and a local probe
in the research described we have constructed a tightly coupled set of methods for monitoring steering and applying visual_analysis to large scale simulations the work shows how a collaborative interdisciplinary process that teams application and computer scientists can result in a powerful integrated approach the integrated design allows great flexibility in the development and use of analysis tools the work also shows that visual_analysis is a necessary component for full understanding of spatially complex time dependent atmospheric processes
three topics of aerodynamic research at dlr are chosen to illustrate the need for visualization these include aircraft configuration design variations adaptation devices and unsteady_flow simulation in the transonic the supersonic and the hypersonic speed regime call for the combined use of a geometry generator a powerful graphic system and video technology projects currently under investigation are illustrated and generic case studies are presented
a general framework for visualization of statistical properties of high-dimensional pattern samples and the related computational steps are introduced these procedures are exemplified on applications in anthropometrical research shape information in faces but can be easily generalized to various other morphometrical questions and data sets with pattern structure eg data stemming from sensor arrays presently the visualization_techniques illustrated concentrate on higher moments of first order it is suggested how moments of second order can be visualized by animations and how this approach can be used in the context of comparative_visualization
volume_rendering generates 2d images by ray tracing 3d volume data this technique imposes considerable demands on storage space as the data set grows in size in this paper we describe a method to render compressed volume data directly to reduce the memory requirements of the rendering process the volume data was compressed by a technique called the laplacian pyramid a compression ratio of 101 was achieved by uniform quantization over the laplacian pyramid the quality of the images obtained by this technique as virtually indistinguishable from that of the images generated from the uncompressed volume data a significant improvement in computational performance was achieved by using a cache algorithm to temporarily retain the reconstructed voxels to be used by the adjacent rays
proposes as a generalization of isosurfaces the `interval_volume' which is a new type of geometric model representing 3d subvolumes with field values belonging to a closed interval a dominant surface fitting algorithm called `marching_cubes' is extended to obtain a solid fitting algorithm which extracts from a given volumetric_dataset a high-resolution polyhedral solid data structure of the interval_volume_rendering methods for the interval_volume and principal related operations are also presented the effectiveness of this approach is illustrated with 4d simulated data from atomic collision research
visual realism is necessary for many virtual_reality applications in order to convince the user that the virtual environment is real the scene presented should faithfully model the expected actual environment a highly accurate fully modeled interactive environment is thus seen as “virtually real” the paper addresses the problem of interactive visual realism and discusses a possible solution a hybrid rendering paradigm that ties distributed graphics_hardware and ray tracing systems together for use in interactive high visual realism applications this new paradigm is examined in the context of a working rendering system this system is capable of producing images of higher fidelity than possible through the use of graphics_hardware alone able both to render images at speeds useful for interactive_systems and to progressively refine static high_quality snapshots
spot noise is a technique for texture_synthesis which is very useful for vector_field_visualization this paper describes improvements and extensions of the basic principle of spot noise first better visualization of highly curved vector_fields with spot noise is achieved by adapting the shape of the spots to the local velocity field second filtering of spots is proposed to eliminate undesired low frequency components from the spot noise texture third methods are described to utilize graphics_hardware to generate the texture and to produce variable viewpoint animations of spot noise on surfaces fourth the synthesis of spot noise on grids with highly irregular cell sizes is described
flow volumes are extended for use in unsteady time-dependent flows the resulting unsteady_flow volumes are the 3d analogs of streaklines there are few examples where methods other than particle_tracing have been used to visualize time-varying flows since particle paths can become convoluted in time there are additional considerations to be made when extending any visualization_technique to unsteady_flows we present some solutions to the problems which occur in subdivision rendering and system design we apply the unsteady_flow volumes to a variety of field types including moving multi-zoned curvilinear grids
presents a new method for adaptive surface meshing and triangulation which controls the local level-of-detail of the surface approximation by local spectral estimates these estimates are determined by a wavelet representation of the surface data the basic idea is to decompose the initial data set by means of anorthogonal or semi-orthogonal tensor product wavelet transform wt and to analyze the resulting coefficients in surface regions where the partial energy of the resulting coefficients is low the polygonal approximation of the surface can be performed with larger triangles without losing too much fine-grain detail however since the localization of the wt is bound by the heisenberg principle the meshing method has to be controlled by the detail signals rather than directly by the coefficients the dyadic scaling of the wt stimulated us to build a hierarchical meshing algorithm which transforms the initially regular data grid into a quadtree representation by rejection of unimportant mesh vertices the optimum triangulation of the resulting quadtree cells is carried out by selection from a look-up table the tree grows recursively as controlled by the detail signals which are computed from a modified inverse wt in order to control the local level-of-detail we introduce a new class of wavelet space filters acting as “magnifying glasses” on the data
parallel program visualization and debugging require new techniques for gathering and displaying execution trace and profile data interaction with the program during execution is also required to facilitate parallel debugging we discuss the difficulties associated with runtime user/program interaction and how the data-parallel programming paradigm facilitates much more liberal runtime interaction than typical mimd-based models we present a model for data-parallel program visualization that addresses both data collection/interaction and visualization issues we follow our model presentation with the design andimplementation of a subset of our visualization model we discuss our preliminary findings and propose future research directions
this paper presents an analysis of progress in the use of sound as a tool in support of visualisation and gives an insight into its development and future needs special emphasis is given to the use of sound in scientific and engineering applications a system developed to support surface data_presentation and interaction by using sound is presented and discussed
maximum projection is a volume_rendering technique that for each pixel finds the maximum intensity along a projector for certain important classes of data this is an approximation to summation rendering which produces superior visualizations we show how maximum projection rendering with additional depth cues can be implemented using simple affine transformations in object space this technique can be used together with 3d graphics libraries and standard graphics_hardware thus allowing interactive_manipulations of the volume data the algorithm presented allows for a wide range of tradeoffs between interactivity and image quality
diagrams are data representations that convey information predominantly through combinations of graphical elements rather than through other channels such as text or interaction we have implemented a prototype called ave automatic visualization environment that generates diagrams automatically based on a generative theory of diagram design according to this theory diagrams are constructed based on the data to be visualized rather than by selection from a predefined set of diagrams this approach can be applied to knowledge represented by semantic_networks we give a brief introduction to the underlying theory then describe theimplementation and finally discuss strategies for extending the algorithm
advances in computer_graphics_hardware and algorithms visualization and interactive techniques for analysis offer the components for a highly integrated efficient real-time 3d geographic_information system we have developed “virtual gis” a system with truly immersive capability for navigating and understanding complex and dynamic terrain-based databases the system provides the means for visualizing terrain models consisting of elevation and imagery data along with gis raster layers protruding features buildings vehicles and other objects we have implemented window-based and virtual_reality versions and in both cases provide a direct_manipulation visual interface for accessing the gis data unique terrain data structures and algorithms allow rendering of large high resolution datasets at interactive rates
visualization animation and simulation techniques are applied to the problem of rotor design for helicopters periodic unsteady experimental velocity data laser doppler velocimetry or ldv in two dimensions and velocity data derived from simulated vortex systems in three dimensions are compared using the same visual tools animations show the development of rotor wake systems and induced velocities over time modified particle trace integration schemes are used to calculate steady streamlines and unsteady particle paths for both kinds of data in an extension of this work a virtual environment ve system was used to view the wake vortex system and an interactive probe was used to explore the induced velocity field future work will enable interactive visual_debugging and simulation_steering
as the use of 3d information presentation becomes more prevalent the need for effective viewing tools grows accordingly much work has been done in developing tools for 2d spaces which allow for detail in context views we examine the extension of such 2d methods to 3d and explore the limitations encountered in accessing internal regions of the data with these methods we then describe a novel solution to this problem of internal access with the introduction of a distortion function which creates a clear line of sight to the focus revealing sections previously obscured the distortion is symmetric about the line of sight and is smoothly integrated back into the original 3d layout
we present the h3 layout technique for drawing large directed graphs as node-link_diagrams in 3d hyperbolic_space we can lay out much larger structures than can be handled using traditional techniques for drawing general graphs because we assume a hierarchical nature of the data we impose a hierarchy on the graph by using domain-specific knowledge to find an appropriate spanning tree links which are not part of the spanning tree do not influence the layout but can be selectively drawn by user request the volume of hyperbolic 3-space increases exponentially as opposed to the familiar geometric increase of euclidean 3-space we exploit this exponential amount of room by computing the layout according to the hyperbolic metric we optimize the cone tree layout_algorithm for 3d hyperbolic_space by placing children on a hemisphere around the cone mouth instead of on its perimeter hyperbolic navigation affords a focus+context view of the structure with minimal visual_clutter we have successfully laid out hierarchies of over 20000 nodes ourimplementation accommodates navigation through graphs too large to be rendered interactively by allowing the user to explicitly prune or expand subtrees
we describe a method for the visualization of information units on spherical domains which is employed in the banking industry for risk analysis stock prediction and other tasks the system is based on a quantification of the similarity of related objects that governs the parameters of a mass-spring system unlike existing approaches we initialize all information units onto the inner surface of two concentric spheres and attach them with springs to the outer sphere since the spring stiffnesses correspond to the computed similarity measures the system converges into an energy minimum which reveals multidimensional relations and adjacencies in terms of spatial neighborhoods depending on the application scenario our approach supports different topological arrangements of related objects in order to cope with large_data sets we propose a blobby clustering mechanism that enables encapsulation of similar objects by implicit shapes in addition we implemented various interaction techniques allowing semantic analysis of the underlying data sets our prototype system ivory is written in java and its versatility is illustrated by an example from financial service providers
in information_visualization as the volume and complexity of the data increases researchers require more powerful visualization tools that enable them to more effectively explore multidimensional datasets we discuss the general utility of a novel visualization spreadsheet framework just as a numerical spreadsheet enables exploration of numbers a visualization spreadsheet enables exploration of visual forms of information we show that the spreadsheet approach facilitates certain information_visualization tasks that are more difficult using other approaches unlike traditional spreadsheets which store only simple data elements and formulas in each cell a visualization spreadsheet cell can hold an entire complex data set selection criteria viewing specifications and other information needed for a full-fledged information_visualization similarly inter-cell operations are far more complex stretching beyond simple arithmetic and string operations to encompass a range of domain-specific operators we have built two prototype systems that illustrate some of these research issues the underlying approach in our work allows domain experts to define new data types and data operations and enables visualization experts to incorporate new visualizations viewing parameters and view operations
we introduce the adaptive information_visualization method for hypermedia and the www based on the user's multiple viewpoints we propose two graphical interfaces the cvi and the rf-cone the cvi is the interface for interactive viewpoint_selection we can select a viewpoint reflecting our interests by using the cvi according to the given viewpoint the rf-cone adaptively organizes the 3d representation of the hypermedia so that we can understand the semantic and structural relationship of the hypermedia and easily retrieve the information combining these methods we have developed the www visualization system which can provide highly efficient navigation
managing large projects is a very challenging task requiring the tracking and scheduling of many resources although new technologies have made it possible to automatically collect data on project resources it is very difficult to access this data because of its size and lack of structure we present three novel glyphs for simplifying this process and apply them to visualizing statistics from a multi-million line software project these glyphs address four important needs in project management viewing time dependent data managing large_data volumes dealing with diverse data types and correspondence of data to real-world concepts
this paper describes the shrimp visualization_technique for seamlessly exploring software structure and browsing source code with a focus on effectively assisting hybrid program comprehension strategies the technique integrates both pan+zoom and fisheye-view visualization approaches for exploring a nested graph view of software structure the fisheye-view approach handles multiple focal points which are necessary when examining several subsystems and their mutual interconnections source code is presented by embedding code fragments within the nodes of the nested graph finer connections among these fragments are represented by a network that is navigated using a hypertext link-following metaphor shrimp combines this hypertext metaphor with animated panning and zooming motions over the nested graph to provide continuous orientation and contextual cues for the user the shrimp tool is being evaluated in several user studies observations of users performing program understanding tasks with the tool are discussed
we describe a system that allows the user to rapidly construct program visualizations over a variety of data sources such a system is a necessary foundation for using visualization as an aid to software understanding the system supports an arbitrary set of data sources so that information from both static and dynamic analysis can be combined to offer meaningful software_visualizations it provides the user with a visual universal-relation front end that supports the definition of queries over multiple data sources without knowledge of the structure or contents of the sources it uses a flexible back end with a range of different visualizations most geared to the efficient display of large amounts of data the result is a high-quality easy-to-define program visualization that can address specific problems and hence is useful for software understanding the overall system is flexible and extensible in that both the underlying data model and the set of visualizations are defined in resource files
we introduce nonlinear magnification fields as an abstract representation of nonlinear magnification providing methods for converting transformation routines to magnification fields and vice-versa this new representation provides ease of manipulation and power of expression by removing the restrictions of explicit foci and allowing precise specification of magnification values we can achieve magnification effects which were not previously possible of particular interest are techniques we introduce for expressing complex and subtle magnification effects through magnification brushing and allowing intrinsic properties of the data being visualized to create data-driven magnifications
the table lens focus+context_visualization for large_data tables allows users to see 100 times as many data values as a spreadsheet in the same screen space in a manner that enables an extremely immediate form of exploratory_data_analysis in the original table lens design data are shown in the context area using graphical representations in a single pixel row scaling up the table lens technique beyond approximately 500 cases rows by 40 variables columns requires not showing every value individually and thus raises challenges for preserving the exploratory and navigational ease and power of the original design we describe two design enhancements for introducing regions of less than a pixel row for each data value and discuss the issues raised by each
interactive_visualization_techniques allow data_exploration to be a continuous process rather than a discrete sequence of queries and results as in traditional database systems however limitations in expressive power of current visualization_systems force users to go outside the system and form a new dataset in order to perform certain operations such as those involving the relationship among multiple objects further there is no support for integrating data from the new dataset into previous visualizations so users must recreate them visage's information centric paradigm provides an architectural hook for linking data across multiple queries removing this overhead this paper describes the addition to visage of a visual_query_language called vqe which allows users to express more complicated queries than in previous interactive_visualization_systems visualizations can be created from queries and vice versa when either is updated the other changes to maintain consistency
the bead visualization system employs a fast algorithm for laying out high-dimensional_data in a low-dimensional space and a number of features added to 3d_visualizations to improve imageability we describe recent work on both aspects of the system in particular a generalization of the data types laid out and theimplementation of imageability features in a 2d visualization tool the variety of data analyzed in a financial institution such as ubs and the ubiquity of spreadsheets as a medium for analysis led us to extend our layout tools to handle data in a generic spreadsheet format we describe the metrics of similarity used for this data type and give examples of layouts of sets of records of financial trades conservatism and scepticism with regard to 3d_visualization along with the lack of functionality of widely available 3d web browsers led to the development of a 2d visualization tool with refinements of a number of our imageability features
a dynamic_query interface dqi is a database access mechanism that provides continuous real-time feedback to the user during query formulation previous work shows that dqis are elegant and powerful interfaces to small databases unfortunately when applied to large_databases previous dqi algorithms slow to a crawl we present a new incremental approach to dqi algorithms and display updates that work well with large_databases both in theory and in practice
a method for efficiently volume_rendering dense scatterplots of relational_data is described plotting difficulties that arise from large numbers of data points categorical variables interaction with non-axis dimensions and unknown values are addressed by this method the domain of the plot is voxelized using binning and then volume_rendering since a table is used as the underlying data structure no storage is wasted on regions with no data the opacity of each voxel is a function of the number of data points in a corresponding bin a voxel's color is derived by averaging the value of one of the variables for all the data points that fall in a bin other variables in the data may be mapped to external query sliders a dragger object permits a user to select regions inside the volume
research on information_visualization has reached the point where a number of successful point designs have been proposed and a variety of techniques have been discovered it is now appropriate to describe and analyze portions of the design space so as to understand the differences among designs and to suggest new possibilities this paper proposes an organization of the information_visualization literature and illustrates it with a series of examples the result is a framework for designing new visualizations and augmenting existing designs
the display of multivariate datasets in parallel_coordinates transforms the search for relations among the variables into a 2-d pattern_recognition problem this is the basis for the application to visual_data_mining the knowledge_discovery process together with some general guidelines are illustrated on a dataset from the production of a vlsi chip the special strength of parallel_coordinates is in modeling relations as an example a simplified economic model is constructed with data from various economic sectors of a real country the visual model shows the interelationship and dependencies between the sectors circumstances where there is competition for the same resource and feasible economic policies interactively the model can be used to do trade-off analyses discover sensitivities do approximate optimization monitor as in a process and provide decision support
metrics for information_visualization will help designers create and evaluate 3d_information_visualizations based on experience from 60+ 3d_information_visualizations the metrics we propose are number of data points and data_density number of dimensions and cognitive overhead occlusion percentage and reference context and percentage of identifiable points
the task of reconstructing the derivative of a discrete function is essential for its shading and rendering as well as being widely used in image_processing and analysis we survey the possible methods for normal_estimation in volume_rendering and divide them into two classes based on the delivered numericalaccuracy the three members of the first class determine the normal in two steps by employing both interpolation and derivative filters among these is a new method which has never been realized the members of the first class are all equally accurate the second class has only one member and employs a continuous derivative filter obtained through the analytic derivation of an interpolation filter we use the new method to analytically compare theaccuracy of the first class with that of the second as a result of our analysis we show that even inexpensive schemes can in fact be more accurate than high order methods we describe the theoretical computational cost of applying the schemes in a volume_rendering application and provide guidelines for helping one choose a scheme for estimating derivatives in particular we find that the new method can be very inexpensive and can compete with the normal_estimations which pre-shade and pre-classify the volume m levoy 1988
we propose a probability model for the handling of complicated interactions between volumetric objects in our model each volume is associated with a "probability map" that assigns a "surface crossing" probability to each space point according to local volume properties the interaction between two volumes is then described by finding the intersecting regions between the volumes and calculating the "collision probabilities" at each intersecting point from the surface crossing probabilities to enable fast and efficient calculations we introduce the concept of a distance map and develop two hierarchical collision_detection algorithms taking advantage of the uniform structure of volumetric_datasets
different techniques have been proposed for rendering volumetric scalar_data sets usually these approaches are focusing onorthogonal cartesian grids but in the last years research did also concentrate on arbitrary structured or even unstructured topologies in particular direct_volume_rendering of these data types is numerically complex and mostly requires sorting the whole database we present a new approach to direct rendering of convex voluminous polyhedra on arbitrary grid topologies which efficiently use hardware assisted polygon drawing to support the sorting procedure the key idea of this technique lies in a two pass rendering approach first the volume primitives are drawn in polygon mode to obtain their cross sections in the vsbufferorthogonal to the viewing plane second this buffer is traversed in front to back order and the volume integration is performed thus the complexity of the sorting procedure is reduced furthermore any connectivity information can be completely neglected which allows for the rendering of arbitrary scattered convex polyhedra
one well known application area of volume_rendering is the reconstruction and visualization of output from medical scanners like computed_tomography ct 2d greyscale slices produced by these scanners can be reconstructed and displayed onscreen as a 3d model volume_visualization of medical images must address two important issues first it is difficult to segment medical scans into individual materials based only on intensity values second although greyscale images are the normal method for displaying medical volumes these types of images are not necessarily appropriate for highlighting regions of interest within the volume studies of the human_visual_system have shown that individual intensity values are difficult to detect in a greyscale image in these situations colour is a more effective visual feature we addressed both problems during the visualization of ct scans of abdominal aortic aneurysms we have developed a classification method that empirically segments regions of interest in each of the 2d slices we use a perceptual colour selection technique to identify each region of interest in both the 2d slices and the 3d reconstructed volumes the result is a colourized volume that the radiologists are using to rapidly and accurately identify the locations and spatial_interactions of different materials from their scans our technique is being used in an experimental post operative environment to help to evaluate the results of surgery designed to prevent the rupture of the aneurysm in the future we hope to use the technique during the planning of placement of support grafts prior to the actual operation
presents a new method for auralization of the vorticity of a streamline in a vector field this technique involves using a composite tone formed by superimposing sine waves of various amplitudes whose frequency and amplitude vary in such a way as to give the perception that the resulting sound increases or decreases endlessly in pitch without ever extending beyond the listener's range of audible frequencies continuous clockwise or counterclockwise rotations of a streamline resulting from vorticity can then be displayed aurally as an apparently continuous increase or decrease in pitch
studies the topology of 2nd-order symmetric_tensor_fields degenerate points are basic constituents of tensor_fields from the set of degenerate points an experienced researcher can reconstruct a whole tensor_field we address the conditions for the existence of degenerate points and based on these conditions we predict the distribution of degenerate points inside the field every tensor can be decomposed into a deviator and an isotropic tensor a deviator determines the properties of a tensor_field while the isotropic part provides a uniform bias deviators can be 3d or locally 2d the triple-degenerate points of a tensor_field are associated with the singular points of its deviator and the double-degenerate points of a tensor_field have singular local 2d deviators this provides insights into the similarity of topological structure between 1st-order or vectors and 2nd-order tensors control functions are in charge of the occurrences of a singularity of a deviator these singularities can further be linked to important physical properties of the underlying physical phenomena for a deformation tensor in a stationary flow the singularities of its deviator actually represent the area of the vortex core in the field for a stress_tensor the singularities represent the area with no stress for a newtonian flow compressible flow and incompressible flow as well as stress and deformation tensors share similar topological features due to the similarity of their deviators for a viscous flow removing the large isotropic pressure contribution dramatically enhances the anisotropy due to viscosity
presents an algorithm for the visualization of vector_field_topology based on clifford algebra it allows the detection of higher-order singularities this is accomplished by first analysing the possible critical_points and then choosing a suitable polynomial approximation because conventional methods based on piecewise linear or bilinear approximation do not allow higher-order critical_points and destroy the topology in such cases the algorithm is still very fast because of using linear approximation outside the areas with several critical_points
the use of stream_surfaces and streamlines is well established in vector visualization however the proper placement of starting points is critical for these constructs to clearly illustrate the flow topology in this paper we present the principal stream surface algorithm which automatically generates stream_surfaces that properly depict the topology of an irrotational flow for each velocity point in the fluid field we construct the normal to the principal stream surface through the point the set of all such normal vectors is used to construct the principal stream function which is a scalar field describing the direction of velocity in the fluid field volume_rendering can then be used to visualize the principal stream function which is directly related to the flow topology thus topology in a fluid field can be easily modeled and rendered
terrain_visualization is a difficult problem for applications requiring accurate images of large_datasets at high frame rates such as flight simulation and ground-based aircraft testing using synthetic sensor simulation on current graphics_hardware the problem is to maintain dynamic view-dependent triangle meshes and texture maps that produce good images at the required frame rate we present an algorithm for constructing triangle meshes that optimizes flexible view-dependent error metrics produces guaranteed error bounds achieves specified triangle counts directly and uses frame-to-frame coherence to operate at high frame rates for thousands of triangles per frame our method dubbed real-time optimally adapting meshes roam uses two priority queues to drive split and merge operations that maintain continuous triangulations built from pre-processed bintree triangles we introduce two additional performance optimizations incremental triangle stripping and priority-computation deferral lists roam's execution time is proportional to the number of triangle changes per frame which is typically a few percent of the output mesh size hence roam's performance is insensitive to the resolution and extent of the input terrain dynamic terrain and simple vertex morphing are supported
the paper discusses a unique way to visualize height field data-the use of solid fabricated parts with a photomapped texture to display scalar information in this process the data in a height field are turned into a 3d solid representation through solid freeform fabrication techniques in this case laminated object manufacturing next that object is used as a 3d "photographic plate" to allow a texture image representing scalar_data to be permanently mapped onto it the paper discusses this process and how it can be used in different visualization situations
the authors describe a software system supporting interactive_visualization of large terrains in a resource-limited environment ie a low-end client computer accessing a large terrain database server through a low-bandwidth network by "large" they mean that the size of the terrain database is orders of magnitude larger than the computer ram superior performance is achieved by manipulating both geometric and texture data at a continuum of resolutions and at any given moment using the best resolution dictated by the cpu and bandwidth constraints the geometry is maintained as a delaunay_triangulation of a dynamic subset of the terrain data points and the texture compressed by a progressive wavelet scheme a careful blend of algorithmic techniques enables the system to achieve superior rendering performance on a low-end computer by optimizing the number of polygons and texture pixels sent to the graphics pipeline it guarantees a frame rate depending only on the size and quality of the rendered image independent of the viewing parameters and scene database size an efficient paging scheme minimizes data i/o thus enabling the use of the system in a low-bandwidth client/server data-streaming scenario such as on the internet
the authors consider the multi-triangulation a general model for representing surfaces at variable resolution based on triangle meshes they analyse characteristics of the model that make it effective for supporting basic operations such as extraction of a surface approximation and point location an interruptible algorithm for extracting a representation at a resolution variable over the surface is presented different heuristics for building the model are considered and compared results on both the construction and the extraction algorithm are presented
the authors present an efficient visualization approach to support multivariate data_exploration through a simple but effective low dimensional data overview based on metric scaling a multivariate dataset is first transformed into a set of dissimilarities between all pairs of data records a graph configuration algorithm based on principal components is then wed to determine the display coordinates of the data records in the low dimensional data overview this overview provides a graphical summary of the multivariate data with reduced data dimensions reduced data size and additional data semantics it can be used to enhance multidimensional data brushing or to arrange the layout of other conventional multivariate visualization_techniques real life data is used to demonstrate the approach
the paper introduces a tool for visualizing a multidimensional relevance space abstractly the information to be displayed consists of a large number of objects a set of features that are likely to be of interest to the user and some function that measures the relevance level of every object to the various features the goal is to provide the user with a concise and comprehensible visualization of that information for the type of applications concentrated on the exact relevance measures of the objects are not significant this enablesaccuracy to be traded for a clearer display the idea is to "flatten" the multidimensionality of the feature space into a 2d "relevance map" capturing the inter-relations among the features without causing too many ambiguous interpretations of the results to better reflect the nature of the data and to resolve the ambiguity the authors refine the given set of features and introduce the notion of composed features the layout of the map is then obtained by grading it according to a set of rules and using a simulated annealing algorithm which optimizes the layout with respect to these rules the technique proposed has been implemented and tested in the context of visualizing the result of a web search in the rmap relevance map prototype system
the authors present a multiresolution framework called multi-tetra framework that approximates volume data with different levels-of-detail tetrahedra the framework is generated through a recursive subdivision of the volume data and is represented by binary trees instead of using a certain level of the multi-tetra framework for approximation an error-based model ebm is generated by recursively fusing a sequence of tetrahedra from different levels of the multi-tetra framework the ebm significantly reduces the number of voxels required to model an object while preserving the original topology the approach provides continuous distribution of rendered intensity or generated isosurfaces along boundaries of different levels-of-detail thus solving the crack problem the model supports typical rendering approaches such as marching_cubes direct volume projection and splatting experimental results demonstrate the strengths of the approach
some new piecewise constant wavelets defined over nested triangulated domains are presented and applied to the problem of multiresolution analysis of flow over a spherical domain these new nearlyorthogonal wavelets have advantages over the existing weaker biorthogonal wavelets in the planar case of uniform areas the wavelets converge to one of two fullyorthogonal haar wavelets these new fullyorthogonal wavelets are proven to be the only possible wavelets of this type
the paper addresses multiresolutional representation of datasets arising from a computational field simulation the approach determines the regions of interest breaks the volume into variable size blocks to localize the information and then codes each block using a wavelet transform the blocks are then ranked by visual information content so that the most informative wavelet coefficients can be embedded in a bit stream for progressive transmission or access the technique is demonstrated on a widely-used computational field simulation dataset
color is widely and reliably used to display the value of a single scalar variable it is more rarely and far less reliably used to display multivariate data dynamic control over the parameters of the color mapping results in a more effective environment for the exploration of multivariate spatial distributions the paper describes an empirical_study comparing the effectiveness of static versus dynamic representations for the exploration of qualitative aspects of bivariate distributions in this experiment subjects made judgments about the correspondence of the shape location and magnitude of two patterns under conditions with varying amounts of random noise subjects made significantly more correct judgements p
the authors introduce the contour spectrum a user interface component that improves qualitative user_interaction and provides real-time exact quantification in the visualization of isocontours the contour spectrum is a signature consisting of a variety of scalar_data and contour attributes computed over the range of scalar values /spl omega//spl isin/r they explore the use of surface area volume and gradient integral of the contour that are shown to be univariate b-spline functions of the scalar value /spl omega/ for multi-dimensional unstructured triangular grids these quantitative properties are calculated in real-time and presented to the user as a collection of signature graphs plots of functions of /spl omega/ to assist in selecting relevant isovalues /spl omega//sub 0/ for informative visualization for time-varying_data these quantitative properties can also be computed over time and displayed using a 2d interface giving the user an overview of the time-varying function and allowing interaction in both isovalue and time step the effectiveness of the current system and potential extensions are discussed
navigation through 3d spaces is required in many interactive_graphics and virtual_reality applications the authors consider the subclass of situations in which a 2d device such as a mouse controls smooth movements among viewpoints for a "through the screen" display of a 3d world frequently there is a poor match between the goal of such a navigation activity the control device and the skills of the average user they propose a unified mathematical framework for incorporating context-dependent constraints into the generalized viewpoint generation problem these designer-supplied constraint modes provide a middle ground between the triviality of a single camera animation path and the confusing excess freedom of common unconstrained control paradigms they illustrate the approach with a variety of examples including terrain models interior architectural spaces and complex molecules
volume navigation is the interactive_exploration of volume data sets by "flying" the view point through the data producing a volume rendered view at each frame the authors present an inexpensive perspective volume navigation method designed to run on a pc platform with accelerated 3d graphics_hardware they compute perspective projections at each frame allow trilinear_interpolation of sample points and render both gray scale and rgb volumes by volumetric compositing theimplementation handles arbitrarily large_volumes by dynamically swapping data within the local depth-limited frustum into main memory as the viewpoint moves through the volume they describe a new ray casting algorithm that takes advantage of the coherence inherent in adjacent frames to generate a sequence of approximate animated frames much faster than they could be computed individually they also take advantage of the 3d graphics acceleration hardware to offload much of the alpha blending and resampling from the cpu
previous accelerated volume_rendering techniques have used auxiliary hierarchical_datastructures to skip empty and homogeneous regions although some recent research has taken advantage of more efficient direct encoding techniques to skip empty regions no work has been done to directly encode homogeneous but not empty regions 3d distance transforms previously used to encode empty space can be extended to preprocess homogeneous regions as well and these regions can be efficiently encoded and incorporated into volume ray-casting and back projection algorithms with a high degree of flexibility
splatting is a popular direct_volume_rendering algorithm however the algorithm does not correctly render cases where the volume sampling rate is higher than the image sampling rate eg more than one voxel maps into a pixel this situation arises with orthographic_projections of high-resolution volumes as well as with perspective projections of volumes of any resolution the result is potentially severe spatial and temporal aliasing artifacts some volume ray-casting algorithms avoid these artifacts by employing reconstruction kernels which vary in width as the rays diverge unlike ray-casting algorithms existing splatting algorithms do not have an equivalent mechanism for avoiding these artifacts the authors propose such a mechanism which delivers high-quality splatted images and has the potential for a very efficient hardwareimplementation
triangle decimation techniques reduce the number of triangles in a mesh typically to improve interactive rendering performance or reduce data storage and transmission requirements most of these algorithms are designed to preserve the original topology of the mesh unfortunately this characteristic is a strong limiting factor in overall reduction capability since objects with a large number of holes or other topological constraints cannot be effectively reduced the author presents an algorithm that yields a guaranteed reduction level modifying topology as necessary to achieve the desired result in addition the algorithm is based on a fast local decimation technique and its operations can be encoded for progressive storage transmission and reconstruction he describes the new progressive decimation algorithm introduces mesh splitting operations and shows how they can be encoded as a progressive mesh he also demonstrates the utility of the algorithm on models ranging in size from 1132 to 168 million triangles and reduction ratios of up to 2001
the paper discusses the problem of subdividing unstructured_mesh topologies containing hexahedra prisms pyramids and tetrahedra into a consistent set of only tetrahedra while preserving the overall mesh topology efficient algorithms for volume_rendering iso-contouring and particle advection exist for mesh topologies comprised solely of tetrahedra general finite-element simulations however consist mainly of hexahedra and possibly prisms pyramids and tetrahedra arbitrary subdivision of these mesh topologies into tetrahedra can lead to discontinuous behaviour across element faces this will show up as visible artifacts in the iso-contouring and volume_rendering algorithms and lead to impossible face adjacency graphs for many algorithms the authors present various properties of tetrahedral subdivisions and an algorithm sop determining a consistent subdivision containing a minimal set of tetrahedra
the interval_volume is a generalization of the isosurface commonly associated with the marching_cubes algorithm based upon samples at the locations of a 3d rectilinear grid the algorithm produces a triangular approximation to the surface defined by fxyz=c the interval_volume is defined by α≤fxyz≤β the authors describe an algorithm for computing a tetrahedrization of a polyhedral approximation to the interval_volume
an algorithm for computing a triangulated surface which separates a collection of data points that have been segmented into a number of different classes is presented the problem generalizes the concept of an isosurface which separates data points that have been segmented into only two classes those for which data function values are above the threshold and those which are below the threshold value the algorithm is very simple easy to implement and applies without limit to the number of classes
in the area of scientific_visualization input data sets are often very large in visualization of computational_fluid_dynamics_(cfd) in particular input data sets today can surpass 100 gbytes and are expected to scale with the ability of supercomputers to generate them some visualization tools already partition large_data sets into segments and load appropriate segments as they are needed however this does not remove the problem for two reasons 1 there are data sets for which even the individual segments are too large for the largest graphics workstations 2 many practitioners do not have access to workstations with the memory capacity required to load even a segment especially since the state-of-the-art visualization tools tend to be developed by researchers with much more powerful machines when the size of the data that must be accessed is larger than the size of memory some form of virtual memory is simply required this may be by segmentation paging or by paged segments the authors demonstrate that complete reliance on operating system virtual memory for out-of-core visualization leads to egregious performance they then describe a paged segment system that they have implemented and explore the principles of memory management that can be employed by the application for out-of-core visualization they show that application control over some of these can significantly improve performance they show that sparse traversal can be exploited by loading only those data actually required
modular visualization_environments mves have recently been regarded as the de facto standard for scientific data visualization mainly due to adoption of the visual programming style reusability and extendability however since scientists and engineers as the mve principal user are not always familiar with how to map numerical data to proper graphical primitives the set of built-in modules is not fully used to construct necessary application networks therefore a certain mechanism needs to be incorporated into mves which makes use of heuristics and expertise of visualization specialists visineers and which supports the user in designing his/her applications with mves the wehrend's goal-oriented taxonomy of visualization_techniques is adopted as the basic philosophy to develop a system called gadget for application design guidance for mves the gadget system interactively helps the user design appropriate applications according to the specific visualization goals temporal efficiency versusaccuracy requirements and such properties as dimension and mesh type of a given target dataset also the gadget system is capable of assisting the user in customizing a prototype modular network for his/her desired applications by showing execution examples involving datasets of the same type the paper provides an overview of the gadget guidance mechanism and system architecture with an emphasis on its knowledge base design sample data visualization problems are used to demonstrate the usefulness of the gadget system
current visualization_systems are designed around a single user_model making it awkward for large research teams to collectively analyse large_data sets the paper shows how the popular data flow approach to visualization can be extended to allow multiple users to collaborate-each running their own visualization pipeline but with the opportunity to connect in data generated by a colleague thus collaborative_visualizations are 'programmed' in exactly the same 'plug-and-play' style as is now customary for single-user mode the paper describes a system architecture that can act as a basis for the collaborative extension of any data flow_visualization system and the ideas are demonstrated through a particularimplementation in terms of iris explorer
vizwiz is a java applet that provides basic interactive scientific_visualization functionality such as isosurfaces cutting_planes and elevation plots for 2d and 3d datasets that can be loaded into the applet by the user via the applet's web server vizwiz is unique in that it is a completely platform independent scientific_visualization tool and is usable over the web without being manually downloaded or installed its 3d graphics are implemented using only the java awt api making them portable across all java supporting platforms the paper describes theimplementation of vizwiz including design tradeoffs graphics performance figures are provided for a number of different platforms a solution to the problem of uploading user data files into a java applet working around security limitations is demonstrated the lessons learned from this project are discussed
the authors present an image synthesis methodology and a system built around it given a sparse set of photographs taken from unknown viewpoints the system generates images from new different viewpoints with correct perspective and handles occlusion it achieves this without requiring any knowledge about the 3d structure of the scene nor the intrinsic camera parameters the photo-realistic rendering process is polygon based and can be potentially implemented as real time texture_mapping the system is robust to noise by taking advantage of duplicate information from multiple_views they present results on several example scenes
virtualized reality is a modeling technique that constructs full 3d virtual representations of dynamic events from multiple video streams image-based stereo is used to compute a range image corresponding to each intensity image in each video stream each range and intensity image pair encodes the scene structure and appearance of the scene visible to the camera at that moment and is therefore called a visible surface model vsm a single time instant of the dynamic event can be modeled as a collection of vsms from different viewpoints and the full event can be modeled as a sequence of static scenes-the 3d equivalent of video alternatively the collection of vsms at a single time can be fused into a global 3d surface model thus creating a traditional virtual representation out of real world events global modeling has the added benefit of eliminating the need to hand-edit the range images to correct errors made in stereo a drawback of previous techniques like image-based_rendering models these virtual representations can be used to synthesize nearly any view of the virtualized event for this reason the paper includes a detailed comparison of existing view synthesis techniques with the authors' own approach in the virtualized representations however scene structure is explicitly represented and therefore easily manipulated for example by adding virtual objects to or removing virtualized objects from the model without interfering with real event virtualized reality then is a platform not only for image-based_rendering but also for 3d scene manipulation
the paper discusses techniques for extracting feature lines from three-dimensional unstructured_grids the twin objectives are to facilitate the interactive_manipulation of these typically very large and dense meshes and to clarify the visualization of the solution data that accompanies them the authors describe the perceptual importance of specific viewpoint-dependent and view-independent features discuss the relative advantages and disadvantages of several alternative algorithms for identifying these features taking into consideration both local and global criteria and demonstrate the results of these methods on a variety of different data sets
the authors give i/o-optimal techniques for the extraction of isosurfaces from volumetric_data by a novel application of the i/o-optimal interval tree of arge and vitter 1996 the main idea is to preprocess the data set once and for all to build an efficient search structure in disk and then each time one wants to extract an isosurface they perform an output-sensitive query on the search structure to retrieve only those active cells that are intersected by the isosurface during the query operation only two blocks of main memory space are needed and only those active cells are brought into the main memory plus some negligible overhead of disk accesses this implies that one can efficiently visualize very large_data sets on workstations with just enough main memory to hold the isosurfaces themselves theimplementation is delicate but not complicated they give the firstimplementation of the i/o-optimal interval tree and also implement their methods as an i/o filter for vtk's isosurface_extraction for the case of unstructured_grids they show that in practice the algorithms improve the performance of isosurface_extraction by speeding up the active-cell searching process so that it is no longer a bottleneck moreover this search time is independent of the main memory available the practical efficiency of the techniques reflects their theoretical optimality
the paper discusses cavevis and a related set of tools for the interactive_visualization and exploration of large sets of time-varying scalar and vector_fields using the cave virtual_reality environment since visualization of large_data sets can be very time-consuming in both computation and rendering time the task is distributed over multiple machines each of which is specialized for some aspect of the visualization process all modules must run asynchronously to maintain the highest level of interactivity a model of distributed_visualization is introduced that addresses important issues related to the management of time-dependent_data module synchronization and interactivity bottlenecks
oriented line_integral_convolution olic illustrates flow_fields by convolving a sparse texture with an anisotropic convolution kernel the kernel is aligned to the underlying flow of the vector field olic does not only show the direction of the flow but also its orientation the paper presents fast rendering of oriented line_integral_convolution frolic which is approximately two orders of magnitude faster than olic costly convolution operations as done in olic are replaced in frolic by approximating a streamlet through a set of disks with varying intensity the issue of overlapping streamlets is discussed two efficient animation techniques for animating frolic images are described frolic has been implemented as a java applet this allows researchers from various disciplines typically with inhomogenous hardware environments to conveniently explore and investigate analytically defined 2d vector_fields
the paper presents an algorithm uflic unsteady_flow lic to visualize vector data in unsteady_flow_fields using line_integral_convolution lic as the underlying method a new convolution algorithm is proposed that can effectively trace the flow's global features over time the new algorithm consists of a time-accurate value depositing scheme and a successive feedforward method the value depositing scheme accurately models the flow advection and the successive feedforward method maintains the coherence between animation frames the new algorithm can produce time-accurate highly coherent flow animations to highlight global features in unsteady_flow_fields cfd scientists for the first time are able to visualize unsteady surface flows using the algorithm
the paper presents a new approach for animating 2d steady flow_fields it is based on an original data structure called the motion map the motion map contains not only a dense representation of the flow_field but also all the motion information required to animate the flow an important feature of this method is that it allows in a natural way cyclical variable-speed animations as far as efficiency is concerned the advantage of this method is that computing the motion map does not take more time than computing a single still image of the flow and the motion map has to be computed only once another advantage is that the memory requirements for a cyclical animation of an arbitrary number of frames amounts to the memory cost of a single still image
volumetric_data sets require enormous storage capacity even at moderate resolution levels the excessive storage demands not only stress the capacity of the underlying storage andcommunications systems but also seriously limit the speed of volume_rendering due to data movement and manipulation a novel volumetric_data visualization scheme is proposed and implemented in this work that renders 2d images directly from compressed 3d data sets the novelty of this algorithm is that rendering is performed on the compressed representation of the volumetric_data without pre-decompression as a result the overheads associated with both data movement and rendering processing are significantly reduced the proposed algorithm generalizes previously proposed whole-volume frequency-domain rendering schemes by first dividing the 3d data set into subcubes transforming each subcube to a frequency-domain representation and applying the fourier projection theorem to produce the projected 2d images according to given viewing angles compared to the whole-volume approach the subcube-based scheme not only achieves higher compression efficiency by exploiting local coherency but also improves the quality of resultant rendering images because it approximates the occlusion effect on a subcube by subcube basis
the paper presents a framework for multiresolution compression and geometric reconstruction of arbitrarily dimensioned data designed for distributed applications although being restricted to uniform sampled data the versatile approach enables the handling of a large variety of real world elements examples include nonparametric parametric and implicit lines surfaces or volumes all of which are common to large scale data sets the framework is based on two fundamental steps compression is carried out by a remote server and generates a bit-stream transmitted over the underlying network geometric reconstruction is performed by the local client and renders a piecewise linear approximation of the data more precisely the compression scheme consists of a newly developed pipeline starting from an initial b-spline wavelet precoding the fundamental properties of wavelets allow progressive transmission and interactive control of the compression gain by means of global and local oracles in particular the authors discuss the problem of oracles in semiorthogonal settings and propose sophisticated oracles to remove unimportant coefficients in addition geometric constraints such as boundary lines can be compressed in a lossless manner and are incorporated into the resulting bit-stream the reconstruction pipeline performs a piecewise adaptive linear approximation of data using a fast and easy to use point removal strategy which works with any subsequent triangulation technique
most existing visualization_applications use 3d geometry as their basic rendering primitive as users demand more complex data sets the memory requirements for retrieving and storing large 3d models are becoming excessive in addition the current 3d rendering hardware is facing a large memory bus bandwidth bottleneck at the processor to graphics pipeline interface rendering 1 million triangles with 24 bytes per triangle at 30 hz requires as much as 720 mb/sec memory bus bandwidth this transfer rate is well beyond the current low-cost graphics systems a solution is to compress the static 3d geometry as an off-line pre-process then only the compressed geometry needs to be stored in main memory and sent down to the graphics pipeline for real-time decompression and rendering the author presents several new techniques for compression of 3d geometry that produce 2 to 3 times better compression ratios than existing methods they first introduce several algorithms for the efficient encoding of the original geometry as generalized triangle meshes this encoding allows most of the mesh vertices to be reused when forming new triangles their second contribution allows various parts of a geometric model to be compressed with different precision depending on the level of details present together the meshifying algorithms and the variable compression method achieve compression ratios of 30 and 37 to one over ascii encoded formats and 10 and 15 to one over binary encoded triangle strips the experimental results show a dramatically lowered memory bandwidth required for real-time_visualization of complex data sets
this paper outlines a method to dynamically replace portals with textures in a cell-partitioned model the rendering complexity is reduced to the geometry of the current cell thus increasing interactive performance a portal is a generalization of windows and doors it connects two adjacent cells or rooms each portal of the current cell that is some distance away from the viewpoint is rendered as a texture the portal texture smoothly returns to geometry when the viewpoint gets close to the portal this way all portal sequences not too close to the viewpoint have a depth complexity of one the size of each texture and distance at which the transition occurs is configurable for each portal
we describe an algorithm for repairing polyhedral cad models that have errors in their b-rep errors like cracks degeneracies duplication holes and overlaps are usually introduced in solid models due to imprecise arithmetic model transformations designer errors programming bugs etc such errors often hamper further processing such as finite_element analysis radiosity computation and rapid prototyping our fault-repair algorithm converts an unordered collection of polygons to a shared-vertex representation to help eliminate errors this is done by choosing for each polygon edge the most appropriate edge to unify it with the two edges are then geometrically merged into one by moving vertices at the end of this process each polygon edge is either coincident with another or is a boundary edge for a polygonal hole or a dangling wall and may be appropriately repaired finally in order to allow user-inspection of the automatic corrections we produce a visualization of the repair and let the user mark the corrections that conflict with the original design intent a second iteration of the correction algorithm then produces a repair that is commensurate with the intent this by involving the users in a feedback loop we are able to refine the correction to their satisfaction
recursive subdivision schemes have been extensively used in computer_graphics and scientific_visualization for modeling smooth_surfaces of arbitrary topology recursive subdivision generates a visually pleasing smooth surface in the limit from an initial user-specified polygonal mesh through the repeated application of a fixed set of subdivision rules in this paper we present a new dynamic surface model based on the catmull-clark 1978 subdivision scheme which is a very popular method to model complicated objects of arbitrary genus because of many of its nice properties our new dynamic surface model inherits the attractive properties of the catmull-clark subdivision scheme as well as that of the physics-based modeling paradigm this new model provides a direct and intuitive means of manipulating geometric shapes a fast robust and hierarchical approach for recovering complex geometric shapes from range and volume data using very few degrees of freedom control vertices we provide an analytic formulation and introduce the physical quantities required to develop the dynamic subdivision surface model which can be interactively deformed by applying synthesized forces in real time the governing dynamic differential equation is derived using lagrangian mechanics and a finite_element discretization our experiments demonstrate that this new dynamic model has a promising future in computer_graphics geometric shape design and scientific_visualization
multilevel representations and mesh reduction techniques have been used for accelerating the processing and the rendering of large_datasets representing scalar- or vector-valued functions defined on complex 2d or 3d meshes we present a method based on finite_element approximations which combines these two approaches in a new and unique way that is conceptually simple and theoretically sound the main idea is to consider mesh reduction as an approximation problem in appropriate finite_element spaces starting with a very coarse triangulation of the functional domain a hierarchy of highly non-uniform tetrahedral or triangular in 2d meshes is generated adaptively by local refinement this process is driven by controlling the local error of the piecewise linear finite_element approximation of the function on each mesh element a reliable and efficient computation of the global approximation error and a multilevel preconditioned conjugate gradient solver are the key components of theimplementation in order to analyze the properties and advantages of the adaptively generated tetrahedral_meshes we implemented two volume_visualization algorithms an iso-surface extractor and a ray-caster both algorithms while conceptually simple show significant speedups over conventional methods delivering comparable rendering quality from adaptively compressed datasets
we present the use of mapping functions to automatically generate levels of detail with known error bounds for polygonal models we develop a piece-wise linear mapping function for each simplification operation and use this function to measure deviation of the new surface from both the previous level of detail and from the original surface in addition we use the mapping function to compute appropriate texture coordinates if the original map has texture coordinates at its vertices our overall algorithm uses edge collapse operations we present rigorous procedures for the generation of local planar projections as well as for the selection of a new vertex position for the edge collapse operation as compared to earlier methods our algorithm is able to compute tight error bounds on surface deviation and produce an entire continuum of levels of detail with mappings between them we demonstrate the effectiveness of our algorithm on several models a ford bronco consisting of over 300 parts and 70000 triangles a textured lion model consisting of 49 parts and 86000 triangles and a textured wrinkled torus consisting of 79000 triangles
an eigenvector method for vortex identification has been applied to recent numerical and experimental studies in external flow aerodynamics it is shown to be an effective way to extract and visualize features such as vortex cores spiral vortex_breakdowns vortex bursting and vortex diffusion several problems are reported and illustrated these include disjointed line segments detecting non-vortical flow features and vortex core displacement future research and applications are discussed such as using vortex cores to guide automatic grid refinement
software has been developed to apply visualization_techniques to aeronautics data collected during wind tunnel experiments interaction between the software developers and the aeroscientists has been crucial in making the software the interaction has also been important in building the scientists' confidence in the use of interactive computer-mediated analysis tools
this paper discusses strategies for effectively portraying 3d flow using volume line_integral_convolution issues include defining an appropriate input texture clarifying the distinct identities and relative depths of the advected texture elements and selectively highlighting regions of interest in both the input and output volumes apart from offering insights into the greater potential of 3d lic as a method for effectively representing flow in a volume a principal contribution of this work is the suggestion of a technique for generating and rendering 3d visibility-impeding "halos" that can help to intuitively indicate the presence of depth discontinuities between contiguous elements in a projection and thereby clarify the 3d spatial organization of elements in the flow the proposed techniques are applied to the visualization of a hot supersonic laminar jet exiting into a colder subsonic coflow
large simulation grids and multi-grid configurations impose many constraints on commercial visualization software when available ram is limited and graphics primitives are numbered in millions alternative techniques for data access and processing are necessary in this case_study we present our contributions to a visualization environment based on the avs/express software we demonstrate how the efficient visualization of large_datasets relies upon several forms of resource sharing and alternate and efficient data access techniques
describes several visualization_techniques based on the notion of multi-resolution brushing to browse large 3d volume datasets our software is implemented using public-domain libraries and is designed to run on average-equipped desktop computers such as a linux machine with 32 mbytes of memory empirically our system allows scientists to obtain information from a large_dataset with over 83 million numbers in interactive time we show that very large scientific volume datasets can be accessed and utilized without expensive hardware and software
3d virtual_colonoscopy has recently been proposed as a non-invasive alternative procedure for the visualization of the human colon surface rendering is sufficient for implementing such a procedure to obtain an overview of the interior surface of the colon at interactive rendering speeds unfortunately physicians can not use it to explore tissues beneath the surface to differentiate between benign and malignant structures in this paper we present a direct_volume_rendering approach based on perspective ray casting as a supplement to the surface navigation to accelerate the rendering speed surface-assistant techniques are used to adapt the resampling rates by skipping the empty space inside the colon in addition a parallel version of the algorithm has been implemented on a shared-memory multiprocessing architecture experiments have been conducted on both simulation and patient data sets
describes data_exploration techniques designed to classify dna_sequences several visualization and data_mining techniques were used to validate and attempt to discover new methods for distinguishing coding dna_sequences exons from non-coding dna_sequences introns the goal of the data_mining was to see whether some other possibly non-linear combination of the fundamental position-dependent dna nucleotide frequency values could be a better predictor than the ami average mutual information we tried many different classification techniques including rule-based classifiers and neural_networks we also used visualization of both the original data and the results of the data_mining to help verify patterns and to understand the distinction between the different types of data and classifications in particular the visualization helped us develop refinements to neural network classifiers which have accuracies as high as any known method finally we discuss the interactions between visualization and data_mining and suggest an integrated approach
an interactive cerebral blood vessel exploration system is described it has been designed on the basis of neurosurgeons' requirements in order to assist them in the diagnosis of vascular pathologies the system is based on the construction of a symbolic model of the vascular tree with automatic identification and labelling of vessel bifurcations aneurysms and stenoses it provides several types of visualization individual mra magnetic resonance angiography slices mip maximum intensity projection shaded rendering symbolic schemes and surface_reconstruction
we describe a multidisciplinary effort for creating interactive 3d graphical modules for visualizing optical phenomena these modules are designed for use in an upper-level undergraduate course the modules are developed in open inventor which allows them to run under both unix and windows the work is significant in that it applies contemporary interactive 3d_visualization_techniques to instructional courseware which represents a considerable advance compared to the current state of the practice
the ability to forecast the progress of crisis events would significantly reduce human suffering and loss of life the destruction of property and expenditures for assessment and recovery los alamos national laboratory has established a scientific thrust in crisis forecasting to address this national challenge in the initial phase of this project scientists at los alamos are developing computer models to predict the spread of a wildfire visualization of the results of the wildfire simulation will be used by scientists to assess the quality of the simulation and eventually by fire personnel as a visual forecast of the wildfire's evolution the fire personnel and scientists want the visualization to look as realistic as possible without compromising scientificaccuracy this paper describes how the visualization was created analyzes the tools and approach that were used and suggests directions for future work and research
this paper investigates the visualization and animation of geometric computing in a distributed electronic classroom we show how focusing in a well-defined domain makes it possible to develop a compact system that is accessible to even naive users we present a conceptual_model and a system gasp-ii geometric animation system princeton ii that realizes this model in the geometric domain the system allows the presentation and interactive_exploration of 3d geometric_algorithms over a network
we present collaborative scientific_visualization in studierstube studierstube is an augmented_reality system that has several advantages over conventional desktop and other virtual_reality environments including true stereoscopy 3d-interaction individual viewpoints and customized views for multiple users unhindered natural collaboration and low cost we demonstrate the application of this concept for the interaction of multiple users and illustrate it with several visualizations of dynamical_systems in dynsys3d a visualization system running on top of avs
we describe a set of visualization programs developed for understanding segmentations of customer records produced by a self organizing map som algorithm a som produces segments of similar customer records that can then be used as the basis of a marketing campaign since the characteristics that each segment will have in common are not specified a priori visualization is essential to understanding the segment to design specific marketing strategies two different styles of visualizations were found to be useful for the two types of observers of the data abstract overviews of the entire segmentation were designed for analysts applying the som algorithm detailed scatterplots of individual records were designed for communicating the results to decision makers specifying marketing strategy
since 1991 our team of computer scientists chemists and physicists have worked together to develop an advanced virtual-environment interface to scanned-probe microscopes the interface has provided insights and useful capabilities well beyond those of the traditional interface this paper lists the particular visualization and control techniques that have enabled actual scientific discovery including specific examples of insight gained using each technique this information can help scientists determine which features are likely to be useful in their particular application and which would be just sugar coating it can also guide computer scientists to suggest the appropriate type of interface to help solve a particular problem we have found benefit in advanced rendering with natural viewpoint control but not always from semi-automatic control techniques from force feedback during manipulation and from storing/replaying data for an entire experiment these benefits come when the system is well-integrated into the existing tool and allows export of the data to standard visualization packages
this paper describes our experiences with using the virtual_reality modeling language vrml to view files in the initial graphics exchange specification iges format using a java-based translator from iges to vrml and html hypertext markup language the paper examines the conversion problems between iges and vrml and presents some results of the process
the measurement analysis and visualization of plant growth is of primary interest to plant biologists we are developing software tools to support such investigations there are two parts in this investigation namely growth visualization of i a plant root and ii a plant stem for both domains the input data is a stream of images taken by cameras the tools being developed make it possible to measure various time-varying quantities such as differential growth for both domains the plant is modeled by using flexible templates to represent non-rigid motions
climatological data about thunderstorms is traditionally collected by balloons or planes traveling through the storm along straight tracts such data lends itself to simple 2d representations the data described in this paper was gathered by a sail plane spiraling in an updraft within a thundercloud the more complex organization of data samples demands more complex representation methods this paper describes a system developed using the visualization toolkit vtk to explore such data the data consists of several scalar values and a set of vector values associated with positional data on the measuring devices the goal of this visualization is to explore the location of point charges suggested by the electromagnetic field vectors and determine if any correlation exists between the point charge location and standard cloud microstructure scalar measurements such as temperature there are several problems associated with visualizing this rather unique set of data they stem from the fact that the data is a sparse spiraling sample of scalars and vectors the system allows the track of the plane to be displayed as a line a tube or a ribbon scalar values can be displayed as transparent isosurfaces and the vector data as an arrow plot along that track given a color that is constant based on orientation or related to the value of a scalar any combination of methods can be used to display the data a single primitive can be overloaded in many ways or several different variables can all be displayed simultaneously
presents a system for interactively visualizing large polygonal environments such as those produced by cad systems during the design of aircraft and power generation engines our method combines view frustum culling with level-of-detail modeling to create a visualization system that supports part motion and has the ability to view arbitrary sets of data to avoid long system start-up delays due to data loading we have implemented our system using a dynamic loading strategy this also allows us to interactively visualize more data than could fit in memory at one time
numerical finite_element simulations of the behaviour of a car body in frontal side or rear impact collision scenarios have become increasingly complex as well as reliable and precise they are well-established as a standard evaluation tool in the automotive development process both the increased complexity and the advances in computer_graphics technology have resulted in the need for new visualization_techniques to facilitate the analysis of the immense amount of data originating from such scientific engineering computations expanding the effectiveness of traditional post-processing techniques is one key to achieve shorter design cycles and faster time-to-market in this paper we describe how the extensive use of texture_mapping and new visualization mappings like force tubing can considerably enhance the post-processing of structural and physical properties of car components in crash simulations we show that using these techniques both the calculation costs and the rendering costs are reduced and the quality of the visualization is improved
we define a rotation field by extending the notion of a vector field to rotations a vector field has a vector as a value at each point of its domain a rotation field has a rotation as a value at each point of its domain rotation fields result from mapping the orientation error of tracking systems we build upon previous methods for the visualization of vector_fields tensor_fields and rotations at a point to visualize a rotation field resulting from calibration of a commonly-used magnetic tracking system
presents a new approach to isosurface_extraction from volume data using particle_systems particle behavior is dynamic and can be based on laws of physics or artificial rules for isosurface_extraction we program particles to be attracted towards a specific surface value while simultaneously repelling adjacent particles the repulsive forces are based on the curvature of the surface at that location a birth-death process results in a denser concentration of particles in areas of high curvature and sparser populations in areas of lower curvature the overall level of detail is controlled through a scaling factor that increases or decreases the repulsive forces of the particles once particles reach equilibrium their locations are used as vertices in generating a triangular_mesh of the surface the advantages of our approach include vertex densities are based on surface features rather than on the sampling rate of the volume a single scaling factor simplifies level-of-detail control and meshing is efficient because it uses neighbor information that has already been generated during the force calculations
currently the most popular method of visualizing music is music notation through music notation an experienced musician can gain an impression of how a particular piece of music sounds simply by looking at the notes on paper however most listeners are unfamiliar or uncomfortable with the complex nature of music notation the goal of this project is to present an alternate method for visualizing music that makes use of color and 3d space this paper describes one method of visualizing music in 3d space theimplementation of this method shows that music visualization is an effective technique although it is certainly not the only possible method for accomplishing the task throughout the course of this project several variations and alternative approaches were discussed the final version of this project reflects the decisions that were made in order to present the best possible representation of music data
the paper investigates the visualization of distributed_algorithms we present a conceptual_model and a system vade that realizes this model since in asynchronous distributed_systems there is no way of knowing let alone visualizing the “real” execution we show how to generate a visualization which is consistent with the execution of the distributed algorithm we also present the design andimplementation of our system vade is designed so that the algorithm runs on the server's machines while the visualization is executed on a web page on the client's machine programmers can write animations quickly and easily with the assistance of vade's libraries
geographic_visualization sometimes called cartographic visualization is a form of information_visualization in which principles from cartography geographic_information systems gis exploratory_data_analysis eda and information_visualization more generally are integrated in the development and assessment of visual methods that facilitate the exploration analysis synthesis and presentation of georeferenced information the authors report on development and use of one component of a prototype gvis environment designed to facilitate exploration by domain experts of time_series multivariate georeferenced health statistics emphasis is on how manipulable dynamic gvis tools may facilitate visual thinking pattern noticing and hypothesis generation the prototype facilitates the highlighting of data extremes examination of change in geographic patterns over time and exploration of similarity among georeferenced variables a qualitative exploratory_analysis of verbal protocols and transaction logs is used to characterize system use evidence produced through the characterization highlights differences among experts in data_analysis strategies particularly in relation to the use of attribute “focusing” combined with time_series animation and corresponding differences in success at noticing spatiotemporal patterns
information_visualization focuses on the use of visual means for exploring non-visual information while free-form text is a rich common source of information_visualization of text is a challenging problem since text is inherently non-spatial the paper explores the use of implicit surface models for visualizing text the authors describe several techniques for text_visualization that aid in understanding document content and document relationships a simple method is defined for mapping document content to shape by comparing the shapes of multiple documents global content similarities and differences may be noted in addition they describe a visual_clustering method in which documents are arranged in 3d based upon similarity scoring documents deemed closely related blend together as a single connected shape hence a document corpus becomes a collection of shapes that reflect inter-document relationships these techniques provide methods to visualize individual documents as well as corpus meta-data they then combine the two techniques to produce transparent clusters enclosing individual document shapes this provides a way to visualize both local and global contextual information finally they elaborate on several potential applications of these methods
one very effective method for managing large_data sets is aggregation or binning we consider two aggregation methods that are tightly coupled with interactive_manipulation and the visual representation of the data through this integration we hope to provide effective support for the aggregation process specifically by enabling 1 automatic aggregation 2 continuous change and control of the aggregation level 3 spatially based aggregates 4 context maintenance across different aggregate levels and 5 feedback on the level of aggregation
the author proposes a simple and powerful graphical interface tool called the lensbar for filtering and visualizing large lists of data browsing and querying are the most important tasks in retrieving information and lensbar integrates the two techniques into a simple scroll window with slider while it looks familiar to users of conventional graphical interface tools its filtering and zooming features offer sophisticated handling of large lists of textual data
decision tables like decision_trees or neural nets are classification models used for prediction they are induced by machine_learning algorithms a decision table consists of a hierarchical table in which each entry in a higher level table gets broken down by the values of a pair of additional attributes to form another table the structure is similar to dimensional_stacking a visualization method is presented that allows a model based on many attributes to be understood even by those unfamiliar with machine_learning various forms of interaction are used to make this visualization more useful than other static designs
the order and arrangement of dimensions variates is crucial for the effectiveness of a large number of visualization_techniques such as parallel_coordinates scatterplots recursive pattern and many others we describe a systematic approach to arrange the dimensions according to their similarity the basic idea is to rearrange the data dimensions such that dimensions showing a similar behavior are positioned next to each other for the similarity clustering of dimensions we need to define similarity measures which determine the partial or global similarity of dimensions we then consider the problem of finding an optimal one- or two-dimensional arrangement of the dimensions based on their similarity theoretical considerations show that both the one- and the two-dimensional arrangement problem are surprisingly hard problems ie they are np complete our solution of the problem is therefore based on heuristic algorithms an empirical_evaluation using a number of different visualization_techniques shows the high impact of our similarity clustering of dimensions on the visualization results
a number of usability studies report that many users of the www cannot find pages already visited additionally many users cannot visualise where they are or where they have been browsing currently readily available www browsers provide history mechanisms that offer little or no support in the presentation and manipulation of visited sites manipulation and presentation of usage data such as a browse history has been used in a number of cases to aid users in searching for previously attained data and to teach or assist other users in their browse or searching techniques the paper presents a virtual_reality vr based application to be used alongside traditional web browsers which provides them with a flexibly tailorable real time visualisation of their history
we present ivory a newly developed platform-independent framework for physics based visualization ivory is especially designed for information_visualization_applications and multidimensional graph_layout it is fully implemented in java 11 and its architecture features client server setup which allows us to run the visualization even on thin clients in addition vrml 20 exports can be viewed by any vrml plugged-in www browser individual visual metaphors are invoked into ivory via an advanced plug-in mechanism where plug-ins can be implemented by any experienced user the configuration of ivory is accomplished using a script language called ivml some interactive_visualization examples such as the integration of a haptic interface illustrate the performance and versatility of our system our currentimplementation supports nt 40
to gain insight and understanding of complex information collections users must be able to visualize and explore many facets of the information the paper presents several novel visual methods from an information analyst's perspective the authors present a sample scenario using the various methods to gain a variety of insights from a large information collection they conclude that no single paradigm or visual method is sufficient for many analytical tasks often a suite of integrated methods offers a better analytic environment in today's emerging culture of information overload and rapidly changing issues they also conclude that the interactions among these visual paradigms are equally as important as if not more important than the paradigms themselves
the purpose of the paper is to develop a visualization system of a document space called bibliomapper for cisi collections one of the bibliographic databases available on the internet the major function of bibliomapper is to visualize the document space with a cluster-based_visualization_technique the cluster-based_visualization_technique assembles a set of documents according to semantic similarities one advantage of this technique is that users are able to focus on and assess each cluster and the documents which the cluster comprises according to their information needs
the paper describes a visualization of a general hierarchical_clustering algorithm that allows the user to manipulate the number of classes produced by the clustering method without requiring a radical re-drawing of the clustering tree the visual method used a space filling recursive division of a rectangular_area keeps the items under consideration at the same screen position even while the number of classes is under interactive control as well as presenting a compact representation of the clustering with different cluster numbers this method is particularly useful in a linked_views environment where additional information can be added to a display to encode other information without this added level of detail being perturbed when changes are made to the number of clusters
the authors propose a methodology for automatically realizing communicative goals in graphics it features a task model that mediates the communicative intent and the selection of graphical techniques the methodology supports the following functions isolating assertions presentable in graphics mapping such assertions into tasks for the potential reader and selecting graphical techniques that support those tasks they illustrate the methodology by redesigning a textual argument into a multimedia one with the same rhetorical and content structures but employing graphics to achieve some of the intentions
we present a new visualization_technique called rdt reconfigurable disc tree which can alleviate the disadvantages of cone trees significantly for large hierarchies while maintaining its context of using 3d depth in rdt each node is associated with a disc around which its children are placed using discs instead of cones as the basic shape in rdt has several advantages significant reduction of occluded region sharp increase in number of displayed nodes and easy projection onto plane without visual overlapping we show that rdt can greatly enhance user perception by transforming its shapes dynamically in several ways 1 disc tree which can significantly reduce the occluded region by the foreground objects 2 compact disc tree which can increase the number of nodes displayed on the screen and 3 plane disc tree which can be mapped onto the plane without visual overlapping we describe animplementation of our visualization system called visit visual information system for reconfigurable disc tree it provides 2d and 3d layouts for rdt and various user interface features such as tree reconfiguration tree transformation tree shading viewing transformation animation selection and browsing which can enhance the user perception and navigation capabilities we also evaluate our system using the following three metrics percentage of occlusion density of displayed nodes on a screen and number of identifiable nodes
information_visualization encounters a wide variety of different data domains the visualization community has developed representation methods and interactive techniques as a community we have realized that the requirements in each domain are often dramatically different in order to easily apply existing methods researchers have developed a semiology of graphic representations we have extended this research into a framework that includes operators and interactions in visualization_systems such as a visualization spreadsheet we discuss properties of this framework and use it to characterize operations spanning a variety of different visualization_techniques the framework developed in the paper enables a new way of exploring and evaluating the design space of visualization operators and helps end users in their analysis tasks
the paper describes a general formulation of the “detail-in-context” problem which is a central issue of fundamental importance to a wide variety of nonlinear magnification systems a number of tools are described for dealing with this problem effectively these tools can be applied to any continuous nonlinear magnification system and are not tied to specificimplementation features of the system that produced the original transformation of particular interest is the development of “seamless multi level views” which allow multiple global views of aninformation_space each having different information content to be integrated into a single view without discontinuity
algorithm animation systems and graphical debuggers perform the task of translating program state into visual representations while algorithm animations typically rely on user augmented source code to produce visualizations debuggers make use of symbolic information in the target program as a result visualizations produced by debuggers often lack important semantic content making them inferior to algorithm animation systems the paper presents a method to provide higher level more informative visualizations in a debugger using a technique called traversal based visualization the debugger traverses a data structure using a set of user supplied patterns to identify parts of the data structure to be drawn a similar way a declarative language is used to specify the patterns and the actions to take when the patterns are encountered alternatively the user can construct traversal specifications through a graphical user interface to the declarative language furthermore the debugger supports modification of data changes made to the on-screen representation are reflected in the underlying data
real-time rendering of triangulated surfaces has attracted growing interest in the last few years however interactive_visualization of very large scale grid digital elevation models is still difficult the graphics load must be controlled by adaptive surface triangulation and by taking advantage of different levels of detail furthermore management of the visible scene requires efficient access to the terrain database we describe an all-in-one visualization system which integrates adaptive triangulation dynamic scene management and spatial_data handling the triangulation model is based on the restricted quadtree triangulation furthermore we present new algorithms of restricted quadtree triangulation these include among others exact error approximation progressive meshing performance enhancements and spatial access
the key to real-time rendering of large-scale surfaces is to locally adapt surface geometric complexity to changing view parameters several schemes have been developed to address this problem of view-dependent level-of-detail control among these the view-dependent progressive mesh vdpm framework represents an arbitrary triangle mesh as a hierarchy of geometrically optimized refinement transformations from which accurate approximating meshes can be efficiently retrieved in this paper we extend the general vdpm framework to provide temporal_coherence through the run-time creation of geomorphs these geomorphs eliminate "popping" artifacts by smoothly interpolating geometry theirimplementation requires new output-sensitive data structures which have the added benefit of reducing memory use we specialize the vdpm framework to the important case of terrain_rendering to handle huge terrain grids we introduce a block-based simplification scheme that constructs a progressive mesh as a hierarchy of block refinements we demonstrate the need for an accurate approximation metric during simplification our contributions are highlighted in a real-time flyover of a large rugged terrain notably the use of geomorphs results in visually smooth rendering even at 72 frames/sec on a graphics workstation
multi-triangulation mt is a general framework for managing the level-of-detail in large triangle meshes which we have introduced in our previous work in this paper we describe an efficientimplementation of an mt based on vertex decimation we present general techniques for querying an mt which are independent of a specific application and which can be applied for solving problems such as selective refinement windowing point location and other spatial interference queries we describe alternative data structures for encoding an mt which achieve different trade-offs between space and performance experimental results are discussed
scalar_fields arise in every scientific application existing scalar visualization_techniques require that the user infers the global scalar structure from what is frequently an insufficient display of information we present a visualization_technique which numerically detects the structure at all scales removing from the user the responsibility of extracting information implicit in the data and presenting the structure explicitly for analysis we further demonstrate how scalar_topology detection proves useful for correct visualization and image_processing applications such as image co-registration isocontouring and mesh_compression
many sophisticated solutions have been proposed to reduce the geometric complexity of 3d meshes a problem studied less often is how to preserve on a simplified mesh the detail eg color high frequency shape detail scalar_fields etc which is encoded in the original mesh we present a general approach for preserving detail on simplified meshes the detail or high frequency information lost after simplification is encoded through texture or bump maps the original contribution is that preservation is performed after simplification by building set of triangular texture patches that are then packed in a single texture map each simplified mesh face is sampled to build the associated triangular texture patch a new method for storing this set of texture patches into a standard rectangular texture is presented and discussed our detail preserving approach makes no assumptions about the simplification process adopted to reduce mesh complexity and allows highly efficient rendering the solution is very general allowing preservation of any attribute value defined on the high resolution mesh we also describe an alternative application the conversion of 3d models with 3d static procedural textures into standard 3d models with 2d textures
generation of a three-dimensional model from an unorganized set of points is an active area of research in computer_graphics alpha shapes can be employed to construct a surface which most closely reflects the object described by the points however no α-shape for any value of α can properly detail discontinuous regions of a model we introduce herein two methods of improving the results of reconstruction using α-shapes density-scaling which modulates the value of a depending on the density of points in a region and anisotropic shaping which modulates the form of the α-ball based on point normals we give experimental results that show the successes and limitations of our method
in this article we build a multi-resolution framework intended to be used for the visualization of continuous piecewise linear functions defined over triangular planar or spherical meshes in particular the data set can be viewed at different level of detail that's to say as a piecewise linear function defined over any simplification of the base mesh in his multi-resolution form the function requires strictly the same volume of data than the original input it is then possible to go through consecutive levels by the use of so-called detail coefficients with exact reconstruction if desired we also show how to choose a decimation sequence that leads to a good compromise between the resulting approximation error and the number of removed vertices the theoretical tools used here are inspired from wavelet-based techniques and extended in the sense that they can handle non-nested approximation spaces
3d time-varying unstructured and structured data sets are difficult to visualize and analyze because of the immense amount of data involved these data sets contain many evolving amorphous regions and standard visualization_techniques provide no facilities to aid the scientist to follow regions of interest in this paper we present a basic framework for the visualization of time-varying_data sets and a new algorithm and data structure to track volume features in unstructured scalar_data sets the algorithm and data structure are general and can be used for structured curvilinear adaptive and hybrid grids as well the features tracked can be any type of connected regions examples are shown from ongoing research
this paper describes by example a strategy for plotting and interacting with data in multiple metric spaces the example system was designed for use with time-varying computational_fluid_dynamics_(cfd) data sets but the methodology is directly applicable to other types of field data the central objects embodied by the tool are portraits which show the data in various coordinate systems while preserving their spatial connectivity and temporal variability the coordinates are derived in various ways from the field data and an important feature is that new and derived portraits can be created interactively the primary operations supported by the tool are brushing_and_linking the user can select a subset of a given portrait and this subset is highlighted in all portraits the user can combine highlighted subsets from an arbitrary number of portraits with the usual logical operators thereby indicating where an arbitrarily complex set of conditions holds the system is useful for exploratory_visualization and feature_detection in multivariate data
we are interested in feature_extraction from volume data in terms of coherent surfaces and 3d space curves the input can be an inaccurate scalar or vector field sampled densely or sparsely on a regular 3d grid in which poor resolution and the presence of spurious noisy samples make traditional iso-surface techniques inappropriate in this paper we present a general-purpose methodology to extract surfaces or curves from a digital 3d potential vector field {sv~} in which each voxel holds a scalar s designating the strength and a vector v~ indicating the direction for scalar sparse or low-resolution data we "vectorize" and "densify" the volume by tensor voting to produce dense vector_fields that are suitable as input to our algorithms the extremal_surface and curve algorithms both algorithms extract with sub-voxel precision coherent features representing local extrema in the given vector field these coherent features are a hole-free triangulation mesh in the surface case and a set of connected oriented and non-intersecting polyline segments in the curve case we demonstrate the general usefulness of both extremal algorithms on a variety of real data by properly extracting their inherent extremal properties such as a shock waves induced by abrupt velocity or direction changes in a flow_field b interacting vortex cores and vorticity lines in a velocity field c crest-lines and ridges implicit in a digital terrain map and d grooves anatomical lines and complex surfaces from noisy dental data
a novel approach is introduced to define a quantitative measure of closeness between vector_fields the usefulness of this measurement can be seen when comparing computational and experimental flow_fields under the same conditions furthermore its applicability can be extended to more cumbersome tasks such as navigating through a large_database searching for similar topologies this new measure relies on the use of critical_points which are a key feature in vector_field_topology in order to characterize critical_points α and β parameters are introduced they are used to form a closed set of eight unique patterns for simple critical_points these patterns are also basic building blocks for higher-order nonlinear vector_fields in order to study and compare a given set of vector_fields a measure of distance between different patterns of critical_points is introduced the basic patterns of critical_points are mapped onto a unit circle in α-β space the concept of the "earth mover's distance" is used to compute the closeness between various pairs of vector_fields and a nearest-neighbor query is thus produced to illustrate the relationship between the given set of vector_fields this approach quantitatively measures the similarity and dissimilarity between vector_fields it is ideal for data_compression of a large flow_field since only the number and types of critical_points along with their corresponding α and β parameters are necessary to reconstruct the whole field it can also be used to better quantify the changes in time-varying_data sets
presents a new method for using texture to visualize multi-dimensional_data elements arranged on an underlying 3d height field we hope to use simple texture patterns in combination with other visual_features like hue and intensity to increase the number of attribute values we can display simultaneously our technique builds perceptual texture elements or pexels to represent each data element attribute values encoded in the data element are used to vary the appearance of a corresponding pexel texture patterns that form when the pexels are displayed can be used to rapidly and accurately explore the dataset our pexels are built by controlling three separate texture dimensions height density and regularity results from computer_graphics computer vision and cognitive psychology have identified these dimensions as important for the formation of perceptual texture patterns we conducted a set of controlled experiments to measure the effectiveness of these dimensions and to identify any visual interference that may occur when all three are displayed simultaneously at the same spatial location results from our experiments show that these dimensions can be used in specific combinations to form perceptual textures for visualizing multidimensional datasets we demonstrate the effectiveness of our technique by applying it to two real-world visualization_environments tracking typhoon activity in southeast asia and analyzing ocean conditions in the northern pacific
presents an efficient algorithm for the reconstruction of a multivariate function from multiple sets of scattered data given n sets of scattered data representing n distinct dependent variables that have been sampled independently over a common domain and n error tolerance values the algorithm constructs a triangulation of the domain of the data and associates multivariate values with the vertices of the triangulation the resulting linear_interpolation of these multivariate values yields a multivariate function called a co-triangulation that represents all of the dependent data up to the given error tolerance a simple iterative algorithm for the construction of a co-triangulation from any number of data sets is presented and analyzed the main contribution of this paper lies in the description of a highly efficient framework for the realization of this approximation algorithm while the asymptotic time complexity of the algorithm certainly remains within the theoretical bounds we demonstrate that it is possible to achieve running times that depend only linearly on the number of data even for very large problems with more than two million samples this efficient realization of the algorithm uses adapted dynamic_data structures and careful caching in an integrated framework
within biological systems water molecules undergo continuous stochastic brownian motion the diffusion rate can give clues to the structure of the underlying tissues in some tissues the rate is anisotropic_diffusion-rate images can be calculated from diffusion-weighted mri a 2d diffusion tensor image dti and an associated anatomical scalar field define seven values at each spatial location we present two new methods for visually representing dtis the first method displays an array of ellipsoids where the shape of each ellipsoid represents one tensor value the ellipsoids are all normalized to approximately the same size so that they can be displayed simultaneously in context the second method uses concepts from oil painting to represent the seven-valued data with multiple layers of varying brush strokes both methods successfully display most or all of the information in dtis and provide exploratory methods for understanding them the ellipsoid method has a simpler interpretation and explanation than the painting-motivated method the painting-motivated method displays more of the information and is easier to read quantatively we demonstrate the methods on images of the mouse spinal cord the visualizations show significant differences between spinal cords from mice suffering from experimental allergic encephalomyelitis and spinal cords from wild-type mice the differences are consistent with differences shown histologically and suggest that our new non-invasive imaging methodology and visualization of the results could have early diagnostic value for neurodegenerative diseases
the success of using a streamline technique for visualizing a vector field usually depends largely on the choice of adequate seed points g turk and d banks 1996 developed an elegant technique for automatically placing seed points to achieve a uniform distribution of streamlines on a 2d vector field their method uses an energy function calculated from the low-pass filtered streamline image to guide the optimization process of the streamline distribution this paper proposes a new technique for creating evenly distributed streamlines on 3d parametric surfaces found in curvilinear grids we make use of turk and banks's 2d algorithm by first mapping the vectors on a 3d surface into the computational space of the curvilinear grid to take into the consideration the mapping distortion caused by the uneven grid density in a curvilinear grid a new energy function is designed and used for guiding the placement of streamlines in the computational space with desired local densities
this paper presents a novel method to extract vortical structures from 3d cfd computational fluid dynamics vector_fields automatically it discusses the underlying theory and some aspects of theimplementation making use of higher-order derivatives the method is able to locate bent vortices in order to structure the recognition procedure we distinguish locating the core line from calculating attributes of strength and quality results are presented on several flow_fields from the field of turbomachinery
a fully automatic feature_detection algorithm is presented that locates and distinguishes lines of flow separation and attachment on surfaces in 3d numerical flow_fields the algorithm is based on concepts from 2d phase-plane analysis of linear vector_fields unlike prior visualization_techniques based on particle_tracing or flow topology the phase-plane algorithm detects separation using local analytic tests the results show that it not only detects the standard closed separation lines but also the illusive open separation lines which are not captured by flow topology methods
many high-performance isosurface_extraction algorithms have been proposed in the past several years as a result of intensive research efforts when applying these algorithms to large-scale time-varying fields the storage overhead incurred from storing the search index often becomes overwhelming this paper proposes an algorithm for locating isosurface cells in time-varying fields we devise a new data structure called the temporal hierarchical index tree which utilizes the temporal_coherence that exists in a time-varying field and adaptively coalesces the cells' extreme values over time the resulting extreme values are then used to create the isosurface cell search index for a typical time-varying scalar_data set not only does this temporal hierarchical index tree require much less storage space but also the amount of i/o required to access the indices from the disk at different time steps is substantially reduced we illustrate the utility and speed of our algorithm with data from several large-scale time-varying cfd simulations our algorithm can achieve more than 80% of disk-space savings when compared with the existing techniques while the isosurface_extraction time is nearly optimal
we present a novel out-of-core technique for the interactive computation of isosurfaces from volume data our algorithm minimizes the main memory and disk space requirements on the visualization workstation while speeding up isosurface_extraction queries our overall approach is a two-level indexing scheme first by our meta-cell technique we partition the original dataset into clusters of cells called meta-cells secondly we produce meta-intervals associated with the meta-cells and build an indexing data structure on the meta-intervals we separate the cell information kept only in meta-cells on disk from the indexing structure which is also on disk and only contains pointers to meta-cells our meta-cell technique is an i/o-efficient approach for computing a k-d-tree-like partition of the dataset our indexing data structure the binary blocked i/o interval tree is a new i/o-optimal data structure to perform stabbing queries that report from a set of meta-intervals or intervals those containing a query value q our tree is simpler to implement and is also more space-efficient in practice than existing structures to perform an isosurface query we first query the indexing structure and then use the reported meta-cell pointers to read from disk the active meta-cells intersected by the isosurface the isosurface itself can then be generated from active meta-cells rather than being a single cost indexing approach our technique exhibits a smooth trade-off between query time and disk space
we propose a new approach to polygonal isosurface_extraction that is based on extracting only the visible portion of the isosurface the visibility tests are done in two phases first coarse visibility tests are performed in software to determine the visible cells these tests are based on hierarchical tiles and shear-warp factorization the second phase resolves the visible portions of the extracted triangles and is accomplished by the graphics_hardware while the latest isosurface_extraction methods have effectively eliminated the search phase bottleneck the cost of constructing and rendering the isosurface remains high many of today's large_datasets contain very large and complex isosurfaces that can easily overwhelm even state-of-the-art graphics_hardware the proposed approach is output sensitive and is thus well suited for remote_visualization_applications where the extraction and rendering phases are done on a separate machines
in a large number of applications data is collected and referenced by their spatial locations visualizing large amounts of spatially referenced data on a limited-size screen display often results in poor visualizations due to the high degree of overplotting of neighboring datapoints we introduce a new approach to visualizing large amounts of spatially referenced data the basic idea is to intelligently use the unoccupied pixels of the display instead of overplotting data points after formally describing the problem we present two solutions which are based on placing overlapping data points on the nearest unoccupied pixel and shifting data points along a screen-filling curve eg hilbert-curve we then develop a more sophisticated approach called gridfit which is based on a hierarchical partitioning of the data space we evaluate all three approaches with respect to their efficiency and effectiveness and show the superiority of the gridfit approach for measuring the effectiveness we not only present the resulting visualizations but also introduce mathematical effectiveness criteria measuring properties of the generated visualizations with respect to the original data such as distance- and position-preservation
area cartograms are used for visualizing geographically distributed data by attaching measurements to regions of a map and scaling the regions such that their areas are proportional to the measured quantities a continuous area cartogram is a cartogram that is constructed without changing the underlying map topology we present a new algorithm for the construction of continuous area cartograms that was developed by viewing their construction as a constrained optimization problem the algorithm uses a relaxation method that exploits hierarchical resolution constrained dynamics and a scheme that alternates goals of achieving correct region areas and adjusting region shapes it is compared favorably to existing methods in its ability to preserve region shape recognition cues while still achieving highaccuracy
the paper discusses a concept for virtual_reality tools for use in design reviews of mechanical products in this discussion the special requirements of a virtual environment are given consideration the focus of this paper is on suggestions for the visualization and arrangement of a product its structure its components and their alternatives together in one environment the realization of these concepts results in a 3d-interface that allows users especially engineers to evaluate different configurations of a product and gives them direct access to the product structure by applying various visualization_techniques product components and their attributes eg their price can be brought together into one visualization thus in contrast to state-of-the-art software the product structure three-dimensional real-sized components and attribute values can be combined together in 3d-visualizations this research was done in cooperation with christoph brandt member of the heinz nixdorf institute's virtual_reality group
this paper presents efficient image-based_rendering techniques used in the context of an architectural walkthrough system portals doors and windows are rendered by warping layered depth images ldis in a preprocessing phase for every portal a number of pre-rendered images are combined into an ldi the resulting ldi stores exactly once all surfaces visible in at least one of the images used in the construction so most of the exposure errors are efficiently eliminated the ldi can be warped in the mcmillan occlusion compatible ordering a substantial increase in performance is obtained by warping in parallel our parallelization scheme achieves good load balancing scales with the number of processors and preserves the occlusion compatible ordering a fast conservative reference-image-space clipping algorithm also reduces the warping effort
this paper discusses techniques for visualizing structure in video data and other data sets that represent time snapshots of physical phenomena individual frames of a movie are treated as vectors and projected onto a low-dimensional subspace spanned by principal components movies can be compared and their differences visualized by analyzing the nature of the subspace and the projections of multiple movies onto the same subspace the approach is demonstrated on an application in neurobiology in which the electrical response of a visual cortex to optical stimulation is imaged onto a high-speed photodiode array to produce a cortical movie techniques for sampling movies over a single trial and multiple trials are discussed the approach provides the traditional benefits of principal_component_analysis compression noise reduction and classification and also allows the visual separation of spatial and temporal behavior
in this paper the motivation design and application of a distributed blackboard architecture for interactive data visualization is discussed the main advantages of the architecture are twofold first it allows visualization tools to be tightly integrated with simulations second it allows qualitative and quantitative analysis to be combined during the visualization process
splatting is a fast volume_rendering algorithm which achieves its speed by projecting voxels in the form of pre-integrated interpolation kernels or splats presently two main variants of the splatting algorithm exist i the original method in which all splats are composited back-to-front and ii the sheet-buffer method in which the splats are added in cache-sheets aligned with the volume face most parallel to the image plane which are subsequently composited back-to-front the former method is prone to cause bleeding artifacts from hidden objects while the latter method reduces bleeding but causes very visible color popping artifacts when the orientation of the compositing sheets changes suddenly as the image screen becomes more parallel to another volume face we present a new variant of the splatting algorithm in which the compositing sheets are always parallel to the image plane eliminating the condition for popping while maintaining the insensitivity to color bleeding this enables pleasing animated viewing of volumetric objects without temporal color and lighting discontinuities the method uses a hierarchy of partial splats and employs an efficient list-based volume traversal scheme for fast splat access it also offers moreaccuracy for perspective splatting as the decomposition of the individual splats facilitates a better approximation to the diverging nature of the rays that traverse the splatting kernels
we present an efficient and robust ray-casting algorithm for directly rendering a curvilinear volume of arbitrarily-shaped cells we designed the algorithm to alleviate the consumption of cpu power and memory space by incorporating the essence of the projection paradigm into the ray-casting process we have successfully accelerated the ray traversal through the grid and data interpolations at sample points our algorithm also overcomes the conventional limitation requiring the cells to be convex application of this algorithm to several commonly-used curvilinear data sets has produced a favorable performance when compared with recently reported algorithms
for high_quality rendering of objects segmented from tomographic volume data the precise location of the boundaries of adjacent objects in subvoxel resolution is required we describe a new method that determines the membership of a given sample point to an object by reclassifying the sample point using interpolation of the original intensity values and searching for the best fitting object in the neighbourhood using a ray-casting approach we then compute the surface location between successive sample points along the viewing-ray by interpolation or bisection the accurate calculation of the object boundary enables a much more precise computation of the gray-level-gradient yielding the surface normal our new approach significantly improves the quality of reconstructed and shaded surfaces and reduces aliasing artifacts for animations and magnified views we illustrate the results on different cases including the visible-human-data where we achieve nearly photo-realistic images
there are a variety of application areas in which there is a need for simplifying complex polygonal surface models these models often have material properties such as colors textures and surface normals our surface simplification algorithm based on iterative edge contraction and quadric error metrics can rapidly produce high_quality approximations of such models we present a natural extension of our original error metric that can account for a wide range of vertex attributes
we present a new approach for simplifying models composed of polygons or spline patches given an input model the algorithm computes a new representation of the model in terms of triangular bezier patches it performs a series of geometric operations consisting of patch merging and swapping diagonals and makes use of batch connectivity information to generate c-lods curved levels-of-detail each c-lod is represented using cubic triangular bezier patches the c-lods provide a compact representation for storing the model the algorithm tries to minimize the surface deviation error and maintains continuity at patch boundaries given the clods the algorithm can generate their polygonal approximations using static and dynamic tessellation schemes it has been implemented and we highlight its performance on a number of polygonal and spline models
conventional wisdom says that in order to produce high-quality simplified polygonal models one must retain and use information about the original model during the simplification process we demonstrate that excellent simplified models can be produced without the need to compare against information from the original geometry while performing local changes to the model we use edge collapses to perform simplification as do a number of other methods we select the position of the new vertex so that the original volume of the model is maintained and we minimize the per-triangle change in volume of the tetrahedra swept out by those triangles that are moved we also maintain surface area near boundaries and minimize the per-triangle area changes calculating the edge collapse priorities and the positions of the new vertices requires only the face connectivity and the the vertex locations in the intermediate model this approach is memory efficient allowing the simplification of very large polygonal models and it is also fast moreover simplified models created using this technique compare favorably to a number of other published simplification methods in terms of mean geometric error
we present a method for the construction of multiple levels of tetrahedral_meshes approximating a trivariate function at different levels of detail starting with an initial high-resolution triangulation of a three-dimensional region we construct coarser representation levels by collapsing tetrahedra each triangulation defines a linear spline function where the function values associated with the vertices are the spline coefficients based on predicted errors we collapse tetrahedron in the grid that do not cause the maximum error to exceed a use-specified threshold bounds are stored for individual tetrahedra and are updated as the mesh is simplified we continue the simplification process until a certain error is reached the result is a hierarchical_data description suited for the efficient visualization of large_data sets at varying levels of detail
this paper presents techniques for interactively visualizing tensor_fields using deformations the conceptual idea behind this approach is to allow the tensor_field to manifest its influence on idealized objects placed within the tensor_field this is similar though not exactly the same to surfaces deforming under load in order to relieve built up stress and strain we illustrate the effectiveness of the deviator-isotropic tensor_decomposition in deformation visualizations of cfd strain rate we also investigate how directional flow techniques can be extended to distinguish between regions of tensile versus compressive forces
visualization of three-dimensional steady flow has to overcome a lot of problems to be effective among them are occlusion of distant details lack of directional and depth hints and occlusion we present methods which address these problems for real-time graphic representations applicable in virtual environments we use dashtubes ie animated opacity-mapped streamlines as a visualization icon for 3d-flow_visualization we present a texture_mapping technique to keep the level of texture detail along a streamline nearly constant even when the velocity of the flow varies considerably an algorithm is described which distributes the dashtubes evenly in space we apply magic_lenses and magic boxes as interaction techniques for investigating densely filled areas without overwhelming the observer with visual detailimplementation details of these methods and their integration in our virtual environment conclude the paper
we develop multiresolution_models for analyzing and visualizing two-dimensional flows over curvilinear grids our models are based upon nested spaces of piecewise defined functions defined over nested curvilinear grid domains the nested domains are selected so as to maintain the original geometry of the inner boundary we first give the refinement and decomposition equations for haar wavelets over these domains next using lifting techniques we develop and show examples of piecewise linear wavelets over curvilinear grids
transfer_function_design is an integrated component in volume_visualization and data_exploration the common trial-and-error approach for transfer_function searching is a very difficult and time consuming process a goal oriented and parameterized transfer_function model is therefore crucial in guiding the transfer_function searching process for better and more meaningful visualization results the paper presents an image based transfer_function model that integrates 3d image_processing tools into the volume_visualization pipeline to facilitate the search for an image based transfer_function in volume data visualization and exploration the model defines a transfer_function as a sequence of 3d image_processing procedures and allows the users to adjust a set of qualitative and descriptive parameters to achieve their subjective visualization goals 3d image_enhancement and boundary detection tools and their integration methods with volume_visualization algorithms are described the application of this approach for 3d microscopy data_exploration and analysis is also discussed
we attack the problem of image based rendering with occlusions and general camera motions by using distorted multiperspective images such images provide multiple viewpoint photometry similar to the paintings of cubist artists we take scene geometry in contrast to be embodied in mappings of viewing rays from their original 3d intercepts into the warped multiperspective image space this approach allows us to render approximations of scenes with occlusions using time dense and spatially sparse sequences of camera rays which is a significant improvement over the storage requirements of an equivalent animation sequence additional data_compression can be achieved using sparse time keyframes as well interpolating the paths of sparse time key rays correctly in image space requires singular interpolation functions with spatial discontinuities while there are many technical questions yet to be resolved the employment of these singular interpolation functions in the multiperspective image space appears to be of potential interest for generating general viewpoint scene renderings with minimal data storage
one common problem in the practical application of volume_visualization is the proper choice of transfer_functions in order to color different parts of the volume meaningfully this interactive process can be very complicated and time consuming an alternative to the adjustment of transfer_functions is the application of segmentation algorithms these algorithms are often dedicated to a limited range of data sets and tend to be very compute intensive we propose a morphology based hierarchical analysis to estimate the optical properties of the volume to be rendered this approach requires fewer parameters and incorporates also spatial information but it is far less compute intensive than most of the segmentation methods the hierarchical analysis is constructed in analogy to the wavelet analysis except for the fact that nonlinear filters are used in our case these morphological operators have a lower distortional influence on the analyzed structures than the usual linear filters a special decomposition of the morphological operators is discussed which leads to an efficientimplementation of this approach this technique reduces the three dimensional analysis to a one dimensional computation as it is done in tensor product based linear filters the resulting decomposition may also be parallelized easily we demonstrate the usefulness of the proposed technique by applying it to medical and technical data sets
large textures cause bottlenecks in real time applications that often lead to a loss of interactivity these performance bottlenecks occur because of disk and network transfer texture translation and memory swapping we present a software solution that alleviates the problems associated with large textures by treating texture as a bandwidth limited resource rather than a finite resource as a result the display of large textures is reduced to a caching problem in which texture memory serves as the primary cache for texture data main memory the secondary cache and local disk the tertiary cache by using this cache hierarchy applications are able to maintain real time performance while displaying textures hundreds of times larger than can fit into texture memory
rendering objects transparently gives additional insight in complex and overlapping structures however traditional techniques for the rendering of transparent objects such as alpha blending are not very well suited for the rendering of multiple transparent objects in dynamic scenes screen door transparency is a technique to render transparent objects in a simple and efficient way no sorting is required and intersecting polygons can be handled without further preprocessing with this technique polygons are rendered through a mask only where the mask is present pixels are set however artifacts such as incorrect opacities and distracting patterns can easily occur if the masks are not carefully designed the requirements on the masks are considered next three algorithms are presented for the generation of pixel masks one algorithm is designed for the creation of small eg 4×4 masks the other two algorithms can be used for the creation of larger masks eg 32×32 for each of these algorithms results are presented and discussed
spot noise and line_integral_convolution lic are two texture_synthesis techniques for vector_field_visualization the two techniques are compared continuous directional convolution is used as a common basis for comparing the techniques it is shown that the techniques are based on the same mathematical concept comparisons of the visual appearance of the output and performance of the algorithms are made
we propose a general paradigm for computing optimal coordinate frame fields that may be exploited to visualize curves and surfaces parallel transport framings which work well for open curves generally fail to have desirable properties for cyclic curves and for surfaces we suggest that minimal quaternion measure provides an appropriate heuristic generalization of parallel transport our approach differs from minimal tangential acceleration approaches due to the addition of "sliding ring" constraints that fix one frame axis but allow an axial rotational freedom whose value is varied in the optimization process our fundamental tool is the quaternion gauss map a generalization to quaternion space of the tangent map for curves and of the gauss map for surfaces the quaternion gauss map takes 3d coordinate frame fields for curves and surfaces into corresponding curves and surfaces constrained to the space of possible orientations in quaternion space standard optimization tools provide application specific means of choosing optimal eg length- or area-minimizing quaternion frame fields in this constrained space
many real world polygonal surfaces contain topological singularities that represent a challenge for processes such as simplification compression smoothing etc we present an algorithm for removing such singularities thus converting non manifold sets of polygons to manifold polygonal surfaces orientable if necessary we identify singular vertices and edges multiply singular vertices and cut through singular edges in an optional stitching phase we join surface boundary edges that were cut or whose endpoints are sufficiently close while guaranteeing that the surface is a manifold we study two different stitching strategies called "edge pinching" and "edge snapping" when snapping special care is required to avoid re-creating singularities the algorithm manipulates the polygon vertex indices surface topology and essentially ignores vertex coordinates surface geometry except for the optional stitching the algorithm has a linear complexity in the number of vertices edges and faces and require no floating point operation
we consider interpolation between keyframe hierarchies we impose a set of weak constraints that allows smooth interpolation between two keyframe hierarchies in an animation or more generally allows the interpolation in an n-parameter family of hierarchies we use hierarchical triangulations obtained by the rivara element bisection algorithm m rivara 1984 and impose a weak compatibility constraint on the set of root elements of all keyframe hierarchies we show that the introduced constraints are rather weak the strength of our approach is that the interpolation works in the class of conforming triangulations and simplifies the task of finding the intermediate hierarchy which is the union of the two or more keyframe hierarchies involved in the interpolation process this allows for an efficient generation of the intermediate connectivity and additionally ensures that the intermediate hierarchy is again a conforming hierarchy satisfying the same constraints
the paper describes some fundamental issues for robustimplementations of progressively refined tetrahedralizations generated through sequences of edge collapses we address the definition of appropriate cost functions and explain on various tests which are necessary to preserve the consistency of the mesh when collapsing edges although considered a special case of progressive simplicial complexes j popovic and h hoppe 1997 the results of our method are of high practical importance and can be used in many different applications such as finite_element meshing scattered_data_interpolation or rendering of unstructured volume data
efforts to create highly generic visualizations both content and interface often when applied to non research oriented or operational activities are composed of several goals although these goals may appear to be related they are often composed of distinct tasks generic solutions even if domain-specific may lack sufficient focus to be effective for such purposes the design of different visualization tools matched to a set of tasks but built on top of a common framework with a similar approach to content is a promising alternative this hypothesis is tested in detail by application to a demanding problem-operational weather forecasting
this case_study describes the design and development of visor visual integration of simulated and observed results a tool which supports the visualization and analysis of a wide variety of data relevant to aerospace engineering design integrating data from such disparate sources is challenging overcoming the obstacles results in a powerful tool the process has also been valuable in exposing requirements for the libraries of reusable software tools for visualization and data_analysis being developed at nasa ames
the paper describes the architecture of a data level comparative_visualization system and experiences using it to study computational fluid dynamics data and experimental wind tunnel data we illustrate how the system can be used to compare data sets from different sources data sets with different resolutions and data sets computed using different mathematical models of fluid flow suggested improvements to the system based on user feedback are also discussed
vortices are important features in many research and engineering fields visualization is an important step in gaining more understanding and control of vortices vortex_detection criteria fall into two categories point based scalar quantities calculated at single points and curve based geometric criteria calculated for eg streamlines the first category is easy to compute but does not work in all cases the second category is more intuitive and should work in all cases but currently only works in 2d or 3d projected flows we show applications of both approaches in hydrodynamic flows
medical image_analysis is shifting from current film oriented light screen environments to computer environments that involve viewing and analyzing large sets of images on a computer screen magnetic_resonance_imaging mri studies in particular can involve many images the paper examines how best to meet the needs of radiologists in a computational environment to this end a field study was conducted to observe radiologists' interactions during mri analysis in the traditional light screen environment key issues uncovered involve control over focus_and_context dynamic grouping of images and retrieval of images and image groups to address the problem of focus_and_context existing layout adjustment and magnification techniques are explored to provide the most appropriate solution our interest is in combining the methodologies of human computer interaction studies with computational presentation possibilities to design a visual environment for the crucial field of medical image_analysis
the anterior surface of the eye 'cornea' is extremely important for good sight instruments measuring corneal shape conventionally visualize the surface characteristics by mapping the instantaneous radius of curvature onto a rainbow colour scale this technique is known to have important drawbacks firstly not corneal shape itself is visualized but rather second order surface properties secondly the type of colouring produces well documented artifacts we discuss visualization_techniques for a more direct representation of the data in a three part display shape deviations are presented as a height surface in one window height lines superimposed over the input image in another and a colour mapped representation of the mean normal radius of curvature in a third with the aid of some typical examples it is shown that these visualizations are easy to interpret by the physician and overcome the limitations of the conventional techniques
the paper details the use of a virtual environment for reconstructive surgery vers in the case of a 17 year-old boy with a severe facial defect arising from the removal of a soft tissue tumor computed_tomography ct scans were taken of the patient the data were segmented a mesh was generated and this patient-specific mesh was used in a virtual environment by the surgeons for preoperative visualization of the defect planning of the surgery and production of a custom surgical template to aid in repairing the defect the paper details the case of this patient provides a background on the virtual environment technology used discusses the difficulties encountered and describes the lessons learned
virtual angioscopy is a non invasive medical procedure for exploring parts of the human vascular system we have developed an interactive tool that takes as input data acquired with standard medical imaging modalities and regards it as a virtual environment to be interactively inspected the system supports real time navigation with stereoscopic direct_volume_rendering and dynamic endoscopic camera_control interactive tissue classification and interactive point picking for morphological feature measurement we provide an overview of the system discuss the techniques used in our prototype and present experimental results on human data sets
a new tool for real time visualization of acoustic sound fields has been developed for a new sound spatialization theatre the theatre is described and several applications of the acoustic and volumetric modeling software are presented the visualization system described is a valuable tool for spatial sound researchers sound engineers and composers using cnmat's sound spatialization theatre further work is in progress on the adaptation of better acoustic_simulation methods m monks et al 1996 for more accurate display of the quality of the reverberant field the room database will be automatically extracted from a model built with 3d modeling software volume_visualization strategies are being explored to display sounds in spectral and impulse response form
this paper presents a tool for the visual_exploration of dna_sequences represented as h-curves although very long sequences can be plotted using h-curves micro-features are lost as sequences get longer we present a new three-dimensional distortion algorithm to allow the magnification of a sub-segment of an h-curve while preserving a global view of the curve this is particularly appropriate for h-curves as they provide useful visual information at several resolutions our approach also extends the current possibilities of detail-in-context viewing in 3d it provides a non-occludingorthogonal technique that preserves uniform scaling within regions and maintains geometric continuity between regions
a computer animated movie was produced illustrating both 2d and 3d hilbert curves and showing the transition from 2d to 3d with the help of volume_rendering
this paper describes a project that combined physical model fabrication and virtual computer-based data display to create a unique visualization presentation usgs terrain information on prince of wales island alaska was used to create a physical prototype in sdsc's telemanufacturing facility this model was then used as a mold to create a translucent plate of the terrain finally deforestation data from the island was color mapped and rear-projected onto the translucent plate within a light box the result is a very compelling display in which both the senses of sight and touch are used to make relationships between terrain features and the data more readily apparent
this paper considers how out-of-core visualization applies to terrain datasets which are among the largest now presented for interactive_visualization and can range to sizes of 20 gb and more it is found that a combination of out-of-core visualization which tends to focus on 3d data and visual simulation which places an emphasis on visual perception and real-time display of multiresolution data results in interactive terrain_visualization with significantly improved data access and quality of presentation further the visual simulation approach provides qualities that are useful for general data not just terrain
the delivery of the first one tera-operations/sec computer has significantly impacted production data visualization affecting data transfer post processing and rendering terascale computing has motivated a need to consider the entire data visualization system improving a single algorithm is not sufficient this paper presents a systems approach to decrease by a factor of four the time required to prepare large_data sets for visualization for daily production use all stages in the processing pipeline from physics simulation code to pixels on a screen must be balanced to yield good overall performance performance of the initial visualization system is compared with recent improvements "lessons learned" from the coordinated deployment of improved algorithms also are discussed including the need for 64 bit addressing and a fully parallel data visualization pipeline
in this paper we describe a battlefield visualization system called dragon which we have implemented on a virtual_reality responsive_workbench the dragon system has been successfully deployed as part of two large military exercises the hunter warrior advanced warfighting experiment in march 1997 and the joint counter mine advanced concept tactical demonstration in august and september 1997 we describe battlefield visualization the dragon system and the workbench and we describe our experiences as part of these two real-world deployments with an emphasis on lessons learned and needed future work
sediments in many parts of the new york and new jersey estuary system are contaminated with toxic organic and inorganic compounds by different sources because of the potential environmental consequences detailed information on the spatial distribution of sediment contaminants is essential in order to carry out routine shipping channel dredging in an environmentally responsible way and to remediate hot spots cost-effectively and safely scientific_visualization and scatter data modeling techniques have been successfully applied in analyzing the sparse_sampling data of sediment contaminants in new york and new jersey estuaries the underlying spatial characteristics of which are otherwise difficult to comprehend continuous realizations of contaminant concentrations in the region were obtained by using a spectral domain-decomposition scattered data model and ibm data explorer which is a software package for scientific data visualization
global circulation models are used to gain an understanding of the processes that affect the earth's climate and may ultimately be used to assess the impact of humanity's activities on it the pop ocean model developed at los alamos is an example of such a global circulation model that is being used to investigate the role of the ocean in the climate system data output from pop has traditionally been visualized using video technology which precludes rapid modification of visualization parameters and techniques this paper describes a visualization system that leverages high speed graphics_hardware specifically texture_mapping hardware to accelerate data_exploration to interactive rates we describe the design of the system the specific hardware features used and provide examples of its use the system is capable of viewing ocean circulation simulation results at up to 60 frames per second while loading texture memory at approximately 72 million texels per second
visualization and quantification methods are being developed to analyze our acoustic images of thermal plumes containing metallic mineral particles that discharge from hot springs on the deep seafloor the acoustic images record intensity of backscattering from the particulate matter suspended in the plumes the visualization methods extract classify visualize measure and track reconstructions of the plumes depicted by isointensity surfaces as 3d volume objects and 2d slices the parameters measured including plume volume cross sectional area centerline location trajectory surface area and isosurfaces at percentages of maximum backscatter intensity are being used to derive elements of plume behavior including expansion with height dilution and mechanisms of entrainment of surrounding seawater our aim is to compare the observational data with predictions of plume theory to test and advance models of the behavior of hydrothermal plumes through the use of multiple representations
the development of a high speed multi-frequency continuous scan sonar at sonar research & development ltd has resulted in the acquisition of extremely accurate high resolution bathymetric data this rich underwater data provides new challenges and possibilities within the field of seabed visualization this paper introduces the reader to seabed visualization by describing two example case studies which use the seabed visualization system developed at srd both case studies harbour wall and shipwreck visualization are implemented using real survey data the high resolution of the data obtained means slight changes in the seabed topography are easily distinguishable annual survey inspections in both case studies enable comparisons to be made between the data sets making the visualization system an important tool for management and planning
we are studying difficult geometric problems in computer-aided mechanical design where visualization plays a key role the research addresses the fundamental design task of contact analysis deriving the part contacts and the ensuing motion constraints in a mechanical system we have automated contact analysis of general planar systems via configuration space computation configuration space is a geometric representation of rigid-body interaction that encodes quantitative information such as part motion paths and qualitative information such as system failure modes the configuration space dimension equals the number of degrees of freedom in the system three-dimensional spaces are most important but higher-dimensions are often useful the qualitative aspects which relate to the topology of the configuration space are best understood by visualization we explain what configuration space is how it encodes contact information and what research challenges it poses for visualization
this case_study describes a technique for the three-dimensional analysis of the internal microscopic structure microstructure of materials this technique consists of incrementally polishing through a thin layer approximately 02 μm of material chemically etching the polished surface applying reference marks and performing optical or scanning electron microscopy on selected areas the series of images are then processed employing avs and other visualization software to obtain a 3d_reconstruction of the material we describe how we applied this technique to an alloy steel to study the morphology connectivity and distribution of cementite precipitates formed during thermal processing the results showed microstructural features not previously identified with traditional 2d techniques
we describe an aircraft design problem in high dimensional space with d typically being 10 to 30 in some respects this is a classic optimization problem where the goal is to find the point that minimizes an objective function while satisfying a set of constraints however evaluating an individual point is expensive and the high dimensionality makes many approaches to solving the problem infeasible the difficulty of the problem means that aircraft designers would benefit from any insights that can be provided we discuss how simple visualizations have already proved beneficial and then describe how visualization might be of further help in the future
we show that it is feasible to perform interactive isosurfacing of very large rectilinear datasets with brute-force ray tracing on a conventional distributed shared-memory multiprocessor machine rather than generate geometry representing the isosurface and render with a z-buffer for each pixel we trace a ray through a volume and do an analytic isosurface intersection computation although this method has a high intrinsic computational cost its simplicity and scalability make it ideal for large_datasets on current high-end systems incorporating simple optimizations such as volume_bricking and a shallow hierarchy enables interactive rendering ie 10 frames per second of the 1 gbyte full resolution visible woman dataset on an sgi reality monster the graphics capabilities of the reality monster are used only for display of the final color image
we examine how animating a viewpoint change in a spatial information system affects a user's ability to build a mental_map of the information in the space we found that animation improves users' ability to reconstruct theinformation_space with no penalty on task_performance time we believe that this study provides strong evidence for adding animated transitions in many applications with fixed spatial_data where the user navigates around the data space
constellation is a visualization system for the results of queries from the mindnet natural language semantic_network constellation is targeted at helping mindnet's creators and users refine their algorithms as opposed to understanding the structure of language we designed a special-purpose graph_layout_algorithm which exploits higher-level structure in addition to the basic node and edge connectivity our layout prioritizes the creation of a semantic space to encode plausibility instead of traditional graph_drawing metrics like minimizing edge crossings we make careful use of several perceptual channels both to minimize the visual impact of edge crossings and to emphasize highlighted constellations of nodes and edges
this paper describes concepts that underlie the design andimplementation of an information exploration system that allows users to impose arbitrary hierarchical organizations on their data such hierarchies allow a user to embed important semantic information into the hierarchy definition our goal is to recognize the significance of this implicit information and to utilize it in the hierarchy_visualization the innovative features of our system include the dynamic modification of the hierarchy definitions and the definition andimplementation of a set of layout_algorithms that utilize semantic information implicit in the tree construction
automation has arrived to parallel_coordinates a geometrically motivated classifier is presented and applied with both training and testing stages to 3 real datasets our results compared to those from 33 other classifiers have the least error the algorithm is based on parallel_coordinates and has very low computational complexity in the number of variables and the size of the dataset-contrasted with the very high or unknown often unstated complexity of other classifiers the low complexity enables the rule derivation to be done in near real-time hence making the classification adaptive to changing conditions provides comprehensible and explicit rules-contrasted to neural_networks which are “black boxes” does dimensionality selection-where the minimal set of original variables not transformed new variables as in principal_component_analysis required to state the rule is found orders these variables so as to optimize the clarity of separation between the designated set and its complement-this solves the pesky “ordering problem” in parallel_coordinates the algorithm is display independent hence it can be applied to very large in size and number of variables datasets though it is instructive to present the results visually the input size is no longer display-limited as for visual_data_mining
this paper aims to give a systematic account of focus+context_visualization_techniques ie visualizations which aim to give users integrated visual access to details and context in a data set we introduce the notion that there are different orders of information_visualization with focus+context being a second-order visualization and provide a formal framework for describing and constructing focus+context_visualizations
a similarity metric based on the low-level content of images can be used to create a visualisation in which visually similar images are displayed close to each other we are carrying out a series of experiments to evaluate the usefulness of this type of visualisation as an image browsing aid the initial experiment described considered whether people would find a given photograph more quickly in a visualisation than in a randomly arranged grid of images the results show that the subjects were faster with the visualisation although in post-experiment interviews many of them said that they preferred the clarity and regularity of the grid we describe an algorithm with which the best aspects of the two layout types can be combined
this paper introduces the sunflower visual metaphor for information_visualization the visual metaphor is presented as an alternative to current techniques of dimensional compression and the visualization tools that employ them the paper discusses the motivation for the sunflower paradigm itsimplementation and criticalfactors for producing an effective visualization a primary driver in this research effort has been to develop a visualization tool that facilitates browsing knowledge_discovery and that supports learning through sense making and integration of new information
many real-world kdd knowledge_discovery & data_mining applications involve the navigation of large_volumes of information on the web such as internet resources hot topics and telecom phone switches quite often users feel lost confused and overwhelmed with displays that contain too much information this paper discusses a new content-driven visual mining infrastructure called vismine that uses several innovative techniques 1 hidden visual_structure and relationships for uncluttering displays 2 simultaneous visual presentations for high-dimensional knowledge_discovery and 3 a new visual interface to plug in existing graphic toolkits for expanding its use in a wide variety of visual applications we have applied this infrastructure to three data_mining visualization_applications-topic hierarchy for document navigation web-based trouble shooting and telecom switch mining
in the process of knowledge_discovery workers examine available information in order to make sense of it by sensemaking we mean interacting with and operating on the information with a variety of information processing mechanisms previously we introduced a concept that uses the spreadsheet metaphor with cells containing visualizations of complex data we extend and apply a cognitive model called “visual sensemaking” to the visualization spreadsheet we use the task of making sense of a large web site as a concrete example throughout the paper for demonstration using a variety of visualization_techniques such as the disk tree and cone tree we show that the interactions of the visualization spreadsheet help users draw conclusions from the overall relationships of the entire information set
a new method is presented to get an insight into univariate time_series_data the problem addressed is how to identify patterns and trends on multiple time scales days weeks seasons simultaneously the solution presented is to cluster similar daily data patterns and to visualize the average patterns as graphs and the corresponding days on a calendar this presentation provides a quick insight into both standard and exceptional patterns furthermore it is well suited to interactive_exploration two applications numbers of employees present and energy_consumption are presented
interactive selection is a critical component in exploratory_visualization allowing users to isolate subsets of the displayed information for highlighting deleting analysis or focussed investigation brushing a popular method for implementing the selection process has traditionally been performed in either screen space or data space we introduce the concept of a structure-based brush which can be used to perform selection in hierarchically structured data sets our structure-based brush allows users to navigate hierarchies by specifying focal extents and level-of-detail on a visual representation of the structure proximity-based coloring which maps similar colors to data that are closely related within the structure helps convey both structural relationships and anomalies we describe the design andimplementation of our structure-based brushing tool we also validate its usefulness using two distinct hierarchical visualization_techniques namely hierarchical parallel_coordinates and tree-maps
this paper proposes a new technique to visualize dependencies among cells in a spreadsheet in this way the system firstly visualizes a spreadsheet on a plane in three-dimensional space and draws arcs between interrelated cells by allowing a user to select an arbitrary cell and lift it up with direct_manipulation the system utilizes the third dimension to ameliorate visual occlusion of crossing arcs as the user lifts a focused cell up the interrelated cells are lifted up together thus hidden dataflow networks can be visually intelligible interactively because spreadsheets are aimed at calculation itself rather than appearances of outputs their mechanism is relatively invisible and not obvious for ordinary users our visualization helps such users to understand structures and mechanism of spreadsheets
the advent of superscalar processors with out-of-order execution makes it increasingly difficult to determine how well an application is utilizing the processor and how to adapt the application to improve its performance we describe a visualization system for the analysis of application behavior on superscalar processors our system provides an overview-plus-detail display of the application's execution a timeline view of pipeline performance data shows the overall utilization of the pipeline this information is displayed using multiple time scales enabling the user to drill down from a high-level application overview to a focus region of hundreds of cycles this region of interest is displayed in detail using an animated cycle-by-cycle view of the execution this view shows how instructions are reordered and executed and how functional units are being utilized additional context views correlate instuctions in this detailed view with the relevant source code for the application this allows the user to discover the root cause of the poor pipeline utilization and make changes to the application to improve its performance this visualization system can be easily configured to display a variety of processor models and configurations we demonstrate it for both the mxs and mmix processor models
domain analysis for data visualization dadv is a technique to use when investigating a domain where data visualizations are going to be designed and added to existing software systems dadv was used to design the data visualization in viseio-lca which is a framework to visualize environmental data about products most of the visualizations are designed using the following stages formatting data in tables selecting visual_structures and rendering the data on the screen although many visualization authors perform implicit domain analysis in this paper domain analysis is added explicitly to the process of designing visualizations with the goal of producing move usable software tools environmental life-cycle assessment lca is used as a test bed for this technique
we have developed a technique aggregate towers that allows geospatial_data to be visualized across a range of map scales we use a combination of data_aggregation algorithms and dynamically aggregating data markers eg icons or symbols to accommodate interactive zooming by a user while maintaining a representation that remains intuitive consistent across multiple scales and uncluttered this approach implicitly generates multiple levels of overview displays from a single set of underlying data
an association rule in data_mining is an implication of the form x→y where x is a set of antecedent items and y is the consequent item for years researchers have developed many tools to visualize association rules however few of these tools can handle more than dozens of rules and none of them can effectively manage rules with multiple antecedents thus it is extremely difficult to visualize and understand the association information of a large_data set even when all the rules are available this paper presents a novel visualization_technique to tackle many of these problems we apply the technology to a text mining study on large corpora the results indicate that our design can easily handle hundreds of multiple antecedent association rules in a three-dimensional display with minimum human interaction low occlusion percentage and no screen swapping
dynamic queries offer continuous feedback during range queries and have been shown to be effective and satisfying recent work has extended them to datasets of 100000 objects and separately to queries involving relations among multiple objects the latter work enables filtering houses by properties of their owners for instance our primary concern is providing feedback from histograms during dynamic_query the height of each histogram bar shows the count of selected objects whose attribute value falls into a given range unfortunately previous efficient algorithms for single object queries overcount in the case of multiple objects if for instance a house has multiple owners this paper presents an efficient algorithm that with high probability closely approximates the true counts
visageweb is an information-centric user interface to the world_wide_web built within the visage data visualization environment this paper traces the development of the visageweb project using it to motivate an exploration of how an information-centric architecture copes with new visualization challenges we conclude with a presentation of the visageweb prototype itself
a new method is presented for the visualization of hierarchical information such as directory structures and organization structures cushion treemaps inherit the elegance of standard treemaps compact space-filling displays of hierarchical information based on recursive subdivision of a rectangular image space intuitive shading is used to provide insight in the hierarchical structure during the subdivision ridges are added per rectangle which are rendered with a simple shading model the result is a surface that consists of recursive cushions the method is efficient effective easy to use and implement and has a wide applicability
presents a method for the hierarchical_representation of vector_fields our approach is based on iterative refinement using clustering and principal_component_analysis the input to our algorithm is a discrete set of points with associated vectors the algorithm generates a top-down segmentation of the discrete field by splitting clusters of points we measure the error of the various approximation levels by measuring the discrepancy between streamlines generated by the original discrete field and its approximations based on much smaller discrete data sets our method assumes no particular structure of the field nor does it require any topological connectivity information it is possible to generate multi-resolution representations of vector_fields using this approach
presents a system designed for the interactive definition and visualization of fields derived from large_data sets the demand-driven visualizer ddv the system allows the user to write arbitrary expressions to define new fields and then apply a variety of visualization_techniques to the result expressions can include differential operators and numerous other built-in functions determination of field values both in space and in time is directed automatically by the demands of the visualization_techniques the payoff of following a demand-driven design philosophy throughout the visualization system becomes particularly evident when working with large time-series_data where the costs of eager evaluation alternatives can be prohibitive
vector_field_visualization remains a difficult task many local and global visualization methods for vector_fields such as flow data exist but they usually require extensive user_experience on setting the visualization parameters in order to produce images communicating the desired insight we present a visualization method that produces simplified but suggestive images of the vector field automatically based on a hierarchical_clustering of the input data the resulting clusters are then visualized with straight or curved arrow icons the presented method has a few parameters with which users can produce various simplified vector_field_visualizations that communicate different insights on the vector data
our ability to accumulate large complex multivariate data sets has far exceeded our ability to effectively process them in searching for patterns anomalies and other interesting features conventional multivariate visualization_techniques generally do not scale well with respect to the size of the data set the focus of this paper is on the interactive_visualization of large multivariate data sets based on a number of novel extensions to the parallel_coordinates display technique we develop a multi-resolution view of the data via hierarchical_clustering and use a variation of parallel_coordinates to convey aggregation information for the resulting clusters users can then navigate the resulting structure until the desired focus region and level of detail is reached using our suite of navigational and filtering tools we describe the design andimplementation of our hierarchical parallel_coordinates system which is based on extending the xmdvtool system lastly we show examples of the tools and techniques applied to large hundreds of thousands of records multivariate data sets
complex triangle meshes arise naturally in many areas of computer_graphics and visualization previous work has shown that a quadric error metric allows fast and accurate geometric simplification of meshes this quadric approach was recently generalized to handle meshes with appearance attributes in this paper we present an improved quadric error metric for simplifying meshes with attributes the new metric based on geometric correspondence in 3d requires less storage evaluates more quickly and results in more accurate simplified meshes meshes often have attribute discontinuities such as surface creases and material boundaries which require multiple attribute vectors per vertex we show that a wedge-based mesh data structure captures such discontinuities efficiently and permits simultaneous optimization of these multiple attribute vectors in addition to the new quadric metric we experiment with two techniques proposed in geometric simplification memoryless simplification and volume preservation and show that both of these are beneficial within the quadric framework the new scheme is demonstrated on a variety of meshes with colors and normals
we present a method for compressing non-manifold polygonal_meshes ie polygonal_meshes with singularities which occur very frequently in the real-world most efficient polygonal compression methods currently available are restricted to a manifold mesh they require a conversion process and fail to retrieve the original model connectivity after decompression the present method works by converting the original model to a manifold model encoding the manifold model using an existing mesh_compression technique and clustering or stitching together during the decompression process vertices that were duplicated earlier to faithfully recover the original connectivity this paper focuses on efficiently encoding and decoding the stitching information by separating connectivity from geometry and properties the method avoids encoding vertices and properties bound to vertices multiple times thus a reduction of the size of the bit-stream of about 10% is obtained compared with encoding the model as a manifold
for types of data visualization where the cost of producing images is high and the relationship between the rendering parameters and the image produced is less than obvious a visual representation of the exploration process can make the process more efficient and effective image graphs represent not only the results but also the process of data visualization each node in an image graph consists of an image and the corresponding visualization parameters used to produce it each edge in a graph shows the change in rendering parameters between the two nodes it connects image graphs are not just static representations users can interact with a graph to review a previous visualization session or to perform new rendering operations which cause changes in rendering parameters can propagate through the graph the user can take advantage of the information in image graphs to understand how certain parameter changes affect visualization results users can also share image graphs to streamline the process of collaborative_visualization we have implemented a volume_visualization system using the image graph interface and the examples presented come from this application
we present a novel forward image mapping algorithm which speeds up perspective warping as in texture_mapping it processes the source image in a special scanline order instead of the normal raster scanline order this special scanline has the property of preserving parallelism when projecting to the target image the algorithm reduces the complexity of perspective-correct image warping by eliminating the division per pixel and replacing it with a division per scanline the method also corrects the perspective distortion in gouraud shading with negligible overhead furthermore the special scanline order is suitable for antialiasing using a more accurate antialiasing conic filter with minimum additional cost the algorithm is highlighted by incremental calculations and optimized memory bandwidth by reading each source pixel only once suggesting a potential hardwareimplementation
often images or datasets have to be compared to facilitate choices of visualization and simulation parameters respectively common comparison techniques include side-by-side viewing and juxtaposition in order to facilitate visual verification of verisimilitude we propose quantitative techniques which accentuate differences in images and datasets the comparison is enabled through a collection of partial metrics which essentially measure the lack of correlation between the datasets or images being compared that is they attempt to expose and measure the extent of the inherent structures in the difference between images or datasets besides yielding numerical attributes the metrics also produce images which can visually highlight differences our metrics are simple to compute and operate in the spatial domain we demonstrate the effectiveness of our metrics through examples for comparing images and datasets
we present a technique for optimizing the rendering of high-depth complexity scenes prioritized-layered projection plp does this by rendering an estimation of the visible set the novelty in our work lies in the fact that we do not explicitly compute visible sets instead our work is based on computing on demand a priority order for the polygons that maximizes the likelihood of rendering visible polygons before occluded ones for any given scene given a fixed budget eg time or number of triangles our rendering algorithm makes sure to render geometry respecting the computed priority there are two main steps to our technique 1 an occupancy based tessellation of space and 2 a solidity based traversal algorithm plp works by computing an occupancy based tessellation of space which tends to have smaller cells where there are more geometric primitives eg polygons in this spatial tessellation each cell is assigned a solidity value which is directly proportional to its likelihood of occluding other cells in its simplest form a cell's solidity value is directly proportional to the number of polygons contained within it during our traversal algorithm cells are marked for projection and the geometric primitives contained within them actually rendered the traversal algorithm makes use of the cells' solidity and other view-dependent information to determine the ordering in which to project cells by tailoring the traversal algorithm to the occupancy based tessellation we can achieve very good frame rates with low preprocessing and rendering costs we describe our technique and itsimplementation in detail we also provide experimental evidence of its performance and briefly discuss extensions of our algorithm
view-dependent simplification has emerged as a powerful tool for graphics acceleration in visualization of complex environments however view-dependent simplification techniques have not been able to take full advantage of the underlying graphics_hardware specifically triangle strips are a widely used hardware-supported mechanism to compactly represent and efficiently render static triangle meshes however in a view-dependent framework the triangle mesh connectivity changes at every frame making it difficult to use triangle strips we present a novel data structure skip strip that efficiently maintains triangle strips during such view-dependent changes a skip strip stores the vertex hierarchy nodes in a skip-list-like manner with path compression we anticipate that skip strips will provide a road map to combine rendering acceleration techniques for static datasets typical of retained-mode graphics applications with those for dynamic_datasets found in immediate-mode applications
the reconstruction of isosurfaces from scalar volume data has positioned itself as a fundamental visualization_technique in many different applications but the dramatically increasing size of volumetric_data sets often prohibits the handling of these models on affordable low-end single processor architectures distributed client-server systems integrating high-bandwidth transmission channels and web based visualization tools are one alternative to attack this particular problem but therefore new approaches to reduce the load of numerical processing and the number of generated primitives are required we outline different scenarios for distributed isosurface_reconstruction from large scale volumetric_data sets we demonstrate how to directly generate stripped surface_representations and we introduce adaptive and hierarchical concepts to minimize the number of vertices that have to be reconstructed transmitted and rendered furthermore we propose a novel computation scheme which allows the user to flexibly exploit locally available resources the proposed algorithms have been merged together in order to build a platform-independent web based application extensive use of vrml and java opengl bindings allows for the exploration of large scale volume data quite efficiently
the temporal branch-on-need tree t-bon extends the three dimensional branch-on-need octree for time-varying isosurface_extraction at each time step only those portions of the tree and data necessary to construct the current isosurface are read from disk this algorithm can thus exploit the temporal locality of the isosurface and as a geometric technique spatial locality between cells in order to improve performance experimental results demonstrate the performance gained and memory overhead saved using this technique
the paper describes new techniques for minimally immersive visualization of 3d scalar and vector_fields and visualization of document corpora in our glyph based visualization system the user interacts with the 3d volume of glyphs using a pair of button-enhanced 3d position and orientation trackers the user may also examine the volume using an interactive lens which is a rectangle that slices through the 3d volume and displays scalar information on its surface a lens allows the display of scalar_data in the 3d volume using a contour diagram and a texture based volume_rendering
conventional projector-based display systems are typically designed around precise and regular configurations of projectors and display surfaces while this results in rendering simplicity and speed it also means painstaking construction and ongoing maintenance in previously published work we introduced a vision of projector-based displays constructed from a collection of casually-arranged projectors and display surfaces in this paper we present flexible yet practical methods for realizing this vision enabling low-cost mega-pixel display systems with large physical dimensions higher resolution or both the techniques afford new opportunities to build personal 3d_visualization_systems in offices conference rooms theaters or even your living room as a demonstration of the simplicity and effectiveness of the methods that we continue to perfect we show in the included video that a 10-year old child can construct and calibrate a two-camera two-projector head-tracked display system all in about 15 minutes
this paper describes tools and techniques for the exploration of gee-scientific data from the oil and gas domain in stereoscopic virtual environments the two main sources of data in the exploration task are seismic volumes and multivariate well logs of physical properties down a bore hole we have developed a props-based interaction device called the cubic mouse to allow more direct and intuitive interaction with a cubic seismic volume this device effectively places the seismic cube in the user's hand geologists who have tried this device have been enthusiastic about the ease of use and were adept only a few moments after picking it up we have also developed a multi-modal visualisation and sonification technique for the dense multivariate well log data the visualisation can show two well log variables mapped along the well geometry in a bivariate colour scheme and another variable on a sliding lens a sonification probe is attached to the lens so that other variables can be heard the sonification is based on a geiger-counter metaphor that is widely understood and which makes it easy to explain the data is sonified at higher or lower resolutions depending on the speed of the lens sweeps can be made at slower rates and over smaller intervals to home in on peaks boundaries or other features in the full resolution data set
this paper describes a method to simulate realistic wrinkles on clothes without fine mesh and large computational overheads cloth has very little in-plane deformations as most of the deformations come from buckling this can be looked at as area conservation property of cloth the area conservation formulation of the method modulates the user defined wrinkle pattern based on deformation of individual triangle the methodology facilitates use of small in-plane deformation stiffnesses and a coarse mesh for the numerical simulation this makes cloth simulation fast and robust moreover the ability to design wrinkles even on generalized deformable models makes this method versatile for synthetic image generation the method inspired from cloth wrinkling problem being geometric in nature can be extended to other wrinkling phenomena
with the development of magnetic_resonance_imaging techniques for acquiring diffusion tensor data from biological tissue visualization of tensor data has become a new research focus the diffusion tensor describes the directional dependence of water molecules' diffusion and can be represented by a three-by-three symmetric matrix_visualization of second-order tensor_fields is difficult because the data values have many degrees of freedom existing visualization_techniques are best at portraying the tensor's properties over a two-dimensional field or over a small subset of locations within a three-dimensional field a means of visualizing the global structure in measured diffusion tensor data is needed we propose the use of direct_volume_rendering with novel approaches for the tensors' coloring lighting and opacity assignment hue-balls use a two-dimensional colormap on the unit sphere to illustrate the tensor's action as a linear operator lit-tensors provide a lighting model for tensors which includes as special cases both lit-lines from streamline vector visualization and standard phong surface lighting together with an opacity assignment based on a novel two-dimensional barycentric space of anisotropy these methods are shown to produce informative renderings of measured diffusion tensor data from the human brain
we present an algorithm which renders opaque and/or translucent polygons embedded within volumetric_data the processing occurs such that all objects are composited in the correct order by rendering thin slabs of the translucent polygons between volume slices using slice-order volume_rendering we implemented our algorithm with opengl on current general-purpose graphics systems we discuss our systemimplementation speed and image quality as well as the renderings of several mixed scenes
a rigorous mathematical review of ray tracing is presented the concept of a generic voxel decoder acting on flexible voxel formats is introduced the necessity of interpolating opacity weighted colors is proved using a new definition of the blending process in terms of functional integrals the continuum limit of the discrete opacity accumulation formula is presented and its convexity properties are investigated the issues pertaining to interpolation/classification order are discussed the lighting equation is expressed in terms of opacity weighted colors the multi-resolution along the ray correction of the opacity-weighted color is derived the mathematics of filtering on the image plane are studied and an upper limit of the local pixel size on the image plane is obtained interpolation of pixel values on the image plane is shown to be in-equivalent to blending of interpolated samples
we present a new technique which enables direct_volume_rendering based on 3d texture_mapping hardware enabling shading as well as classification of the interpolated data our technique supports accurate lighting for a one directional light source semi-transparent classification and correct blending to circumvent the limitations of one general classification we introduce multiple classification spaces which are very valuable to understand the visualized data and even mandatory to comprehensively grasp the 3d relationship of different materials present in the volumetric_data furthermore we illustrate how multiple classification spaces can be realized using existing graphics_hardware in contrast to previously reported algorithms our technique is capable of performing all the above mentioned tasks within the graphics pipeline therefore it is very efficient the three dimensional texture needs to be stored only once and no load is put onto the cpu besides using standard opengl functionality we exploit advanced per pixel operations and make use of available opengl extensions
recent interest in large displays has led to renewed development of tiled_displays which are comprised of several individual displays arranged in an array and used as one large logical display stanford's "interactive mural" is an example of such a display using an overlapping four by two array of projectors that back-project onto a diffuse screen to form a 6' by 2' display area with a resolution of over 60 dpi writing software to make effective use of the large display space is a challenge because normal window system interaction metaphors break down one promising approach is to switch to immersive applications another approach the one we are investigating is to emulate office conference room or studio environments which use the space to display a collection of visual material to support group activities we describe a virtual graphics system that is designed to support multiple simultaneous rendering streams from both local and remote sites the system abstracts the physical number of computers graphics subsystems and projectors used to create the display we provide performance measurements to show that the system scales well and thus supports a variety of different hardware configurations the system is also interesting because it uses transparent "layers" instead of windows to manage the screen
computer simulation and digital measuring systems are now generating data of unprecedented size the size of data is becoming so large that conventional visualization tools are incapable of processing it which is in turn is impacting the effectiveness of computational tools in this paper we describe an object-oriented architecture that addresses this problem by automatically breaking data into pieces and then processes the data piece-by-piece within a pipeline of filters the piece size is user specified and can be controlled to eliminate the need for swapping ie relying on virtual memory in addition because piece size can be controlled any size problem can be run on any size computer at the expense of extra computational time furthermore pieces are automatically broken into sub-pieces and each piece assigned to a different thread for parallel_processing this paper includes numerical performance studies and references to the source code which is freely available on the web
line_integral_convolution lic is an effective technique for visualizing vector_fields the application of lic to 3d flow_fields has yet been limited by difficulties to efficiently display and animate the resulting 3d-images texture-based volume_rendering allows interactive_visualization and manipulation of 3d-lic textures in order to ensure the comprehensive and convenient exploration of flow_fields we suggest interactive functionality including transfer_functions and different clipping mechanisms thereby we efficiently substitute the calculation of lic based on sparse noise textures and show the convenient visual access of interior structures further on we introduce two approaches for animating static 3d-flow_fields without the computational expense and the immense memory requirements for pre-computed 3d-textures and without loss of interactivity this is achieved by using a single 3d-lic texture and a set of time surfaces as clipping geometries in our first approach we use the clipping geometry to pre-compute a special 3d-lic texture that can be animated by time-dependent color tables our second approach uses time volumes to actually clip the 3d-lic volume interactively during rasterization additionally several examples demonstrate the value of our strategy in practice
we approach the problem of exploring a virtual space by exploiting positional and camera-model constraints on navigation to provide extra assistance that focuses the user's explorational wanderings on the task objectives our specific design incorporates not only task-based constraints on the viewer's location gaze and viewing parameters but also a personal "glide" that serves two important functions keeping the user oriented in the navigation space and "pointing" to interesting subject areas as they are approached the guide's cues may be ignored by continuing in motion but if the user stops the gaze shifts automatically toward whatever the guide was interested in this design has the serendipitous feature that it automatically incorporates a nested collaborative paradigm simply by allowing any given viewer to be seen as the "guide" of one or more viewers following behind the leading automated guide we tend to select a guide dog for this avatar can remind the leading live human guide of interesting sites to point out while each real human collaborator down the chain has some choices about whether to follow the local leader's hints we have chosen vrml as our initial development medium primarily because of its portability and we have implemented a variety of natural modes for leading and collaborating including ways for collaborators to attach to and detach from a particular leader
we propose an elementary operation on a pair of vector_fields as a building block for defining and computing global line-type features of vector or scalar_fields while usual feature definitions often are procedural and therefore implicit our operator allows precise mathematical definitions it can serve as a basis for comparing feature definitions and for reuse of algorithms andimplementations applications focus on vortex core methods
multiresolution analysis based on fwt fast wavelet transform is now widely used in scientific_visualization spherical biorthogonal wavelets for spherical triangular grids were introduced by p schroder and w sweldens 1995 in order to improve on theorthogonality of the wavelets the concept of nearlyorthogonality and two new piecewise-constant haar bases were introduced by gm nielson 1997 we extend the results of nielson first we give two one-parameter families of triangular haar wavelet bases that are nearlyorthogonal in the sense of nielson then we introduce a measure oforthogonality this measure vanishes fororthogonal bases eventually we show that we can find an optimal parameter of our wavelet families for which the measure oforthogonality is minimized several numerical and visual examples for a spherical topographic data set illustrates our results
we present a novel approach to solving the cracking problem the cracking problem arises in many contexts in scientific_visualization and computer_graphics modeling where there is need for an approximation based upon domain decomposition that is fine in certain regions and coarse in others this includes surface rendering approximation of images and multiresolution terrain_visualization in general algorithms based upon adaptive_refinement strategies must deal with this problem the approach presented here is simple and general it is based upon the use of a triangular coons patch both the basic idea of using a triangular coons patch in this context and the particular coons patch that is used constitute the novel contributions of the paper
we present a novel rendering technique termed lod-sprite rendering which uses a combination of a level-of-detail lod representation of the scene together with reusing image sprites previously rendered images our primary application is accelerating terrain_rendering the lod-sprite technique renders an initial frame using a high-resolution model of the scene geometry it renders subsequent frames with a much lower-resolution model of the scene geometry and texture-maps each polygon with the image sprite from the initial high-resolution frame as it renders these subsequent frames the technique measures the error associated with the divergence of the view position from the position where the initial frame was rendered once this error exceeds a user-defined threshold the technique re-renders the scene from the high-resolution model we have efficiently implemented the lod-sprite technique with texture_mapping graphics_hardware although to date we have only applied lod-sprite to terrain_rendering it could easily be extended to other applications we feel lod-sprite holds particular promise for real time rendering systems
the recent growth in the size and availability of large triangular surface models has generated interest in compact multi-resolution progressive representation and data transmission an ongoing challenge is to design an efficient data structure that encompasses both compactness of geometric representations and visual quality of progressive representations we introduce a topological layering based data structure and an encoding scheme to build a compact progressive representation of an arbitrary triangular_mesh a 2d simplicial complex in 3d with attached attribute data this compact representation is composed of multiple levels of detail that can be progressively transmitted and displayed the global topology which is the number of holes and connected components can be flexibly changed among successive levels while still achieving guaranteed size of the coarsest level mesh for very complex models the flexibility in our encoding scheme also allows topology preserving progressivity
many applications produce three-dimensional points that must be further processed to generate a surface surface_reconstruction algorithms that start with a set of unorganized points are extremely time-consuming sometimes however points are generated such that there is additional information available to the reconstruction algorithm we present spiraling edge a specialized algorithm for surface_reconstruction that is three orders of magnitude faster than algorithms for the general case in addition to sample point locations our algorithm starts with normal information and knowledge of each point's neighbors our algorithm produces a localized approximation to the surface by creating a star-shaped triangulation between a point and a subset of its nearest neighbors this surface patch is extended by locally triangulating each of the points along the edge of the patch as each edge point is triangulated it is removed from the edge and new edge points along the patch's edge are inserted in its place the updated edge spirals out over the surface until the edge encounters a surface boundary and stops growing in that direction or until the edge reduces to a small hole that is filled by the final triangle
vector_field_visualization is an important topic in scientific_visualization its aim is to graphically represent field data in an intuitively understandable and precise way here a new approach based on anisotropic nonlinear diffusion is introduced it enables an easy perception of flow data and serves as an appropriate scale_space method for the visualization of complicated flow patterns the approach is closely related to nonlinear diffusion methods in image_analysis where images are smoothed while still retaining and enhancing edges an initial noisy image is smoothed along streamlines whereas the image is sharpened in theorthogonal direction the method is based on a continuous model and requires the solution of a parabolic pde problem it is discretized only in the finalimplementational step therefore many important qualitative aspects can already be discussed on a continuous level applications are shown in 2d and 3d and the provisions for flow segmentation are outlined
we present a new visualization method for 2d flows which allows us to combine multiple data values in an image for simultaneous viewing we utilize concepts from oil painting art and design as introduced in laidlaw et al 1998 to examine problems within fluid mechanics we use a combination of discrete and continuous visual elements arranged in multiple layers to visually represent the data the representations are inspired by the brush strokes artists apply in layers to create an oil painting we display commonly visualized quantities such as velocity and vorticity together with three additional mathematically derived quantities the rate of strain_tensor and the turbulent charge and turbulent current we describe the motivation for simultaneously examining these quantities and use the motivation to guide our choice of visual representation for each particular quantity we present visualizations of three flow examples and observations concerning some of the physical relationships made apparent by the simultaneous display technique that we employed
this paper explores mapping strategies for generating lic-like images from streamlines and streamline-like images from lic the main contribution of this paper is a technique which we call pseudo-lic or plic by adjusting a small set of key parameters plic can generate flow_visualizations that span the spectrum of streamline-like to lic-like images among the advantages of plic are image quality comparable with lic performance speedup over lic use of a template texture that is independent of the size of the flow_field handles the problem of multiple streamlines occupying the same pixel in image space reduced aliasing applicability totime_varying data sets and variable speed animation
splatting is a volume_rendering algorithm that combines efficient volume projection with a sparse data representation only voxels that have values inside the iso-range need to be considered and these voxels can be projected via efficient rasterization schemes in splatting each projected voxel is represented as a radially symmetric interpolation kernel equivalent to a fuzzy ball projecting such a basis_function leaves a fuzzy impression called a footprint or splat on the screen splatting traditionally classifies and shades the voxels prior to projection and thus each voxel footprint is weighted by the assigned voxel color and opacity projecting these fuzzy color balls provides a uniform screen image for homogeneous object regions but leads to a blurry appearance of object edges the latter is clearly undesirable especially when the view is zoomed on the object in this work we manipulate the rendering pipeline of splatting by performing the classification and shading process after the voxels have been projected onto the screen in this way volume contributions outside the iso-range never affect the image since shading requires gradients we not only splat the density volume using regular splats but we also project the gradient volume using gradient splats however alternative to gradient splats we can also compute the gradients on the projection plane using central differencing this latter scheme cuts the number of footprint rasterization by a factor of four since only the voxel densities have to be projected
we present a fast volume_rendering algorithm for time-varying fields we propose a new data structure called time-space partitioning tsp tree that can effectively capture both the spatial and the temporal_coherence from a time-varying field using the proposed data structure the rendering speed is substantially improved in addition our data structure helps to maintain the memory access locality and to provide the sparse data traversal so that our algorithm becomes suitable for large-scale out-of-core applications finally our algorithm allows flexible error control for both the temporal and the spatial coherence so that a trade-off between image quality and rendering speed is possible we demonstrate the utility and speed of our algorithm with data from several time-varying cfd simulations our rendering algorithm can achieve substantial speedup while the storage space overhead for the tsp tree is kept at a minimum
we present a system for interactive_explorations of extra- and intracranial blood_vessels starting with a stack of images from 3d angiography we use virtual clips to limit the segmentation of the vessel tree to the parts the neuroradiologists are interested in furthermore methods of interactive virtual_endoscopy are applied in order to provide an interior view of the blood_vessels
we present the design of a simulator for a prototype interventional magnetic_resonance_imaging scanner this mri scanner is integrated with an operating theater enabling new techniques in minimally invasive surgery the simulator is designed with a threefold purpose to provide a rehearsal apparatus for practicing and modifying conventional procedures for use in the magnetic environment to serve as a visualization workstation for procedure planning and previewing as well as a post-operative review and to form the foundation of a laboratory workbench for the development of new surgical tools and procedures for minimally invasive surgery the simulator incorporates pre-operative data either mri or ct exams as well as data from commercial surgical planning systems dynamic control of the simulation and interactive_display of pre-operative data in lieu of intra-operative data is handled via an opto-electronic tracking system the resulting system is contributing insights into how best to perform visualization for this new surgical environment
we present an interactive navigation system for virtual_colonoscopy which is based solely on high performance volume_rendering previous colonic navigation systems have employed either a surface rendering or a z-buffer-assisted volume_rendering method that depends on the surface rendering results our method is a fast direct_volume_rendering technique that exploits distance information stored in the potential field of the camera_control model and is parallelized on a multiprocessor experiments have been conducted on both a simulated pipe and patients' data sets acquired with a ct scanner
the molecular events involved in the activation of g protein-coupled receptors represent a fundamental biochemical process these events were selected for animation because the mechanism involves both a ligand-receptor conformational shape change and an enzyme-substrate conformational shape change expository animation brought this biochemical process to life
much of the research in scientific_visualization has focused on complete sets of gridded data the paper presents our experience dealing with gridded data sets with a large number of missing or invalid data and some of our experiments in addressing the shortcomings of standard off-the-shelf visualization algorithms in particular we discuss the options in modifying known algorithms to adjust to the specifics of sparse datasets and provide a new technique to smooth out the side-effects of the operations we apply our findings to data acquired from nexrad next generation radars weather radars which usually have no more than 3 to 4 percent of all possible cell points filled
the detection of vortical phenomena in vector data is one of the key issues in many technical applications in particular in flow_visualization many existing approaches rely on purely local evaluation of the vector data in order to overcome the limits of a local approach we choose to combine a local method with a correlation of a pre-defined generic vortex with the data in a medium-scale region two different concepts of generic vortices were tested on various sets of flow velocity vector data the approach is not limited to the two generic patterns suggested here the method was found to successfully detect vortices in cases were other methods fail
visualization_systems are complex dynamic software systems debugging such systems is difficult using conventional debuggers because the programmer must try to imagine the three-dimensional geometry based on a list of positions and attributes in addition the programmer must be able to mentally animate changes in those positions and attributes to grasp dynamic behaviors within the algorithm we show that representing geometry attributes and relationships graphically permits visual pattern_recognition skills to be applied to the debugging problem the particular application is a particle system used for isosurface_extraction from volumetric_data coloring particles based on individual attributes is especially helpful when these colorings are viewed as animations over successive iterations in the program although we describe a particular application the types of tools that we discuss can be applied to a variety of problems
perhaps the most effective instrument to simplify and to clarify the comprehension of any complex mathematical or scientific theory is through visualisation moreover using interactivity and 3d real time representations one can easily explore and hence learn quickly in the virtual environments the concept of virtual and safe laboratories has vast potentials in education with the aid of computer simulations and 3d visualisations many dangerous or cumbersome experiments may be implemented in the virtual environments with rather small effort nonetheless visualisation alone is of little use if the respective simulation is not scientifically accurate hence a rigorous combination of precise computation as well as sophisticated visualisation presented through some intuitive user interface is required to realise a virtual laboratory for education we introduce delta's virtual physics laboratory comprising a wide range of applications in the field of physics and astronomy which can be implemented and used as an interactive_learning tool on the world_wide_web
we describe a visualization tool to aid aircraft designers during the conceptual design stage the conceptual design for an aircraft is defined by a vector of 10-30 parameters the goal is to find a vector that minimizes an objective function while meeting a series of constraints vizcraft integrates the simulation code that evaluates the design with visualizations for analyzing the design individually or in contrast to other designs vizcraft allows the designer to easily switch between the view of a design in the form of a parameter set and a visualization of the corresponding aircraft the user can easily see which if any constraints are violated vizcraft also allows the user to view a database of designs using parallel_coordinates
having a better way to represent and to interact with large geological models are topics of high interest in geoscience and especially for oil and gas companies we present the design andimplementation of a visualization program that involves two main features it is based on the central data model in order to display in real time the modifications caused by the modeler furthermore it benefits from the different immersive_environments which give the user a much more accurate insight of the model than a regular computer screen then we focus on the difficulties that come in the way of performance
"free flight" will change today's air traffic control system by giving pilots increased flexibility to choose and modify their routes in real time reducing costs and increasing system capacity this increased flexibility comes at the price of increased complexity if free flight is to become a reality future air traffic controllers pilots and airline managers will require new conflict detection resolution and visualization decision support tools the paper describes a testbed system for building and evaluating such tools including its current capabilities lessons we learned and feedback received from expert users the visualization system provides an overall plan view supplemented with a detailed perspective view allowing a user to examine highlighted conflicts and select from a list of proposed solutions as the scenario runs in real time future steps needed to improve this system are described
this paper presents results for real-time_visualization of out-of-core collections of 3d objects this is a significant extension of previous methods and shows the generality of hierarchical paging procedures applied both to global terrain and any objects that reside on it applied to buildings the procedure shows the effectiveness of using a screen-based paging and display criterion within a hierarchical framework the results demonstrate that the method is scalable since it is able to handle multiple collections of buildings eg cities placed around the earth with full interactivity and without extensive memory load further the method shows efficient handling of culling and is applicable to larger extended collections of buildings finally the method shows that levels of detail can be incorporated to provide improved detail management
situational awareness applications require a highly detailed geospatial visualization covering a large geographic area conventional polygon based terrain modeling would exceed the capacity of current computer rendering terrain_visualization_techniques for a situational awareness application are described in this case_study visualizing large amounts of terrain data has been achieved using very large texture maps sun shading is applied to the terrain texture map to enhance perception of relief features perception of submarine positions has been enhanced using a translucent textured water surface
the detailed underwater bathymetric data provided by sonar research and development's high speed multi-frequency sonar transducer system provides new challenges in the development of interactive seabed visualization tools the paper introduces a "whole field modelling" system developed at sonar research and development ltd and the department of computer science university of hull this system provides the viewer with a new 3d underwater visualization environment that allows the user to pilot a virtual underwater vehicle around an accurate seabed model we consider two example case studies that use the whole field modelling system for visualizing sonar data both case studies visualizing real time pipeline dredging and pipe restoration visualization are implemented using real survey data
we explore the potential of information_visualization_techniques in enhancing existing methodologies for domain analysis and modeling in this case_study we particularly focus on visualizing the evolution of the hypertext field based on author co-citation patterns including the use of a sliding-window scheme to generate a series of annual snapshots of the domain structure and a factor-referenced color-coding scheme to highlight predominant specialties in the field
analyzing options is a complex multi-variate process option behavior depends on a variety of market conditions which vary over the time course of the option the goal of this project is to provide an interactive visual environment which allows the analyst to explore these complex interactions and to select and construct specific views for communicating information to non-analysts eg marketing managers and customers in this paper we describe an environment for exploring 2- and 3-dimensional representations of options data dynamically varying parameters examining how multi-variate relationships develop over time and exploring the likelihood of the development of different outcomes over the life of the option we also demonstrate how this tool has been used by analysts to communicate to non-analysts how particular options no longer deliver the behavior they were originally intended to provide
visual_exploration of massive datasets arising from telecommunication networks and services is a challenge this paper describes swift-3d an integrated data visualization and exploration system created at at&t labs for large scale network analysis swift-3d integrates a collection of interactive tools that includes pixel-oriented 2d maps interactive 3d maps statistical displays network topology diagrams and an interactive drill-down query interface example applications are described demonstrating a successful application to analyze unexpected network events high volumes of unanswered calls and comparison of usage of an internet service with voice network traffic and local access coverage
microsatellite genotypes can have problems that are difficult to detect with existing tools one such problem is null alleles this paper presents a new visualization tool that helps to find and characterize these errors the paper explains how the tool is used to analyze groups of genotypes and proposes other possible uses
this paper describes initial results of a 3d field topology analysis for automating transfer_function_design aiming at comprehensible volume_rendering the conventional reeb_graph-based approach to describing topological features of 3d surfaces is extended to capture the topological skeleton of a volumetric field based on the analysis result which is represented in the form of a hyper reeb_graph a procedure is proposed for designing appropriate color/opacity transfer_functions two analytic volume datasets are used to preliminarily prove the feasibility of the present design methodology
many volume filtering operations used for image_enhancement data processing or feature_detection can be written in terms of three-dimensional convolutions it is not possible to yield interactive frame rates on todays hardware when applying such convolutions on volume data using software filter routines as modern graphics workstations have the ability to render two-dimensional convoluted images to the frame buffer this feature can be used to accelerate the process significantly this way generic 3d convolution can be added as a powerful tool in interactive_volume_visualization toolkits
one area in need of new research in information_visualization is the operation and analysis of large-scale electric power systems in analyzing power systems one is usually confronted with a large amount of multivariate data with systems containing tens of thousands of electrical nodes buses a key challenge is to present this data in a form so the user can assess the state of the system in an intuitive and quick manner this is particularly true when trying to analyze relationships between actual network power flows the scheduled power flows and the capacity of the transmission system with electric industry restructuring and the move towards having a single entity such as an independent system operator or pool operate a much larger system this need has become more acute this paper presents several power system visualization_techniques to help in this task these techniques include animation of power system flow values contouring of bus and transmission line flow values data_aggregation techniques and interactive 3d data visualization
we describe mgv an integrated visualization and exploration system for massive multi-digraph_navigation mgv's only assumption is that the vertex set of the underlying digraph corresponds to the set of leaves of a predetermined tree t mgv builds an out-of-core graph hierarchy and provides mechanisms to plug in arbitrary visual representations for each graph hierarchy slice navigation from one level to another of the hierarchy corresponds to theimplementation of a drill-down interface in order to provide the user with navigation control and interactive response mgv incorporates a number of visualization_techniques like interactive pixel-oriented 2d and 3d maps statistical displays multi-linked_views and a zoomable label based interface this makes the association of geographic_information and graph data very natural mgv follows the client-server paradigm and it is implemented in c and java-3d we highlight the main algorithmic and visualization_techniques behind the tools and point out along the way several possible application scenarios our techniques are being applied to multi-graphs defined on vertex sets with sizes ranging from 100 million to 250 million vertices
drawing on ethnographic studies of landscape architects at work this paper presents a human-centered approach to information_visualization a 3d collaborative electronic workspace allows people to configure save and browse arrangements of heterogeneous work materials spatial arrangements and links are created and maintained as an integral part of ongoing work with `live' documents and objects the result is an extension of the physicalinformation_space of the architects' studio that utilizes the potential of electronic data storage visualization and network technologies to support work with information in context
anyone who has ever experienced three-dimensional 3d interfaces will agree that navigating in a 3d world is not a trivial task the user interface of traditional 3d browsers provides simple navigation tools that allow the user to modify the camera parameters such as orientation position and focal using these tools it is frequent that after some movements the user is lost in the virtual 3d space and usually tries to restart from the beginning we present how the 3d navigation problem is addressed in the context of the cybernet project abel et al 2000 our underlying principle is to help the user navigate by adapting the navigation tool to the virtual world we feel that the navigation schemes provided by the 3d browsers are too generic for some specific 3d tools and we have developed adaptive navigation features that are dependent on the 3d metaphor used for visualizing the information and on the user's task
by virtue of their spatio-cognitive abilities humans are able to navigate through geographic space as well as meaningfully communicate geographic_information represented in cartographic form the current dominance of spatial metaphors in information_visualization research is the result of the realization that those cognitive skills also have value in the exploration and analysis of non-geographic_information while mapping or landscape metaphors are routinely used in this field there is a noticeable lack of consideration for existing cartographic expertise this is especially apparent whenever problematic issues are encountered such as graphic complexity or feature labeling there are a number of areas in which a cartographic outlook could provide a valuable perspective this paper discusses how geographic and cartographic notions may influence the design of visualizations for textualinformation_spaces map projections generalization feature labeling and map design issues are discussed
radial space-filling_visualizations can be useful for depicting information hierarchies but they suffer from one major problem as the hierarchy grows in size many items become small peripheral slices that are difficult to distinguish we have developed three visualization/interaction techniques that provide flexible browsing of the display the techniques allow viewers to examine the small items in detail while providing context within the entire information hierarchy additionally smooth transitions between views help users maintain orientation within the completeinformation_space
the paper describes major concepts of a scalable information_visualization_framework we assume that the exploration of heterogeneousinformation_spaces at arbitrary levels of detail requires a suitable preprocessing of information quantities the combination of different graphical interfaces and the illustration of the frame of reference of given information sets the innovative features of our system include dynamic hierarchy computation and user controlled refinement of those hierarchies for preprocessing unstructuredinformation_spaces a new focus+context technique for visualizing complex hierarchy graphs a new paradigm for visualizing information structures within their frame of reference and a new graphical interface that utilizes textual similarities to arrange objects of high dimensionalinformation_space in 3-dimensional visualization space
the increasing diversity of computers especially among small mobile devices such as mobile phones and pdas raise new questions about information_visualization_techniques developed for the desktop computer using a series of examples ranging from applications for ordinary desktop displays to web-browsers and other applications for pdas we describe how a focus+context technique flip zooming is changed due to the situation it is used in based on these examples we discuss how the use of “focus” and “context” in focus+context techniques change in order to fit new areas of use for information_visualization
we describe a prototype same-time/different-place collaborative geo visualization environment we outline an approach to understanding use and usability and present results of interviews with domain experts about the ways in which collaborative_visualization might enable groups to work at a distance one goal for our research is to design an effective and flexible system that can support group_work on environmental science research mediated through dynamic geo visualization displays we are addressing this goal using a four-step human-centered system design process modeled on that proposed by gabbard et al 1999 for development and evaluation of virtual environments the steps they delineate are user task analysis expert guideline-based evaluation formative user-centered evaluation and summative comparative evaluation
with an increase in the number of different visualization_techniques it becomes necessary to develop a measure for evaluating the effectiveness of visualizations metrics to evaluate visual displays were developed based on measures of information content developed by shannon and used incommunication theory these measures of information content can be used to quantify the relative effectiveness of displays
lighthouse is an on-line interface for a web-based information_retrieval system it accepts queries from a user collects the retrieved documents from the search_engine organizes and presents them to the user the system integrates two known presentations of the retrieved results the ranked list and clustering visualization in a novel and effective way it accepts the user's input and adjusts the document_visualization accordingly we give a brief overview of the system
two tasks in graph_visualization require partitioning the assignment of visual attributes and divisive clustering often we would like to assign a color or other visual attributes to a node or edge that indicates an associated value in an application involving divisive clustering we would like to partition the graph into subsets of graph elements based on metric values in such a way that all subsets are evenly populated assuming a uniform distribution of metric values during either partitioning or coloring can have undesired effects such as empty clusters or only one level of emphasis for the entire graph probability density functions derived from statistics about a metric can help systems succeed at these tasks
comind is a tool for conceptual design of industrial products it helps designers define and evaluate the initial design space by using search algorithms to generate sets of feasible solutions two algorithm visualization_techniques kaleidoscope and lattice and one visualization of n-dimensional data map are used to externalize the machine's problem_solving strategies and the tradeoffs as a result of using these strategies after a short training period users are able to discover tactics to explore design space effectively evaluate new design solutions and learn important relationships among design criteria search speed and solution quality we thus propose that visualization can serve as a tool for interactive intelligence ie human-machine collaboration for solving complex problems
in previous work researchers have attempted to construct taxonomies of information_visualization_techniques by examining the data domains that are compatible with these techniques this is useful because implementers can quickly identify various techniques that can be applied to their domain of interest however these taxonomies do not help the implementers understand how to apply and implement these techniques the author extends and proposes a new way to taxonomize information_visualization_techniques by using the data state model eh chi and jt reidl 1998 in fact as the taxonomic analysis in the paper will show many of the techniques share similar operating steps that can easily be reused the paper shows that the data state model not only helps researchers understand the space of design but also helps implementers understand how information_visualization_techniques can be applied more broadly
data visualization_environments help users understand and analyze their data by permitting interactive browsing of graphical representations of the data to further facilitate understanding and analysis many visualization_environments have special features known as portals which are sub-windows of a data canvas portals provide a way to display multiple graphical representations simultaneously in a nested fashion this makes portals an extremely powerful and flexible paradigm for data visualization unfortunately with this flexibility comes complexity there are over a hundred possible ways each portal can be configured to exhibit different behaviors many of these behaviors are confusing and certain behaviors can be inappropriate for a particular setting it is desirable to eliminate confusing and inappropriate behaviors the authors construct a taxonomy of portal behaviors and give recommendations to help designers of visualization_systems decide which behaviors are intuitive and appropriate for a particular setting they apply these recommendations to an example setting that is fully visually programmable and analyze the resulting reduced set of behaviors finally the authors consider a real visualization environment and demonstrate some problems associated with behaviors that do not follow their recommendations
since novice users of visualization_systems lack knowledge and expertise in data visualization it is a tough task for them to generate efficient and effective visualizations that allow them to comprehend information that is embedded in the data therefore systems supporting the users to design appropriate visualizations are of great importance the gadget goal-oriented application design guidance for modular visualization_environments system which has been developed by the authors 1997 interactively helps users to design scientific_visualization_applications by presenting appropriate mve modular visualization environment prototypes according to the specification of the visualization goals expressed mainly with the wehrend matrix s wehrend & c lewis 1990 this paper extends this approach in order to develop a system named gadget/iv which is intended to provide the users with an environment for semi-automatic design of information_visualization iv applications to this end a novel goal-oriented taxonomy of iv techniques is presented also an initial design of the system architecture and user assistance flow is described the usefulness of the gadget/iv system is illustrated with example problems of web site access frequency analysis
a sequential pattern in data_mining is a finite series of elements such as a→b→c→d where a b c and d are elements of the same domain the mining of sequential patterns is designed to find patterns of discrete events that frequently happen in the same arrangement along a timeline like association and clustering the mining of sequential patterns is among the most popular knowledge_discovery techniques that apply statistical measures to extract useful information from large_datasets as out computers become more powerful we are able to mine bigger datasets and obtain hundreds of thousands of sequential patterns in full detail with this vast amount of data we argue that neither data_mining nor visualization by itself can manage the information and reflect the knowledge effectively subsequently we apply visualization to augment data_mining in a study of sequential patterns in large text corpora the result shows that we can learn more and more quickly in an integrated visual data-mining environment
in the last several years large multi-dimensional_databases have become common in a variety of applications such as data warehousing and scientific computing analysis and exploration tasks place significant demands on the interfaces to these databases because of the size of the data sets dense graphical representations are more effective for exploration than spreadsheets and charts furthermore because of the exploratory nature of the analysis it must be possible for the analysts to change visualizations rapidly as they pursue a cycle involving first hypothesis and then experimentation the authors present polaris an interface for exploring large multi-dimensional_databases that extends the well-known pivot table interface the novel features of polaris include an interface for constructing visual specifications of table based graphical displays and the ability to generate a precise set of relational queries from the visual specifications the visual specifications can be rapidly and incrementally developed giving the analyst visual feedback as they construct complex queries and visualizations
we present a new technique for extracting regions of interest roi applying a local watershed transformation the proposed strategy for computing catchment basins in a given region of interest is based on a rain-falling simulation unlike the standard watershed algorithms which flood the complete gradient magnitude of an image the proposed approach allows us to perform this task locally thus a controlled region growth is performed saving time and reducing the memory requirement especially when applied on volume data a second problem arising from the standard watershed transformation is the over-segmented result and the lack of sound criteria for merging the computed basins for overcoming this drawback we present a basin-merging strategy introducing four criteria for merging adjacent basins the threshold values applied in this strategy are derived from the user input and match rather the attributes of the selected object than of all objects in the image in doing so the user is not required to adjust abstract numbers but to simply select a coarse region of interest moreover the proposed algorithm is not limited to the 2d case as we show in this work it is suitable for volume data processing as well finally we present the results of applying the proposed approach on several example images and volume data sets
we present a new visibility determination algorithm for interactive virtual_endoscopy the algorithm uses a modified version of template-based ray casting to extract a view dependent set of potentially visible voxels from volume data the voxels are triangulated by marching_cubes and the triangles are rendered onto the display by a graphics accelerator early ray termination and space_leaping are used to accelerate the ray casting step and a quadtree subdivision algorithm is used to reduce the number of cast rays compared to other recently proposed rendering algorithms for virtual_endoscopy our rendering algorithm does not require a long preprocessing step or a high-end graphics workstation but achieves interactive frame rates on a standard pc equipped with a low-cost graphics accelerator
we propose a novel approach for segmentation and digital cleansing of endoscopic organs our method can be used for a variety of segmentation needs with little or no modification it aims at fulfilling the dual and often conflicting requirements of a fast and accurate segmentation and also eliminates the undesirable partial volume effect which contemporary approaches cannot for segmentation and digital cleansing we use the peculiar characteristics exhibited by the intersection of any two distinct-intensity regions to detect these intersections we cast rays through the volume which we call the segmentation rays as they assist in the segmentation we then associate a certain task of reconstruction and classification with each intersection the ray detects we further use volumetric contrast enhancement to reconstruct surface lost by segmentation if any which aids in improving the quality of the volume_rendering
we present ceasar a centerline extraction algorithm that delivers smooth accurate and robust results centerlines are needed for accurate measurements of length along winding tubular structures centerlines are also required in automatic virtual navigation through human organs such as the colon or the aorta as they are used to control movement and orientation of the virtual camera we introduce a concise but general definition of a centerline and provide an algorithm that finds the centerline accurately and rapidly our algorithm is provably correct for general geometries our solution is fully automatic which frees the user from having to engage in data preprocessing for a number of test datasets we show the smooth and accurate centerlines computed by our ceasar algorithm on a single 194 mhz mips r10000 cpu within five minutes
we present a new hierarchical_clustering and visualization algorithm called h-blob which groups and visualizes cluster hierarchies at multiple levels-of-detail our method is fundamentally different to conventional clustering algorithms such as c-means k-means or linkage methods that are primarily designed to partition a collection of objects into subsets sharing similar attributes these approaches usually lack an efficient level-of-detail strategy that breaks down the visual complexity of very large_datasets for visualization in contrast our method combines grouping and visualization in a two stage process constructing a hierarchical setting in the first stage a cluster tree is computed making use of an edge contraction operator exploiting the inherent hierarchical structure of this tree a second stage visualizes the clusters by computing a hierarchy of implicit_surfaces we believe that h-blob is especially suited for the visualization of very large_datasets and for visual decision_making in information_visualization the versatility of the algorithm is demonstrated using examples from visual_data_mining
as the size and complexity of data sets continues to increase the development of user_interfaces and interaction techniques that expedite the process of exploring that data must receive new attention regardless of the speed of rendering it is important to coherently organize the visual process of exploration this information both grants insights about the data to a user and can be used by collaborators to understand the results to fulfil these needs we present a spreadsheet-like interface to data_exploration the interface displays a 2-dimensional window into visualization parameter space which users manipulate as they search for desired results through tabular organization and a clear correspondence between parameters and results the interface eases the discovery comparison and analysis of the underlying data users can utilize operators and the integrated interpreter to further explore and automate the visualization process using a method introduced in this paper these operations can be applied to cells in different stacks of the interface via illustrations using a variety of data sets we demonstrate the efficacy of this novel interface
in many applications of scientific_visualization a large quantity of data is being processed and displayed in order to enable a viewer to make informed and effective decisions since little data is perfect there is almost always some degree of associated uncertainty this uncertainty is an important part of the data and should be taken into consideration when interpreting the data uncertainty however should not overshadow the data values many methods that address the problem of visualizing data with uncertainty can distort the data and emphasize areas with uncertain values we have developed a method for showing the uncertainty information together with data with minimal distraction this method uses procedurally generated annotations which are deformed according to the uncertainty information as another possible technique we propose distorting glyphs according to the uncertainty information
the techniques for reducing the size of a volume dataset by preserving both the geometrical/topological shape and the information encoded in an attached scalar field are attracting growing interest given the framework of incremental 3d mesh simplification based on edge collapse we propose an approach for the integrated evaluation of the error introduced by both the modification of the domain and the approximation of the field of the original volume dataset we present and compare various techniques to evaluate the approximation error or to produce a sound prediction a flexible simplification tool has been implemented which provides a different degree ofaccuracy and computational efficiency for the selection of the edge to be collapsed techniques for preventing a geometric or topological degeneration of the mesh are also presented
we present a new method for the modeling of freehand collected three-dimensional ultrasound data the model is piecewise linear and based upon progressive tetrahedral domains created by a subdivision scheme which splits a tetrahedron on on its longest edge and guarantees a valid tetrahedrization least squares error is used to characterize the model and an effective iterative technique is used to compute the values of the model at the vertices of the tetrahedral grid since the subdivision strategy is adaptive the complexity of the model conforms to the complexity of the data leading to an extremely efficient and highly compressed volume model the model is evaluated in real time using piecewise linear_interpolation and gives a medical professional the chance to see images which would not be possible using conventional ultrasound techniques
very large irregular-grid data sets are represented as tetrahedral_meshes and may incur significant disk i/o access overhead in the rendering process an effective way to alleviate the disk i/o overhead associated with rendering a large tetrahedral mesh is to reduce the i/o bandwidth requirement through compression existing tetrahedral mesh_compression algorithms focus only on compression efficiency and cannot be readily integrated into the mesh rendering process and thus demand that a compressed tetrahedral mesh be decompressed before it can be rendered into a 2d image this paper presents an integrated tetrahedral mesh_compression and rendering algorithm called gatun which allows compressed tetrahedral_meshes to be rendered incrementally as they are being decompressed thus leading to an efficient irregular grid rendering pipeline both compression and rendering algorithms in gatun exploit the same local connectivity information among adjacent tetrahedra and thus can be tightly integrated into a unifiedimplementation framework our tetrahedral compression algorithm is specifically designed to facilitate the integration with an irregular grid renderer without any compromise in compression efficiency a unique performance advantage of gatun is its ability to reduce the runtime memory footprint requirement by releasing memory allocated to tetrahedra as early as possible
we present two beneficial rendering extensions to the projected_tetrahedra pt algorithm proposed by shirley and tuchman 1990 these extensions are compatible with any cell sorting technique for example the bsp-xmpvo sorting algorithm for unstructured_meshes using 3d texture_mapping our first extension solves the longstanding problem of hardware-accelerated but accurate rendering of tetrahedral volume cells with arbitrary transfer_functions by employing 2d texture_mapping our second extension realizes the hardware-accelerated rendering of multiple shaded isosurfaces within the pt algorithm without reconstructing the isosurfaces additionally two methods are presented to combine projected_tetrahedral volumes with isosurfaces the time complexity of all our algorithms is linear in the number of tetrahedra and does neither depend on the number of isosurfaces nor on the employed transfer_functions
large area tiled_displays are gaining popularity for use in collaborative immersive virtual environments and scientific_visualization while recent work has addressed the issues of geometric registration rendering architectures and human interfaces there has been relatively little work on photometric_calibration in general and photometric non-uniformity in particular for example as a result of differences in the photometric characteristics of projectors the color and intensity of a large area display varies from place to place further the imagery typically appears brighter at the regions of overlap between adjacent projectors we analyze and classify the causes of photometric non-uniformity in a tiled display we then propose a methodology for determining corrections designed to achieve uniformity that can correct for the photometric variations across a tiled projector display in real time using per channel color look-up-tables lut
a scalable high-resolution display may be constructed by tiling many projected images over a single display surface one fundamental challenge for such a display is to avoid visible seams due to misalignment among the projectors traditional methods for avoiding seams involve sophisticated mechanical devices and expensive crt projectors coupled with extensive human effort for fine-tuning the projectors the paper describes an automatic alignment method that relies on an inexpensive uncalibrated camera to measure the relative mismatches between neighboring projectors and then correct the projected imagery to avoid seams without significant human effort
specific rendering modes are developed for a combined visual/haptic interface to allow exploration and understanding of fluid dynamics data the focus is on visualization of shock surfaces and vortex cores advantages provided by augmenting traditional graphical rendering modes with haptic rendering modes are discussed particular emphasis is placed on synergistic combinations of visual and haptic modes which enable rapid exploratory interaction with the dataimplementation issues are also discussed
we present an algorithm for haptic display of moderately complex polygonal models with a six degree of freedom dof force feedback device we make use of incremental algorithms for contact determination between convex primitives the resulting contact information is used for calculating the restoring forces and torques and thereby used to generate a sense of virtual touch to speed up the computation our approach exploits a combination of geometric locality temporal_coherence and predictive methods to compute object-object contacts at khz rates the algorithm has been implemented and interfaced with a 6-dof phantom premium 15 we demonstrate its performance on force display of the mechanical interaction between moderately complex geometric structures that can be decomposed into convex primitives
we propose a technique for visualizing steady flow using this technique we first convert the vector_field_data into a scalar level-set representation we then analyze the dynamic behavior and subsequent distortion of level-sets and interactively monitor the evolving structures by means of texture-based surface rendering next we combine geometrical and topological considerations to derive a multiscale representation and to implement a method for the automatic placement of a sparse set of graphical primitives depicting homogeneous streams in the fields using the resulting algorithms we have built a visualization system that enables us to effectively display the flow direction and its dynamics even for dense 3d fields
we present a novel hardware-accelerated texture_advection algorithm to visualize the motion of two-dimensional unsteady_flows making use of several proposed extensions to the opengl-12 specification we demonstrate animations of over 65000 particles at 2 frames/sec on an sgi octane with emxi graphics high image quality is achieved by careful attention to edge effects noise frequency and image_enhancement we provide a detailed description of the hardwareimplementation including temporal and spatial coherence techniques dye advection techniques and feature_extraction
the paper presents a seed placement strategy for streamlines based on flow features in the dataset the primary goal of our seeding_strategy is to capture flow patterns in the vicinity of critical_points in the flow_field even as the density of streamlines is reduced secondary goals are to place streamlines such that there is sufficient coverage in non-critical regions and to vary the streamline_placements and lengths so that the overall presentation is aesthetically pleasing avoid clustering of streamlines avoid sharp discontinuities across several streamlines etc the procedure is straightforward and non-iterative first critical_points are identified next the flow_field is segmented into regions each containing a single critical point the critical point in each region is then seeded with a template depending on the type of critical point finally additional seed points are randomly distributed around the field using a poisson disk distribution to minimize closely spaced seed points the main advantage of this approach is that it does not miss the features around critical_points since the strategy is not image-guided and hence not view dependent significant savings are possible when examining flow_fields from different viewpoints especially for 3d flow_fields
the work presents a method to enable matching of level-of-detail lod models to image-plane resolution over large variations in viewing distances often present in exterior images a relationship is developed between image sampling rate viewing distance object projection and expected image error due to lod approximations this is employed in an error metric to compute error profiles for lod models multirate filtering in the frequency space of a reference object image is utilized to approximate multiple distant views over a range of orientations an importance sampling method is described to better characterize perspective projection over view distance a contrast sensitivity function csf is employed to approximate the response of the vision system examples are presented for multiresolution spheres and a terrain height field feature future directions for extending this method are described
this is basic research for assigning color values to voxels of multichannel mri volume data the mri volume data sets obtained under different scanning conditions are transformed into their components by independent component analysis ica which enhances the physical characteristics of the tissue the transfer_functions for generating color values from independent components are obtained using a radial basis_function network a kind of neural net by training the network with sample data chosen from the visible female data set the resultant color volume data sets correspond well with the full-color cross-sections of the visible human data sets
accurately and automatically conveying the structure of a volume model is a problem that has not been fully solved by existing volume_rendering approaches physics-based volume_rendering approaches create images which may match the appearance of translucent materials in nature but may not embody important structural details transfer_function approaches allow flexible design of the volume appearance but generally require substantial hand-tuning for each new data set in order to be effective we introduce the volume_illustration approach combining the familiarity of a physics-based illumination model with the ability to enhance important features using non-photorealistic_rendering techniques since the features to be enhanced are defined on the basis of local volume characteristics rather than volume sample values the application of volume_illustration techniques requires less manual tuning than the design of a good transfer_function volume_illustration provides a flexible unified framework for enhancing structural perception of volume models through the amplification of features and the addition of illumination effects
presents a two-level approach for fusing direct_volume_rendering dvr and maximum-intensity projection mip within a joint rendering method different structures within the data set are rendered locally by either mip or dvr on an object-by-object basis globally all the results of subsequent object renderings are combined in a merging step usually compositing in our case this allows us to selectively choose the most suitable technique for depicting each object within the data while keeping the amount of information contained in the image at a reasonable level this is especially useful when inner structures should be visualized together with semi-transparent outer parts similar to the focus-and-context approach known from information_visualization we also present animplementation of our approach which allows us to explore volumetric_data using two-level rendering at interactive frame rates
splatting is widely applied in many areas including volume point-based and image-based_rendering improvements to splatting such as eliminating popping and color bleeding occasion-based acceleration post-rendering classification and shading have all been recently accomplished these improvements share a common need for efficient frame-buffer accesses we present an optimized software splatting package using a newly designed primitive called fastsplat to scan-convert footprints our approach does not use texture_mapping hardware but supports the whole pipeline in memory in such an integrated pipeline we are then able to study the optimization strategies and address image quality issues while this research is meant for a study of the inherent trade-off of splatting our renderer purely in software achieves 3- to 5-fold speedups over a top-end texture hardwareimplementation for opaque data sets we further propose a method of efficient occlusion_culling using a summed area table of opacity 3d solid texturing and bump mapping capabilities are demonstrated to show the flexibility of such an integrated rendering pipeline a detailed numerical error analysis in addition to the performance and storage issues is also presented our approach requires low storage and uses simple operations thus it is easily implementable in hardware
presents a new rendering technique for processing multiple multi-resolution textures of lod level-of-detail terrain models and describes its application to interactive animated terrain content design the approach is based on a multi-resolution model for terrain texture which cooperates with a multi-resolution model for terrain geometry for each texture layer an image pyramid and a texture tree are constructed multiple texture layers can be associated with one terrain model and can be combined in different ways eg by blending and masking the rendering algorithm simultaneously traverses the multi-resolution geometry model and the multi-resolution texture model and takes into account geometric and texture approximation errors it uses multi-pass rendering and exploits multi-texturing to achieve real-time performance applications include interactive texture lenses texture animation and topographic textures these techniques offer an enormous potential for developing new visualization_applications for presenting exploring and manipulating spatio-temporal_data
geometric models are often annotated to provide additional information during visualization maps may be marked with rivers roads or topographical information and cad data models may highlight the underlying mesh structure while this additional information may be extremely useful there is a rendering cost associated with it texture maps have often been used to convey this information at relatively low cost but they suffer from blurring and pixelization at high magnification we present a technique for simplifying surface annotations based on directed asymmetric tolerance by maintaining the annotations as geometry as opposed to textures we are able to simplify them while still maintaining the overall appearance of the model over a wide range of magnifications texture maps may still be used to provide low-resolution surface detail such as color we demonstrate a significant gain in rendering performance while retaining the original appearance of objects from many application domains
discusses the concept of uniform frequency images which exhibit uniform local frequency properties such images make optimal use of space when sampled close to their nyquist limit a warping function may be applied to an arbitrary image to redistribute its local frequency content reducing its highest frequencies and increasing its lowest frequencies in order to approach this uniform frequency ideal the warped image may then be downsampled according to its new reduced nyquist limit thereby reducing its storage requirements to reconstruct the original image the inverse warp is applied we present a general top-down algorithm to automatically generate a piecewise-linear warping function with this frequency balancing property for a given input image the image size is reduced by applying the warp and then downsampling we store this warped downsampled image plus a small number of polygons with texture coordinates to describe the inverse warp the original image is later reconstructed by rendering the associated polygons with the warped image applied as a texture map a process which is easily accelerated by current graphics_hardware as compared to previous image compression techniques we generate a similar graceful space-quality tradeoff with the advantage of being able to "uncompress" images during rendering we report results for several images with sizes ranging from 15000 to 300000 pixels achieving reduction rates of 70-90% with improved quality over downsampling alone
multiresolution methods are becoming increasingly important tools for the interactive_visualization of very large_data sets multiresolution isosurface visualization allows the user to explore volume data using simplified and coarse representations of the isosurface for overview images and finer resolution in areas of high interest or when zooming into the data ideally a coarse isosurface should have the same topological structure as the original the topological genus of the isosurface is one important property which is often neglected in multiresolution algorithms this results in uncontrolled topological changes which can occur whenever the level-of-detail is changed the scope of this paper is to propose an efficient technique which allows preservation of topology as well as controlled topology simplification in multiresolution isosurface_extraction
visualization algorithms have seen substantial improvements in the past several years however very few algorithms have been developed for directly studying data in dimensions higher than three most algorithms require a sampling in three-dimensions before applying any visualization algorithms this sampling typically ignores vital features that may be present when examined in oblique cross-sections and places an undo burden on system resources when animation through additional dimensions is desired for time-varying_data of large_data sets smooth animation is desired at interactive rates we provide a fast marching_cubes like algorithm for hypercubes of any dimension to support this we have developed a new algorithm to automatically generate the isosurface and triangulation tables for any dimension this allows the efficient calculation of 4d isosurfaces which can be interactively sliced to provide smooth animation or slicing through oblique hyperplanes the former allows for smooth animation in a very compressed format the latter provide better tools to study time-evolving features as they move downstream we also provide examples in using this technique to show interval_volumes or the sensitivity of a particular isovalue threshold
we present a novel method to extract iso-surfaces from distance volumes it generates high_quality semi-regular multiresolution_meshes of arbitrary topology our technique proceeds in two stages first a very coarse mesh with guaranteed topology is extracted subsequently an iterative multi-scale force-based solver refines the initial mesh into a semi-regular mesh with geometrically adaptive sampling rate and good aspect_ratio triangles the coarse mesh extraction is performed using a new approach we call surface wavefront propagation a set of discrete iso-distance ribbons are rapidly built and connected while respecting the topology of the iso-surface implied by the data subsequent multi-scale refinement is driven by a simple force-based solver designed to combine good iso-surface fit and high_quality sampling through reparameterization in contrast to the marching_cubes technique our output meshes adapt gracefully to the iso-surface geometry have a natural multiresolution structure and good aspect_ratio triangles as demonstrated with a number of examples
a standard way to segment medical imaging datasets is by tracing contours around regions of interest in parallel planar slices unfortunately the standard methods for reconstructing three dimensional surfaces from those planar contours tend to be either complicated or not very robust furthermore they fail to consistently mesh abutting structures which share portions of contours we present a novel straight-forward algorithm for accurately and automatically reconstructing surfaces from planar contours our algorithm is based on scanline rendering and separating_surface_extraction by rendering the contours as distinctly colored polygons and reading back each rendered slice into a segmented volume we reduce the complex problem of building a surface from planar contours to the much simpler problem of extracting separating_surfaces from a classified volume our scanline surfacing algorithm robustly handles complex surface topologies such as bifurcations embedded features and abutting surfaces
throughout the design cycle visualization whether a sketch scribbled on the back of a spare piece of paper or a fully detailed drawing has been the mainstay of design we need to see the product one of the most important stages of the design cycle is the initial or concept stage and it is here that design variants occur in large numbers to be vetted quickly at this initial stage the human element the designer is crucial to the success of the product we describe an interactive environment for concept design which recognises the needs of the designer not only to see the product and make rapid modifications but also to monitor the progress of their design towards some preferred solution this leads to the notion of a design parameter space typically high-dimensional which must also be visualized in addition to the product itself using a module developed for iris explorer design steering is presented as a navigation of this space in order to search for optimal designs either manually or by local optimisation
multi-dimensional entities are modeled displayed and understood with a new algorithm vectorizing data of any dimensionality this algorithm is called sbp it is a vectorized generalization of parallel_coordinates classic geometries of any dimensionality can be demonstrated to facilitate perception and understanding of the shapes generated by this algorithm sbp images of a 4d line a circle and 3d and 4d spherical helices are shown a strategy for synthesizing multi-dimensional models matching multi-dimensional_data is presented current applications include data_mining modeling data-defined structures of scientific interest such as protein structure and calabi-yau figures as multi-dimensional geometric entities generating vector-fused data signature fingerprints of classic frequency spectra that identify substances and treating complex targets as multi-dimensional entities for automatic target recognition sbp vector data signatures apply to all pattern_recognition problems
this paper describes a novel rendering technique for special relativistic visualization it is an image-based method which allows to render high speed flights through real-world scenes filmed by a standard camera the relativistic effects on image generation are determined by the relativistic aberration_of_light the doppler_effect and the searchlight_effect these account for changes of apparent geometry color and brightness of the objects it is shown how the relativistic effects can be taken into account by a modification of the plenoptic function therefore all known image-based nonrelativistic rendering methods can easily be extended to incorporate relativistic rendering ourimplementation allows interactive viewing of relativistic panoramas and the production of movies which show super-fast travel examples in the form of snapshots and film sequences are included
one of the main research topics in scientific_visualization is to "visualize the appropriate features" of a certain structure or data set geodesics are very important in geometry and physics but there is one major problem which prevents scientists from using them as a visualization tool the differential equations for geodesics are very complicated and in most cases numerical algorithms must be used there is always a certain approximation error involved how can you be sure to visualize the features and not only the approximation quality the paper presents an algorithm to overcome this problem it consists of two parts in the first a geometric method for the construction of geodesics of arbitrary surfaces is introduced this method is based on the fundamental property that geodesics are a generalization of straight lines on plains in the second part these geodesics are used to generate local nets on the surfaces
the compression of geometric structures is a relatively new field of data_compression since about 1995 several articles have dealt with the coding of meshes using for most of them the following approach the vertices of the mesh are coded in an order that partially contains the topology of the mesh in the same time some simple rules attempt to predict the position of each vertex from the positions of its neighbors that have been previously coded we describe a compression algorithm whose principle is completely different the coding order of the vertices is used to compress their coordinates and then the topology of the mesh is reconstructed from the vertices this algorithm achieves compression ratios that are slightly better than those of the currently available algorithms and moreover it allows progressive and interactive transmission of the meshes
in 1998 we introduced the idea for a project we call the office of the future our long-term vision is to provide a better every-day working environment with high-fidelity scene reconstruction for life-sized 3d tele-collaboration in particular we want a true sense of presence with our remote collaborator and their real surroundings the challenges related to this vision are enormous and involve many technical tradeoffs this is true in particular for scene reconstruction researchers have been striving to achieve real-time approaches and while they have made respectable progress the limitations of conventional technologies relegate them to relatively low resolution in a restricted volume we present a significant step toward our ultimate goal via a slightly different path in lieu of low-fidelity dynamic scene modeling we present an exceedingly high fidelity reconstruction of a real but static office by assembling the best of available hardware and software technologies in static scene acquisition modeling algorithms rendering tracking and stereo projective display we are able to demonstrate a portal to a real office occupied today by a mannequin and in the future by a real remote collaborator we now have both a compelling sense of just how good it could be and a framework into which we will later incorporate dynamic scene modeling as we continue to head toward our ultimate goal of 3d collaborative telepresence
we present an algorithm for compressing 2d vector_fields that preserves topology our approach is to simplify the given data set using constrained_clustering we employ different types of global and local error metrics including the earth mover's distance metric to measure the degradation in topology as well as weighted magnitude and angular errors as a result we obtain precise error bounds in the compressed vector_fields experiments with both analytic and simulated data sets are presented results indicate that one can obtain significant compression with low errors without losing topology information
topology analysis of plane turbulent vector_fields results in visual_clutter caused by critical_points indicating vortices of finer and finer scales a simplification can be achieved by merging critical_points within a prescribed radius into higher order critical_points after building clusters containing the singularities to merge the method generates a piecewise linear representation of the vector field in each cluster containing only one higher order singularity any visualization method can be applied to the result after this process using different maximal distances for the critical_points to be merged results in a hierarchy of simplified vector_fields that can be used for analysis on different scales
we present a new algorithm for material boundary interface reconstruction from data sets containing volume fractions we transform the reconstruction problem to a problem that analyzes the dual data set where each vertex in the dual mesh has an associated barycentric coordinate tuple that represents the fraction of each material present after constructing the dual tetrahedral mesh from the original mesh we construct material boundaries by mapping a tetrahedron into barycentric space and calculating the intersections with voronoi cells in barycentric space these intersections are mapped back to the original physical space and triangulated to form the boundary surface approximation this algorithm can be applied to any grid structure and can treat any number of materials per element/vertex
we present a novel approach to surface_reconstruction based on the delaunay complex first we give a simple and fast algorithm that picks locally a surface at each vertex for that we introduce the concept of λ-intervals it turns out that for smooth regions of the surface this method works very well and at difficult parts of the surface yields an output well-suited for postprocessing as a postprocessing step we propose a topological clean up and a new technique based on linear programming in order to establish a topologically correct surface these techniques should be useful also for many other reconstruction schemes
polyhedral meshes are used for visualization computer_graphics or geometric modeling purposes and result from many applications like iso-surface_extraction surface_reconstruction or cad/cam the paper introduces a method for constructing smooth_surfaces from a triangulated polyhedral mesh of arbitrary topology it presents a new algorithm which generalizes and improves the triangle 4-split method s hahmann and g-p bonneau in the crucial point of boundary curve network construction this network is then filled in by a visual smooth surface from which an explicit closed form parametrization is given furthermore the method becomes now completely local and can interpolate normal vector input at the mesh vertices
a new multiscale method in surface processing is presented which combines the image_processing methodology based on nonlinear diffusion equations and the theory of geometric evolution problems its aim is to smooth discretized surfaces while simultaneously enhancing geometric features such as edges and corners this is obtained by an anisotropic curvature evolution where time is the multiscale parameter here the diffusion tensor depends on the shape operator of the evolving surface a spatial finite_element discretization on arbitrary unstructured triangular_meshes and a semi-implicit finite difference discretization in time are the building blocks of the easy to code algorithm presented the systems of linear equations in each timestep are solved by appropriate preconditioned iterative solvers different applications underline the efficiency and flexibility of the presented type of surface processing tool
multi-resolution techniques and models have been shown to be effective for the display and transmission of large static geometric object dynamic environments with internally deforming models and scientific simulations using dynamic meshes pose greater challenges in terms of time and space and need the development of similar solutions we introduce the t-dag an adaptive multi-resolution representation for dynamic meshes with arbitrary deformations including attribute position connectivity and topology changes t-dag stands for time-dependent directed_acyclic_graph which defines the structure supporting this representation we also provide an incremental algorithm in time for constructing the t-dag representation of a given input mesh this enables the traversal and use of the multi-resolution dynamic model for partial playback while still constructing new time-steps
visualization can be an important tool for displaying categorizing and digesting large quantities of inter-related information during laboratory and simulation experiments summary visualizations that compare and represent data sets in the context of a collection are particularly valuable applicable visualizations used in these settings must be fast near real time and should allow the addition of data sets as they are acquired without requiring rerendering of the visualization this paper examines several visualization_techniques for representing collections of data sets in a combustion experiment including spectral displays tiling and geometric mappings of symmetry the application provides insight into how such visualizations might be used in practical real-time settings to assist in exploration and in conducting parameter space surveys
the display of iso-surfaces in medical data sets is an important visualization_technique used by radiologists for the diagnosis of volumetric density data sets the demands put by radiologists on such a display technique are interactivity multiple stacked transparent_surfaces and cutting_planes that allow an interactive clipping of the surfaces this paper presents a java based platform independentimplementation of a very fast surface rendering algorithm which combines the advantages of explicit surface_representation splatting and shear-warp projection to fulfill all these requirements the algorithm is implemented within the context of j-vision an application for viewing and diagnosing medical images which is currently in use at various hospitals
the paper describes a computer modeling and simulation system that supports computational_steering which is an effort to make the typical simulation workflow more efficient our system provides an interface that allows scientists to perform all of the steps in the simulation process in parallel and online it uses a standard network flow_visualization package which has been extended to display graphical output in an immersive virtual environment such as a cave our system allows scientists to interactively manipulate simulation parameters and observe the results it also supports inverse steering where the user specifies the desired simulation result and the system searches for the simulation parameters that achieve this result taken together these capabilities allow scientists to more efficiently and effectively understand model behavior as well as to search through simulation parameter space the paper is also a case_study of applying our system to the problem of simulating microwave interactions with missile bodies because these interactions are difficult to study experimentally and have important effects on missile electronics there is a strong desire to develop and validate simulation models of this phenomena
general relativistic ray tracing is presented as a tool for gravitational physics it is shown how standard three-dimensional ray tracing can be extended to allow for general relativistic visualization this visualization_technique provides images as seen by an observer under the influence of a gravitational field and allows to probe space-time by null geodesics moreover a technique is proposed for visualizing the caustic surfaces generated by a gravitational lens the suitability of general relativistic ray tracing is demonstrated by means of two examples namely the visualization of the rigidly rotating disk of dust and the warp drive metric
for a comprehensive understanding of tomographic image data in medicine interactive and high-quality direct_volume_rendering is an essential prerequisite this is provided by visualization using 3d texture_mapping which is still limited to high-end graphics_hardware in order to make it available in a clinical environment we present a system which uniquely combines local desktop computers and remote high-end graphics_hardware in this context we exploit the standard visualization capabilities to a maximum which are available in the clinical environment for 3d representations of high resolution and quality we access the remote specialized hardware various tools for 2d and 3d_visualization are provided which meet the requirements of a medical diagnosis this is demonstrated with examples from the field of neuroradiology which show the value of our strategy in practice
we describe a toolkit for the design and visualization of flexible artificial heart valves the toolkit consists of interlinked modules with a visual programming interface the user of the toolkit can set the initial geometry and material properties of the valve leaflet solve for the flexing of the leaflet and the flow of blood around it and display the results using the visualization capabilities of the toolkit the interactive nature of our environment is highlighted by the fact that changes in leaflet properties are immediately reflected in the flow_field and response of the leaflet hence the user may in a single session investigate a broad range of designs each one of which provides important information about the blood flow and motion of the valve during the cardiac cycle
we present an immersive system for exploring numerically simulated flow data through a model of a coronary artery graft this tightly-coupled interdisciplinary project is aimed at understanding how to reduce the failure rate of these grafts the visualization system provides a mechanism for exploring the effect of changes to the geometry to the flow and for exploring potential sources of future lesions the system uses gestural and voice interactions exclusively moving away from more traditional windows/icons/menus/point-and-click wimp interfaces we present an example session using the system and discuss our experiences developing testing and using it we describe some of the interaction and rendering techniques that we experimented with and describe their level of success our experience suggests that systems like this are exciting to clinical researchers but conclusive evidence of their value is not yet available
virtual_endoscopy presents the cross-sectional acquired 3d-data of a computer tomograph as an endoluminal view the common approach for the visualization of a virtual_endoscopy is surface rendering yielding images close to a real endoscopy if external structures are of interest volume_rendering techniques have to be used these methods do not display the exact shape of the inner lumen very well for certain applications eg operation_planning of a transbronchial biopsy both the shape of the inner lumen as well as outer structures like blood_vessels and the tumor have to be delineated a method is described that allows a quick and easy hybrid_visualization using overlays of different visualization methods like different surfaces or volume_renderings with different transfer_functions in real time on a low-end pc to achieve real time frame rates image based rendering techniques have been used
the study of time dependent characteristics of proteins is important for gaining insight into many biological processes however visualizing protein_dynamics by animating atom trajectories does not provide satisfactory results when the trajectory is sampled with large times steps the impression of smooth motion will be destroyed due to the effects of temporal aliasing sampling with small time steps will result in the camouflage of interesting motions in this case_study we discuss techniques for the interactive 3d_visualization of the dynamics of the photoactive yellow protein we use essential dynamics methods to filter out uninteresting atom motions from the larger concerted motions in this way clear and concise 3d animations of protein motions can be produced in addition we discuss various interactive techniques that allow exploration of the essential subspace of the protein we discuss the merits of these techniques when applied to the analysis of the yellow protein
the authors present a visualization system for interactive real time animation and visualization of simulation results from a parallel particle-in-cell code the system was designed and implemented for the onyx2 infinite reality hardware a number of different visual objects such as volume rendered particle density functionals were implemented to provide sufficient frame rates for interactive_visualization the system was designed to provide performance close to the hardware specifications both in terms of the i/o and graphics subsystems the presented case_study applies the developed system to the evolution of an instability that gives rise to a plasma surfatron a mechanism which rapidly can accelerate particles to very high velocities and thus be of great importance in the context of electron acceleration in astrophysical shocks in the solar corona and in particle accelerators the produced visualizations have allowed us to identify a previously unknown saturation mechanism for the surfatron and direct research efforts into new areas of interest
the microscopic analysis of time dependent 3d live cells provides considerable challenges to visualization effective visualization can provide insight into the structure and functioning of living cells the paper presents a case_study in which a number of visualization_techniques were applied to analyze a specific problem in cell biology the condensation and de-condensation of chromosomes during cell division the spatial complexity of the data required sophisticated presentation techniques the interactive virtual_reality enabled visualization system proteus specially equipped for time dependent 3d data sets is described an important feature of proteus is that it is extendible to cope with application-specific demands
non-traditional applications of scientific data challenge the typical approaches to visualization in particular popular scientific_visualization strategies fail when the expertise of the data consumer is in a different field than the one that generated the data and data from the user's domain must be utilized as well this problem occurs when predictive weather simulations are used for a number of weather-sensitive applications a data fusion approach is adopted for visualization design and utilized for specific example problems
applications_of_visualization_techniques that facilitate comparison of simulation and field datasets of seafloor hydrothermal plumes are demonstrated in order to explore and confirm theories of plume behavior in comparing these datasets there is no one-to-one correspondence we show the comparison by performing quantitative capturing of large scale observable features the comparisons are needed not only to improve the relevance of the simulations to the field observations but also to enable real time adjustment of shipboard data collection systems our approach for comparing simulation and field datasets is to use skeletonization and centerline representation features representing plumes are skeletonized skeleton points are used to construct a centerline and to quantify plume properties on planes normal to the centerline these skeleton points are further used to construct an idealized cone representing a plume isosurface the difference between the plume feature and the cone is identified as protrusions of turbulent eddies comparison of the simulation and field data sets through these abstractions illustrates how these abstractions characterize a plume
in our study of regional climate modeling and simulation we frequently encounter vector_fields that are crowded with large numbers of critical_points a critical point in a flow is where the vector field vanishes while these critical_points accurately reflect the topology of the vector_fields in our study only a subset of them is worth further investigation we present a filtering technique based on the vorticity of the vector_fields to eliminate the less interesting and sometimes sporadic critical_points in a multiresolution fashion the neighboring regions of the preserved features which are characterized by strong shear and circulation are potential locations of weather instability we apply our feature filtering technique to a regional climate modeling data set covering east asia in the summer of 1991
weave workbench environment for analysis and visual_exploration is an environment for creating interactive_visualization_applications weave differs from previous systems in that it provides transparent linking between custom 3d_visualizations and multidimensional statistical representations and provides interactive color brushing between all visualizations the authors demonstrate how weave can be used to rapidly prototype a biomedical application weaving together simulation data measurement data and 3d anatomical data concerning the propagation of excitation in the heart these linked statistical and custom three-dimensional visualizations of the heart can allow scientists to more effectively study the correspondence of structure and behavior
using inductive learning techniques to construct classification models from large high-dimensional_data sets is a useful way to make predictions in complex domains however these models can be difficult for users to understand we have developed a set of visualization methods that help users to understand and analyze the behavior of learned models including techniques for high-dimensional_data space projection display of probabilistic predictions variable/class correlation and instance mapping we show the results of applying these techniques to models constructed from a benchmark data set of census data and draw conclusions about the utility of these methods for model understanding
visualization_techniques enable scientists to interactively explore 3d data sets segmenting and cutting them to reveal inner structure while powerful these techniques suffer from one serious flaw-the images they create are displayed on a flat piece of glass or paper it is not really 3d-it can only be made to appear 3d we describe the construction of 3d physical models from volumetric_data using solid freeform fabrication equipment these models are built as separate interlocking pieces that express in physical form the segmentation and cutting operations common in display-based visualization
we demonstrate the use of a combination of perceptually effective techniques for visualizing magnetic field data from the diii-d tokamak these techniques can be implemented to run very efficiently on machines with hardware support for opengl interactive speeds facilitate clearcommunication of magnetic field structure enhancing fusion scientists' understanding of their data and thereby accelerating their research
the paper describes the effective real-time_visualization of the clear-up operation of a former us nuclear submarine base located in holy loch scotland the whole field modelling system has provided an extremely accurate real-time_visualization of a large number of varying parameters such as remotely operated vehicles cranes barges grabs magnets and detailed seabed topography the system has improved the field staffs' spatial and temporal awareness of the underwater environment and facilitated decision-making within the complex offshore working environment
scaling of simulations challenges the effectiveness of conventional visualization methods this problem becomes two-fold for mesoscale weather models that operate in near-real-time at cloud-scale resolution for example typical approaches to vector_field_visualization eg wind are based upon global methods which may not illustrate detailed structure in addition such computations employ multi-resolution meshes to capture small-scale phenomena which are not properly reflected in both vector and scalar realizations to address the former critical point analysis and simple bandpass filtering of wind fields is employed for better seed point identification of streamline calculations for the latter an encapsulation of nested computational meshes is developed for general realization it is then combined with the seed point calculation for an improved vector visualization of multi-resolution weather forecasting data
in this paper we present angular brushing for parallel_coordinates pc as a new approach to highlighting rational data-properties ie features which - in a non-separable way - depend on two data dimensions we also demonstrate smooth brushing as an intuitive tool for specifying nonbinary degree-of-interest functions for focus+context_visualization we also briefly describe ourimplementation as well as its application to the visualization of cfd data
cartograms are a well-known technique for showing geography-related statistical information such as population demographics and epidemiological data the basic idea is to distort a map by resizing its regions according to a statistical parameter but in a way that keeps the map recognizable we deal with the problem of making continuous cartograms that strictly retain the topology of the input mesh we compare two algorithms to solve the continuous cartogram problem the first one uses an iterative relocation of the vertices based on scanlines the second one is based on the gridfit technique which uses pixel-based distortion based on a quadtree-like data structure
a large body of results on the characteristics of human spatial vision suggests that space perception is distorted recent studies indicate that the geometry of visual space is best understood as affine if this is the case it has far reaching implications on how 3d_visualizations can be successfully employed for instance all attempts to build visualization_systems where users are expected to discover relations based on euclidean distances or shapes will be ineffective because visualization can and sometimes do employ all possible types of depth information and because the results from vision research usually concentrates on one or two such types three experiments were performed under near optimal viewing conditions the aim of the experiments was twofold to test whether the earlier findings generalize to optimal viewing conditions and to get a sense of the size of the error under such conditions the results show that the findings do generalize and that the errors are large the implications of these results for successful visualizations are discussed
information analysis often involves decomposing data into sub-groups to allow for comparison and identification of relationships breakdown visualization provides a mechanism to support this analysis through user guided drill-down of polyarchical metadata this metadata describes multiple hierarchical structures for organizing tuple aggregations and table attributes this structure is seen in financial data organizational structures sport statistics and other domains a spreadsheet format enables comparison of visualizations at any level of the hierarchy breakdown visualization allows users to drill-down a single hierarchy then pivot into another hierarchy within the same view it utilizes a fix and move technique that allows users to select multiple foci for drill-down
we present a novel tree browser that builds on the conventional node link tree diagrams it adds dynamic rescaling of branches of the tree to best fit the available screen space optimized camera movement and the use of preview icons summarizing the topology of the branches that cannot be expanded in addition it includes integrated search and filter functions this paper reflects on the evolution of the design and highlights the principles that emerged from it a controlled experiment showed benefits for navigation to already previously visited nodes and estimation of overall tree topology
data-mining of information by the process of pattern discovery in protein sequences has been predominantly algorithm based we discuss a visualization approach which uses texture_mapping and blending techniques to perform visual data-mining on text data obtained from discovering patterns in protein sequences this visual approach investigates the possibilities of representing text data in three dimensions and provides new possibilities of representing more dimensions of information in text data visualization and analysis we also present a generic framework derived from this visualization approach to visualize text in biosequence data
we describe a new method for the visualization of tree structured relational_data it can be used especially for the display of very large hierarchies in a 2-dimensional space we discuss the advantages and limitations of current techniques of tree_visualization our strategy is to optimize the drawing of trees in a geometrical plane and maximize the utilization of display space by allowing more nodes and links to be displayed at a limit screen resolution we use the concept of enclosure to partition the entire display space into a collection of local regions that are assigned to all nodes in tree t for the display of their sub-trees and themselves to enable the exploration of large hierarchies we use a modified semantic_zooming technique to view the detail of a particular part of the hierarchy at a time based on user's interest layout animation is also provided to preserve the mental_map while the user is exploring the hierarchy by changing zoomed views
most analysts start with an overview of the data before gradually refining their view to be more focused and detailed multiscale pan-and-zoom systems are effective because they directly support this approach however generating abstract overviews of large_data sets is difficult and most systems take advantage of only one type of abstraction visual_abstraction furthermore these existing systems limit the analyst to a single zooming path on their data and thus a single set of abstract views this paper presents 1 a formalism for describing multiscale visualizations of data_cubes with both data and visual_abstraction and 2 a method for independently zooming along one or more dimensions by traversing a zoom graph with nodes at different levels of detail as an example of how to design multiscale visualizations using our system we describe four design_patterns using our formalism these design_patterns show the effectiveness of multiscale visualization of general relational_databases
for a decade the ruling common wisdom for internet traffic held that it was everywhere bursty over periods lasting tens of milliseconds to hundreds the traffic was either much below its average rate or much above in other words the traffic was not smooth not staying at all times close to its average it was bursty on the cable running down a street carrying the merged traffic of a small number of cable modem users in one section of a town it was bursty on the core fiber of an internet service provider carrying the merged traffic of thousands of users from all over the country the internet was designed to accommodate the bursty traffic the routers and switches that forward traffic from one place to the next were designed for burstiness and internet service providers allocated traffic loads on the devices based on an assumption of burstiness recently it was discovered that the old common wisdom is not true visualization played a fundamental role in the discovery the old wisdom held up for links with a small numbers of users but as the number of users increases the burstiness dissipates and the traffic becomes smooth design of the high-load part of the internet needs to be rethought the old wisdom had persisted for high-load links because the databases of traffic measurements from them are immense and the traffic measurements had not been studied in their fullest detail which is necessary to see the smoothing visualization tools allowed the detail to be seen and allowed the verification of a mathematical theory that predicts the smoothing to see the detail individual visual displays were created that take up an amount of virtual screen real estate measured in hundreds of pages it is a simple idea if you have a lot of data and you want to see it in detail you need a lot of space what is needed now is a rich set of ideas and methods for navigating such very large displays
we describe a visualization tool which allows a biologist to explore a large set of hypothetical evolutionary trees interacting with such a dataset allows the biologist to identify distinct hypotheses about how different species or organisms evolved which would not have been clear from traditional analyses our system integrates a point-set_visualization of the distribution of hypothetical trees with detail views of an individual tree or of a consensus tree summarizing a subset of trees efficient algorithms were required for the key tasks of computing distances between trees finding consensus trees and laying out the point-set_visualization
visualization is a powerful way to facilitate data_analysis but it is crucial that visualization_systems explicitly convey the presence nature and degree of uncertainty to users otherwise there is a danger that data will be falsely interpreted potentially leading to inaccurate conclusions a common method for denoting uncertainty is to use error bars or similar techniques designed to convey the degree of statistical uncertainty while uncertainty can often be modeled statistically a second form of uncertainty bounded uncertainty can also arise that has very different properties than statistical uncertainty error bars should not be used for bounded uncertainty because they do not convey the correct properties so a different technique should be used instead we describe a technique for conveying bounded uncertainty in visualizations and show how it can be applied systematically to common displays of abstract charts and graphs interestingly it is not always possible to show the exact degree of uncertainty and in some cases it can only be displayed approximately
existing information_visualization_techniques are usually limited to the display of a few thousand items this article describes new interactive techniques capable of handling a million items effectively visible and manageable on screen we evaluate the use of hardware-based techniques available with newer graphics cards as well as new animation techniques and non-standard graphical features such as stereovision and overlap count these techniques have been applied to two popular information_visualizations treemaps and scatter plot diagrams but are generic enough to be applied to other 2d representations as well
example-based graphics generation systems automatically create new information_visualizations by learning from existing graphic examples as part of the effort on developing a general-purpose example-based generation system we are building a visual database of graphic examples in this paper we address two main issues involved in constructing such a database example selection and example modeling as a result our work offers three unique contributions first we build a visual database that contains a diverse collection of well-designed examples second we develop a feature-based scheme to model all examples uniformly and accurately third our visual database brings several important implications to the area of information_visualization
we present an extremely fast graph_drawing algorithm for very large graphs which we term ace for algebraic multigrid computation of eigenvectors ace exhibits an improvement of something like two orders of magnitude over the fastest algorithms we are aware of it draws graphs of millions of nodes in less than a minute ace finds an optimal drawing by minimizing a quadratic energy function the minimization problem is expressed as a generalized eigenvalue problem which is rapidly solved using a novel algebraic multigrid technique the same generalized eigenvalue problem seems to come up also in other fields hence ace appears to be applicable outside of graph_drawing too
beamtrees are a new method for the visualization of large hierarchical_data sets nodes are shown as stacked circular beams such that both the hierarchical structure as well as the size of nodes are depicted the dimensions of beams are calculated using a variation of the treemap algorithm a small user_study indicated that beamtrees are significantly more effective than nested treemaps and cushion treemaps for the extraction of global hierarchical information
a new method for visualizing the class of incrementally evolving networks is presented in addition to the intermediate states of the network it conveys the nature of the change between them by unrolling the dynamics of the network each modification is shown in a separate layer of a three-dimensional representation where the stack of layers corresponds to a time line of the evolution we focus on discourse networks as the driving application but our method extends to any type of network evolving in similar ways
we demonstrate how we apply information_visualization_techniques to process monitoring virtual instruments are enhanced using history encoding instruments are capable of displaying the current value and the value from the near past multi-instruments are capable of displaying several data sources simultaneously levels of detail for virtual instruments are introduced where the screen area is inversely proportional to the information amount displayed furthermore the monitoring system is enhanced by using 3d anchoring attachment of instruments to positions on a 3d model collision avoidance a physically based spring model prevents instruments from overlapping and focus+context rendering - giving the user a possibility to examine particular instruments in detail without loosing the context information
we describe a system for analyzing the flow of traffic through web sites we decomposed the general path analysis problem into a set of distinct subproblems and created a visual metaphor for analyzing each of them our system works off of multiple representations of the clickstream and exposes the path extraction algorithms and data to the visual metaphors as web services we have combined the visual metaphors into a web-based "path analysis portal" that lets the user easily switch between the different modes of analysis
research in several areas provides scientific guidance for use of graphical encoding to convey information in an information_visualization display by graphical encoding we mean the use of visual display elements such as icon color shape size or position to convey information about objects represented by the icons literature offers inconclusive and often conflicting viewpoints including the suggestion that the effectiveness of a graphical encoding depends on the type of data represented our empirical_study suggests that the nature of the users' perceptual task is more indicative of the effectiveness of a graphical encoding than the type of data represented
this paper introduces a new visualization method the arc diagram which is capable of representing complex patterns of repetition in string data arc diagrams improve over previous methods such as dotplots because they scale efficiently for strings that contain many instances of the same subsequence this paper describes design andimplementation issues related to arc diagrams and shows how they may be applied to visualize such diverse data as music text and compiled code
relational_databases provide significant flexibility to organize store and manipulate an infinite variety of complex data collections this flexibility is enabled by the concept of relational_data schemas which allow data owners to easily design custom databases according to their unique needs however user_interfaces and information_visualizations for accessing and utilizing databases have not kept pace with this level of flexibility this paper introduces the concept of visualization schemas based on the snap-together visualization model which are analogous to relational_data schemas visualization schemas enable users to rapidly construct customized multiple-view visualizations for databases in a similarly flexible fashion without programming since the design of appropriate visualizations for a given database depends on the data schema visualization schemas are a natural analogy to the data schema concept
since the crash of the dotcoms investors have gotten a lot more careful with where they place their money now more than ever it becomes really important for venture capitalists vcs to monitor the state of the startups market and continually update their investment strategy to suit the rapidly changing market conditions this paper presents three new visualization_metaphors spiral map timeticker and double histogram for monitoring the startups market while we are focusing on the vc domain the visual metaphors developed are general and can be easily applied to other domains
radial space-filling rsf techniques for hierarchy_visualization have several advantages over traditional node-link_diagrams including the ability to efficiently use the display space while effectively conveying the hierarchy structure several rsf systems and tools have been developed to date each with varying degrees of support for interactive operations such as selection and navigation we describe what we believe to be a complete set of desirable operations on hierarchical structures we then present interring an rsf hierarchy_visualization system that supports a significantly more extensive set of these operations than prior systems in particular interring supports multi-focus distortions interactive hierarchy reconfiguration and both semi-automated and manual selection we show the power and utility of these and other operations and describe our on-going efforts to evaluate their effectiveness and usability
we discuss 3d_interaction techniques for the quantitative analysis of spatial relations in medical visualizations we describe the design andimplementation of measurement tools to measure distances angles and volumes in 3d_visualizations the visualization of measurement tools as recognizable 3d objects and a 3d_interaction which is both intuitive and precise determines the usability of such facilities measurements may be carried out in 2d visualizations of the original radiological data and in 3d_visualizations the result of a measurement carried out in one view is also displayed in the other view appropriately we discuss the validation of the obtained measures finally we describe how some important measurement tasks may be solved automatically
we present a robust noise-resistant criterion characterizing plane-like skeletons in binary voxel objects it is based on a distance map and the geodesic distance along the object's boundary a parameter allows us to control the noise sensitivity if needed homotopy with the original object might be reconstructed in a second step using an improved distance ordered thinning algorithm the skeleton is analyzed to create a geometric representation for rendering plane-like parts are transformed into an triangulated surface not enclosing a volume by a suitable triangulation scheme the resulting surfaces have lower triangle count than those created with standard methods and tend to maintain the original geometry even after simplification with a high decimation rate our algorithm allows us to interactively render expressive images of complex 3d structures emphasizing independently plane-like and rod-like structures the methods are applied for visualization of the microstructure of bone biopsies
visualization of tubular structures such as blood_vessels is an important topic in medical imaging one way to display tubular structures for diagnostic purposes is to generate longitudinal cross-sections in order to show their lumen wall and surrounding tissue in a curved plane this process is called curved_planar_reformation cpr we present three different methods to generate cpr images a tube-phantom was scanned with computed_tomography ct to illustrate the properties of the different cpr methods furthermore we introduce enhancements to these methods thick-cpr rotating-cpr and multi-path-cpr
this paper presents a new technique for the extraction of surfaces from 3d ultrasound data surface_extraction from ultrasound data is challenging for a number of reasons including noise and artifacts in the images and nonuniform data sampling a method is proposed to fit an approximating radial basis_function to the group of data samples an explicit surface is then obtained by iso-surfacing the function in most previous 3d ultrasound research a pre-processing step is taken to interpolate the data into a regular voxel array and a corresponding loss of resolution we are the first to represent the set of semi-structured ultrasound pixel data as a single function from this we are able to extract surfaces without first reconstructing the irregularly spaced pixels into a regular 3d voxel array
we present a new algorithm for rendering very large volume data sets at interactive frame rates on standard pc_hardware the algorithm accepts scalar_data sampled on a regular grid as input the input data is converted into a compressed hierarchical wavelet representation in a preprocessing step during rendering the wavelet representation is decompressed on-the-fly and rendered using hardware texture_mapping the level of detail used for rendering is adapted to the local frequency spectrum of the data and its position relative to the viewer using a prototypeimplementation of the algorithm we were able to perform an interactive walkthrough of large_data sets such as the visible human on a single off-the-shelf pc
by offering more detail and precision large_data sets can provide greater insights to researchers than small data sets however these data sets require greater computing resources to view and manage remote_visualization_techniques allow the use of computers that cannot be operated locally the semotus visum framework applies a high-performance client-server paradigm to the problem the framework utilizes both client and server resources via multiple rendering methods experimental results show the framework delivers high frame rates and low latency across a wide range of data sets
we present an external memory algorithm for fast display of very large and complex geometric environments we represent the model using a scene graph and employ different culling techniques for rendering acceleration our algorithm uses a parallel approach to render the scene as well as fetch objects from the disk in a synchronous manner we present a novel prioritized prefetching technique that takes into account lod-switching and visibility-based events between successive frames we have applied our algorithm to large gigabyte-sized environments that are composed of thousands of objects and tens of millions of polygons the memory overhead of our algorithm is output sensitive and is typically tens of megabytes in practice our approach scales with the model sizes and its rendering performance is comparable to that of an in-core algorithm
in this paper we are presenting a novel approach for rendering large_datasets in a view-dependent manner in a typical view-dependent rendering framework an appropriate level of detail is selected and sent to the graphics_hardware for rendering at each frame in our approach we have successfully managed to speed up the selection of the level of detail as well as the rendering of the selected levels we have accelerated the selection of the appropriate level of detail by not scanning active nodes that do not contribute to the incremental update of the selected level of detail our idea is based on imposing a spatial subdivision over the view-dependence trees data-structure which allows spatial tree cells to refine and merge in real-time rendering to comply with the changes in the active nodes list the rendering of the selected level of detail is accelerated by using vertex arrays to overcome the dynamic changes in the selected levels of detail we use multiple small vertex arrays whose sizes depend on the memory on the graphics_hardware these multiple vertex arrays are attached to the active cells of the spatial tree and represent the active nodes of these cells these vertex arrays which are sent to the graphics_hardware at each frame merge and split with respect to the changes in the cells of the spatial tree
this paper describes an efficient algorithm to model the light attenuation due to a participating_media with low albedo the light attenuation is modeled using splatting volume renderer for both the viewer and the light source during the rendering a 2d shadow buffer attenuates the light for each pixel when the contribution of a footprint is added to the image buffer as seen from the eye we add the contribution to the shadow buffer as seen from the light source we have generated shadows for point lights and parallel lights using this algorithm the shadow algorithm has been extended to deal with multiple light sources and projective textured lights
we propose new clipping methods that are capable of using complex geometries for volume clipping the clipping tests exploit per-fragment operations on the graphics_hardware to achieve high frame rates in combination with texture-based volume_rendering these techniques enable the user to interactively select and explore regions of the data set we present depth-based clipping techniques that analyze the depth structure of the boundary representation of the clip geometry to decide which parts of the volume have to be clipped in another approach a voxelized clip object is used to identify the clipped regions
we describe a method for volume_rendering using a spectral representation of colour instead of the traditional rgb model it is shown how to use this framework for a novel exploration of datasets through enhanced transfer_function_design furthermore our framework is extended to allow real-time re-lighting of the scene created with any rendering method the technique of post-illumination is introduced to generate new spectral images for arbitrary light colours in real-time also a tool is described to design a palette of lights and materials having certain properties such as selective metamerism or colour constancy applied to spectral transfer_functions different light colours can accentuate or hide specific qualities of the data in connection with post-illumination this provides a new degree of freedom for guided exploration of volumetric_data which cannot be achieved using the rgb model
direct_volume_rendering is a commonly used technique in visualization_applications many of these applications require sophisticated shading models to capture subtle lighting effects and characteristics of volumetric_data and materials many common objects and natural phenomena exhibit visual quality that cannot be captured using simple lighting models or cannot be solved at interactive rates using more sophisticated methods we present a simple yet effective interactive shading model which captures volumetric light attenuation effects to produce volumetric shadows and the subtle appearance of translucency we also present a technique for volume displacement or perturbation that allows realistic interactive modeling of high frequency detail for real and synthetic volumetric_data
we present a new multiphase method for efficiently simplifying polygonal surface models of arbitrary size it operates by combining an initial out-of-core uniform clustering phase with a subsequent in-core iterative edge contraction phase these two phases are both driven by quadric error metrics and quadrics are used to pass information about the original surface between phases the result is a method that produces approximations of a quality comparable to quadric-based iterative edge contraction but at a fraction of the cost in terms of running time and memory consumption
this paper introduces a method for smoothing complex noisy surfaces while preserving and enhancing sharp geometric features it has two main advantages over previous approaches to feature preserving surface smoothing first is the use of level set surface models which allows us to process very complex shapes of arbitrary and changing topology this generality makes it well suited for processing surfaces that are derived directly from measured data the second advantage is that the proposed method derives from a well-founded formulation which is a natural generalization of anisotropic_diffusion as used in image_processing this formulation is based on the proposition that the generalization of image filtering entails filtering the normals of the surface rather than processing the positions of points on a mesh
this paper introduces an algorithm for rapid progressive simplification of tetrahedral_meshes tetfusion we describe how a simple geometry decimation operation steers a rapid and controlled progressive simplification of tetrahedral_meshes while also taking care of complex mesh-inconsistency problems the algorithm features a high decimation ratio per step and inherently discourages any cases of self-intersection of boundary element-boundary intersection at concave boundary-regions and negative volume tetrahedra flipping we achieved rigorous reduction ratios of up to 98% for meshes consisting of 827904 elements in less than 2 minutes progressing through a series of level-of-details lods of the mesh in a controlled manner we describe how the approach supports a balanced re-distribution of space between tetrahedral elements and explain some useful control parameters that make it faster and more intuitive than 'edge collapse'-based decimation methods for volumetric meshes finally we discuss how this approach can be employed for rapid lod prototyping of large time-varying_datasets as an aid to interactive_visualization
we present a generalization of the geometry coder by touma and gotsman 1998 to polygon_meshes we let the polygon information dictate where to apply the parallelogram rule that they use to predict vertex positions since polygons tend to be fairly planar and fairly convex it is beneficial to make predictions within a polygon rather than across polygons this for example avoids poor predictions due to a crease angle between polygons up to 90 percent of the vertices can be predicted this way our strategy improves geometry compression by 10 to 40 percent depending on a how polygonal the mesh is and b on the quality planarity/convexity of the polygons
efficient and informative visualization of surfaces with uncertainties is an important topic with many applications in science and engineering examples include environmental pollution borderline identification identification of the limits of an oil basin or discrimination between contaminated and healthy tissue in medicine this paper presents an approach for such visualization using points as display primitives the approach is to render each polygon as a collection of points and to displace each point from the surface in the direction of the surface normal by an amount proportional to some random number multiplied by the uncertainty level at that point this approach can be used in combination with other techniques such as pseudo-coloring and shading to give rise to efficient and revealing visualizations the method is used to visualize real and simulated tumor formations with uncertainty of tumor boundaries
within the field of computer_graphics and visualization it is often necessary to visualize polygonal models with large number of polygons display quality is mandatory but it is also desirable to have the ability to rapidly update the display in order to facilitate interactive use point based rendering methods have been shown effective for this task building on this paradigm we introduce the pmr system which uses a hierarchy both in points and triangles for rendering this hierarchy is fundamentally different from the ones used in existing methods it is based on the feature geometry in the object space rather than its projection in the screen space this provides certain advantages over the existing methods
we introduce analyze and quantitatively compare a number of surface simplification methods for point-sampled geometry we have implemented incremental and hierarchical_clustering iterative simplification and particle simulation algorithms to create approximations of point-based models with lower sampling density all these methods work directly on the point cloud requiring no intermediate tesselation we show how local variation estimation and quadric error metrics can be employed to diminish the approximation error and concentrate more samples in regions of high curvature to compare the quality of the simplified surfaces we have designed a new method for computing numerical and visual error estimates for point-sampled surfaces our algorithms are fast easy to implement and create high-quality surface approximations clearly demonstrating the effectiveness of point-based surface simplification
isosurfaces are commonly used to visualize scalar_fields critical isovalues indicate isosurface topology changes the creation of new surface components merging of surface components or the formation of holes in a surface component therefore they highlight interesting isosurface behavior and are helpful in exploration of large trivariate data sets we present a method that detects critical isovalues in a scalar field defined by piecewise trilinear_interpolation over a rectilinear grid and describe how to use them when examining volume data we further review varieties of the marching_cubes mc algorithm with the intention of preserving topology of the trilinear interpolant when extracting an isosurface we combine and extend two approaches in such a way that it is possible to extract meaningful isosurfaces even when a critical value is chosen as the isovalue
typically 3-d mr and ct scans have a relatively high resolution in the scanning x-y plane but much lower resolution in the axial z direction this non-uniform sampling of an object can miss small or thin structures one way to address this problem is to scan the same object from multiple directions in this paper we describe a method for deforming a level set model using velocity information derived from multiple volume datasets with non-uniform resolution in order to produce a single high-resolution 3d model the method locally approximates the values of the multiple datasets by fitting a distance-weighted polynomial using moving least-squares the proposed method has several advantageous properties its computational cost is proportional to the object surface area it is stable with respect to noise imperfect registrations and abrupt changes in the data it provides gain-correction and it employs a distance-based weighting to ensures that the contributions from each scan are properly merged into the final result we have demonstrated the effectiveness of our approach on four multi-scan datasets a griffin laser scan reconstruction a ct scan of a teapot and mr scans of a mouse embryo and a zucchini
simulating hand-drawn illustration techniques can succinctly express information in a manner that is communicative and informative we present a framework for an interactive direct volume_illustration system that simulates traditional stipple drawing by combining the principles of artistic and scientific_illustration we explore several feature enhancement techniques to create effective interactive_visualizations of scientific and medical datasets we also introduce a rendering mechanism that generates appropriate point lists at all resolutions during an automatic preprocess and modifies rendering styles through different combinations of these feature enhancements the new system is an effective way to interactively preview large complex volume datasets in a concise meaningful and illustrative manner volume stippling is effective for many applications and provides a quick and efficient method to investigate volume models
we present a method for interactive rendering of large outdoor scenes complex polygonal plant models and whole plant populations are represented by relatively small sets of point and line primitives this enables us to show landscapes faithfully using only a limited percentage of primitives in addition a hierarchical_data structure allows us to smoothly reduce the geometrical representation to any desired number of primitives the scene is hierarchically divided into local portions of geometry to achieve large reductionfactors for distant regions additionally the data reduction is adapted to the visual importance of geometric objects this allows us to maintain the visual fidelity of the representation while reducing most of the geometry drastically with our system we are able to interactively render very complex landscapes with good visual quality
we propose the use of textured splats as the basic display primitives for an open surface fire model the high-detail textures help to achieve a smooth boundary of the fire and gain the small-scale turbulence appearance we utilize the lattice boltzmann model lbm to simulate physically-based equations describing the fire evolution and its interaction with the environment eg obstacles wind and temperature the property of fuel and non-burning objects are defined on the lattice of the computation domain a temperature field is also incorporated to model the generation of smoke from the fire due to incomplete combustion the linear and local characteristics of the lbm enable us to accelerate the computation with graphics_hardware to reach real-time simulation speed while the texture splat primitives enable interactive rendering frame rates
the bioactivity of a molecule strongly depends on its metastable conformational shapes and the transitions between these therefore conformation analysis and visualization is a basic prerequisite for the understanding of biochemical processes we present techniques for visual_analysis of metastable molecular conformations core of these are flexibly applicable methods for alignment of molecular geometries as well as methods for depicting shape and 'fuzziness' of metastable conformations all analysis tools are provided in an integrated working environment the described techniques are demonstrated with pharmaceutically active biomolecules
genevis provides a visual environment for exploring the dynamics of genetic regulatory networks at present time genetic regulation is the focus of intensive research worldwide and computational aids are being called for to help in the research offactors that are difficult to observe directly genevis provides a particle-based simulation of genetic networks and visualizes the process of this simulation as it occurs two dynamic_visualization_techniques are provided a visualization of the movement of the regulatory proteins and a visualization of the relative concentrations of these proteins several interactive tools relate the dynamic_visualizations to the underlying genetic network structure
to display the intuitive meaning of an abstract metric it is helpful to look on an embedded surface with the same inner geometry as the given metric the resulting partial_differential_equations have no standard solution only for some special cases satisfactory methods are known i present a new algorithmic approach which is not based on differential equations in contrast to other methods this technique also works if the embedding exists only locally the fundamental idea is to estimate euclidean distances from which the surface is built up in this paper i focus on the reconstruction of a surface from these estimated distances particular the influence of a perturbation of the distances on the shape of the resulting surface is investigated
level-of-detail rendering is essential for rendering very large detailed worlds in real-time unfortunately level-of-detail computations can be expensive creating a bottleneck at the cpu this paper presents the cabtt algorithm an extension to existing binary-triangle-tree-based level-of-detail algorithms instead of manipulating triangles the cabtt algorithm instead operates on clusters of geometry called aggregate triangles this reduces cpu overhead eliminating a bottleneck common to level-of-detail algorithms since aggregate triangles stay fixed over several frames they may be cached on the video card this further reduces cpu load and fully utilizes the hardware accelerated rendering pipeline on modern video cards these improvements result in a fourfold increase in frame rate over roam at high detail levels ourimplementation renders an approximation of an 8 million triangle height field at 42 frames per second with an maximum error of 1 pixel on consumer hardware
for some graphics applications object interiors and hard-to-see regions contribute little to the final images and need not be processed in this paper we define a view-independent visibility measure on mesh surfaces based on the visibility function between the surfaces and a surrounding sphere of cameras we demonstrate the usefulness of this measure with a visibility-guided simplification algorithm mesh simplification reduces the polygon counts of 3d models and speeds up the rendering process many mesh simplification algorithms are based on sequences of edge collapses that minimize geometric and attribute errors by combining the surface visibility measure with a geometric error measure we obtain simplified models with improvement proportional to the number of low visibility regions in the original models
finding the "best" viewing parameters for a scene is a difficult but very important problem fully automatic procedures seem to be impossible as the notion of "best" strongly depends on human judgment as well as on the application in this paper a solution to the sub-problem of placing light sources for given camera parameters is proposed a light position is defined to be optimal when the resulting illumination reveals more about the scene than illuminations from all other light positions ie the light position maximizes information that is added to the image through the illumination with the help of an experiment with several subjects we could adapt the information measure to the actually perceived information content we present fast global optimization procedures and solutions for two and more light sources
critical_points of a vector field are key to their characterization their positions as well as their indexes are crucial for understanding vector_fields considerable work exists in 2d but less is available for 3d or higher dimensions geometric algebra is a derivative of clifford algebra that not only enables a succinct definition of the index of a critical point in higher dimension it also provides insight and computational pathways for calculating the index we describe the problems in terms of geometric algebra and present an octree based solution using the algebra for finding critical_points and their index in a 3d vector field
surface texturing aids the visualization of polygonal_meshes by providing additional surface orientation cues and feature annotations such texturing is usually implemented via texture_mapping which is easier and more effective when the distortion of the mapping from the surface to the texture map is kept small we have previously shown that distortion occurs when areas of high surface curvature are flattened into the texture map by cutting the surface in these areas one can reduce texture map distortion at the expense of additional seam artifacts this paper describes a faster technique for guiding a texture map seam through high distortion regions while restricting the seam to regions of low visibility this results in distortion reducing seams that are less visually distracting and take less time to compute we have also observed that visibility considerations improve the speed of a recent method that adds cuts to reduce a surface genus
most systems used for creating and displaying colormap-based visualizations are not photometrically calibrated that is the relationship between rgb input levels and perceived luminance is usually not known due to variations in the monitor hardware configuration and the viewing environment however the luminance component of perceptually based colormaps should be controlled due to the central role that luminance plays in our visual processing we address this problem with a simple and effective method for performing luminance matching on an uncalibrated monitor the method is akin to the minimally distinct border technique a previous method of luminance matching used for measuring luminous efficiency but our method relies on the brain's highly developed ability to distinguish human faces we present a user_study showing that our method produces equivalent results to the minimally distinct border technique but with significantly improved precision we demonstrate how results from our luminance matching method can be directly applied to create new univariate colormaps
in this paper we present a verification algorithm for swirling features in flow_fields based on the geometry of streamlines the features of interest in this case are vortices without a formal definition existing detection algorithms lack the ability to accurately identify these features and the current method for verifying theaccuracy of their results is by human visual inspection our verification algorithm addresses this issue by automating the visual inspection process it is based on identifying the swirling streamlines that surround the candidate vortex cores we apply our algorithm to both numerically simulated and procedurally generated datasets to illustrate the efficacy of our approach
comparative evaluation of visualization and experimental results is a critical step in computational_steering in this paper we present a study of image comparison metrics for quantifying the magnitude of difference between visualization of a computer simulation and a photographic image captured from an experiment we examined eleven metrics including three spatial domain four spatial-frequency domain and four hvs human-vision system metrics among these metrics a spatial-frequency domain metric called 2nd-order fourier comparison was proposed specifically for this work our study consisted of two stages base cases and field trials the former is a general study on a controlled comparison space using purposely selected data and the latter involves imagery results from computational fluid dynamics and a rheological experiment this study has introduced a methodological framework for analyzing image-level methods used in comparative_visualization for the eleven metrics considered it has offered a set of informative indicators as to the strengths and weaknesses of each metric in particular we have identified three image comparison metrics that are effective in separating "similar" and "different" image groups our 2nd-order fourier comparison metric has compared favorably with others in two of the three tests and has shown its potential to be used for steering computer simulation quantitatively
the current state of the art in visualization research places strong emphasis on different techniques to derive insight from disparate types of data however little work has investigated the visualization process itself the information content of the visualization process - the results history and relationships between those results - is addressed by this work a characterization of the visualization process is discussed leading to a general model of the visualization exploration process the model based upon a new parameter derivation calculus can be used for automated reporting analysis or visualized directly an xml-based language for expressing visualization sessions using the model is also described these sessions can then be shared and reused by collaborators the model along with the xml representation provides an effective means to utilize information within the visualization process to further data_exploration
a long-standing research problem in computer_graphics is to reproduce the visual experience of walking through a large photorealistic environment interactively on one hand traditional geometry-based rendering systems fall short of simulating the visual realism of a complex environment on the other hand image-based_rendering systems have to date been unable to capture and store a sampled representation of a large environment with complex lighting and visibility effects in this paper we present a "sea of images" a practical approach to dense sampling storage and reconstruction of the plenoptic function in large complex indoor environments we use a motorized cart to capture omnidirectional images every few inches on a eye-height plane throughout an environment the captured images are compressed and stored in a multiresolution hierarchy suitable for real-time prefetching during an interactive walkthrough later novel images are reconstructed for a simulated observer by resampling nearby captured images our system acquires 15254 images over 1050 square feet at an average image spacing of 15 inches the average capture and processing time is 7 hours we demonstrate realistic walkthroughs of real-world environments reproducing specular reflections and occlusion effects while rendering 15-25 frames per second
this paper presents a vision-based geometric alignment system for aligning the projectors in an arbitrarily large display wall existing algorithms typically rely on a single camera view and degrade inaccuracy as the display resolution exceeds the camera resolution by several orders of magnitude naive approaches to integrating multiple zoomed camera views fail since small errors in aligning adjacent views propagate quickly over the display surface to create glaring discontinuities our algorithm builds and refines a camera homography tree to automatically register any number of uncalibrated camera images the resulting system is both faster and significantly more accurate than competing approaches reliably achieving alignment errors of 055 pixels on a 24-projector display in under 9 minutes detailed experiments compare our system to two recent display wall alignment algorithms both on our 18 megapixel display wall and in simulation these results indicate that our approach achieves sub-pixelaccuracy even on displays with hundreds of projectors
we present a method to code the multiresolution structure of a 3d triangle mesh in a manner that allows progressive decoding and efficient rendering at a client machine the code is based on a special ordering of the mesh vertices which has good locality and continuity properties inducing a natural multiresolution structure this ordering also incorporates information allowing efficient rendering of the mesh at all resolutions using the contemporary vertex buffer mechanism the performance of our code is shown to be competitive with existing progressive mesh_compression methods while achieving superior rendering speed
many computer_graphics operations such as texture_mapping 3d painting remeshing mesh_compression and digital_geometry_processing require finding a low-distortion parameterization for irregular connectivity triangulations of arbitrary genus 2-manifolds this paper presents a simple and fast method for computing parameterizations with strictly bounded distortion the new method operates by flattening the mesh onto a region of the 2d plane to comply with the distortion bound the mesh is automatically cut and partitioned on-the-fly the method guarantees avoiding global and local self-intersections while attempting to minimize the total length of the introduced seams to our knowledge this is the first method to compute the mesh partitioning and the parameterization simultaneously and entirely automatically while providing guaranteed distortion bounds our results on a variety of objects demonstrate that the method is fast enough to work with large complex irregular meshes in interactive applications
we present a novel disk-based multiresolution triangle mesh data structure that supports paging and view-dependent rendering of very large meshes at interactive frame rates from external memory our approach called xfastmesh is based on a view-dependent mesh simplification framework that represents half-edge collapse operations in a binary hierarchy known as a merge-tree forest the proposed technique partitions the merge-tree forest into so-called detail blocks which consist of binary subtrees that are stored on disk we present an efficient external memory data structure and file format that stores all detail information of the multiresolution triangulation method using significantly less storage then previously reported approaches furthermore we present a paging algorithm that provides efficient loading and interactive rendering of large meshes from external memory at varying and view-dependent level-of-detail the presented approach is highly efficient both in terms of space cost and paging performance
while many methods exist for visualising scalar and vector data visualisation of tensor data is still troublesome we present a method for visualising second order tensors in three dimensions using a hybrid between direct_volume_rendering and glyph_rendering an overview scalar field is created by using three-dimensional adaptive filtering of a scalar field containing noise the filtering process is controlled by the tensor_field to be visualised creating patterns that characterise the tensor_field by combining direct_volume_rendering of the scalar field with standard glyph_rendering methods for detailed tensor visualisation a hybrid solution is created a combined volume and glyph renderer was implemented and tested with both synthetic tensors and strain-rate tensors from the human heart muscle calculated from phase contrast magnetic resonance image data a comprehensible result could be obtained giving both an overview of the tensor_field as well as detailed information on individual tensors
visualizing second-order 3d tensor_fields continue to be a challenging task although there are several algorithms that have been presented no single algorithm by itself is sufficient for the analysis because of the complex nature of tensor_fields in this paper we present two new methods based on volume_deformation to show the effects of the tensor_field upon its underlying media we focus on providing a continuous representation of the nature of the tensor_fields each of these visualization algorithms is good at displaying some particular properties of the tensor_field
in this paper we develop a new technique for tracing anatomical fibers from 3d tensor_fields the technique extracts salient tensor features using a local regularization technique that allows the algorithm to cross noisy regions and bridge gaps in the data we applied the method to human brain dt-mri data and recovered identifiable anatomical structures that correspond to the white_matter brain-fiber pathways the images in this paper are derived from a dataset having 121×88×60 resolution we were able to recover fibers with less than the voxel size resolution by applying the regularization technique ie using a priori assumptions about fiber smoothness the regularization procedure is done through a moving least squares filter directly incorporated in the tracing algorithm
interactive_visualization of large digital elevation models is of continuing interest in scientific_visualization gis and virtual_reality applications taking advantage of the regular structure of grid digital elevation models efficient hierarchical multiresolution triangulation and adaptive level-of-detail lod rendering algorithms have been developed for interactive terrain_visualization despite the higher triangle count these approaches generally outperform mesh simplification methods that produce irregular triangulated network tin based lod representations in this project we combine the advantage of a tin based mesh simplification preprocess with high-performance quadtree based lod triangulation and rendering at run-time this approach called quadtin generates an efficient quadtree triangulation hierarchy over any irregular point set that may originate from irregular terrain sampling or from reducing oversampling in high-resolution grid digital elevation models
we present a technique to perform occlusion_culling for hierarchical terrains at run-time the algorithm is simple to implement and requires minimal pre-processing and additional storage yet leads to 2-4 times improvement in framerate for views with high degrees of occlusion our method is based on the well-known occlusion horizon algorithm we show how to adapt the algorithm for use with hierarchical terrains the occlusion horizon is constructed as the terrain is traversed in an approximate front to back ordering regions of the terrain are compared to the horizon to determine when they are completely occluded from the viewpoint culling these regions leads to significant savings in rendering
novel speech and/or gesture interfaces are candidates for use in future mobile or ubiquitous applications this paper describes an evaluation of various interfaces for visual navigation of a whole earth 3d terrain model a mouse driven interface a speech interface a gesture interface and a multimodal speech and gesture interface were used to navigate to targets placed at various points on the earth this study measured each participant's recall of target identity order and location as a measure of cognitive load timing information as well as a variety of subjective measures including discomfort and user preference were taken while the familiar and mature mouse interface scored best by most measures the speech interface also performed well the gesture and multimodal interface suffered from weaknesses in the gesture modality weaknesses in the speech and multimodal modalities are identified and areas for improvement are discussed
this paper presents a new technique for visualizing large complex collections of data the size and dimensionality of these datasets make them challenging to display in an effective manner the images must show the global structure of spatial relationships within the dataset yet at the same time accurately represent the local detail of each data element being visualized we propose combining ideas from information_and_scientific_visualization together with a navigation assistant a software system designed to help users identify and explore areas of interest within their data the assistant locates data elements of potential importance to the user clusters them into spatial regions and builds underlying graph structures to connect the regions and the elements they contain graph traversal algorithms constraint-based viewpoint construction and intelligent camera planning techniques can then be used to design animated tours of these regions in this way the navigation assistant can help users to explore any of the areas of interest within their data we conclude by demonstrating how our assistant is being used to visualize a multidimensional weather dataset
this paper describes bm3d a method for the analysis of motion in time dependent volume data from a sequence of volume data sets a sequence of vector data sets representing the movement of the data is computed a block matching technique is used for the reconstruction of data movement the derived vector field can be used for the visualization of time dependent volume data the method is illustrated in two applications
motion provides strong visual_cues for the perception of shape and depth as demonstrated by cognitive scientists and visual artists this paper presents a novel visualization_technique-kinetic visualization -that uses particle_systems to add supplemental motion cues which can aid in the perception of shape and spatial relationships of static objects based on a set of rules following perceptual and physical principles particles flowing over the surface of an object not only bring out but also attract attention to essential information on the shape of the object that might not be readily visible with conventional rendering that uses lighting and view changes replacing still images with animations in this fashion we demonstrate with both surface and volumetric models in the accompanying videos that in many cases the resulting visualizations effectively enhance the perception of three-dimensional shape and structure the results of a preliminary user_study that we have conducted also show evidence that the supplemental motion cues help
the analysis of multidimensional functions is important in many engineering disciplines and poses a major problem as the number of dimensions increases previous visualization approaches focus on representing three or fewer dimensions at a time this paper presents a new focus+context_visualization that provides an integrated overview of an entire multidimensional function space with uniform treatment of all dimensions the overview is displayed with respect to a user-controlled polar focal point in the function's parameter space function value patterns are viewed along rays that emanate from the focal point in all directions in the parameter space and represented radially around the focal point in the visualization data near the focal point receives proportionally more screen space than distant data this approach scales smoothly from two dimensions to 10-20 with a 1000 pixel range on each dimension
in this paper we introduce a new and simple algorithm to compress isosurface data this is the data extracted by isosurface algorithms from scalar functions defined on volume grids and used to generate polygon_meshes or alternative representations in this algorithm the mesh connectivity and a substantial proportion of the geometric information are encoded to a fraction of a bit per marching_cubes vertex with a context based arithmetic coder closely related to the jbig binary image compression standard the remaining optional geometric information that specifies the location of each marching_cubes vertex more precisely along its supporting intersecting grid edge is efficiently encoded in scan-order with the same mechanism vertex normals can optionally be computed as normalized gradient vectors by the encoder and included in the bitstream after quantization and entropy encoding or computed by the decoder in a postprocessing smoothing step these choices are determined by trade-offs associated with an in-core vs out-of-core decoder structure the main features of our algorithm are its extreme simplicity and high compression rates
we present some new methods for computing estimates of normal vectors at the vertices of a triangular_mesh surface approximation to an isosurface which has been computed by the marching cube algorithm these estimates are required for the smooth rendering of triangular_mesh surfaces the conventional method of computing estimates based upon divided difference approximations of the gradient can lead to poor estimates in some applications this is particularly true for isosurfaces obtained from a field function which is defined only for values near to the isosurface we describe some efficient methods for computing the topology of the triangular_mesh surface which is used for obtaining local estimates of the normals in addition a new one pass approach for these types of applications is described and compared to existing methods
polygonal approximations of isosurfaces extracted from uniformly sampled volumes are increasing in size due to the availability of higher resolution imaging techniques the large number of i primitives represented hinders the interactive_exploration of the dataset though many solutions have been proposed to this problem many require the creation of isosurfaces at multiple resolutions or the use of additional data structures often hierarchical to represent the volume we propose a technique for adaptive isosurface_extraction that is easy to implement and allows the user to decide the degree of adaptivity as well as the choice of isosurface_extraction algorithm our method optimizes the extraction of the isosurface by warping the volume in a warped volume areas of importance eg containing significant details are inflated while unimportant ones are contracted once the volume is warped any extraction algorithm can be applied the extracted mesh is subsequently unwarped such that the warped areas are rescaled to their initial proportions the resulting isosurface is represented by a mesh that is more densely sampled in regions decided as important
we present an algorithm for interactively extracting and rendering isosurfaces of large volume datasets in a view-dependent fashion a recursive tetrahedral mesh refinement scheme based on longest_edge_bisection is used to hierarchically decompose the data into a multiresolution structure this data structure allows fast extraction of arbitrary isosurfaces to within user specified view-dependent error bounds a data layout scheme based on hierarchical space filling curves provides access to the data in a cache coherent manner that follows the data access pattern indicated by the mesh refinement
line_integral_convolution lic is a promising method for visualizing 2d dense flow_fields direct extensions of the lic method to 3d have not been considered very effective because optical integration in viewing directions tends to spoil the coherent_structures along 3d local streamlines in our previous reports we have proposed a selective approach to volume_rendering of lic solid texture using 3d significance map s-map derived from the characteristics of flow structures and a specific illumination model for 3d streamlines in this paper we take full advantage of scalar volume_rendering hardware such as volumepro to realize a realtime 3d flow_field visualization environment with the lic volume_rendering method
we report on using computed_tomography ct as a model acquisition tool for complex objects in computer_graphics unlike other modeling and scanning techniques the complexity of the object is irrelevant in ct which naturally enables to model objects with for example concavities holes twists or fine surface details once the data is scanned one can apply post-processing techniques for data enhancement modification or presentation for demonstration purposes we chose to scan a christmas tree which exhibits high complexity which is difficult or even impossible to handle with other techniques however care has to be taken to achieve good scanning results with ct further we illustrate post-processing by means of data segmentation and photorealistic as well as non-photorealistic surface and volume_rendering techniques
this paper is a documentation of techniques invented results obtained and lessons learned while creating visualization algorithms to render outputs of large-scale seismic simulations the objective is the development of techniques for a collaborative simulation and visualization shared between structural engineers seismologists and computer scientists the computer_graphics research community has been witnessing a large number of exemplary publications addressing the challenges faced while trying to visualize both large-scale surface and volumetric_datasets lately from a visualization perspective issues like data preprocessing simplification sampling filtering etc rendering algorithms surface and volume and interaction paradigms large-scale highly interactive highly immersive etc have been areas of study in this light we outline and describe the milestones achieved in a large-scale_simulation_and_visualization project which opened the scope for combining existing techniques with new methods especially in those cases where no existing methods were suitable we elucidate the data simplification and reorganization schemes that we used and discuss the problems we encountered and the solutions we found we describe both desktop high-end local as well as remote interfaces and immersive visualization_systems that we developed to employ interactive surface and volume_rendering algorithms finally we describe the results obtained challenges that still need to be addressed and ongoing efforts to meet the challenges of large-scale visualization
for most of the time we enjoy and appreciate music performances as they are once we try to understand the performance not in subjective terms but in an objective way and share it with other people visualizing the performance parameters is indispensable in this paper a figure for visualizing performance expressions is described this figure helps people understand the cause and position of the performance expression as it has expressive cues which coincide with the cognitive meaning of musical performance and not by using only midi parameter values the differences we hear between performances are clarified by visualized figures
internet connectivity is defined by a set of routing protocols which let the routers that comprise the internet backbone choose the best route for a packet to reach its destination one way to improve the security and performance of internet is to routinely examine the routing data in this case_study we show how interactive_visualization of border gateway protocol bgp data helps characterize routing behavior identify weaknesses in connectivity which could potentially cripple the internet as well as detect and explain actual anomalous events
we have created an application called prima patient record intelligent monitoring and analysis which can be used to visualize and understand patient record data it was developed to better understand a large collection of patient records of bone marrow transplants at hadassah hospital in jerusalem israel it is based on an information_visualization toolkit opal which has been developed at the ibm tj watson research center opal allows intelligent interactive_visualization of a wide variety of different types of data the prima application is generally applicable to a wide range of patient record data as the underlying toolkit is flexible with regard to the form of the input data this application is a good example of the usefulness of information_visualization_techniques in the bioinformatics domain as these techniques have been developed specifically to deal with diverse sets of often unfamiliar data we illustrate several unanticipated findings which resulted from the use of a flexible and interactive_information_visualization environment
with the completion of the human genome sequence and with the proliferation of genome-related annotation data the need for scalable and more intuitive means for analysis becomes critical at variagenics and small design firm we have addressed this problem with a coherent three-dimensional space in which all data can be seen in a single context this tool aids in integrating information at vastly divergent scales while maintaining accurate spatial and size relationships our visualization was successful in communicating to project teams with diverse backgrounds the magnitude and biological implication of genetic variation
we present an innovative application developed at sandia national laboratories for visual_debugging of unstructured finite_element physics codes our tool automatically locates anomalous regions such as inverted elements or nodes whose variable values lie outside a prescribed range then extracts mesh subsets around these features for detailed examination the subsets are viewed using color coding of variable values superimposed on the mesh structure this allows the values and their relative spatial locations within the mesh to be correlated at a glance both topological irregularities and hot spots within the data stand out visually allowing the user to explore the exact numeric values of the grid at surrounding points over time we demonstrate the utility of this approach by debugging a cell inversion in a simulation of an exploding wire
adaptive_mesh_refinement amr is a popular computational simulation technique used in various scientific and engineering fields although amr data is organized in a hierarchical multi-resolution data structure the traditional volume_visualization algorithms such as ray-casting and splatting cannot handle the form without converting it to a sophisticated data structure in this paper we present a hierarchical multi-resolution splatting technique using k-d trees and octrees for amr data that is suitable forimplementation on the latest consumer pc graphics_hardware we describe a graphical user interface to set transfer_function and viewing/rendering parameters interactively experimental results obtained on a general purpose pc equipped with nvidia geforce card are presented to demonstrate that the technique can interactively render amr data over 20 frames per second our scheme can easily be applied to parallel_rendering of time-varying amr data
in this case_study we explore techniques for the purpose of visualizing isolated flow structures in time-dependent_data our primary industrial application is the visualization of the vortex rope a rotating helical structure which builds up in the draft tube of a water turbine the vortex rope can be characterized by high values of normalized helicity which is a scalar field derived from the given cfd velocity data in two related applications the goal is to visualize the cavitation regions near the runner blades of a kaplan turbine and a water pump respectively again the flow structure of interest can be defined by a scalar field namely by low pressure values we propose a particle seeding scheme based on quasi-random numbers which minimizes visual artifacts such as clusters or patterns by constraining the visualization to a region of interest occlusion problems are reduced and storage efficiency is gained
ocean model simulations commonly assume the ocean is hydrostatic resulting in near zero vertical motion the vertical motion found is typically associated with the variations of the thermocline depth over time which are mainly a result of the development and movement of ocean fronts eddies and internal waves a new technique extended from lagrangian-eulerian advection is presented to help understand the variation of vertical motion associated with the change in thermocline depth over time a time surface is correctly deformed in a single direction according to the flow the evolution of the time surface is computed via a mixture of eulerian and lagrangian techniques the dominant horizontal motion is textured onto the surface using texture_advection while both the horizontal and vertical motions are used to displace the surface the resulting surface is shaded for enhanced contrast timings indicate that the overhead over standard 2d texture_advection is no more than 12%
weather radars can measure the backscatter from rain drops in the atmosphere a complete radar scan provides three-dimensional precipitation information for the understanding of the underlying atmospheric processes interactive_visualization of these data sets is necessary this is a challenging task due to the size structure and required context of the data in this case_study a multiresolution approach for real-time simultaneous visualization of radar measurements together with the corresponding terrain data is illustrated
for quantitative examination of phenomena that simultaneously occur on very different spatial and temporal scales adaptive hierarchical schemes are required a special numerical multilevel technique associated with a particular hierarchical_data structure is so-called adaptive_mesh_refinement amr it allows one to bridge a wide range of spatial and temporal resolutions and therefore gains increasing popularity we describe the interplay of several visualization and vr software packages for rendering time dependent amr simulations of the evolution of the first star in the universe the work was done in the framework of a television production for discovery channel television "the unfolding universe" parts of the data were taken from one of the most complex amr simulation ever carried out it contained up to 27 levels of resolution requiring modifications to the texture based amr volume_rendering algorithm that was used to depict the density distribution of the gaseous interstellar matter a voice and gesture controlled cave application was utilized to define camera paths following the interesting features deep inside the computational domains background images created from cosmological computational data were combined with the final renderings
this paper examines a series of nasa outreach visualizations created using several layers of remote sensing satellite data ranging from 4-kilometers per pixel to i-meter per pixel the viewer is taken on a seamless cloud free journey from a global view of the earth down to ground level where buildings streets and cars are visible the visualizations were produced using a procedural shader that takes advantage of accurate georegistration and color matching between images the shader accurately and efficiently maps the data sets to geometry allowing for animations with few perceptual transitions among data sets we developed a pipeline to facilitate the production of over twenty zoom visualizations millions of people have seen these visualizations through national and international media coverage
in this paper we address the problem of automatic camera positioning and automatic camera path generation in the context of historical data visualization after short description of the given data we elaborate on the constraints for the positioning of a virtual camera in such a way that not only the projected area is maximized but also the depth of the displayed scene this is especially important when displaying terrain models which do not provide good 3d impression when only the projected area is maximized based on this concept we present a method for computing an optimal camera position for each instant of time since the explored data are not static but change depending on the explored scene time we also discuss a method for animation generation in order to avoid sudden changes of the camera position when the previous method is applied for each frame point in time we introduce pseudo-events in time which expand the bounding box defined by the currently active events of interest in particular this technique allows events happening in a future point in time to be taken into account such that when this time becomes current all events of interest are already within the current viewing frustum of the camera
visualization is required for the effective utilization of data from a weather simulation appropriate mapping of user goals to the design of pictorial content has been useful in the development of interactive applications with sufficient bandwidth for timely access to the model data when remote access to the model visualizations is required the limited bandwidth becomes the primary bottleneck to help address these problems visualizations are presented on a web page as a meta-representation of the model output and serve as an index to simplify finding other visualizations of relevance to provide consistency with extant interactive products and to leverage their cost of development the aforementioned applications are adapted to automatically populate a web site with images and interactions for an operational weather forecasting system
the gauss map projects surface normals to a unit sphere providing a powerful visualization of the geometry of a graphical object it can be used to predict visual events caused by changes in lighting shading and camera_control we present an interactive technique for portraying the gauss map of polygonal models mapping surface normals and the magnitudes of surface curvature using a spherical projection unlike other visualizations of surface curvature we create our gauss map directly from polygonal_meshes without requiring any complex intermediate calculations of differential geometry for anything other than simple shapes surface information is densely mapped into the gaussian normal image inviting the use of visualization_techniques to amplify and emphasize details hidden within the wealth of data we present the use of interactive_visualization tools such as brushing_and_linking to explore the surface properties of solid shapes the gauss map is shown to be simple to compute easy to view dynamically and effective at portraying important features of polygonal models
active stereo has been used by engineers and industrial designers for several years to enhance the perception of computer generated three-dimensional images unfortunately active stereo requires specialized hardware therefore as ubiquitous computing and teleworking gain importance using active stereo becomes a problem the goal of this case_study is to examine the concept of a generic library for polychromatic passive stereo to make stereo vision available everywhere
as part of a larger effort exploring alternative display systems lawrence livermore national laboratory has installed systems in two offices that extend and update the previously described "office of real soon now" project to improve the value for visualization tasks these new systems use higher resolution projectors driven by workstations that run unix-based applications via linux and support hardware-accelerated 3d graphics even across the boundary between displays
the following topics are dealt with computer displays multiscaling graphs high dimensionality occlusion visualization_evaluation linking and design studies
high-resolution wall-size digital displays present significant new and different visual space to show and see imagery the author has been working with two wall-size digital displays at princeton university for five years and directing and producing imax films for a decade and he has noted some unique design considerations for creating effective visual images when they are spread across entire walls the author suggests these "frameless" screens - where images are so large we need to look around to see the entire field - need different ways of thinking about image design and visualization presenting such things as scale and detail take on new meaning when they can be displayed life-size and not shown in the context of one or many small frames such as we see everywhere these design ideas will be of use for pervasive computing interface research and design interactive design control design representations of massive data sets and creating effective displays of data for research and education
large 2dinformation_spaces such as maps images or abstract visualizations require views at various level of detail close ups to inspect details overviews to maintain literally an overview_users often switch between these views we discuss how smooth animations from one view to another can be defined to this end a metric on the effect of simultaneous zooming and panning is defined based on an estimate of the perceived velocity optimal is defined as smooth and efficient given the metric these terms can be translated into a computational model which is used to calculate an analytic solution for optimal animations the model has two free parameters animation speed and zoom/pan trade off a user experiment to find good values for these is described
we propose a new method for assessing the perceptual_organization of information graphics based on the premise that the visual_structure of an image should match the structure of the data it is intended to convey the core of our method is a new formal model of one type of perceptual structure based on classical machine vision techniques for analyzing an image at multiple resolutions the model takes as input an arbitrary grayscale image and returns a lattice structure describing the visual organization of the image we show how this model captures several aspects of traditional design aesthetics and we describe a software tool that implements the model to help designers analyze and refine visual displays our emphasis here is on demonstrating the model's potential as a design aid rather than as a description of human perception but given its initial promise we propose a variety of ways in which the model could be extended and validated
we introduce an approach to visual_analysis of multivariate data that integrates several methods from information_visualization exploratory_data_analysis eda and geo visualization the approach leverages the component-based architecture implemented in geovista studio to construct a flexible multiview tightly but generically coordinated eda toolkit this toolkit builds upon traditional ideas behind both small_multiples and scatterplot matrices in three fundamental ways first we develop a general multiform bivariate matrix and a complementary multiform bivariate small multiple plot in which different bivariate representation forms can be used in combination we demonstrate the flexibility of this approach with matrices and small_multiples that depict multivariate data through combinations of scatterplots bivariate_maps and space-filling displays second we apply a measure of conditional entropy to a identify variables from a high-dimensional_data set that are likely to display interesting relationships and b generate a default order of these variables in the matrix or small multiple display third we add conditioning a kind of dynamic_query/filtering in which supplementary undisplayed variables are used to constrain the view onto variables that are displayed conditioning allows the effects of one or more well understood variables to be removed form the analysis making relationships among remaining variables easier to explore we illustrate the individual and combined functionality enabled by this approach through application to analysis of cancer diagnosis and mortality data and their associated covariates and riskfactors
in this paper we focus on some of the key design decisions we faced during the process of architecting a visualization system and present some possible choices with their associated advantages and disadvantages we frame this discussion within the context of rivet our general visualization environment designed for rapidly prototyping interactive exploratory_visualization tools for analysis as we designed increasingly sophisticated visualizations we needed to refine rivet in order to be able to create these richer displays for larger and more complex data sets the design decisions we discuss in this paper include the internal data model data access semantic meta-data information the visualization can use to create effective visual decodings the need for data_transformations in a visualization tool modular objects for flexibility and the tradeoff between simplicity and expressiveness when providing methods for creating visualizations
an increasing number of tasks require people to explore navigate and search extremely complex data sets visualized as graphs examples include electrical and telecommunication networks web structures and airline routes the problem is that graphs of these real world data sets have many interconnected nodes ultimately leading to edge congestion the density of edges is so great that they obscure nodes individual edges and even the visual information beneath the graph to address this problem we developed an interactive technique called edgelens an edgelens interactively curves graph edges away for a person's focus attention without changing the node positions this opens up sufficient space to disambiguate node and edge relationships and to see underlying information while still preserving node layout initially two methods of creating this interaction were developed and compared in a user_study the results of this study were used in the selection of a basic approach and the subsequent development of the edgelens we then improved the edgelens through use of transparency and colour and by allowing multiple lenses to appear on the graph
graph and tree_visualization_techniques enable interactive_exploration of complex relations while communicating topology however most existing techniques have not been designed for situations where visual information such as images is also present at each node and must be displayed this paper presents moiregraphs to address this need moiregraphs combine a new focus+context radial graph_layout with a suite of interaction techniques focus strength changing radial rotation level highlighting secondary foci animated transitions and node information to assist in the exploration of graphs with visual nodes the method is scalable to hundreds of displayed visual nodes
network evolution is an ubiquitous phenomenon in a wide variety of complex systems there is an increasing interest in statistically modeling the evolution of complex networks such as small-world networks and scale-free networks in this article we address a practical issue concerning the visualizations of co-citation networks of scientific publications derived by two widely known link reduction algorithms namely minimum spanning trees msts and pathfinder networks pfnets our primary goal is to identify the strengths and weaknesses of the two methods in fulfilling the need for visualizing evolving networks two criteria are derived for assessing visualizations of evolving networks in terms of topological properties and dynamical properties we examine the animated visualization_models of the evolution of botulinum toxin research in terms of its co-citation structure across a 58-year span 1945-2002 the results suggest that although high-degree nodes dominate the structure of mst models such structures can be inadequate in depicting the essence of how the network evolves because mst removes potentially significant links from high-order shortest paths in contrast pfnet models clearly demonstrate their superiority in maintaining the cohesiveness of some of the most pivotal paths which in turn make the growth animation more predictable and interpretable we suggest that the design of visualization and modeling tools for network evolution should take the cohesiveness of critical paths into account
in visualising multidimensional data it is well known that different types of algorithms to process them data sets might be distinguished according to volume variable types and distribution and each of these characteristics imposes constraints upon the choice of applicable algorithms for their visualization previous work has shown that a hybrid algorithmic approach can be successful in addressing the impact of data volume on the feasibility of multidimensional scaling mds this suggests that hybrid combinations of appropriate algorithms might also successfully address other characteristics of data this paper presents a system and framework in which a user can easily explore hybrid algorithms and the data flowing through them visual programming and a novel algorithmic architecture let the user semi-automatically define data flows and the co-ordination of multiple_views
we introduce two dynamic_visualization_techniques using multidimensional scaling to analyze transient data_streams such as newswires and remote sensing imagery while the time-sensitive nature of these data_streams requires immediate attention in many applications the unpredictable and unbounded characteristics of this information can potentially overwhelm many scaling algorithms that require a full re-computation for every update we present an adaptive visualization_technique based on data stratification to ingest stream information adaptively when influx rate exceeds processing rate we also describe an incremental_visualization_technique based on data fusion to project new information directly onto a visualization subspace spanned by the singular vectors of the previously processed neighboring data the ultimate goal is to leverage the value of legacy and new information and minimize re-processing of the entire dataset in full resolution we demonstrate these dynamic_visualization results using a newswire corpus and a remote sensing imagery sequence
large number of dimensions not only cause clutter in multi-dimensional_visualizations but also make it difficult for users to navigate the data space effective dimension management such as dimension_ordering spacing and filtering is critical for visual_exploration of such datasets dimension_ordering and spacing explicitly reveal dimension relationships in arrangement-sensitive multidimensional visualization_techniques such as parallel_coordinates star_glyphs and pixel-oriented techniques they facilitate the visual discovery of patterns within the data dimension filtering hides some of the dimensions to reduce clutter while preserving the major information of the dataset in this paper we propose an interactive hierarchical dimension_ordering spacing and filtering approach called dosfa dosfa is based on dimension hierarchies derived from similarities among dimensions it is scalable multi-resolution approach making dimensional management a tractable task on the one hand it automatically generates default settings for dimension_ordering spacing and filtering on the other hand it allows users to efficiently control all aspects of this dimension management process via visual_interaction tools for dimension hierarchy manipulation a case_study visualizing a dataset containing over 200 dimensions reveals high dimensional visualization_techniques
data sets with a large number of nominal variables some with high cardinality are becoming increasingly common and need to be explored unfortunately most existing visual_exploration displays are designed to handle numeric variables only when importing data sets with nominal values into such visualization tools most solutions to date are rather simplistic often techniques that map nominal values to numbers do not assign order or spacing among the values in a manner that conveys semantic relationships moreover displays designed for nominal variables usually cannot handle high cardinality variables well this paper addresses the problem of how to display nominal variables in general-purpose visual_exploration tools designed for numeric variables specifically we investigate 1 how to assign order and spacing among the nominal values and 2 how to reduce the number of distinct values to display we propose that nominal variables be pre-processed using a distance-quantification-classing dqc approach before being imported into a visual_exploration tool in the distance step we identify a set of independent dimensions that can be used to calculate the distance between nominal values in the quantification step we use the independent dimensions and the distance information to assign order and spacing among the nominal values in the classing step we use results from the previous steps to determine which values within a variable are similar to each other and thus can be grouped together each step in the dqc approach can be accomplished by a variety of techniques we extended the xmdvtool package to incorporate this approach we evaluated our approach on several data sets using a variety of evaluation measures
large and high-dimensional_data sets mapped to low-dimensional visualizations often result in perceptual ambiguities one such ambiguity is overlap or occlusion that occurs when the number of records exceeds the number of unique locations in the presentation or when there exist two or more records that map to the same location to lessen the affect of occlusion non-standard visual attributes ie shading and/or transparency are applied or such records may be remapped to a corresponding jittered location the resulting mapping efficiently portrays the crowding of records but fails to provide the insight into the relationship between the neighboring records we introduce a new interactive technique that intelligibly organizes overlapped points a neural network-based smart jittering algorithm we demonstrate this technique on a scatter plot the most widely used visualization the algorithm can be applied to other one two and multi-dimensional_visualizations which represent data as points including 3-dimensional scatter plots radviz polar coordinates
the informedia digital video library user interface summarizes query results with a collage of representative keyframes we present a user_study in which keyframe occlusion caused difficulties to use the screen space most efficiently to display images both occlusion and wasted whitespace should be minimized thus optimal choices will tend toward constant density displays however previous constant density algorithms are based on global density which leads to occlusion and empty space if the density is not uniform we introduce an algorithm that considers the layout of individual objects and avoids occlusion altogether efficiency concerns are important for dynamic summaries of the informedia digital video library which has hundreds of thousands of shots posting multiple queries that take into account parameters of the visualization as well as the original query reduces the amount of work required this greedy algorithm is then compared to an optimal one the approach is also applicable to visualizations containing complex graphical objects other than images such as text icons or trees
dynamic queries facilitate rapid exploration of information by real-time visual display of both query formulation and results dynamic_query sliders are linked to the main visualization to filter data a common alternative to dynamic queries is to link several simple visualizations such as histograms to the main visualization with a brushing interaction strategy selecting data in the histograms highlights that data in the main visualization we compare these two approaches in an empirical experiment on datamaps a geographic data visualization tool dynamic_query sliders resulted in better performance for simple range tasks while brushing histograms was better for complex trend evaluation and attribute relation tasks participants preferred brushing histograms for understanding relationships between attributes and the rich information they provided
this paper presents the results of an experiment aimed at investigating how different methods of viewing visual programs affect users' understanding the first two methods used traditional flat and semantic_zooming models of program representation the third is a new representation that uses semantic_zooming combined with blending and proximity the results of several search tasks performed by approximately 80 participants showed that the new method resulted in both faster and more accurate searches than the other methods
as visualization researchers we are interested in gaining a better understanding of how to effectively use texture to facilitate shape perception if we could design the ideal texture pattern to apply to an arbitrary smoothly curving shape to be most accurately and effectively perceived what would the characteristics of that texture pattern be in this paper we describe the results of a comprehensive controlled observer experiment intended to yield insight into that question here we report the results of a new study comparing the relativeaccuracy of observers' judgments of shape type elliptical cylindrical hyperbolic or flat and shape orientation convex concave both or neither for local views of boundary masked quadric surface patches under six different principal direction texture pattern conditions plus two texture conditions an isotropic pattern and a non-principal direction oriented anisotropic pattern under both perspective and orthographic_projection conditions and from both head-on and oblique viewpoints our results confirm the hypothesis that accurate shape perception is facilitated to a statistically significantly greater extent by some principal direction texture patterns than by others specifically we found that for both views under conditions of perspective projection participants more often correctly identified the shape category and the shape orientation when the surface was textured with the pattern that contained oriented energy along both the first and second principal directions only than in the case of any other texture condition patterns containing markings following only one of the principal directions or containing information along other directions in addition to the principal directions yielded poorer performance overall
microarrays are relatively new high-throughput data acquisition technology for investigating biological phenomena at the micro-level one of the more common procedures for microarray experimentation is that of the microarray time-course experiment the product of microarray time-course experiment is time-series_data which subject to proper analysis has the potential to have significant impact on the diagnosis treatment and prevention of diseases while existing information_visualization_techniques go some way to making microarray time-series_data more manageable requirements analysis has revealed significant limitations the main finding was that users were unable to uncover and quantify common changes in value over a specified time-period this paper describes a novel technique that provides this functionality by allowing the user to visually formulate and modify measurable queries with separate time-period and condition components these visual_queries are supported by the combination of a traditional value against time graph representation of the data with a complementary scatter-plot representation of a specified time-period the multiple_views of the visualization are coordinated so that the user can formulate and modify queries with rapid reversible display of query results in the traditional value against time graph format
this paper proposes a conceptual_model called compound brushing for modeling the brushing techniques used in dynamic_data visualization in this approach brushing techniques are modeled as higraphs with five types of basic entities data selection device renderer and transformation using this model a flexible visual programming tool is designed not only to configure/control various common types of brushing techniques currently used in dynamic_data visualization but also to investigate new brushing techniques
we present growing polygons a novel visualization_technique for the graphical representation of causal relations and information flow in a system of interacting processes using this method individual processes are displayed as partitioned polygons with color-coded segments showing dependencies to other processes the entire visualization is also animated to communicate the dynamic execution of the system to the user the results from a comparative user_study of the method show that the growing polygons technique is significantly more efficient than the traditional hasse_diagram visualization for analysis tasks related to deducing information flow in a system for both small and large executions furthermore our findings indicate that the correctness when solving causality tasks is significantly improved using our method in addition the subjective ratings of the users rank the method as superior in all regards including usability efficiency and enjoyability
satisfaction surveys are an important measurement tool in fields such as market_research or human resources management serious studies consist of numerous questions and contain answers from large population samples aggregation on both sides the questions asked as well as the answers received turns the multidimensional problem into a complex system of interleaved hierarchies traditional ways of presenting the results are limited to one-dimensional charts and cross-tables we developed a visualization method called the parallel coordinate tree that combines multidimensional analysis with a tree structure representation distortion-oriented focus+context techniques are used to facilitate interaction with the visualization in this paper we present a design_study of a commercial application that we built using this method to analyze and communicate results from large-scale customer satisfaction surveys
an equity mutual fund is a financial instrument that invests in a set of stocks any two different funds may partially invest in some of the same stocks thus overlap is common portfolio diversification aims at spreading an investment over many different stocks in search of greater returns helping people with portfolio diversification is challenging because it requires informing them about both their current portfolio of stocks held through funds and the other stocks in the market not invested in yet current stock/fund visualization_systems either waste screen real estate and visualization of all data points we have developed a system called fundexplorer that implements a distorted treemap to visualize both the amount of money invested in a person's fund portfolio and the context of remaining market stocks the fundexplorer system enables people to interactively explore diversification possibilities with their portfolios
this paper describes thread arcs a novel interactive_visualization_technique designed to help people use threads found in email thread arcs combine the chronology of messages with the branching tree structure of a conversational thread in a mixed-model visualization by venolia and neustaedter 2003 that is stable and compact by quickly scanning and interacting with thread arcs people can see various attributes of conversations and find relevant messages in them easily we tested this technique against other visualization_techniques with users' own email in a functional prototype email client thread arcs proved an excellent match for the types of threads found in users' email for the qualities users wanted in small-scale visualizations
traditionally node link diagrams are the prime choice when it comes to visualizing software architectures however node link diagrams often fall short when used to visualize large graph structures in this paper we investigate the use of call matrices as visual aids in the management of large software projects we argue that call matrices have a number of advantages over traditional node link diagrams when the main object of interest is the link instead of the node matrix_visualizations can provide stable and crisp layouts of large graphs and are inherently well suited for large multilevel visualizations because of their recursive structure we discuss a number of visualization issues using a very large software project currently under development at philips medical systems as a running example
unlike traditional information_visualization ambient information_visualizations reside in the environment of the user rather than on the screen of a desktop computer currently most dynamic information that is displayed in public places consists of text and numbers we argue that information_visualization can be employed to make such dynamic_data more useful and appealing however visualizations intended for non-desktop spaces will have to both provide valuable information and present an attractive addition to the environment - they must strike a balance between aesthetical appeal and usefulness to explore this we designed a real-time_visualization of bus departure times and deployed it in a public_space with about 300 potential users to make the presentation more visually appealing we took inspiration from a modern abstract artist the visualization was designed in two passes first we did a preliminary version that was presented to and discussed with prospective users based on their input we did a final design we discuss the lessons learned in designing this and previous ambient information_visualizations including how visual art can be used as a design constraint and how the choice of information and the placement of the display affect the visualization
the following topics are dealt with medical visualization isosurfaces implicit_surfaces flow_visualization terrains and view-dependent methods segmentation and feature analysis haptics and physical_simulation hardware-assisted_volume_rendering volume_rendering_acceleration shading and shape perception volume_reconstruction volumetric techniques sample-based rendering mesh simplification transfer_functions information_visualization scientific and large_data_visualization visualization_in_medicine and biology and visualization software
the extraction of planar sections from volume images is the most commonly used technique for inspecting and visualizing anatomic structures we propose to generalize the concept of planar section to the extraction of curved cross-sections free form surfaces compared with planar slices curved cross-sections may easily follow the trajectory of tubular structures and organs such as the aorta or the colon they may be extracted from a 3d volume displayed as a 3d view and possibly flattened flattening of curved cross-sections allows to inspect spatially complex relationship between anatomic structures and their neighborhood they also allow to carry out measurements along a specific orientation for the purpose of facilitating the interactive specification of free form surfaces users may navigate in real time within the body and select the slices on which the surface control points will be positioned immediate feedback is provided by displaying boundary curves as cylindrical markers within a 3d view composed of anatomic organs planar slices and possibly free form surface sections extraction of curved surface sections is an additional service that is available online as a java applet http//visiblehumanepflch it may be used as an advanced tool for exploring and teaching anatomy
a new method was developed to increase the saliency of changing variables in a cardiovascular_visualization for use by anesthesiologists in the operating room or clinically meaningful changes in patient physiology were identified and then mapped to the inherent psychophysical properties of the visualization a long history of psychophysical research has provided an understanding of the parameters within which the human information processing system is able to detect changes in the size shape and color of visual objects gescheider 1976 spence 1990 and baird 1970 these detection thresholds are known as just noticeable differences jnds which characterize the amount of change in an object's attribute that is recognizable 50% of the time a prototype version of the display has been demonstrated to facilitate anesthesiologist's performance while reducing cognitive workload during simulated cardiac events agutter et al 2002 in order to further improve the utility of the new cardiovascular_visualization the clinically relevant changes in cardiovascular variables are mapped to noticeable perceptual changes in the representational elements of the display the results of the method described in this paper are used to merge information from the psychophysical properties of the cardiovascular_visualization with clinically relevant changes in the patient's cardiovascular physiology as measured by the clinical meaningfulness questionnaire the result of this combination will create a visualization that is sensitive to changes in the cardiovascular health of the patient and communicates this information to the user in a meaningful salient and intuitive manner
traditional volume_visualization_techniques may provide incomplete clinical information needed for applications in medical visualization in the area of vascular_visualization important features such as the lumen of a diseased vessel segment may not be visible curved_planar_reformation cpr has proven to be an acceptable practical solution existing cpr techniques however still have diagnostically relevant limitations in this paper we introduce two advances methods for efficient vessel_visualization based on the concept of cpr both methods benefit from relaxation of spatial coherence in favor of improved feature perception we present a new technique to visualize the interior of a vessel in a single image a vessel is resampled along a spiral around its central axis the helical spiral depicts the vessel volume furthermore a method to display an entire vascular tree without mutually occluding vessels is presented minimal rotations at the bifurcations avoid occlusions for each viewing direction the entire vessel structure is visible
we describe how to count the cases that arise in a family of visualization_techniques including marching_cubes sweeping simplices contour meshing interval_volumes and separating_surfaces counting the cases is the first step toward developing a generic visualization algorithm to produce substitopes geometric substitution of polytopes we demonstrate the method using a software system "gap" for computational group theory the case-counts are organized into a table that provides taxonomy of members of the family numbers in the table are derived from actual lists of cases which are computed by our methods the calculation confirms previously reported case-counts for large dimensions that are too large to check by hand and predicts the number of cases that will arise in algorithms that have not yet been invented
we describe a modification of the widely used marching_cubes method that leads to the useful property that the resulting isosurfaces are locally single valued functions this implies that conventional interpolation and approximation methods can be used to locally represent the surface these representations can be used for computing approximations for local surface properties we utilize this possibility in order to develop algorithms for locally approximating gaussian and mean curvature methods for constrained smoothing of isosurface and techniques for the parameterization of isosurfaces
there are numerous algorithms in graphics and visualization whose performance is known to decay as the topological complexity of the input increases on the other hand the standard pipeline for 3d geometry acquisition often produces 3d models that are topologically more complex than their real forms we present a simple and efficient algorithm that allows us to simplify the topology of an isosurface by alternating the values of some number of voxels its utility and performance are demonstrated on several examples including signed distance functions from polygonal models and ct scans
deformable isosurfaces implemented with level-set methods have demonstrated a great potential in visualization for applications such as segmentation surface processing and surface_reconstruction their usefulness has been limited however by their high computational cost and reliance on significant parameter tuning this paper presents a solution to these challenges by describing graphics processor gpu based on algorithms for solving and visualizing level-set solutions at interactive rates our efficient gpu-based solution relies on packing the level-set isosurface data into a dynamic sparse texture format as the level set moves this sparse data structure is updated via a novel gpu to cpu message passing scheme when the level-set solver is integrated with a real-time volume renderer operating on the same packed format a user can visualize and steer the deformable level-set surface as it evolves in addition the resulting isosurface can serve as a region-of-interest specifier for the volume renderer this paper demonstrates the capabilities of this technology for interactive_volume_visualization and segmentation
this paper presents a signed distance transform algorithm using graphics_hardware which computes the scalar valued function of the euclidean distance to a given manifold of co-dimension one if the manifold is closed and orientable the distance has a negative sign on one side of the manifold and a positive sign on the other triangle meshes are considered for the representation of a two-dimensional manifold and the distance function is sampled on a regular cartesian grid in order to achieve linear complexity in the number of grid points to each primitive we assign a simple polyhedron enclosing its voronoi cell voronoi cells are known to contain exactly all points that lay closest to its corresponding primitive thus the distance to the primitive only has to be computed for grid points inside its polyhedron although voronoi cells partition space the polyhedrons enclosing these cells do overlap in regions where these overlaps occur the minimum of all computed distances is assigned to a grid point in order to speed up computations points inside each polyhedron are determined by scan conversion of grid slices using graphics_hardware for this task a fragment program is used to perform the nonlinear_interpolation and minimization of distance values
we present improved subdivision and isosurface_reconstruction algorithms for polygonizing implicit_surfaces and performing accurate geometric operations our improved reconstruction algorithm uses directed distance_fields kobbelt et al 2001 to detect multiple intersections along an edge separates them into components and reconstructs an isosurface locally within each components using the dual contouring algorithm ju et al 2002 it can reconstruct thin features without creating handles and results in improved surface_extraction from volumetric_data our subdivision algorithm takes into account sharp features that arise from intersecting_surfaces or boolean operations and generates an adaptive_grid such that each voxel has at most one sharp feature the subdivision algorithm is combined with our improved reconstruction algorithm to compute accurate polygonization of boolean combinations or offsets of complex primitives that faithfully reconstruct the sharp features we have applied these algorithms to polygonize complex cad models designed using thousands of boolean operations on curved primitives
we propose unsteady_flow advection-convolution ufac as a novel visualization approach for unsteady_flows it performs time evolution governed by pathlines but builds spatial correlation according to instantaneous streamlines whose spatial extent is controlled by the flow unsteadiness ufac is derived from a generic framework that provides spacetime-coherent dense representations of time dependent-vector_fields by a two-step process 1 construction of continuous trajectories in spacetime for temporal_coherence and 2 convolution along another set of paths through the above spacetime for spatially correlated patterns within the framework known visualization_techniques-such as lagrangian-eulerian advection image-based flow_visualization unsteady_flow lic and dynamic lic-can be reproduced often with better image quality higher performance or increased flexibility of the visualization style finally we present a texture-based discretization of the framework and its interactiveimplementation on graphics_hardware which allows the user to gradually balance visualization speed against quality
in this paper we offer several new insights and techniques for effectively using color and texture to simultaneously convey information about multiple 2d scalar and vector distributions in a way that facilitates allowing each distribution to be understood both individually and in the context of one or more of the other distributions specifically we introduce the concepts of color_weaving for simultaneously representing information about multiple co-located color encoded distributions and texture stitching for achieving more spatially accurate multi-frequency line_integral_convolution representations of combined scalar and vector distributions the target application for our research is the definition detection and visualization of regions of interest in a turbulent boundary layer flow at moderate reynolds number in this work we examine and analyze streamwise-spanwise planes of three-component velocity vectors with the goal of identifying and characterizing spatially organized packets of hairpin vortices
a new method for the synthesis of dense vector-field aligned textures on curved surfaces is presented called ibfvs the method is based on image based flow_visualization ibfv in ibfv two-dimensional animated textures are produced by defining each frame of a flow animation as a blend between a warped version of the previous image and a number of filtered white noise images we produce flow aligned texture on arbitrary three-dimensional triangular_meshes in the same spirit as the original method texture is generated directly in image space we show that ibfvs is efficient and effective high performance typically fifty frames or more per second is achieved by exploiting graphics_hardware also ibfvs can easily be implemented and a variety of effects can be achieved applications are flow_visualization and surface rendering specifically we show how to visualize the wind field on the earth and how to render a dirty bronze bunny
we present a technique for direct visualization of unsteady_flow on surfaces from computational fluid dynamics the method generates dense representations of time-dependent_vector_fields with high spatio-temporal correlation using both lagrangian-eulerian advection and image based flow_visualization as its foundation while the 3d vector_fields are associated with arbitrary triangular surface meshes the generation and advection of texture properties is confined to image space frame rates of up to 20 frames per second are realized by exploiting graphics card hardware we apply this algorithm to unsteady_flow on boundary surfaces of large complex meshes from computational fluid dynamics composed of more than 250000 polygons dynamic meshes with time-dependent geometry and topology as well as medical data
we combine topological and geometric methods to construct a multi-resolution data structure for functions over two-dimensional domains starting with the morse-smale_complex we construct a topological hierarchy by progressively canceling critical_points in pairs concurrently we create a geometric hierarchy by adapting the geometry to the changes in topology the data structure supports mesh traversal operations similarly to traditional multi-resolution representations
we describe an efficient technique for out-of-core management and interactive rendering of planet sized textured terrain surfaces the technique called planet-sized batched dynamic adaptive meshes p-bdam extends the bdam approach by using as basic primitive a general triangulation of points on a displaced triangle the proposed framework introduces several advances with respect to the state of the art thanks to a batched host-to-graphicscommunication model we outperform current adaptive tessellation solutions in terms of rendering speed we guarantee overall geometric continuity exploiting programmable_graphics_hardware to cope with theaccuracy issues introduced by single precision floating points we exploit a compressed out of core representation and speculative prefetching for hiding disk latency during rendering of out-of-core data we efficiently construct high_quality simplified representations with a novel distributed out of core simplification algorithm working on a standard pc network
in this paper we present a generic method for incremental mesh adaptation based on hierarchy of semi-regular meshes our method supports any refinement rule mapping vertices onto vertices such as 1-to-4 split or √3-subdivision resulting adaptive mesh has subdivision connectivity and hence good aspect_ratio of triangles hierarchic representation of the mesh allows incremental local refinement and simplification operations exploiting frame-to-frame coherence we also present an out-of-core storage layout scheme designed for semi-regular meshes of arbitrary subdivision connectivity it provides high cache coherency in the data retrieval and relies on the interleaved storage of resolution levels and maintaining good geometrical proximity within each level the efficiency of the proposed method is demonstrated with applications in physically-based cloth simulation real-time terrain_visualization and procedural modeling
this paper presents an algorithm combining view-dependent rendering and conservative occlusion_culling for interactive_display of complex environments a vertex hierarchy of the entire scene is decomposed into a cluster hierarchy through a novel clustering and partitioning algorithm the cluster hierarchy is then used for view-frustum and occlusion_culling using hardware accelerated occlusion queries and frame-to-frame coherence a potentially visible set of clusters is computed an active vertex front and face list is computed from the visible clusters and rendered using vertex arrays the integrated algorithm has been implemented on a pentium iv pc with a nvidia geforce 4 graphics card and applied in two complex environments composed of millions of triangles the resulting system can render these environments at interactive rates with little loss in image quality and minimal popping artifacts
segmentation of structures from measured volume data such as anatomy in medical imaging is a challenging data-dependent task in this paper we present a segmentation method that leverages the parallel_processing capabilities of modern programmable_graphics_hardware in order to run significantly faster than previous methods in addition collocating the algorithm computation with the visualization on the graphics_hardware circumvents the need to transfer data across the system bus allowing for faster visualization and interaction this algorithm is unique in that it utilizes sophisticated graphics_hardware functionality ie floating point precision render to texture computational masking and fragment programs to enable fast segmentation and interactive_visualization
segmentation of the tracheo-bronchial tree of the lungs is notoriously difficult this is due to the fact that the small size of some of the anatomical structures is subject to partial volume effects furthermore the limited intensity contrast between the participating materials air blood and tissue increases the segmentation of difficulties in this paper we propose a hybrid segmentation method which is based on a pipeline of three segmentation stages to extract the lower airways down to the seventh generation of the bronchi user_interaction is limited to the specification of a seed point inside the easily detectable trachea at the upper end of the lower airways similarly the complementary vascular tree of the lungs can be segmented furthermore we modified our virtual_endoscopy system to visualize the vascular and airway system of the lungs along with other features such as lung tumors
unstructured_meshes are often used in simulations and imaging applications they provide advanced flexibility in modeling abilities but are more difficult to manipulate and analyze than regular data this work provides a novel approach for the analysis of unstructured_meshes using feature-space clustering and feature-detection analyzing and revealing underlying structures in data involve operators on both spatial and functional domains slicing concentrates more on the spatial domain while iso-surfacing or volume_rendering concentrate more on the functional domain nevertheless many times it is the combination of the two domains which provides real insight on the structure of the data in this work a combined feature-space is defined on top of unstructured_meshes in order to search for structure in the data a point in feature-space includes the spatial coordinates of the point in the mesh domain and all chosen attributes defined on the mesh a distance measures between points in feature-space is defined enabling the utilization of clustering using the mean shift procedure previously used for images on unstructured_meshes feature space analysis is shown to be useful for feature-extraction for data_exploration and partitioning
the goal of this paper is to define a convolution operation which transfers image_processing and pattern matching to vector_fields from flow_visualization for this a multiplication of vectors is necessary clifford algebra provides such a multiplication of vectors we define a clifford convolution on vector_fields with uniform grids the clifford convolution works with multivector filter masks scalar and vector masks can be easily converted to multivector_fields so filter masks from image_processing on scalar_fields can be applied as well as vector and scalar masks furthermore a method for pattern matching with clifford convolution on vector_fields is described the method is independent of the direction of the structures this provides an automatic approach to feature_detection the features can be visualized using any known method like glyphs isosurfaces or streamlines the features are defined by filter masks instead of analytical properties and thus the approach is more intuitive
in this paper we present a space efficient algorithm for speeding up isosurface_extraction even though there exist algorithms that can achieve optimal search performance to identify isosurface cells they prove impractical for large_datasets due to a high storage overhead with the dual goals of achieving fast isosurface_extraction and simultaneously reducing the space requirement we introduce an algorithm based on transform_coding to compress the interval information of the cells in a dataset compression is achieved by first transforming the cell intervals minima maxima into a form which allows more efficient compaction it is followed by a dataset optimized non-uniform quantization stage the compressed data is stored in a data structure that allows fast searches in the compression domain thus eliminating the need to retrieve the original representation of the intervals at run-time the space requirement of our search data structure is the mandatory cost of storing every cell id once plus an overhead for quantization information the overhead is typically in the order of a few hundredths of the dataset size
tracking and visualizing local features from a time-varying volumetric_data allows the user to focus on selected regions of interest both in space and time which can lead to a better understanding of the underlying dynamics in this paper we present an efficient algorithm to track time-varying isosurfaces and interval_volumes using isosurfacing in higher dimensions instead of extracting the data features such as isosurfaces or interval_volumes separately from multiple time steps and computing the spatial correspondence between those features our algorithm extracts the correspondence directly from the higher dimensional geometry and thus can more efficiently follow the user selected local features in time in addition by analyzing the resulting higher dimensional geometry it becomes easier to detect important topological events and the corresponding critical time steps for the selected features with our algorithm the user can interact with the underlying time-varying_data more easily the computation cost for performing time-varying volume tracking is also minimized
in this paper we propose a novel out-of-core isosurface_extraction technique for large time-varying fields over irregular grids we employ our meta-cell technique to explore the spatial coherence of the data and our time tree algorithm to consider the temporal_coherence as well our one-time preprocessing phase first partitions the dataset into meta-cells that cluster spatially neighboring cells together and are stored in disk we then build a time tree to index the meta-cells for fast isosurface_extraction the time tree takes advantage of the temporal_coherence among the scalar values at different time steps and uses bbio trees as secondary structures which are stored in disk and support i/o-optimal interval searches the time tree algorithm employs a novel meta-interval collapsing scheme and the buffer technique to take care of the temporal_coherence in an i/o-efficient way we further make the time tree cache-oblivious so that searching on it automatically performs optimal number of block transfers between any two consecutive levels of memory hierarchy such as between cache and main memory and between main memory and disk simultaneously at run-time we perform optimal cache-oblivious searches in the time tree together with i/o-optimal searches in the bbio trees to read the active meta-cells from disk and generate the queried isosurface efficiently the experiments demonstrate the effectiveness of our new technique in particular compared with the query-optimal main-memory algorithm by cignoni et al 1997 extended for time-varying fields when there is not enough main memory our technique can speed up the isosurface queries from more than 18 hours to less than 4 minutes
one of the reasons that topological methods have a limited popularity for the visualization of complex 3d flow_fields is the fact that such topological structures contain a number of separating stream_surfaces since these stream_surfaces tend to hide each other as well as other topological features for complex 3d topologies the visualizations become cluttered and hardly interpretable this paper proposes to use particular stream lines called saddle connectors instead of separating stream_surfaces and to depict single surfaces only on user demand we discuss properties and computational issues of saddle connectors and apply these methods to complex flow data we show that the use of saddle connectors makes topological skeletons available as a valuable visualization tool even for topologically complex 3d flow data
we present a hardware-accelerated method for visualizing 3d flow_fields the method is based on insertion advection and decay of dye to this aim we extend the texture-based ibfv technique presented by van wijk 2001 for 2d flow_visualization in two main directions first we decompose the 3d flow_visualization problem in a series of 2d instances of the mentioned ibfv technique this makes our method benefit from the hardware_acceleration the original ibfv technique introduced secondly we extend the concept of advected gray value or color noise by introducing opacity or matter noise this allows us to produce sparse 3d noise pattern advections thus address the occlusion problem inherent to 3d flow_visualization overall the presented method delivers interactively animated 3d flow uses only standard opengl 11 calls and 2d textures and is simple to understand and implement
in this paper we present an interactive texture-based technique for visualizing three-dimensional vector_fields the goal of the algorithm is to provide a general volume_rendering framework allowing the user to compute three-dimensional flow textures interactively and to modify the appearance of the visualization on the fly to achieve our goal we decouple the visualization pipeline into two disjoint stages first streamlines are generated from the 3d vector data various geometric properties of the streamlines are extracted and converted into a volumetric form using a hardware-assisted slice sweeping algorithm in the second phase of the algorithm the attributes stored in the volume are used as texture coordinates to look up an appearance texture to generate both informative and aesthetic representations of the underlying vector field users can change the input textures and instantaneously visualize the rendering results with our algorithm visualizations with enhanced structural perception using various visual_cues can be rendered in real time a myriad of existing geometry-based and texture-based visualization_techniques can also be emulated
we introduce a new method for visualizing symmetric_tensor_fields the technique produces images and animations reminiscent of line_integral_convolution lic the technique is also slightly related to hyperstreamlines in that it is used to visualize tensor_fields however the similarity ends there hyperlic uses a multi-pass approach to show the anisotropic properties in a 2d or 3d tensor_field we demonstrate this technique using data sets from computational fluid dynamics as well as diffusion-tensor mri
in this paper we propose a quasi-static approximation qsa approach to simulate the movement of the movable object in 6-degrees-of-freedom dof haptic rendering in our qsa approach we solve for static equilibrium during each haptic time step ignoring any dynamical properties such as inertia the major contribution of this approach is to overcome the computational instability problem in overly stiff systems arising from numerical integration of second-order differential equations in previous dynamic models our primary experimental results on both simulated aircraft geometry and a large-scale real-world aircraft engine showed that our qsa approach was capable of maintaining the 1000hz haptic refresh rate with more robust collision avoidance and more reliable force and torque feedback
we present a haptic rendering technique that uses directional constraints to facilitate enhanced exploration modes for volumetric_datasets the algorithm restricts user motion in certain directions by incrementally moving a proxy point along the axes of a local reference frame reaction forces are generated by a spring coupler between the proxy and the data probe which can be tuned to the capabilities of the haptic interface secondary haptic effects including field forces friction and texture can be easily incorporated to convey information about additional characteristics of the data we illustrate the technique with two examples displaying fiber orientation in heart muscle layers and exploring diffusion tensor fiber tracts in brain white_matter tissue initial evaluation of the approach indicates that haptic constraints provide an intuitive means or displaying directional information in volume data
we introduce a method for the animation of fire propagation and the burning consumption of objects represented as volumetric_data sets our method uses a volumetric fire propagation model based on an enhanced distance_field it can simulate the spreading of multiple fire fronts over a specified isosurface without actually having to create that isosurface the distance_field is generated from a specific shell volume that rapidly creates narrow spatial bands around the virtual surface of any given isovalue the complete distance_field is then obtained by propagation from the initial bands at each step multiple fire fronts can evolve simultaneously on the volumetric object the flames of the fire are constructed from streams of particles whose movement is regulated by a velocity field generated with the hardware-accelerated lattice boltzmann model lbm the lbm provides a physically-based simulation of the air flow around the burning object the object voxels and the splats associated with the flame particles are rendered in the same pipeline so that the volume data with its external and internal structures can be displayed along with the fire
weather_visualization is a difficult problem because it comprises volumetric multi-field data and traditional surface-based approaches obscure details of the complex three-dimensional structure of cloud dynamics therefore visually accurate volumetric multi-field_visualization of storm scale and cloud scale data is needed to effectively and efficiently communicate vital information to weather forecasters improving storm forecasting atmospheric dynamics models and weather spotter training we have developed a new approach to multi-field_visualization that uses field specific physically-based opacity transmission and lighting calculations per-field for the accurate visualization of storm and cloud scale weather data our approach extends traditional transfer_function approaches to multi-field data and to volumetric illumination and scattering
nowadays direct_volume_rendering via 3d textures has positioned itself as an efficient tool for the display and visual_analysis of volumetric scalar_fields it is commonly accepted that for reasonably sized data sets appropriate quality at interactive rates can be achieved by means of this technique however despite these benefits one important issue has received little attention throughout the ongoing discussion of texture based volume_rendering the integration of acceleration techniques to reduce per-fragment operations in this paper we address the integration of early ray termination and empty-space skipping into texture based volume_rendering on graphical processing units gpu therefore we describe volume ray-casting on programmable_graphics_hardware as an alternative to object-order approaches we exploit the early z-test to terminate fragment processing once sufficient opacity has been accumulated and to skip empty space along the rays of sight we demonstrate performance gains up to a factor of 3 for typical renditions of volumetric_data sets on the ati 9700 graphics card
a survey of graphics developers on the issue of texture_mapping hardware for volume_rendering would most likely find that the vast majority of them view limited texture memory as one of the most serious drawbacks of an otherwise fine technology in this paper we propose a compression scheme for static and time-varying volumetric_data sets based on vector quantization that allows us to circumvent this limitation we describe a hierarchical quantization scheme that is based on a multiresolution covariance analysis of the original field this allows for the efficient encoding of large-scale_data sets yet providing a mechanism to exploit temporal_coherence in non-stationary fields we show that decoding and rendering the compressed data_stream can be done on the graphics chip using programmable hardware in this way data transfer between the cpu and the graphics processing unit gpu can be minimized thus enabling flexible and memory efficient real-time rendering options we demonstrate the effectiveness of our approach by demonstrating interactive renditions of gigabyte data sets at reasonable fidelity on commodity graphics_hardware
one of the most important goals in volume_rendering is to be able to visually separate and selectively enable specific objects of interest contained in a single volumetric_data set which can be approached by using explicit segmentation information we show how segmented_data sets can be rendered interactively on current consumer graphics_hardware with high image quality and pixel-resolution filtering of object boundaries in order to enhance object perception we employ different levels of object distinction first each object can be assigned an individual transfer_function multiple of which can be applied in a single rendering pass second different rendering modes such as direct_volume_rendering iso-surfacing and non-photorealistic_techniques can be selected for each object a minimal number of rendering passes is achieved by processing sets of objects that share the same rendering mode in a single pass third local compositing modes such as alpha blending and mip can be selected for each object in addition to a single global mode thus enabling high-quality two-level volume_rendering on gpus
non-linear filtering is an important task for volume analysis this paper presents hardware-basedimplementations of various non-linear filters for volume smoothing with edge preservation the cg high-level shading_language is used in combination with latest pc consumer graphics_hardware filtering is divided into pervertex and per-fragment stages in both stages we propose techniques to increase the filtering performance the vertex program pre-computes texture coordinates in order to address all contributing input samples of the operator mask thus additional computations are avoided in the fragment program the presented fragment programs preserve cache coherence exploit 4d vector arithmetic and internal fixed point arithmetic to increase performance we show the applicability of non-linear filters as part of a gpu-based segmentation pipeline the resulting binary mask is compressed and decompressed in the graphics memory on-the-fly
we propose methods to accelerate texture-based volume_rendering by skipping invisible voxels we partition the volume into sub-volumes each containing voxels with similar properties sub-volumes composed of only voxels mapped to empty by the transfer_function are skipped to render the adaptively partitioned sub-volumes in visibility order we reorganize them into anorthogonal bsp tree we also present an algorithm that computes incrementally the intersection of the volume with the slicing planes which avoids the overhead of the intersection and texture coordinates computation introduced by the partitioning rendering with empty_space_skipping is 2 to 5 times faster than without it to skip occluded voxels we introduce the concept oforthogonal opacity map that simplifies the transformation between the volume coordinates and the opacity map coordinates which is intensively used for occlusion detection the map is updated efficiently by the gpu the sub-volumes are then culled and clipped against the opacity map we also present a method that adaptively adjusts the optimal number of the opacity map updates with occlusion clipping about 60% of non-empty voxels can be skipped and an additional 80% speedup on average is gained for iso-surface-like rendering
we present a method to represent unstructured scalar_fields at multiple levels of detail using a parallelizable classification algorithm to build a cluster hierarchy we generate a multiresolution representation of a given volumetric scalar_data set the method uses principal_component_analysis pca for cluster generation and a fitting technique based on radial basis_functions rbfs once the cluster hierarchy has been generated we utilize a variety of techniques for extracting different levels of detail the main strength of this work is its generality regardless of grid type this method can be applied to any discrete scalar field representation even one given as a "point cloud"
we present the firstimplementation of a volume ray casting algorithm for tetrahedral_meshes running on off-the-shelf programmable_graphics_hardware ourimplementation avoids the memory transfer bottleneck of the graphics bus since the complete mesh data is stored in the local memory of the graphics adapter and all computations in particular ray traversal and ray integration are performed by the graphics processing unit analogously to other ray casting algorithms our algorithm does not require an expensive cell sorting provided that the graphics adapter offers enough texture memory ourimplementation performs comparable to the fastest published volume_rendering algorithms for unstructured_meshes our approach works with cyclic and/or non-convex meshes and supports early ray termination accurate ray integration is guaranteed by applying pre-integrated volume_rendering in order to achieve almost interactive modifications of transfer_functions we propose a new method for computing three-dimensional pre-integration tables
visibility_culling has the potential to accelerate large_data_visualization in significant ways unfortunately existing algorithms do not scale well when parallelized and require full re-computation whenever the opacity transfer_function is modified to address these issues we have designed a plenoptic_opacity_function pof scheme to encode the view-dependent opacity of a volume block pofs are computed off-line during a pre-processing stage only once for each block we show that using pofs is i an efficient conservative and effective way to encode the opacity variations of a volume block for a range of views ii flexible for re-use by a family of opacity transfer_functions without the need for additional off-line processing and iii highly scalable for use in massively parallelimplementations our results confirm the efficacy of pofs for visibility_culling in large-scale parallel volume_rendering we can interactively render the visible woman dataset using software ray-casting on 32 processors with interactive modification of the opacity transfer_function on-the-fly
hand-crafted illustrations are often more effective than photographs for conveying the shape and important features of an object but they require expertise and time to produce we describe an image_compositing system and user interface that allow an artist to quickly and easily create technical illustrations from a set of photographs of an object taken from the same point of view under variable lighting conditions our system uses a novel compositing process in which images are combined using spatially-varying light mattes enabling the final lighting in each area of the composite to be manipulated independently we describe an interface that provides for the painting of local lighting effects eg shadows highlights and tangential lighting to reveal texture directly onto the composite we survey some of the techniques used in illustration and lighting_design to convey the shape and features of objects and describe how our system can be used to apply these techniques
this paper presents a shading model for volumetric_data which enhances the perception of surfaces within the volume the model incorporates uniform diffuse illumination which arrives equally from all directions at each surface point in the volume this illumination is attenuated by occlusions in the local vicinity of the surface point resulting in shadows in depressions and crevices experiments by other authors have shown that perception of a surface is superior under uniform diffuse lighting compared to illumination from point source lighting
lightkit is a system for lighting three-dimensional synthetic scenes lightkit simplifies the task of producing visually pleasing easily interpretable images for visualization while making it harder to produce results where the scene illumination distracts from the visualization process lightkit is based on lighting_designs developed by artists and photographers and shown in previous studies to enhance shape perception a key light provides natural overhead illumination of the scene augmented by fill head and back lights by default lights are attached to a normalized subject-centric camera-relative coordinate frame to ensure consistent lighting independent of camera location or orientation this system allows all lights to be positioned by specifying just six parameters the intensity of each light is specified as a ratio to the key light intensity allowing the scene's brightness to be adjusted using a single parameter the color of each light is specified by a single normalized color parameter called warmth that is based on color temperature of natural sources lightkit's default values for light position intensity and color are chosen to produce good results for a variety of scenes lightkit is designed to work with both hardware graphics systems and potentially higher quality off-line rendering systems we provide examples of images created using a lightkitimplementation within the vtk visualization toolkit software framework
2d and 3d views are used together in many visualization domains such as medical imaging flow_visualization oceanographic visualization and computer aided design cad combining these views into one display can be done by 1 orientation icon ie separate windows 2 in-place methods eg clip and cutting_planes and 3 a new method called exovis how 2d and 3d views are displayed affects ease of mental registration understanding the spatial relationship between views an important factor influencing user performance this paper compares the above methods in terms of their ability to support mental registration empirical results show that mental registration is significantly easier with in-place displays than with exovis and significantly easier with exovis than with orientation icons different mental transformation strategies can explain this result the results suggest that exovis may be a better alternative to orientation icons when in-place displays are not appropriate eg when in-place methods hide data or cut the 3d view into several pieces
the quality of volume_visualization depends strongly on the quality of the underlying data in virtual_colonoscopy ct data should be acquired at a low radiation dose that results in a low signal-to-noise ratio alternatively mri data is acquired without ionizing radiation but suffers from noise and bias global signal fluctuations current volume_visualization_techniques often do not produce good results with noisy or biased data this paper describes methods for volume_visualization that deal with these imperfections the techniques are based on specially adapted edge detectors using first and second order derivative filters the filtering is integrated into the visualization process the first order derivative method results in good quality images but suffers from localization bias the second order method has better surface localization especially in highly curved areas it guarantees minimal detail smoothing resulting in a better visualization of polyps
volume_rendering and isosurface_extraction from three-dimensional scalar_fields are mostly based on piecewise trilinear representations in regions of high geometric complexity such visualization methods often exhibit artifacts due to trilinear_interpolation in this work we present an iterative fairing method for scalar_fields interpolating function values associated with grid points while smoothing the contours inside the grid cells based on variational principles we present a local fairing method providing a piecewise bicubic representation of two-dimensional scalar_fields our algorithm generalizes to the trivariate case and can be used to increase the resolution of data sets either locally or globally reducing interpolation artifacts in contrast to filtering methods our algorithm does not reduce geometric detail supported by the data
we develop a new approach to reconstruct non-discrete models from gridded volume samples as a model we use quadratic trivariate super splines on a uniform tetrahedral partition Δ the approximating splines are determined in a natural and completely symmetric way by averaging local data samples such that appropriate smoothness conditions are automatically satisfied on each tetra-hedron of Δ  the quasi-interpolating spline is a polynomial of total degree two which provides several advantages including efficient computation evaluation and visualization of the model we apply bernstein-bezier techniques well-known in cagd to compute and evaluate the trivariate spline and its gradient with this approach the volume data can be visualized efficiently eg with isosurface ray-casting along an arbitrary ray the splines are univariate piecewise quadratics and thus the exact intersection for a prescribed isovalue can be easily determined in an analytic and exact way our results confirm the efficiency of the quasi-interpolating method and demonstrate high visual quality for rendered isosurfaces
many traditional techniques for "looking inside" volumetric_data involve removing portions of the data for example using various cutting tools to reveal the interior this allows the user to see hidden parts of the data but has the disadvantage of removing potentially important surrounding contextual information we explore an alternate strategy for browsing that uses deformations where the user can cut into and open up spread apart or peel away parts of the volume in real time making the interior visible while still retaining surrounding context we consider various deformation strategies and present a number of interaction techniques based on different metaphors our designs pay special attention to the semantic layers that might compose a volume eg the skin muscle bone in a scan of a human users can apply deformations to only selected layers or apply a given deformation to a different degree to each layer making browsing more flexible and facilitating the visualization of relationships between layers our interaction techniques are controlled with direct "in place" manipulation using pop-up menus and 3d widgets to avoid the divided attention and awkwardness that would come with panels of traditional widgets initial user feedback indicates that our techniques are valuable especially for showing portions of the data spatially situated in context with surrounding data
video data generated by the entertainment industry security and traffic cameras video conferencing systems video emails and so on is perhaps most time-consuming to process by human beings in this paper we present a novel methodology for "summarizing" video sequences using volume_visualization_techniques we outline a system pipeline for capturing videos extracting features volume_rendering video and feature data and creating video_visualization we discuss a collection of image comparison metrics including the linear dependence detector for constructing "relative" and "absolute" difference volumes that represent the magnitude of variation between video frames we describe the use of a few volume_visualization_techniques including volume scene graphs and spatial transfer_functions for creating video_visualization in particular we present a stream-based technique for processing and directly rendering video data in real time with the aid of several examples we demonstrate the effectiveness of using video_visualization to convey meaningful information contained in video sequences
we present an alternative method for viewing time-varying volumetric_data we consider such data as a four-dimensional data field rather than considering space and time as separate entities if we treat the data in this manner we can apply high dimensional slicing and projection techniques to generate an image hyperplane the user is provided with an intuitive user interface to specify arbitrary hyperplanes in 4d which can be displayed with standard volume_rendering techniques from the volume specification we are able to extract arbitrary hyperslices combine slices together into a hyperprojection volume or apply a 4d raycasting method to generate the same results in combination with appropriate integration operators and transfer_functions we are able to extract and present different space-time features to the user
this paper introduces a method for converting an image or volume sampled on a regular grid into a space-efficient irregular point hierarchy the conversion process retains the original frequency characteristics of the dataset by matching the spatial distribution of sample points with the required frequency to achieve good blending the spherical points commonly used in volume_rendering are generalized to ellipsoidal point primitives a family of multiresolution oriented gabor wavelets provide the frequency-space analysis of the dataset the outcome of this frequency analysis is the reduced set of points in which the sampling rate is decreased in originally oversampled areas during rendering the traversal of the hierarchy can be controlled by any suitable error metric or quality criteria the local level of refinement is also sensitive to the transfer_function areas with density ranges mapped to high transfer_function variability are rendered at higher point resolution than others our decomposition is flexible and can be used for iso-surface rendering alpha compositing and x-ray rendering of volumes we demonstrate our hierarchy with an interactive splatting volume renderer in which the traversal of the point hierarchy for rendering is modulated by a user-specified frame rate
numerical particle_simulations and astronomical observations create huge data sets containing uncorrelated 3d points of varying size these data sets cannot be visualized interactively by simply rendering millions of colored points for each frame therefore in many visualization_applications a scalar density corresponding to the point distribution is resampled on a regular grid for direct_volume_rendering however many fine details are usually lost for voxel resolutions which still allow interactive_visualization on standard workstations since no surface geometry is associated with our data sets the recently introduced point-based_rendering algorithms cannot be applied as well in this paper we propose to accelerate the visualization of scattered point data by a hierarchical_data structure based on a pca clustering procedure by traversing this structure for each frame we can trade-off rendering speed vs image quality our scheme also reduces memory consumption by using quantized relative coordinates and it allows for fast sorting of semi-transparent clusters we analyze various software and hardwareimplementations of our renderer and demonstrate that we can now visualize data sets with tens of millions of points interactively with sub-pixel screen space error on current pc graphics_hardware employing advanced vertex shader functionality
we present an innovative modeling and rendering primitive called the o-buffer for sample-based graphics such as images volumes and points the 2d or 3d o-buffer is in essence a conventional image or a volume respectively except that samples are not restricted to a regular grid a sample position in the o-buffer is recorded as an offset to the nearest grid point of a regular base grid hence the name o-buffer the offset is typically quantized for compact representation and efficient rendering the o-buffer emancipates pixels and voxels from the regular grids and can greatly improve the modeling power of images and volumes it is a semi-regular structure which lends itself to efficient construction and rendering image quality can be improved by storing more spatial information with samples and by avoiding multiple resamplings and delaying reconstruction to the final rendering stage using o-buffers more accurate multi-resolution representations can be developed for images and volumes it can also be exploited to represent and render unstructured primitives such as points particles curvilinear or irregular volumes the o-buffer is therefore a uniform representation for a variety of graphics primitives and supports mixing them in the same scene we demonstrate the effectiveness of the o-buffer with hierarchical o-buffers layered depth o-buffers and hybrid volume_rendering with o-buffers
in this paper a novel volume-rendering technique based on monte carlo integration is presented as a result of a preprocessing a point cloud of random samples is generated using a normalized continuous reconstruction of the volume as a probability density function this point cloud is projected onto the image plane and to each pixel an intensity value is assigned which is proportional to the number of samples projected onto the corresponding pixel area in such a way a simulated x-ray image of the volume can be obtained theoretically for a fixed image resolution there exists an m number of samples such that the average standard deviation of the estimated pixel intensities us under the level of quantization error regardless of the number of voxels therefore monte carlo volume_rendering mcvr is mainly proposed to efficiently visualize large volume data sets furthermore network applications are also supported since the trade-off between image quality and interactivity can be adapted to the bandwidth of the client/server connection by using progressive refinement
in this paper we propose a new method for the creation of normal maps for recovering the detail on simplified meshes and a set of objective techniques to metrically evaluate the quality of different recovering techniques the proposed techniques that automatically produces a normal-map texture for a simple 3d model that "imitates" the high frequency detail originally present in a second much higher resolution one is based on the computation of per-texel visibility and self-occlusion information this information is used to define a point-to-point correspondence between simplified and hires meshes moreover we introduce a number of criteria for measuring the quality visual or otherwise of a given mapping method and provide efficient algorithms to implement them lastly we apply them to rate different mapping methods including the widely used ones and the new one proposed here
in this paper we show how out-of-core mesh processing techniques can be adapted to perform their computations based on the new processing sequence paradigm isenburg et al 2003 using mesh simplification as an example we believe that this processing concept will also prove useful for other tasks such a parameterization remeshing or smoothing for which currently only in-core solutions exist a processing sequence represents a mesh as a particular interleaved ordering of indexed triangles and vertices this representation allows streaming very large meshes through main memory while maintaining information about the visitation status of edges and vertices at any time only a small portion of the mesh is kept in-core with the bulk of the mesh data residing on disk mesh access is restricted to a fixed traversal order but full connectivity and geometry information is available for the active elements of the traversal this provides seamless and highly efficient out-of-core access to very large meshes for algorithms that can adapt their computations to this fixed ordering the two abstractions that are naturally supported by this representation are boundary-based and buffer-based processing we illustrate both abstractions by adapting two different simplification methods to perform their computation using a prototype of our mesh processing sequence api both algorithms benefit from using processing sequences in terms of improved quality more efficient execution and smaller memory footprints
in this paper a new quadric-based view-dependent simplification scheme is presented the scheme provides a method to connect mesh simplification controlled by a quadric error metric with a level-of-detail hierarchy that is accessed continuously and efficiently based on current view parameters a variety of methods for determining the screen-space metric for the view calculation are implemented and evaluated including an appearance-preserving method that has both geometry- and texture-preserving aspects results are presented and compared for a variety of models
we present a new algorithm for simplifying the shape of 3d objects by manipulating their medial axis transform mat from an unorganized set of boundary points our algorithm computes the mat decomposes the axis into parts then selectively removes a subset of these parts in order to reduce the complexity of the overall shape the result is simplified mat that can be used for a variety of shape operations in addition a polygonal surface of the resulting shape can be directly generated from the filtered mat using a robust surface_reconstruction method the algorithm presented is shown to have a number of advantages over other existing approaches
while there are a couple of transfer_function_design approaches for ct and mri magnetic_resonance_imaging data direct_volume_rendering of ultrasound data still relies on manual adjustment of an inflexible piecewise linear opacity transfer_function otf on a trial-and-error basis the main challenge of automatically designing an otf for visualization of sonographic data is the low signal-to-noise ratio in combination with real time data acquisition at frame rates up to 25 volumes per second in this paper we present an efficient solution of this task our approach is based on the evaluation of tube cores ie collections of voxels gathered by traversing the volume in rendering directions we use information about the probable position of an interface between tissues of different echogenicity to adaptively design an otf in a multiplicative way we show the appropriateness of our approach by examples deliberately on data sets of moderate quality arising frequently in clinical settings
volume_rendering is a flexible technique for visualizing dense 3d volumetric_datasets a central element of volume_rendering is the conversion between data values and observable quantities such as color and opacity this process is usually realized through the use of transfer_functions that are precomputed and stored in lookup tables for multidimensional transfer_functions applied to multivariate data these lookup tables become prohibitively large we propose the direct evaluation of a particular type of transfer_functions based on a sum of gaussians because of their simple form in terms of number of parameters these functions and their analytic integrals along line segments can be evaluated efficiently on current graphics_hardware obviating the need for precomputed lookup tables we have adopted these transfer_functions because they are well suited for classification based on a unique combination of multiple data values that localize features in the transfer_function domain we apply this technique to the visualization of several multivariate datasets ct cryosection that are difficult to classify and render accurately at interactive rates using traditional approaches
in the traditional volume_visualization paradigm the user specifies a transfer_function that assigns each scalar value to a color and opacity by defining an opacity and a color map function the transfer_function has two limitations first the user must define curves based on histogram and value rather than seeing and working with the volume itself second the transfer_function is inflexible in classifying regions of interest where values at a voxel such as intensity and gradient are used to differentiate material not talking into account additional properties such as texture and position we describe an intuitive user interface for specifying the classification functions that consists of the users painting directly on sample slices of the volume these painted regions are used to automatically define high-dimensional classification functions that can be implemented in hardware for interactive rendering the classification of the volume is iteratively improved as the user paints samples allowing intuitive and efficient viewing of materials of interest
direct_volume_rendering of scalar_fields uses a transfer_function to map locally measured data properties to opacities and colors the domain of the transfer_function is typically the one-dimensional space of scalar_data values this paper advances the use of curvature information in multi-dimensional_transfer_functions with a methodology for computing high-quality curvature measurements the proposed methodology combines an implicit formulation of curvature with convolution-based reconstruction of the field we give concrete guidelines for implementing the methodology and illustrate the importance of choosing accurate filters for computing derivatives with convolution curvature-based transfer_functions are shown to extend the expressivity and utility of volume_rendering through contributions in three different application areas nonphotorealistic volume_rendering surface smoothing via anisotropic_diffusion and visualization of isosurface uncertainty
the internet pervades many aspects of our lives and is becoming indispensable to critical functions in areas such as commerce government production and general information dissemination to maintain the stability and efficiency of the internet every effort must be made to protect it against various forms of attacks malicious users and errors a key component in the internet security effort is the routine examination of internet routing data which unfortunately can be too large and complicated to browse directly we have developed an interactive_visualization process which proves to be very effective for the analysis of internet routing data in this application paper we show how each step in the visualization process helps direct the analysis and glean insights from the data these insights include the discovery of patterns detection of faults and abnormal events understanding of event correlations formation of causation hypotheses and classification of anomalies we also discuss lessons learned in our visual_analysis study
we describe a visualization_application intended for operational use in formulating business strategy in the customer service arena the visualization capability provided in this application implicitly allows the user to better formulate the objective function for large optimization runs which act to minimize costs based on certain input parameters visualization is necessary because many of the inputs to the optimization runs are themselves strategic business decisions which are not pre-ordained both information_visualization presentations and three-dimensional visualizations are included to help users better understand the cost/benefit tradeoffs of these strategic business decisions here visualization explicitly provides value not possible algorithmically as the perceived benefit of different combinations of service level does not have an a priori mathematical formulation thus we take advantage of the fundamental power of visualization bringing the user's intuition and pattern_recognition skills into the solution while simultaneously taking advantage of the strength of algorithmic approaches to quickly and accurately find an optimal solution to a well-defined problem
simulation of rigid body dynamics has been a field of active research for quite some time however the presentation of simulation results has received far less attention so far we present an interactive and intuitive 3d_visualization_framework for rigid body simulation data we introduce various glyphs representing vector attributes such as force and velocity as well as angular attributes including angular velocity and torque we have integrated our visualization method into an application developed at one of the leading companies in automotive engine design and simulation we apply our principles to visualization of chain and belt driven timing drives in engines
this paper describes a set of techniques developed for the visualization of high-resolution volume data generated from industrial computed_tomography for nondestructive testing ndt applications because the data are typically noisy and contain fine features direct_volume_rendering methods do not always give us satisfactory results we have coupled region_growing techniques and a 2d histogram interface to facilitate volumetric feature_extraction the new interface allows the user to conveniently identify separate or composite and compare features in the data to lower the cost of segmentation we show how partial region_growing results can suggest a reasonably good classification function for the rendering of the whole volume the ndt applications that we work on demand visualization tasks including not only feature_extraction and visual inspection but also modeling and measurement of concealed structures in volumetric objects an efficient filtering and modeling process for generating surface_representation of extracted features is also introduced four ct data sets for preliminary ndt are used to demonstrate the effectiveness of the new visualization strategy that we have developed
the simulation of breaking of waves the formation of thin spray sheets and the entertainment of air around the next generation of naval surface combatants is an ongoing 3-year department of defense dod challenge project the goal of this project is a validated computation capability to model the full hydrodynamics around a surface combatant including all of the processes that affect mission and performance visualization of these large-scale simulations is paramount to understanding the complex physics involved these simulations produce enormous data sets with both surface and volumetric qualities wave breaking spray sheets and air entertainment can be visualized using isosurfaces of scalar_data visualization of quantities such as the vorticity field also provides insight into the dynamics of droplet and bubble formation this paper documents the techniques used results obtained and lessons learned from the visualization of the hydrodynamics of naval vessels
we present techniques for discovering and exploiting regularity in large curvilinear data sets the data can be based on a single mesh or a mesh composed of multiple submeshes also known as zones multi-zone data are typical in computational_fluid_dynamics_(cfd) simulations regularities include axis-aligned rectilinear and cylindrical meshes as well as cases where one zone is equivalent to a rigid body transformation of another our algorithms can also discover rigid-body motion of meshes in time-series_data next we describe a data model where we can utilize the results from the discovery process in order to accelerate large_data_visualizations where possible we replace general curvilinear zones with rectilinear or cylindrical zones in rigid-body motion cases we replace a time-series of meshes with a transformed mesh object where a reference mesh is dynamically transformed based on a given time value in order to satisfy geometry requests on demand the data model enables us to make these substitutions and dynamic transformations transparently with respect to the visualization algorithms we present results with large_data sets where we combine our mesh replacement and transformation techniques with out-of-core paging in order to achieve analysis speedups ranging from 15 to 2
in this paper we describe a set of 3d and 4d visualization tools and techniques for corie a complex environmental observation and forecasting system eofs for the columbia river the columbia river a complex and highly variable estuary is the target of numerous cross-disciplinary ecosystem research projects and is at the heart of multiple sustainable development issues with long reaching implications for the pacific northwest however there has been until recently no comprehensive and objective system available for modeling this environment and as a consequence researchers and agencies have had inadequate tools for evaluating the effects of natural resource management decisions corie was designed to address this gap and is a major step towards the vision of a scalable multi-use real-time eofs although corie already had a rich set of visualization tools most of them produced 2d visualizations and did not allow for interactive_visualization our work adds advanced interactive 3d tools to corie which can be used for further inspection of the simulated and measured data
this paper describes the work of a team of researchers in computer_graphics geometric computing and civil engineering to produce a visualization of the september 2001 attack on the pentagon the immediate motivation for the project was to understand the behavior of the building under the impact the longer term motivation was to establish a path for producing high-quality visualizations of large scale simulations the first challenge was managing the enormous complexity of the scene to fit within the limits of state-of-the art simulation software systems and supercomputing resources the second challenge was to integrate the simulation results into a high-quality visualization to meet this challenge we implemented a custom importer that simplifies and loads the massive simulation data in a commercial animation system the surrounding scene is modeled using image-based techniques and is also imported in the animation system where the visualization is produced a specific issue for us was to federate the simulation and the animation systems both commercial systems not under our control and following internally different conceptualizations of geometry and animation this had to be done such that scalability was achieved the reusable link created between the two systems allows communicating the results to non-specialists and the public at large as well as facilitatingcommunication in teams with members having diverse technical backgrounds
we describe an interactive_visualization and modeling program for the creation of protein structures "from scratch" the input to our program is an amino acid sequence - decoded from a gene - and a sequence of predicted secondary structure types for each amino acid - provided by external structure prediction programs our program can be used in the set-up phase of a protein structure prediction process the structures created with it serve as input for a subsequent global internal energy minimization or another method of protein structure prediction our program supports basic visualization methods for protein structures interactive_manipulation based on inverse kinematics and visualization guides to aid a user in creating "good" initial structures
we describe an animated electro-holographic visualization of brain lesions due to the progression of multiple sclerosis a research case_study is used which documents the expression of visible brain lesions in a series of magnetic_resonance_imaging mri volumes collected over the interval of one year some of the salient information resident within this data is described and the motivation for using a dynamic spatial display to explore its spatial and temporal characteristics is stated we provide a brief overview of spatial displays in medical imaging applications and then describe our experimental visualization pipeline from the processing of mri datasets through model construction computer graphic rendering and hologram encoding the utility strengths and shortcomings of the electro-holographic visualization are described and future improvements are suggested
in this paper we use advanced tensor_visualization_techniques to study 3d diffusion_tensor_mri data of a heart we use scalar and tensor glyph visualization methods to investigate the data and apply a moving least squares mls fiber tracing method to recover and visualize the helical structure and the orientation of the heart muscle fibers
provides an abstract of the keynote presentation and a brief professional biography of the presenter the complete presentation was not made available for publication as part of the conference proceedings
high-throughput experiments such as gene expression microarrays in the life sciences result in large_datasets in response a wide variety of visualization tools have been created to facilitate data_analysis biologists often face a dilemma in choosing the best tool for their situation the tool that works best for one biologist may not work well for another due to differences in the type of insight they seek from their data a primary purpose of a visualization tool is to provide domain-relevant insight into the data ideally any user wants maximum information in the least possible time in this paper we identify several distinct characteristics of insight that enable us to recognize and quantify it based on this we empirically evaluate five popular microarray visualization tools our conclusions can guide biologists in selecting the best tool for their data and computer scientists in developing and evaluating visualizations data visualization bioinformatics empirical_evaluation high_throughput_experiments insight microarray_data
this paper describes a comparative experiment with five well-known tree_visualization_systems and windows explorer as a baseline system subjects performed tasks relating to the structure of a directory hierarchy and to attributes of files and directories task completion times correctness and user_satisfaction were measured and video recordings of subjects' interaction with the systems were made significant system and task type effects and an interaction between system and task type were found qualitative analyses of the video recordings were thereupon conducted to determine reasons for the observed differences resulting in several findings and design recommendations as well as implications for future experiments with tree_visualization_systemsaccuracy design recommendations experimental_comparison information_visualization task_performance user_interaction user_satisfaction
in this paper we describe a taxonomy of generic graph related tasks and an evaluation aiming at assessing the readability of two representations of graphs matrix-based_representations and node-link_diagrams this evaluation bears on seven generic tasks and leads to important recommendations with regard to the representation of graphs according to their size and density for instance we show that when graphs are bigger than twenty vertices the matrix-based visualization performs better than node-link_diagrams on most tasks only path finding is consistently in favor of node-link_diagrams throughout the evaluation visualization of graphs adjacency matrices evaluation node-link representation readability
analyzing observations over time and geography is a common task but typically requires multiple separate tools the objective of our research has been to develop a method to visualize and work with the spatial interconnectedness of information over time and geography within a single highly interactive 3d view a novel visualization_technique for displaying and tracking events objects and activities within a combined temporal and geospatial display has been developed this technique has been implemented as a demonstratable prototype called geotime in order to determine potential utility initial evaluations have been with military users however we believe the concept is applicable to a variety of government and business analysis tasks -d visualization geospatial interactive_visualization link_analysis spatiotemporal visual_data_analysis
in many application domains data is collected and referenced by its geospatial location nowadays different kinds of maps are used to emphasize the spatial distribution of one or more geospatial attributes the nature of geospatial statistical data is the highly nonuniform distribution in the real world data sets this has several impacts on the resulting map visualizations classical area maps tend to highlight patterns in large areas which may however be of low importance cartographers and geographers used cartograms or value-by-area maps to address this problem long before computers were available although many automatic techniques have been developed most of the value-by-area cartograms are generated manually via human interaction in this paper we propose a novel visualization_technique for geospatial_data sets called recmap our technique approximates a rectangular partition of the rectangular display area into a number of map regions preserving important geospatial constraints it is a fully automatic technique with explicit user control over all exploration constraints within the exploration process experiments show that our technique produces visualizations of geospatial_data sets which enhance the discovery of global and local correlations and demonstrate its performance in a variety of applications database_and_data_mining_visualization geographic_visualization information_visualization
in this paper we present ezel a visual tool we developed for the performance assessment of peer-to-peer file-sharing networks we start by identifying the relevant data transferred in this kind of networks and the main performance assessment questions then we describe the visualization of data from two different points of view first we take servers as focal points and we introduce a new technique faded cushioning which allows visualizing the same data from different perspectives secondly we present the viewpoint of files and we expose the correlations with the server stance via a special scatter plot finally we discuss how our tool based on the described techniques is effective in the performance assessment of peer-to-peer file-sharing networks pp file-sharing networks visualization distributed_file_systems_visualization process_visualization small displays
a major challenge of current visualization and visual_data_mining vdm frameworks is to support users in the orientation in complex visual mining scenarios an important aspect to increase user support and user orientation is to use a history mechanism that first of all provides un- and redoing functionality in this paper we present a new approach to include such history functionality into a vdm framework therefore we introduce the theoretical background outline design andimplementation aspects of a history management unit and conclude with a discussion showing the usefulness of our history management in a vdm framework history undo/redo visual_data_mining visualization
currentimplementations of multidimensional scaling mds an approach that attempts to best represent data point similarity in a low-dimensional representation are not suited for many of today's large-scale_datasets we propose an extension to the spring model approach that allows the user to interactively explore datasets that are far beyond the scale of previousimplementations of mds we present mdsteer a steerable mds computation engine and visualization tool that progressively computes an mds layout and handles datasets of over one million points our technique employs hierarchical_data structures and progressive layouts to allow the user to steer the computation of the algorithm to the interesting areas of the dataset the algorithm iteratively alternates between a layout stage in which a subselection of points are added to the set of active points affected by the mds iteration and a binning stage which increases the depth of the bin hierarchy and organizes the currently unplaced points into separate spatial regions this binning strategy allows the user to select onscreen regions of the layout to focus the mds computation into the areas of the dataset that are assigned to the selected bins we show both real and common synthetic benchmark datasets with dimensionalities ranging from 3 to 300 and cardinalities of over one million points dimensionality_reduction multidimensional scaling
exploratory_analysis of multidimensional data sets is challenging because of the difficulty in comprehending more than three dimensions two fundamental statistical principles for the exploratory_analysis are 1 to examine each dimension first and then find relationships among dimensions and 2 to try graphical displays first and then find numerical summaries ds moore 1999 we implement these principles in a novel conceptual framework called the rank-by-feature framework in the framework users can choose a ranking criterion interesting to them and sort 1d or 2d axis-parallel projections according to the criterion we introduce the rank-by-feature prism that is a color-coded lower-triangular matrix that guides users to desired features statistical graphs histogram boxplot and scatterplot and information_visualization_techniques overview coordination and dynamic_query are combined to help users effectively traverse 1d and 2d axis-parallel projections and finally to help them interactively find interesting features dynamic_query exploratory_data_analysis feature_detection/selection information_visualization statistical_graphics
traditional multidimensional visualization_techniques such as glyphs parallel_coordinates and scatterplot matrices suffer from clutter at the display level and difficult user navigation among dimensions when visualizing high dimensional datasets in this paper we propose a new multidimensional visualization_technique named a value and relation var display together with a rich set of navigation and selection tools for interactive_exploration of datasets with up to hundreds of dimensions by explicitly conveying the relationships among the dimensions of a high dimensional dataset the var display helps users grasp the associations among dimensions by using pixel-oriented techniques to present values of the data items in a condensed manner the var display reveals data patterns in the dataset using as little screen space as possible the navigation and selection tools enable users to interactively reduce clutter navigate within the dimension space and examine data value details within context effectively and efficiently the var display scales well to datasets with large numbers of data items by employing sampling and texture_mapping a case_study on a real dataset as well as the var displays of multiple real datasets throughout the paper reveals how our proposed approach helps users interactively explore high dimensional datasets with large numbers of data items multi-dimensional_visualization high dimensional datasets multi-dimensional_scaling pixel-oriented
the one-to-one strategy of mapping each single data item into a graphical marker adopted in many visualization_techniques has limited usefulness when the number of records and/or the dimensionality of the data set are very high in this situation the strong overlapping of graphical markers severely hampers the user's ability to identify patterns in the data from its visual representation we tackle this problem here with a strategy that computes frequency or density information from the data set and uses such information in parallel_coordinates visualizations to filter out the information to be presented to the user thus reducing visual_clutter and allowing the analyst to observe relevant patterns in the data the algorithms to construct such visualizations and the interaction mechanisms supported inspired by traditional image_processing techniques such as grayscale manipulation and thresholding are also presented we also illustrate how such algorithms can assist users to effectively identify clusters in very noisy large_data sets density-based_visualization information_visualization visual_clustering visual_data_mining
visual_clutter denotes a disordered collection of graphical entities in information_visualization clutter can obscure the structure present in the data even in a small dataset clutter can make it hard for the viewer to find patterns relationships and structure in this paper we define visual_clutter as any aspect of the visualization that interferes with the viewer's understanding of the data and present the concept of clutter-based dimension reordering dimension_order is an attribute that can significantly affect a visualization's expressiveness by varying the dimension_order in a display it is possible to reduce clutter without reducing information content or modifying the data in any way clutter_reduction is a display-dependent task in this paper we follow a three-step procedure for four different visualization_techniques for each display technique first we determine what constitutes clutter in terms of display properties then we design a metric to measure visual_clutter in this display finally we search for an order that minimizes the clutter in a display multidimensional visualization dimension_order visual_clutter visual_structure
this research demonstrates how principles of self-organization and behavior simulation can be used to represent dynamic_data evolutions by extending the concept of information flocking originally introduced by proctor & winter 1998 to time-varying_datasets a rule-based behavior system continuously controls and updates the dynamic actions of individual three-dimensional elements that represent the changing data values of reoccurring data objects as a result different distinguishable motion types emerge that are driven by local interactions between the spatial elements as well as the evolution of time-varying_data values notably this representation technique focuses on the representation of dynamic_data alteration characteristics or how reoccurring data objects change over time instead of depicting the exact data values themselves in addition it demonstrates the potential of motion as a useful information_visualization cue the original information flocking approach is extended to incorporate time-varying_datasets live database querying continuous data_streaming real-time data similarity evaluation automatic shape generation and more stable flocking algorithms different experiments prove that information flocking is capable of representing short-term events as well as long-term temporal_data evolutions of both individual and groups of time-dependent_data objects an historical stock market quote price dataset is used to demonstrate the algorithms and principles of time-varying information flocking d information_visualization artificial_life boids motion time-varying information_visualization
we present artifacts of the presence era a digital installation that uses a geological metaphor to visualize the events in a physical space over time the piece captures video and audio from a museum and constructs an impressionistic visualization of the evolving history in the space instead of creating a visualization tool for data_analysis we chose to produce a piece that functions as a souvenir of a particular time and place we describe the design choices we made in creating this installation the visualization_techniques we developed and the reactions we observed from users and the media we suggest that the same approach can be applied to a more general set of visualization contexts ranging from email archives to newsgroups conversations history public_space visualization
color is often used to convey information and color compositing is often required while visualizing multiattribute information this paper proposes an alternative method for color compositing in order to present understandable color_blending to the general public several techniques are proposed first a paint-inspired ryb color space is used in addition noise patterns are employed to produce subregions of pure color within an overlapped region we show examples to demonstrate the effectiveness of our technique for visualization color mixing perception ryb
many tree browsers allow subtrees under a node to be collapsed or expanded enabling the user to control screen space usage and selectively drill-down however explicit expansion of nodes can be tedious expand-ahead is a space-filling strategy by which some nodes are automatically expanded to fill available screen space without expanding so far that nodes are shown at a reduced size or outside the viewport this often allows a user exploring the tree to see further down the tree without the effort required in a traditional browser it also means the user can sometimes drill-down a path faster by skipping over levels of the tree that are automatically expanded for them expand-ahead differs from many detail-in-context techniques in that there is no scaling or distortion involved we present 1d and 2d prototypeimplementations of expand-ahead and identify various design issues and possible enhancements to our designs our prototypes support smooth animated transitions between different views of a tree we also present the results of a controlled experiment which show that under certain conditions users are able to drill-down faster with expand-ahead than without adaptive_user_interfaces automatic_expansion expandahead focus+context space filling tree_browsing_and_navigation
the infosky visual explorer is a system enabling users to interactively explore large hierarchically structured document collections similar to a real-world telescope infosky employs a planar graphical representation with variable magnification documents of similar content are placed close to each other and displayed as stars while collections of documents at a particular level in the hierarchy are visualised as bounding polygons usability testing of an early prototypeimplementation of infosky revealed several design issues which prevented users from fully exploiting the power of the visual metaphor evaluation results have been incorporated into an advanced prototype and another usability test has been conducted a comparison of test results demonstrates enhanced system performance and points out promising directions for further work voronoi document retrieval force-directed_placement hierarchical_repositories information_management information visualisation knowledge_management navigation
we describe an exploratory technique based on the direct interaction with a 2d modified scatterplot computed from two different metrics calculated over the elements of a network the scatterplot is transformed into an image by applying standard image_processing techniques resulting into blurring effects segmentation of the image allow to easily select patches on the image as a way to extract subnetworks we were inspired by the work of wattenberg and fisher [m wattenberg et al 2003] showing that the blurring process builds into a multiscale perceptual scheme making this type of interaction intuitive to the user we explain how the exploration of the network can be guided by the visual_analysis of the blurred scatterplot and by its possible interpretations graph_navigation blurring clustering exploration filtering multiscale perceptual_organization scatterplot
the design and evaluation of most current information_visualization_systems descend from an emphasis on a user's ability to "unpack" the representations of data of interest and operate on them independently too often successful decision-making and analysis are more a matter of serendipity and user_experience than of intentional design and specific support for such tasks although humans have considerable abilities in analyzing relationships from data the utility of visualizations remains relatively variable across users data sets and domains in this paper we discuss the notion of analytic gaps which represent obstacles faced by visualizations in facilitating higher-level analytic tasks such as decision-making and learning we discuss support for bridging the analytic gap propose a framework for design and evaluation of information_visualization_systems and demonstrate its use information_visualization analytic gap evaluation framework knowledge_tasks theory
we present the novel high-level visualization_taxonomy our taxonomy classifies visualization algorithms rather than data algorithms are categorized based on the assumptions they make about the data being visualized we call this set of assumptions the design_model because our taxonomy is based on design_models it is more flexible than existing taxonomies and considers the user's conceptual_model emphasizing the human aspect of visualization design_models are classified according to whether they are discrete or continuous and by how much the algorithm designer chooses display attributes such as spatialization timing colour and transparency this novel approach provides an alternative view of the visualization field that helps explain how traditional divisions eg information_and_scientific_visualization relates and overlap and that may inspire research ideas in hybrid_visualization areas classification conceptual_model design_model taxonomy user_model visualization
improvise is a fully-implemented system in which users build and browse multiview visualizations interactively using a simple shared-object coordination mechanism coupled with a flexible expression-based visual_abstraction_language by coupling visual_abstraction with coordination users gain precise control over how navigation and selection in the visualization affects the appearance of data in individual views as a result it is practical to build visualizations with more views and richer coordination in improvise than in other visualization_systems building and browsing activities are integrated in a single live user interface that lets users alter visualizations quickly and incrementally during data_exploration coordinated_queries coordination exploratory_visualization multiple_views visual_abstraction_language
this article presents the infovis toolkit designed to support the creation extension and integration of advanced 2d information_visualization components into interactive java swing applications the infovis toolkit provides specific data structures to achieve a fast action/feedback loop required by dynamic queries it comes with a large set of components such as range sliders and tailored control panels required to control and configure the visualizations these components are integrated into a coherent framework that simplifies the management of rich data structures and the design and extension of visualizations supported data structures currently include tables trees and graphs supported visualizations include scatter plots time_series parallel_coordinates treemaps icicle trees node-link_diagrams for trees and graphs and adjacency matrices for graphs all visualizations can use fisheye lenses and dynamic_labeling the infovis toolkit supports hardware_acceleration when available through agile2d animplementation of the java graphics api based on opengl achieving speedups of 10 to 200 times the article also shows how new visualizations can be added and extended to become components enriching visualizations as well as general applications graphics information_visualization integration toolkit
graph_drawing is a basic visualization tool for graphs of up to hundreds of nodes and edges there are many effective techniques available at greater scale data_density and occlusion problems often negate its effectiveness conventional pan-and-zoom and multiscale and geometric fisheye_views are not fully satisfactory solutions to this problem as an alternative we describe a topological zooming method it is based on the precomputation of a hierarchy of coarsened graphs which are combined on the fly into renderings with the level of detail dependent on the distance from one or more foci we also discuss a related distortion method that allows our technique to achieve constant information density displays large graph_visualization topological_fisheye
in web data telecommunications traffic and in epidemiological studies dense subgraphs correspond to subsets of subjects ie users patients that share a collection of attributes values ie accessed web pages email-calling patterns or disease diagnostic profiles visual and computational identification of these "clusters" becomes useful when domain experts desire to determine thosefactors of major influence in the formation of access andcommunication clusters or in the detection and contention of disease spread with the current increases in graphic hardware capabilities and ram sizes it is more useful to relate graph sizes to the available screen real estate s and the amount of available ram m instead of the number of edges or nodes in the graph we offer a visual interface that is parameterized by m and s and is particularly suited for navigation tasks that require the identification of subgraphs whose edge density is above certain threshold this is achieved by providing a zoomable matrix view of the underlying data this view is strongly coupled to a hierarchical view of the essential information elements present in the data domain we illustrate the applicability of this work to the visual navigation of cancer incidence data and to an aggregated sample of phone call traffic cancer_data_clustering external_memory_algorithms graph_visualization hierarchy_trees phone_traffic
this paper presents an algorithm for drawing a sequence of graphs that contain an inherent grouping of their vertex set into clusters it differs from previous work on dynamic graph_drawing in the emphasis that is put on maintaining the clustered structure of the graph during incremental layout the algorithm works online and allows arbitrary modifications to the graph it is generic and can be implemented using a wide range of static force-directed graph_layout tools the paper introduces several metrics for measuring layout quality of dynamic clustered graphs the performance of our algorithm is analyzed using these metrics the algorithm has been successfully applied to visualizing mobile object software dynamic_layout graph_drawing mobile objects software_visualization
many real world graphs have small world characteristics that is they have a small diameter compared to the number of nodes and exhibit a local cluster structure examples are social_networks software structures bibliographic references and biological neural nets their high connectivity makes both finding a pleasing layout and a suitable clustering hard in this paper we present a method to create scalable interactive_visualizations of small_world_graphs allowing the user to inspect local clusters while maintaining a global overview of the entire structure the visualization method uses a combination of both semantical and geometrical distortions while the layout is generated by a spring_embedder algorithm using recently developed force model we use a cross referenced database of 500 artists as a running example clustering graph_drawing graph_visualization small_world_graphs
we present a method by which force-directed algorithms for graph_layouts can be generalized to calculate the layout of a graph in an arbitrary riemannian geometry the method relies on extending the euclidean notions of distance angle and force-interactions to smooth noneuclidean geometries via projections to and from appropriately chosen tangent spaces in particular we formally describe the calculations needed to extend such algorithms to hyperbolic and spherical geometries force-directed algorithms graph_drawing hyperbolic_space information_visualization non-euclidean_geometry spherical_space spring_embedders
in today's fast-paced world it is becoming increasingly difficult to stay abreast of the public discourse with the advent of hundreds of closed-captioned cable channels and internet-based channels such as news feeds blogs or email knowing the "buzz" is a particular challenge textpool addresses this problem by quickly summarizing recent content in live text streams the summarization is a dynamically changing textual collage that clusters related terms we tested textpool with the content of several rss newswire feeds which are updated roughly every five minutes textpool was able to handle this bandwidth well and produced useful summarizations of feed content
many fields of study produce time_series_datasets and both the size and number of theses datasets are increasing rapidly due to the improvement of data accumulation methods such as small cheap sensors and routine logging of events humans often fail to comprehend the structure of a long time_series_dataset because of the overwhelming amount of data and the range of different time scales at which there may be meaningful patterns binx is an interactive tool that provides dynamic_visualization and manipulation of long time_series_datasets the dataset is visualized through user controlled aggregation augmented by various information_visualization_techniques
we explore the use of natures phyllotactic patterns to inform the layout of hierarchical_data these naturally occurring patterns provide a non-overlapping optimal packing when the total number of nodes is not known a priori we present a family of expandable tree layouts based on these patterns graph_layout information_visualization phyllotactic patterns tree_visualization
we have constructed an information_visualization tool for understanding complex arguments the tool enables analysts to construct structured arguments using judicial proof techniques associate evidence with hypotheses and set evidence parameters such as relevance and credibility users manipulate the hypotheses and their associated inference networks using visualization_techniques our tool integrates concepts from structured argumentation analysis of competing hypotheses and hypothesis scoring with information_visualization it presents new metaphors for visualizing and manipulating structured arguments
a digital lens is a user interface mechanism that is a potential solution to information mangement problems we investigated the use of digital lensing applied to imagery analysis participants completed three different types of tasks locate follow and compare using a magnification lens with three different degrees of offset aligned adjacent and docked over a high-resolution aerial photo although no lens offset mode was significantly better than another most participants preferred the adjacent mode for the locate and compare tasks and the docked mode for the follow tasks this paper describes the results of a user_study of magnification lenses and provides new insights into preferences of and interactions with digital lensing
we introduce a semantically zoomable interface that displays emails as interactive objects rather than files containing lines of text as in traditional e-mail interfaces in this system e-mails are displayed as node objects called e-mail nodes within a 25-dimensional world the e-mail nodes are semantically zoomable and each may be rearranged to different locations within the plane to organize threads topics or projects the prototype for this system was built using the piccolo toolkit the successor of pad++ and jazz [2 3]
managing file systems of large organizations can present significant challenges in terms of the number of users shared access to parts of the file system and securing and monitoring critical parts of the file system we present an interactive exploratory tool for monitoring and viewing the complex relationships within the andrews file system afs this tool is targeted as an aid to system administrators to manage users applications and shared access we tested our tool on unc charlottes andrews file system afs file system which contains 4554 users 556 user groups and 22 million directories two types of visualizations are supported to explore file system relationships in addition drill-down features are provided to access the user file system and access control information of any directory within the system all of the views are linked to facilitate easy navigation
arna is an interactive_visualization system that supports comparison and alignment of rna secondary structure we present a new approach to rna alignment that exploits the complex structure of the smith-waterman local distance matrix allowing people to explore the space of possible partial alignments to discover a good global solution the modular software architecture separates the user interface from computation allowing the possibility of incorporating different alignment algorithms into the same framework
we present a model and prototype system for tracking user_interactions within a visualization the history of the interactions are exposed to the user in a way that supports non-linear navigation of the visualization space the interactions can be augmented with annotations which together with the interactions can be shared with other users and applied to other data in a seamless way the techniques constitute a novel approach for documenting information provenance
we propose an interactive_visualization approach to finding a mathematical model for a real world process commonly known in the field of control theory as system identification the use of interactive_visualization_techniques provides the modeller with instant visual feedback which facilitates the model validation process when working interactively with such large_data sets as are common in system identification methods to handle this data efficiently are required we are developing approaches based on data_streaming to meet this need
data repositories around the world hold many thousands of data sets finding information from these data sets is greatly facilitated by being able to quickly and efficiently browse remote data sets in this note we introduce the iconic remote visual_data_exploration toolirvdx which is a visual_data_mining tool used for exploring the features of remote and distributed data without the necessity of downloading the entire data set irvdx employs three kinds of visualizations one provides a reduced representation of the data sets which we call dataset icons these icons show the important statistical characteristics of data sets and help to identify relevant data sets from distributed repositories another one is called the remote dataset visual browser that provides visualizations to browse remote data without downloading the complete data set to identify its content the final one provides visualizations to show the degree of similarity between two data sets and to visually determine whether a join of two remote data sets will be meaningful
this interactive poster proposes a novel explorative way to browse a database containing links to resource systems-related information online our approach is an illustrative one and draws on our combined backgrounds in computer science graphic and interaction_design sustainability community organization and urban design the data visualized in our prototype was collected by students in the course sustainable habits which lauren dietrich taught at stanford university during winter 2004
email has developed into one of the most extensively used computer applications email interfaces on the other hand have gone through very few transformations since their inception and as the growing volumes of email data accumulate in users' email boxes these interfaces fail to provide effective message handling and browsing support saved email messages provide not only a vast record of one's electronic past but also a potential source of valuable insights into the structure and dynamics of one's social network in this paper we present famailiar a novel email visualization that draws upon email's inherently personal character by using intimacy as a key visualization parameter the program presents a visualization of email use over time famailiar facilitates navigation through large email collections enabling the user to discovercommunication rhythms and patterns
this work focuses on visualizing highly cyclic hierarchical_data a user interface is discussed and its interaction is illustrated using a recipe database example this example showcases a database with multiple categories for each recipe database entry
telepresence experiencing a place without physically being there offers an important means for the public experience of remote locations such as distant continents or other planets eventscope presents one such telepresence visualization_interface for bringing scientific missions to the public currently remote experience lessons based on nasas mars exploration rover missions are being made available through the eventscope framework to museums classrooms and the public at large
visualization_systems must intuitively display and allow interaction with large multivariate data on low-dimensional displays one problem often encountered in the process is occlusion the ambiguity that occurs when records from different data sets are mapped to the same display location for example because of occlusion summarizing 1000 graphs by simply stacking them one over another is pointless we solve this problem by adapting the solution to a similar problem in the information murals system [2] mapping the number of data elements at a location to display luminance inspired by histograms which map data frequency to space we call our solution histographs by treating a histograph as a digital image we can blur and highlight edges to emphasize data features we also support interactive_clustering of the data with data zooming and shape-based selection we are currently investigating alternative occlusion blending schemes
this paper addresses the problem of how to enable users to visually explore and compare large sets of documents that have been retrieved by different search_engines or queries the rank-spiral enables users to rapidly scan large numbers of documents and their titles in a single screen it uses a spiral mapping that maximizes information density and minimizes occlusions it solves the labeling problem by exploiting the structure of the special spiral mapping used focus+context interactions enable users to examine document clusters or groupings in more detail
this paper describes the integration of lookmarks into the paraview visualization tool lookmarks are pointers to views of specific parts of a dataset they were so named because lookmarks are to a visualization tool and dataset as bookmarks are to a browser and the world_wide_web a lookmark can be saved and organized among other lookmarks within the context of paraview then at a later time either in the same paraview session or in a different one it can be regenerated displaying the exact view of the data that had previously been saved this allows the user to pick up where they left off to continue to adjust the view or otherwise manipulate the data lookmarks facilitate collaboration between users who wish to share views of a dataset they enable more effective data comparison because they can be applied to other datasets they also serve as a way of organizing a users data ultimately a lookmark is a time-saving tool that automates the recreation of a complex view of the data
a standard method of visualizing high-dimensional_data is reducing its dimensionality to two or three using some algorithm and then creating a scatterplot with data represented by labelled and/or colored dots two problems with this approach are 1 dots do not represent data well 2 reducing to just three dimensions does not make full use of several dimensionality-reduction algorithms we demonstrate how partiview can be used to solve these problems in the context of handwriting recognition and image retrieval
visualizing long-term acoustic data has been an important subject in the field of equipment surveillance and equipment diagnosis this paper proposes a distortion-based visualization method of long-term acoustic data we applied the method to 1 hour observation data of electric discharge sound and our method could visualize the sound data more intelligibly as compared with conventional methods
intelligence analysts receive thousands of facts from a variety of sources in addition to the bare details of the fact — a particular person for example — each fact may have provenance reliability weight and other attributes each fact may also be associated with other facts eg that one person met another at a particular location the analysts task is to examine a huge collection of such loosely-structured facts and try to "connect the dots" to perceive the underlying and unknown causes — and their possible future courses we have designed and implemented a java platform called vim to support intelligence analysts in their work
the presented work aims to identify major research topics co-authorships and trends in the iv contest 2004 dataset co-author paper-citation and burst analysis were used to analyze the dataset the results are visually presented as graphs static pajek [1]visualizations and interactive network_layouts using pajeks svg output feature a complementary web page with all the raw data details of the analyses and high resolution images of all figures is available online at http//ivslisindianaedu/ref/iv04contest/
this is the first part summary of a three-part contest entry submitted to ieee infovis 2004 the contest topic is visualizing infovis symposium papers from 1995 to 2002 and their references the paper introduces the visualization tool in-spire the visualization process and results and presents lessons learned
we present paperlens a visualization that reveals connections trends and activity throughout the infovis conference community for the last 8 years it tightly couples views across papers authors and references this paper describes how we analyzed the data the strengths and weaknesses of paperlens and interesting patterns and relationships we have discovered using paperlens
our visualisation of the ieee infovis citation network is based on 3d graph visualisation techniques to make effective use of the third dimension we use a layered approach constraining nodes to lie on parallel planes depending on parameters such as year of publication or link degree within the parallel planes nodes are arranged using a fast force-directed_layout method a number of clusters representing different research areas were identified using a self organising map approach
in this case_study we attempt to visualize a real-world dataset consisting of 600 recently published information_visualization papers and their references this is done by first creating a global layout of the entire graph that preserves any cluster structure present we then use this layout as a basis to define a hierarchical_clustering the clusters in this hierarchy are labelled using keywords supplied with the dataset allowing insight into the clusters semantics
in this paper we briefly describe 3 tools developed to visualize the history of information_visualization papers the visualization consists of a standard 3d scatterplot view enhanced with "bubbles" lines text and colors aimed at making comparisons between authors and topics found in the papers three components were developed to translate and display raw xml data using opengl and cocoa we use the visualization tool to perform five tasks and discuss its weaknesses
in this paper we describe the process and result of creating a visualization to capture the past 10 years of history in the field of information_visualization as part of the annual infovis conference contest we began with an xml file containing data provided by the contest organizers scrubbed and augmented the data and created a database to hold the information we designed a visualization and implemented it using flash mx 2004 professional with actionscript 20 php and postgresql the resulting visualization provides an overview of the field of information_visualization and allows users to see the connections between areas of the field particular researchers and documents
we show the structure of the infovis publications dataset using tulip a scalable open-source visualization system for graphs and trees tulip supports interactive navigation and many options for layout subgraphs of the full dataset can be created interactively or using a wide set of algorithms based on graph theory and combinatorics including several kinds of clustering we found that convolution clustering and small world clustering were particularly effective at showing the structure of the infovis publications dataset as was coloring by the strahler metric
an overview of the entry is given the techniques used to prepare the infovis contest entry are outlined the strengths and weaknesses are briefly discussed
summary form only given the human_visual_system is the result of evolution by natural selection and hence its design must incorporate detailed knowledge of the physical properties of the natural environment this is an obvious statement but the scientific community has been slow to take it seriously only recently has there been an increased effort to directly measure the statistical properties of natural scenes and compare them to the design and performance of the human_visual_system this work describes some recent studies of the chromatic and geometrical properties of natural materials and natural images as well as some perceptual and physiological studies designed to test how those physical properties are related to human perceptual mechanisms
summary form only given a self-illustrating phenomenon is an image which exposes the science behind it some famous examples are pictures of iron filings aligned along magnetic lines of force sand particles collecting at the stationary points of the standing waves of a violin stress in a mechanical part revealed through birefringence and particle tracks in a bubble chamber such images brilliantly combine experimental design analysis and visualization quoting j tukey "the general purposes of conducting experiments and analyzing data match point by point" we argue in this talk that computer tools for visual_analysis should normally be conceived of as aids in constructing computational visual experiments and that the resulting visualizations be consciously designed to help validate or invalidate the hypothesis being tested by the experiment
resampling is a frequent task in visualization and medical imaging it occurs whenever images or volumes are magnified rotated translated or warped resampling is also an integral procedure in the registration of multimodal datasets such as ct pet and mri in the correction of motion artifacts in mri and in the alignment of temporal volume sequences in fmri it is well known that the quality of the resampling result depends heavily on the quality of the interpolation filter used however high-quality filters are rarely employed in practice due to their large spatial extents we explore a new resampling technique that operates in the frequency-domain where high-quality filtering is feasible further unlike previous methods of this kind our technique is not limited to integer-ratio scalingfactors but can resample image and volume datasets at any rate this would usually require the application of slow discrete fourier_transforms dft to return the data to the spatial domain we studied two methods that successfully avoid these delays the chirp-z transform and the fftw package we also outline techniques to avoid the ringing artifacts that may occur with frequency-domain filtering thus our method can achieve high-quality interpolation at speeds that are usually associated with spatial filters of far lower quality
we derive piecewise linear and piecewise cubic box spline reconstruction_filters for data sampled on the body centered cubic bcc lattice we analytically derive a time domain representation of these reconstruction_filters and using the fourier slice-projection theorem we derive their frequency responses the quality of these filters when used in reconstructing bcc sampled volumetric_data is discussed and is demonstrated with a raycaster moreover to demonstrate the superiority of the bcc sampling the resulting reconstructions are compared with those produced from similar filters applied to data sampled on the cartesian lattice
we present a space_leaping technique for accelerating volume_rendering with very low space and run-time complexity our technique exploits the ray_coherence during ray casting by using the distance a ray traverses in empty space to leap its neighboring rays our technique works with parallel as well as perspective volume_rendering does not require any preprocessing or 3d data structures and is independent of the transfer_function being an image-space technique it is independent of the complexity of the data being rendered it can be used to accelerate both time-coherent and noncoherent animation sequences
hardware-accelerated direct_volume_rendering of unstructured volumetric meshes is often based on tetrahedral cell_projection in particular the projected_tetrahedra pt algorithm and its variants unfortunately evenimplementations of the most advanced variants of the pt algorithm are very prone to rendering artifacts in this work we identify linear_interpolation in screen coordinates as a cause for significant rendering artifacts and implement the correct perspective interpolation for the pt algorithm with programmable_graphics_hardware we also demonstrate how to use features of modern graphics_hardware to improve theaccuracy of the coloring of individual tetrahedra and the compositing of the resulting colors in particular by employing a logarithmic scale for the preintegrated color lookup table using textures with high color resolution rendering to floating-point color buffers and alpha dithering combined with a correct visibility ordering these techniques result in the firstimplementation of the pt algorithm without objectionable rendering artifacts apart from the important improvement in rendering quality our approach also provides a test bed for differentimplementations of the pt algorithm that allows us to study the particular rendering artifacts introduced by these variants
we present a novel multiscale approach for flow_visualization we define a local alignment tensor that encodes a measure for alignment to the direction of a given flow_field this tensor induces an anisotropic differential operator on the flow domain which is discretized with a standard finite_element technique the entries of the corresponding stiffness matrix represent the anisotropically weighted couplings of adjacent nodes of the domain mesh we use an algebraic multigrid algorithm to generate a hierarchy of fine to coarse descriptions for the above coupling data this hierarchy comprises a set of coarse grid nodes a multiscale of basis_functions and their corresponding supports we use these supports to obtain a multilevel decomposition of the flow structure standard streamline icons are used to visualize this decomposition at any user-selected level of detail the method provides a single framework for vector field decomposition independent on the domain dimension or mesh type applications are shown in 2d for flow_fields on curved surfaces and for 3d volumetric flow_fields
a new method for the simplification and the visualization of vector_fields is presented based on the notion of centroidal_voronoi_tessellations cvt's a cvt is a special voronoi tessellation for which the generators of the voronoi regions in the tessellation are also the centers of mass or means with respect to a prescribed density a distance function in both the spatial and vector spaces is introduced to measure the similarity of the spatially distributed vector_fields based on such a distance vector_fields are naturally clustered and their simplified representations are obtained our method combines simple geometric intuitions with the rigorously established optimality properties of the cvts it is simple to describe easy to understand and implement numerical examples are also provided to illustrate the effectiveness and competitiveness of the cvt-based vector simplification and visualization methodology
we investigate two important common fluid flow patterns from computational_fluid_dynamics_(cfd) simulations namely swirl and tumble motion typical of automotive engines we study and visualize swirl and tumble_flow using three different flow_visualization_techniques direct geometric and texture-based when illustrating these methods side-by-side we describe the relative strengths and weaknesses of each approach within a specific spatial dimension and across multiple spatial dimensions typical of an engineer's analysis our study is focused on steady-state flow based on this investigation we offer perspectives on where and when these techniques are best applied in order to visualize the behavior of swirl and tumble motion
the continuing advancement of plasma science is central to realizing fusion as an inexpensive and safe energy source gryokinetic simulations of plasmas are fundamental to the understanding of turbulent transport in fusion plasma this work discusses the visualization challenges presented by gyrokinetic simulations using magnetic field line following coordinates and presents an effective solution exploiting programmable_graphics_hardware to enable interactive_volume_visualization of 3d plasma flow on a toroidal coordinate system the new visualization capability can help scientists better understand three-dimensional structures of the modeled phenomena both the limitations and future promise of the hardware-accelerated approach are also discussed
we present a hardware-accelerated adaptive ewa elliptical weighted average volume_splatting algorithm ewa splatting combines a gaussian reconstruction kernel with a low-pass image filter for high image quality without aliasing artifacts or excessive blurring we introduce a novel adaptive filtering scheme to reduce the computational cost of ewa splatting we show how this algorithm can be efficiently implemented on modern graphics processing units gpus ourimplementation includes interactive classification and fast lighting to accelerate the rendering we store splat geometry and 3d volume data locally in gpu memory we present results for several rectilinear volume datasets that demonstrate the high image quality and interactive rendering speed of our method
a common deficiency of discretized datasets is that detail beyond the resolution of the dataset has been irrecoverably lost this lack of detail becomes immediately apparent once one attempts to zoom into the dataset and only recovers blur we describe a method that generates the missing detail from any available and plausible high-resolution data using texture_synthesis since the detail generation process is guided by the underlying image or volume data and is designed to fill in plausible detail in accordance with the coarse structure and properties of the zoomed-in neighborhood we refer to our method as constrained texture_synthesis regular zooms become "semantic_zooms" where each level of detail stems from a data source attuned to that resolution we demonstrate our approach by a medical application - the visualization of a human liver - but its principles readily apply to any scenario as long as data at all resolutions are available we first present a 2d viewing application called the "virtual microscope" and then extend our technique to 3d volumetric viewing
determining the three-dimensional structure of distant astronomical objects is a challenging task given that terrestrial observations provide only one viewpoint for this task bipolar planetary_nebulae are interesting objects of study because of their pronounced axial symmetry due to fundamental physical processes making use of this symmetry constraint we present a technique to automatically recover the axisymmetric structure of bipolar planetary_nebulae from two-dimensional images with gpu-based volume_rendering driving a nonlinear_optimization we estimate the nebula's local emission density as a function of its radial and axial coordinates and we recover the orientation of the nebula relative to earth the optimization refines the nebula model and its orientation by minimizing the differences between the rendered image and the original astronomical image the resulting model enables realistic 3d_visualizations of planetary_nebulae eg for educational purposes in planetarium shows in addition the recovered spatial distribution of the emissive gas allows validating computer simulation results of the astrophysical formation processes of planetary_nebulae
most data used in the study of seafloor hydrothermal plumes consists of sonar acoustic scans and sensor readings visual data captures only a portion of the sonar data range due to the prohibitive cost and physical infeasibility of taking sufficient lighting and video equipment to such extreme depths however visual images are available from research dives and from the recent imax movie volcanoes of the deep sea in this application paper we apply existing lighting models with forward scattering and light attenuation to the 3d sonar data in order to mimic the visual images available these generated images are compared to existing visual images this can help the geoscientists understand the relationship between these different data modalities and elucidate some of the mechanisms used to capture the data
traditional flow volumes construct an explicit geometrical or parametrical representation from the vector field the geometry is updated interactively and then rendered using an unstructured volume_rendering technique unless a detailed refinement of the flow volume is specified for the interior information inside the underlying flow volume is lost in the linear_interpolation these disadvantages can be avoided and/or alleviated using an implicit flow model an implicit flow is a scalar field constructed such that any point in the field is associated with a termination surface using an advection operator on the flow we present two techniques a slice-based three-dimensional texture_mapping and an interval_volume_segmentation coupled with a tetrahedron projection-based renderer to render implicit_stream_flows in the first method the implicit flow representation is loaded as a 3d texture and manipulated using a dynamic texture operation that allows the flow to be investigated interactively in our second method a geometric_flow volume is extracted from the implicit flow using a high dimensional isocontouring or interval_volume routine this provides a very detailed flow volume or set of flow volumes that can easily change topology while retaining accurate characteristics within the flow volume the advantages and disadvantages of these two techniques are compared with traditional explicit flow volumes
many large scale physics-based simulations which take place on pc clusters or supercomputers produce huge amounts of data including vector_fields while these vector data such as electromagnetic fields fluid flow_fields or particle paths can be represented by lines the sheer number of the lines overwhelms the memory and computation capability of a high-end pc used for visualization further very dense or intertwined lines rendered with traditional visualization_techniques can produce unintelligible results with unclear depth relationships between the lines and no sense of global structure our approach is to apply a lighting model to the lines and sample them into an anisotropic voxel representation based on spherical_harmonics as a preprocessing step then we evaluate and render these voxels for a given view using traditional volume_rendering for extremely large line based datasets conversion to anisotropic voxels reduces the overall storage and rendering for on lines to o1 with a large constant that is still small enough to allow meaningful visualization of the entire dataset at nearly interactive rates on a single commodity pc
effective visualization of vector_fields relies on the ability to control the size and density of the underlying mapping to visual_cues used to represent the field in this paper we introduce the use of a reaction-diffusion model already well known for its ability to form irregular spatio-temporal patters to control the size density and placement of the vector field representation we demonstrate that it is possible to encode vector field information orientation and magnitude into the parameters governing a reaction-diffusion model to form a spot pattern with the correct orientation size and density creating an effective visualization to encode direction we texture the spots using a light to dark fading texture we also show that it is possible to use the reaction-diffusion model to visualize an additional scalar value such as the uncertainty in the orientation of the vector field an additional benefit of the reaction-diffusion visualization_technique arises from its automatic density distribution this benefit suggests using the technique to augment other vector visualization_techniques we demonstrate this utility by augmenting a lic visualization with a reaction-diffusion visualization finally the reaction-diffusion visualization method provides a technique that can be used for streamline and glyph placement
the physical interpretation of mathematical features of tensor_fields is highly application-specific existing visualization methods for tensor_fields only cover a fraction of the broad application areas we present a visualization method tailored specifically to the class of tensor_field exhibiting properties similar to stress and strain_tensors which are commonly encountered in geomechanics our technique is a global method that represents the physical meaning of these tensor_fields with their central features regions of compression or expansion the method is based on two steps first we define a positive definite metric with the same topological structure as the tensor_field second we visualize the resulting metric the eigenvector_fields are represented using a texture-based approach resembling line_integral_convolution lic methods the eigenvalues of the metric are encoded in free parameters of the texture definition our method supports an intuitive distinction between positive and negative eigenvalues we have applied our method to synthetic and some standard data sets and "real" data from earth science and mechanical engineering application
we present a novel approach for interactive view-dependent rendering of massive models our algorithm combines view-dependent simplification occlusion_culling and out-of-core rendering we represent the model as a clustered hierarchy of progressive meshes chpm we use the cluster hierarchy for coarse-grained selective refinement and progressive meshes for fine-grained local refinement we present an out-of-core_algorithm for computation of a chpm that includes cluster decomposition hierarchy generation and simplification we make use of novel cluster dependencies in preprocess to generate crack-free drastic simplifications at runtime the clusters are used for occlusion_culling and out-of-core rendering we add a frame of latency to the rendering pipeline to fetch newly visible clusters from the disk and to avoid stalls the chpm reduces the refinement cost for view-dependent rendering by more than an order of magnitude as compared to a vertex hierarchy we have implemented our algorithm on a desktop pc we can render massive cad isosurface and scanned models consisting of tens or a few hundreds of millions of triangles at 10-35 frames per second with little loss in image quality
this work introduces importance-driven volume_rendering as a novel technique for automatic focus_and_context display of volumetric_data our technique is a generalization of cut-away views which - depending on the viewpoint - remove or suppress less important parts of a scene to reveal more important underlying information we automatize and apply this idea to volumetric_data each part of the volumetric_data is assigned an object importance which encodes visibility priority this property determines which structures should be readily discernible and which structures are less important in those image regions where an object occludes more important structures it is displayed more sparsely than in those areas where no occlusion occurs thus the objects of interest are clearly visible for each object several representations ie levels of sparseness are specified the display of an individual object may incorporate different levels of sparseness the goal is to emphasize important structures and to maximize the information content in the final image this work also discusses several possible schemes for level of sparseness specification and different ways how object importance can be composited to determine the final appearance of a particular object
typically there is a high coherence in data values between neighboring time steps in an iterative scientific software simulation this characteristic similarly contributes to a corresponding coherence in the visibility of volume blocks when these consecutive time steps are rendered yet traditional visibility_culling algorithms were mainly designed for static data without consideration of such potential temporal coherency we explore the use of temporal occlusion coherence toc to accelerate visibility_culling for time-varying volume_rendering in our algorithm the opacity of volume blocks is encoded by means of plenoptic_opacity_functions pofs a coherence-based block fusion technique is employed to coalesce time-coherent data blocks over a span of time steps into a single representative block then pofs need only be computed for these representative blocks to quickly determine the subvolumes that do not require updates in their visibility status for each subsequent time step a hierarchical "toc tree" data structure is constructed to store the spans of coherent time steps to achieve maximal culling potential while remaining conservative we have extended our previous pop into an optimized pop opop encoding scheme for this specific scenario to test our general toc and opof approach we have designed a parallel time-varying volume_rendering algorithm accelerated by visibility_culling results from experimental runs on a 32-processor cluster confirm both the effectiveness and scalability of our approach
grid computing provides a challenge for visualization system designers in this research we evolve the dataflow concept to allow parts of the visualization process to be executed remotely in a secure and seamless manner we see dataflow at three levels an abstract specification of the intent of the visualization a binding of these abstract modules to a specific software system and then a binding of software to processing and other resources we develop an xml application capable of describing visualization at the three levels to complement this we have implemented an extension to a popular visualization system iris explorer which allows modules in a dataflow pipeline to run on a set of grid resources for computational_steering applications we have developed a library that allows a visualization system front-end to connect to a simulation running remotely on a grid resource we demonstrate the work in two applications the dispersion of a pollutant under different wind conditions and the solution of a challenging numerical problem in elastohydrodynamic lubrication
we present a system for enhancing observation of user_interactions in virtual environments in particular we focus on analyzing behavior patterns in the popular team-based first-person perspective game return to castle wolfenstein enemy territory this game belongs to a genre characterized by two moderate-sized teams usually 6 to 12 players each competing over a set of objectives our system allows spectators to visualize global features such as large-scale behaviors and team strategies as opposed to the limited local view that traditional spectating modes provide we also add overlay visualizations of semantic information related to the action that might be important to a spectator in order to reduce the information overload that plagues traditional overview visualizations these overlays can visualize information about abstract concepts such as player distribution over time and areas of intense combat activity and also highlight important features like player paths fire coverage etc this added information allows spectators to identify important game events more easily and reveals large-scale player behaviors that might otherwise be overlooked
quantitative techniques for visualization are critical to the successful analysis of both acquired and simulated scientific data many visualization_techniques rely on indirect mappings such as transfer_functions to produce the final imagery in many situations it is preferable and more powerful to express these mappings as mathematical expressions or queries that can then be directly applied to the data we present a hardware-accelerated system that provides such capabilities and exploits current graphics_hardware for portions of the computational tasks that would otherwise be executed on the cpu in our approach the direct programming of the graphics processor using a concise data parallel language gives scientists the capability to efficiently explore and visualize data sets
vorticity is the quantity used to describe the creation transformation and extinction of vortices it is present not only in vortices but also in shear flow especially in ducted flows most of the overall vorticity is usually contained in the boundary layer when a vortex develops from the boundary layer this can be described by transport of vorticity for a better understanding of a flow it is therefore of interest to examine vorticity in all of its different roles the goal of this application study was not primarily the visualization of vortices but of vorticity distribution and its role in vortex phenomena the underlying industrial case is a design optimization for a pelton turbine an important industrial objective is to improve the quality of the water jets driving the runner jet quality is affected mostly by vortices originating in the distributor ring for a better understanding of this interrelation it is crucial to not only visualize these vortices but also to analyze the mechanisms of their creation we used various techniques for the visualization of vorticity including field_lines and modified isosurfaces for field line based visualization we extended the image-guided streamline_placement algorithm of turk and banks to data-guided field line_placement on three-dimensional unstructured_grids
vortex_breakdowns and flow recirculation are essential phenomena in aeronautics where they appear as a limiting factor in the design of modern aircrafts because of the inherent intricacy of these features standard flow_visualization_techniques typically yield cluttered depictions the paper addresses the challenges raised by the visual_exploration and validation of two cfd simulations involving vortex_breakdown to permit accurate and insightful visualization we propose a new approach that unfolds the geometry of the breakdown region by letting a plane travel through the structure along a curve we track the continuous evolution of the associated projected vector field using the theoretical framework of parametric topology to improve the understanding of the spatial relationship between the resulting curves and lines we use direct_volume_rendering and multidimensional transfer_functions for the display of flow-derived scalar quantities this enriches the visualization and provides an intuitive context for the extracted topological information our results offer clear synthetic depictions that permit new insight into the structural properties of vortex_breakdowns
feature_detection in flow_fields is a well-researched area but practical application is often difficult due to the numerical complexity of the algorithms preventing interactive use and due to noise in experimental or high-resolution simulation data sets we present an integrated system that provides interactive denoising vortex_detection and visualisation of vector data on cartesian grids all three major phases are implemented in such a way that the system runs completely on a modern gpu once the vector field is downloaded into graphics memory the application aspect of our paper is twofold first we show how recently presented prototypical gpu-based algorithms for filtering numerical computation and volume_rendering can be combined into one productive system by handling all idiosyncrasies of a chosen graphics card second we demonstrate that the significant speedup achieved compared to an optimized softwareimplementation now allows interactive_exploration of characteristic structures in turbulent flow_fields
we describe a new technique for fitting scattered point cloud data given a scattered point cloud of 3d data points and associated normal vectors our new method produces an implicit volume model whose zero level isosurface interpolates the given points and associated normal vectors we concentrate on certain application of these new volume modeling techniques we take existing polygon_mesh surfaces and use the present methods to construct implicit volume models for these surfaces implicit models allow for the application of boolean operations on these surfaces through the techniques of constructive_solid_geometry also standard wavelet and filter operators can be applied to the implicit volume model leading to effective smoothing and filtering algorithms which are simple to implement
we describe a general algorithm to produce compatible 3d triangulations from spatial decompositions such triangulations match edges and faces across spatial cell boundaries solving several problems in graphics and visualization including the crack problem found in adaptive isosurface generation triangulation of arbitrary grids including unstructured_grids clipping and the interval tetrahedrization problem the algorithm produces compatible triangulations on a cell-by-cell basis using a modified delaunay_triangulation with a simple point ordering rule to resolve degenerate cases and produce unique triangulations across cell boundaries the algorithm is naturally parallel since it requires no neighborhood cell information only a unique global point numbering we show application of this algorithm to adaptive contour generation tetrahedrization of unstructured_meshes clipping and interval_volume mesh generation
we address the texture_level-of-detail problem for extremely large surfaces such as terrain during realtime view-dependent rendering a novel texture hierarchy is introduced based on 4-8 refinements of raster tiles in which the texture grids in effect rotate 45 degrees for each level of refinement this hierarchy provides twice as many levels of detail as conventional quadtree-style refinement schemes such as mipmaps and thus provides per-pixel view-dependent_filtering that is twice as close to the ideal cutoff frequency for an average pixel because of this more gradual change in low-pass filtering and due to the more precise emulation of the ideal cutoff frequency we find in practice that the transitions between texture levels of detail are not perceptible this allows rendering systems to avoid the complexity and performance costs of per-pixel blending between texture levels of detail the 4-8 texturing scheme is integrated into a variant of the real-time optimally adapting meshes roam algorithm for view-dependent multiresolution mesh generation improvements to roam included here are the diamond data structure as a streamlined replacement for the triangle bintree elements the use of low-pass-filtered geometry patches in place of individual triangles integration of 4-8 textures and a simple out-of-core data access mechanism for texture and geometry tiles
this work presents an experimental immersive interface for designing dna components for application in nanotechnology while much research has been done on immersive visualization this is one of the first systems to apply advanced interface techniques to a scientific design problem this system uses tangible 3d input devices tongs a raygun and a multipurpose handle tool to create and edit a purely digital representation of dna the tangible controllers are associated with functions not data while a virtual display is used to render the model this interface was built in collaboration with a research group investigating the design of dna tiles a user_study shows that scientists find the immersive interface more satisfying than a 2d interface due to the enhanced understanding gained by directly interacting with molecules in 3d space
the evolving technology of computer auto-fabrication "3d printing" now makes it possible to produce physical models for complex biological molecules and assemblies we report on an application that demonstrates the use of auto-fabricated tangible models and augmented_reality for research and education in molecular biology and for enhancing the scientific environment for collaboration and exploration we have adapted an augmented_reality system to allow virtual 3d representations generated by the python molecular viewer to be overlaid onto a tangible molecular model users can easily change the overlaid information switching between different representations of the molecule displays of molecular properties such as electrostatics or dynamic information the physical model provides a powerful intuitive interface for manipulating the computer models streamlining the interface between human intent the physical model and the computational activity
while molecular_visualization software has advanced over the years today most tools still operate on individual molecular structures with limited facility to manipulate large multicomponent complexes we approach this problem by extending 3d image-based_rendering via programmable graphics units resulting in an order of magnitude speedup over traditional triangle-based rendering by incorporating a biochemically sensitive level-of-detail hierarchy into our molecular representation we communicate appropriate volume occupancy and shape while dramatically reducing the visual_clutter that normally inhibits higher-level spatial_comprehension our hierarchical image based rendering also allows dynamically computed physical properties data eg electrostatics potential to be mapped onto the molecular surface tying molecular structure to molecular function finally we present another approach to interactive molecular exploration using volumetric and structural rendering in tandem to discover molecular properties that neither rendering mode alone could reveal these visualization_techniques are realized in a high-performance interactive molecular exploration tool we call texmol short for texture molecular viewer
we present a new level_set_method for reconstructing interfaces from point aggregations although level-set-based methods are advantageous because they can handle complicated topologies and noisy data most tend to smooth the inherent roughness of the original data our objective is to enhance the quality of a reconstructed surface by preserving certain roughness-related characteristics of the original dataset our formulation employs the total variation of the surface as a roughness measure the algorithm consists of two steps a roughness-capturing flow and a roughness-preserving flow the roughness capturing step attempts to construct a surface for which the original roughness is captured - distance flow is well suited for roughness capturing surface_reconstruction is enhanced by using a total variation preserving tvp scheme for the roughness-preserving flow the shock_filter formulation of osher and rudin is exploited to achieve this goal in practice we have found that better results arc obtained by balancing the tvp term with a smoothing term based on curvature the algorithm is applied to both fractal surface growth simulations and scanned data sets to demonstrate the efficacy of our approach
we present a novel surface_reconstruction algorithm that can recover high-quality surfaces from noisy and defective data sets without any normal or orientation information a set of new techniques is introduced to afford extra noise tolerability robust orientation alignment reliable outlier removal and satisfactory feature recovery in our algorithm sample points are first organized by an octree the points are then clustered into a set of monolithically singly-oriented groups the inside/outside orientation of each group is determined through a robust voting algorithm we locally fit an implicit quadric surface in each octree cell the locally fitted implicit_surfaces are then blended to produce a signed distance_field using the modified shepard's method we develop sophisticated iterative fitting algorithms to afford improved noise tolerance both in topology recognition and geometryaccuracy furthermore this iterative fitting algorithm coupled with a local model_selection scheme provides a reliable sharp feature recovery mechanism even in the presence of bad input
all orientable metric surfaces are riemann surfaces and admit global conformal parameterizations riemann surface structure is a fundamental structure and governs many natural physical phenomena such as heat_diffusion and electro-magnetic fields on the surface a good parameterization is crucial for simulation and visualization this paper provides an explicit method for finding optimal global conformal parameterizations of arbitrary surfaces it relies on certain holomorphic differential_forms and conformal_mappings from differential geometry and riemann surface theories algorithms are developed to modify topology locate zero points and determine cohomology types of differential_forms theimplementation is based on a finite dimensional optimization method the optimal parameterization is intrinsic to the geometry preserves angular structure and can play an important role in various applications including texture_mapping remeshing morphing and simulation the method is demonstrated by visualizing the riemann surface structure of real surfaces represented as triangle meshes
we introduce local and global comparison_measures for a collection of k /spl les/ d real-valued smooth_functions on a common d-dimensional riemannian manifold for k = d = 2 we relate the measures to the set of critical_points of one function restricted to the level_sets of the other the definition of the measures extends to piecewise linear functions for which they are easy to compute the computation of the measures forms the centerpiece of a software tool which we use to study scientific datasets riemannian_manifolds visualization comparison_measure differential_forms smooth_functions time-varying_data
we introduce light collages - a lighting_design system for effective visualization based on principles of human perception artists and illustrators enhance perception of features with lighting that is locally consistent and globally inconsistent inspired by these techniques we design the placement of light sources to convey a greater sense of realism and better perception of shape with globally inconsistent_lighting our algorithm segments the objects into local surface patches and uses a number of perceptual heuristics such as highlights shadows and silhouettes to enhance the perception of shape we show our results on scientific and sculptured datasets
an important task in volume_rendering is the visualization of boundaries between materials this is typically accomplished using transfer_functions that increase opacity based on a voxel's value and gradient lighting also plays a crucial role in illustrating surfaces in this paper we present a multi-dimensional transfer_function method for enhancing surfaces not through the variation of opacity but through the modification of surface shading the technique uses a lighting transfer_function that takes into account the distribution of values along a material boundary and features a novel interface for visualizing and specifying these transfer_functions with our method the user is given a means of visualizing boundaries without modifying opacity allowing opacity to be used for illustrating the thickness of homogeneous materials through the absorption of light
surface texture is among the most salient haptic characteristics of objects it can induce vibratory contact forces that lead to perception of roughness we present a new algorithm to display haptic texture information resulting from the interaction between two textured objects we compute contact forces and torques using low-resolution geometric representations along with texture images that encode surface details we also introduce a novel force model based on directional penetration depth and describe an efficientimplementation on programmable_graphics_hardware that enables interactive haptic texture rendering of complex models our force model takes into account importantfactors identified by psychophysics studies and is able to haptically display interaction due to fine surface textures that previous algorithms do not capture
although luminance contrast plays a predominant role in motion perception significant additional effects are introduced by chromatic contrasts in this paper relevant results from psychophysical and physiological research are described to clarify the role of color in motion_detection interpreting these psychophysical experiments we propose guidelines for the design of animated visualizations and a calibration procedure that improves the reliability of visual motion representation the guidelines are applied to examples from texture-based flow_visualization as well as graph and tree visualisation
visualization of 3d tensor_fields continues to be a major challenge in terms of providing intuitive and uncluttered images that allow the users to better understand their data the primary focus of this paper is on finding a formulation that lends itself to a stable numerical algorithm for extracting stable and persistent topological features from 2nd order real symmetric 3d tensors while features in 2d tensors can be identified as either wedge or trisector points in 3d the corresponding stable features are lines not just points these topological feature lines provide a compact representation of the 3d tensor_field and are essential in helping scientists and engineers understand their complex nature existing techniques work by finding degenerate points and are not numerically stable and worse produce both false positive and false negative feature points this work seeks to address this problem with a robust algorithm that can extract these features in a numerically stable accurate and complete manner
topological methods aim at the segmentation of a vector field into areas of different flow behavior for 2d time-dependent_vector_fields two such segmentations are possible either concerning the behavior of stream lines or of path_lines while stream line oriented topology is well established we introduce path line oriented topology as a new visualization approach in this paper as a contribution to stream line oriented topology we introduce new methods to detect global bifurcations like saddle connections and cyclic fold bifurcations to get the path line oriented topology we segment the vector field into areas of attracting repelling and saddle-like behavior of the path_lines we compare both kinds of topologies and apply them to a number of data sets
we present an approach for monitoring the positions of vector field singularities and related structural changes in time-dependent_datasets the concept of singularity index is discussed and extended from the well-understood planar case to the more intricate three-dimensional setting assuming a tetrahedral grid with linear_interpolation in space and time vector field singularities obey rules imposed by fundamental invariants poincare index which we use as a basis for an efficient tracking algorithm we apply the presented algorithm to cfd datasets to illustrate its purpose we examine structures that exhibit topological variations with time and describe some of the insight gained with our method examples are given that show a correlation in the evolution of physical quantities that play a role in vortex_breakdown
an ideal visualization tool that has not been used before in studying the optical behavior of near-field apertures is three-dimensional vector_field_topology the global view of the vector field structure is deduced by locating singularities critical_points within the field and augmenting these points with nearby streamlines we have used for the first time to the best of our knowledge three-dimensional topology to analyze the topological differences between a resonant c-shaped nano-aperture and various nonresonant conventional apertures the topological differences between these apertures are related to the superiority in power throughput of the c-aperture versus conventional round and square sub-wavelength apertures we demonstrate how topological visualization_techniques provide significant insight into the energy enhancement mechanism of the c aperture and also shed light on critical issues related to the interaction between multiple apertures located in close proximity to each other which gives rise to cross-talk for example as a function of distance topological techniques allow us to develop design rules for the geometry of these apertures and their desired spot sizes and brightness the performance of various sub-wavelength apertures can also be compared quantitatively based on their topology since topological methods are generically applicable to tensor and vector_fields our approach can be readily extended to provide insight into the broader category of finite-difference-time-domain nano-photonics and nano-science problems
datasets of tens of gigabytes are becoming common in computational and experimental science this development is driven by advances in imaging technology producing detectors with growing resolutions as well as availability of cheap processing power and memory capacity in commodity-based computing clusters we describe the design of a visualization system that allows scientists to interactively explore large remote data sets in an efficient and flexible way the system is broadly applicable and currently used by medical scientists conducting an osteoporosis research project human vertebral bodies are scanned using a high resolution microct scanner producing scans of roughly 8 gb size each all participating research groups require access to the centrally stored data due to the rich internal bone structure scientists need to interactively explore the full dataset at coarse levels as well as visualize subvolumes of interest at the highest resolution our solution is based on hdf5 and gridftp when accessing data remotely the hdf5 data processing pipeline is modified to support efficient retrieval of subvolumes we reduce the overall latency and optimize throughput by executing high-level operations on the remote side the gridftp protocol is used to pass the hdf5 requests to a customized server the approach takes full advantage of local graphics_hardware for rendering interactive_visualization is accomplished using a background thread to access the datasets stored in a multiresolution format a hierarchical volume tenderer provides seamless integration of high resolution details with low resolution overviews
this work describes the methods used to produce an interactive_visualization of a 2 tb computational_fluid_dynamics_(cfd) data set using particle_tracing streaklines we use the method introduced by bruckschen el al 2001 that precomputes a large number of particles stores them on disk using a space-filling curve ordering that minimizes seeks then retrieves and displays the particles according to the user's command we describe how the particle computation can be performed using a pc cluster how the algorithm can be adapted to work with a multiblock curvilinear mesh how scalars can be extracted and used to color the particles and how the out-of-core visualization can be scaled to 293 billion particles while still achieving interactive performance on pc_hardware compared to the earlier work our data set size and total number of particles are an order of magnitude larger we also describe a new compression technique that losslessly reduces the amount of particle storage by 41% and speeds the particle retrieval by about 20%
virtual_prototyping is increasingly replacing real mock-ups and experiments in industrial product development part of this process is the simulation of structural and functional properties which is in many cases based on finite_element analysis fea one prominent example from the automotive industry is the safety improvement resulting from crash worthiness simulations a simulation model for this purpose usually consists of up to one million finite_elements and is assembled from many parts which are individually meshed out of their cad representation in order to accelerate the development cycle simulation engineers want to be able to modify their fe models without going back to the cad department furthermore valid cad models might even not be available in preliminary design stages however in contrast to cad there is a lack of tools that offer the possibility of modification and processing of finite_element components while maintaining the properties relevant to the simulation in this application paper we present interactive algorithms for intuitive and fast editing of fe models and appropriate visualization_techniques to support engineers in understating these models this includes new kinds of manipulators feedback mechanisms and facilities for virtual_reality and immersion at the workplace eg autostereoscopic displays and haptic devices
an important challenge encountered during post-processing of finite_element analyses is the visualizing of three-dimensional fields of real-valued second-order tensors namely as finite_element meshes become more complex and detailed evaluation and presentation of the principal stresses becomes correspondingly problematic in this paper we describe techniques used to visualize simulations of perturbed in-situ stress fields associated with hypothetical salt bodies in the gulf of mexico we present an adaptation of the mohr diagram a graphical paper and pencil method used by the material mechanics community for estimating coordinate transformations for stress_tensors as a new tensor glyph for dynamically exploring tensor variables within three-dimensional finite_element models this interactive glyph can be used as either a probe or a filter through brushing_and_linking
diffusion_tensor_imaging dti is a magnetic_resonance_imaging method that can be used to measure local information about the structure of white_matter within the human brain combining dti data with the computational methods of mr_tractography neuroscientists can estimate the locations and sizes of nerve bundles white_matter pathways that course through the human brain neuroscientists have used visualization_techniques to better understand tractography data but they often struggle with the abundance and complexity of the pathways we describe a novel set of interaction techniques that make it easier to explore and interpret such pathways specifically our application allows neuroscientists to place and interactively manipulate box-shaped regions or volumes of interest to selectively display pathways that pass through specific anatomical areas a simple and flexible query language allows for arbitrary combinations of these queries using boolean logic operators queries can be further restricted by numerical path properties such as length mean fractional anisotropy and mean curvature by precomputing the pathways and their statistical properties we obtain the speed necessary for interactive question-and-answer sessions with brain researchers we survey some questions that researchers have been asking about tractography data and show how our system can be used to answer these questions efficiently
accurate and reliable visualization of blood_vessels is still a challenging problem notably in the presence of morphologic changes resulting from atherosclerotic diseases we take advantage of partially segmented_data with approximately identified vessel centerlines to comprehensively visualize the diseased peripheral arterial tree we introduce the vesselglyph as an abstract notation for novel focus & context_visualization_techniques of tubular structures such as contrast-medium enhanced arteries in ct-angiography cta the proposed techniques combine direct_volume_rendering dvr and curved_planar_reformation cpr within a single image the vesselglyph consists of several regions where different rendering methods are used the region type the used visualization method and the region parameters depend on the distance from the vessel centerline and on viewing parameters as well by selecting proper rendering techniques for different regions vessels are depicted in a naturally looking and undistorted anatomic context this may facilitate the diagnosis and treatment_planning of patients with peripheral arterial occlusive disease in this paper we furthermore present a way of how to implement the proposed techniques in software and by means of modern 3d graphics accelerators
accurate estimation of vessel parameters is a prerequisite for automated visualization and analysis of healthy and diseased blood_vessels the objective of this research is to estimate the dimensions of lower extremity arteries imaged by computed_tomography ct these parameters are required to get a good quality visualization of healthy as well as diseased arteries using a visualization_technique such as curved_planar_reformation cpr the vessel is modeled using an elliptical or cylindrical structure with specific dimensions orientation and blood vessel mean density the model separates two homogeneous regions its inner side represents a region of density for vessels and its outer side a region for background taking into account the point spread function psf of a ct scanner a function is modeled with a gaussian kernel in order to smooth the vessel boundary in the model a new strategy for vessel parameter estimation is presented it stems from vessel model and model parameter optimization by a nonlinear_optimization procedure ie the levenberg-marquardt technique the method provides center location diameter and orientation of the vessel as well as blood and background mean density values the method is tested on synthetic_data and real patient data with encouraging results
waves are a fundamental mechanism for conveying information in many physical problems direct visualization_techniques are often used to display wave fronts however the information derived from such visualizations may not be as central to an investigation as an understanding of how the location structure and time course of the wave change as key experimental parameters are varied in experimental data these questions are confounded by noise and incomplete data recognition of waves in networks of neurons is additionally complicated by the presence of long-range physical connections and recurrent excitation this work applies visual techniques to analyze the structural details of waves in response data from the turtle visual cortex we emphasize low-cost visualizations that allow comparisons across neural data sets and variables to reconstruct the choreography for a complex response
coloring higher order scientific data is problematic using standard linear methods as found in opengl the visual results are inaccurate when there is a large scalar gradient over an element or when the scalar field is nonlinear in addition to shading nonlinear data last and accurate rendering of planar cuts through parametric elements can be implemented using programmable_shaders on current graphics_hardware the intersection of a planar cut with geometrically curved volume elements can be rendered using a combination of selective refinement and programmable_shaders this hybrid algorithm also handles curved 2d planar triangles
a new multiple resolution volume_rendering method for finite_element analysis fea data is presented our method is composed of three stages in the first stage the gauss points of the fea cells are calculated the function values gradients diffusions and influence scopes of the gauss points are computed by representing the gauss points as graph vertices and connecting adjacent gauss points with edges an adjacency graph is created the adjacency graph is used to represent the fea data in the subsequent computation in the second stage a hierarchical structure is established upon the adjacency graph any two neighboring vertices with similar function values are merged into a new vertex the similarity is measured by using a user-defined threshold consequently a new adjacency graph is constructed then the threshold is increased and the graph reduction is triggered again to generate another adjacency graph by repeating the processing multiple adjacency graphs are computed and a level of detail lod representation of the fea data is established in the third stage the lod structure is rendered by using a splatting_method at first a level of adjacency graph is selected by users the graph vertices arc sorted based on their visibility orders and projected onto the image plane in back-to-front order billboards are used to render the vertices in the projection the function values gradients and influence scopes of the vertices are utilized to decide the colors opacities orientations and shapes of the billboards the billboards are then modulated with texture maps to generate the footprints of the vertices finally these footprints are composited to produce the volume_rendering image
computational simulation of time-varying physical processes is of fundamental importance for many scientific and engineering applications most frequently time-varying simulations are performed over multiple spatial grids at discrete points in time we investigate a new approach to time-varying simulation spacetime discontinuous galerkin finite_element_methods the result of this simulation method is a simplicial tessellation of spacetime with per-element polynomial solutions for physical quantities such as strain stress and velocity to provide accurate visualizations of the resulting solutions we have developed a method for per-pixel evaluation of solution data on the gpu we demonstrate the importance of per-pixel rendering versus simple linear_interpolation for producing high_quality visualizations we also show that our system can accommodate reasonably large_datasets - spacetime meshes containing up to 20 million tetrahedra are not uncommon in this domain
we present a novel approach to interactive_visualization and exploration of large unstructured tetrahedral_meshes these massive 3d meshes are used in mission-critical cfd and structural mechanics simulations and typically sample multiple field values on several millions of unstructured grid points our method relies on the preprocessing of the tetrahedral mesh to partition it into nonconvex boundaries and internal fragments that are subsequently encoded into compressed multiresolution data representations these compact hierarchical_data structures are then adaptively rendered and probed in real-time on a commodity pc our point-based_rendering algorithm which is inspired by qsplat employs a simple but highly efficient splatting technique that guarantees interactive frame-rates regardless of the size of the input mesh and the available rendering hardware it furthermore allows for real-time probing of the volumetric_data-set through constructive_solid_geometry operations as well as interactive editing of color_transfer_functions for an arbitrary number of field values thus the presented visualization_technique allows end-users for the first time to interactively render and explore very large unstructured tetrahedral_meshes on relatively inexpensive hardware
we introduce a novel span-triangle data structure based on the span-space representation for isosurfaces it stores all necessary cell information for dynamic manipulation of the isovalue in an efficient way we have found that using our data structure in combination with point-based techniques implemented on graphics_hardware effects in real-time rendering and exploration our extraction algorithm utilizes an incremental and progressive update scheme enabling smooth interaction without significant latency moreover the corresponding visualization pipeline is capable of processing large_data sets by utilizing all three levels of memory disk system and graphics we address practical usability in actual medical applications achieving a new level of interactivity
we propose an interpolating refinement method for two- and three-dimensional scalar_fields defined on hexahedral grids iterative fairing of the underlying contours isosurfaces provides the function values of new grid points our method can be considered as a nonlinear variational subdivision scheme for volumes it can be applied locally for adaptive_mesh_refinement in regions of high geometric complexity we use our scheme to increase the quality of low-resolution data sets and to reduce interpolation artifacts in texture-based volume_rendering
we propose a novel point-based approach to view dependent isosurface_extraction we introduce a fast visibility query system for the view dependent traversal which exhibits moderate memory requirements this technique allows for an interactive interrogation of the full visible woman dataset 1gb at four to fifteen frames per second on a desktop computer the point-based approach is built on an extraction scheme that classifies different sections of the isosurface into four categories depending on the size of the geometry when projected onto the screen in particular we use points to represent small and subpixel triangles as well as larger sections of the isosurface whose projection has subpixel size to assign consistent and robust normals to individual points representing such regions we propose to compute them during post processing of the extracted isosurface and provide the corresponding hardwareimplementation
we explore techniques to detect and visualize features in data from molecular_dynamics md simulations although the techniques proposed are general we focus on silicon si atomic systems the first set of methods use 3d location of atoms defects are detected and categorized using local operators and statistical modeling our second set of exploratory techniques employ electron density data this data is visualized to glean the defects we describe techniques to automatically detect the salient isovalues for isosurface_extraction and designing transfer_functions we compare and contrast the results obtained from both sources of data essentially we find that the methods of defect feature_detection are at least as robust as those based on the exploration of electron density for si systems
new high-throughput proteomic techniques generate data faster than biologists can analyze it hidden within this massive and complex data are answers to basic questions about how cells function the data afford an opportunity to take a global or systems approach studying whole proteomes comprising all the proteins in an organism however the tremendous size and complexity of the high-throughput data make it difficult to process and interpret existing tools for studying a few proteins at a time are not suitable for global analysis visualization provides powerful analysis capabilities for enormous complex data at multiple resolutions we developed a novel interactive_visualization tool pquad for the visual_analysis of proteins and peptides identified from high-throughput data on biological samples pquad depicts the peptides in the context of their source protein and dna thereby integrating proteomic and genomic information a wrapped line metaphor is applied across key resolutions of the data from a compressed view of an entire chromosome to the actual nucleotide sequence pquad provides a difference_visualization for comparing peptides from samples prepared under different experimental conditions we describe the requirements for such a visual_analysis tool the design decisions and the novel aspects of pquad
we present an efficient algorithm to mesh the macromolecules surface model represented by the skin surface defined by edelsbrunner our algorithm overcomes several challenges residing in current surface meshing methods first we guarantee the mesh quality with a provable lower bound of 21° on its minimum angle second we ensure the triangulation is homeomorphic to the original surface third we improve the efficiency of constructing the restricted delaunay_triangulation rdt of smooth_surfaces we achieve this by constructing the rdt using the advancing_front method without computing the delaunay tetrahedrization of the sample points on the surfaces the difficulty of handling the front collision problem is tackled by employing the morse_theory in particular we construct the morse-smale_complex to simplify the topological changes of the front ourimplementation results suggest that the algorithm decrease the time of generating high_quality homeomorphic skin mesh from hours to a few minutes
we present the definition and computational algorithms for a new class of surfaces which are dual to the isosurface produced by the widely used marching_cubes mc algorithm these new isosurfaces have the same separating properties as the mc surfaces but they are comprised of quad patches that tend to eliminate the common negative aspect of poorly shaped triangles of the mc isosurfaces based upon the concept of this new dual operator we describe a simple but rather effective iterative scheme for producing smooth separating_surfaces for binary enumerated volumes which are often produced by segmentation algorithms both the dual surface algorithm and the iterative smoothing scheme are easily implemented
the contour_tree an abstraction of a scalar field that encodes the nesting relationships of isosurfaces can be used to accelerate isosurface_extraction to identify important isovalues for volume-rendering transfer_functions and to guide exploratory_visualization through a flexible isosurface interface many real-world data sets produce unmanageably large contour_trees which require meaningful simplification we define local geometric measures for individual contours such as surface area and contained volume and provide an algorithm to compute these measures in a contour_tree we then use these geometric measures to simplify the contour_trees suppressing minor topological features of the data we combine this with a flexible isosurface interface to allow users to explore individual contours of a dataset interactively
we present a fast topology-preserving approach for isosurface simplification the underlying concept behind our approach is to preserve the disconnected surface components in cells during isosurface simplification we represent isosurface components in a novel representation called enhanced cell where each surface component in a cell is represented by a vertex and its connectivity information a topology-preserving vertex_clustering algorithm is applied to build a vertex octree an enhanced dual contouring algorithm is applied to extract error-bounded multiresolution isosurfaces from the vertex octree while preserving the finest resolution isosurface topology cells containing multiple vertices are properly handled during contouring our approach demonstrates better results than existing octree-based simplification techniques
endonasal transsphenoidal pituitary_surgery is a minimally invasive endoscopic procedure applied to remove various kinds of pituitary tumors to reduce the risk associated with this treatment the surgeon must be skilled and well-prepared virtual_endoscopy can be beneficial as a tool for training preoperative planning and intraoperative support this work introduces steps a virtual_endoscopy system designed to aid surgeons in getting acquainted with the endoscopic view the handling of instruments the transsphenoidal approach and challenges associated with the procedure steps also assists experienced surgeons in planning a real endoscopic intervention by getting familiar with the individual patient anatomy identifying landmarks planning the approach and deciding upon the ideal target position of the actual surgical activity besides interactive_visualization using two different first-hit ray casting techniques the application provides navigation and perception aids and the possibility to simulate the procedure including haptic feedback and simulation of surgical instruments
this work describes a method to visualize the thickness of curved thin objects given the mri volume data of articular cartilage medical doctors investigate pathological changes of the thickness since the tissue is very thin it is impossible to reliably map the thickness information by direct_volume_rendering our idea is based on unfolding of such structures preserving their thickness this allows to perform anisotropic geometrical operations eg scaling the thickness however flattening of a curved structure implies a distortion of its surface the distortion problem is alleviated through a focus-and-context minimization approach distortion is smallest close to a focal point which can be interactively selected by the user
imagesurfer is a tool designed to explore correlations between two 3d_scalar_fields our scientific goal was to determine where a protein is located and how much its concentration varies along the membrane of a neuronal dendrite the 3d scalar field data sets fall into two categories dendritic plasma membranes defining the structure and immunofluorescent staining defining protein concentration along the structure imagesurfer enables scientists to analyze relationships between multiple data sets obtained with confocal_microscopy by providing 3d surface view height field and graphing tools each tool reduces the complexity of the problem by extracting a restricted subset of data finding a region of interest in 3d getting a sense of relative concentrations in 2d and getting exact concentration values in 1d the current design is presented along with the rationale for each design decision the tool is already proving useful for data_exploration analysis and presentation
multiperspective images are a useful way to visualize extended roughly planar scenes such as landscapes or city_blocks however constructing effective multiperspective images is something of an art we describe an interactive_system for creating multiperspective images composed of serially blended cross-slits images beginning with a sideways-looking video of the scene as might be captured from a moving vehicle we allow the user to interactively specify a set of cross-slits cameras possibly with gaps between them in each camera one of the slits is defined to be the camera path which is typically horizontal and the user is left to choose the second slit which is typically vertical the system then generates intermediate views between these cameras using a novel interpolation scheme thereby producing a multiperspective image with no seams the user can also choose the picture surface in space onto which viewing rays are projected thereby establishing a parameterization for the image we show how the choice of this surface can be used to create interesting visual effects we demonstrate our system by constructing multiperspective images that summarize city_blocks including corners blocks with deep plazas and other challenging urban situations
we present a tool for real-time_visualization of motion_features in 2d image sequences the motion is estimated through an eigenvector_analysis of the spatio-temporal structure tensor at every pixel location this approach is computationally demanding but allows reliable velocity estimates as well as quality indicators for the obtained results we use a 2d color map and a region of interest selector for the visualization of the velocities on the selected velocities we apply a hierarchical smoothing scheme which allows the choice of the desired scale of the motion field we demonstrate several examples of test sequences in which some persons are moving with different velocities than others these persons are visually marked in the real-time display of the image sequence the tool is also applied to angiography sequences to emphasize the blood flow and its distribution an efficient processing of the data_streams is achieved by mapping the operations onto the stream architecture of standard graphics cards the card receives the images and performs both the motion_estimation and visualization taking advantage of the parallelism in the graphics processor and the superior memory bandwidth the integration of data processing and visualization also saves on unnecessary data transfers and thus allows the real-time analysis of 320×240 images we expect that on the newest generation of graphics_hardware our tool could run in real time for the standard vga format
we present a system for simulating and visualizing the propagation of dispersive contaminants with an application to urban security in particular we simulate airborne contaminant propagation in open environments characterised by sky-scrapers and deep urban canyons our approach is based on the multiple relaxation time lattice boltzmann model mrtlbm which can efficiently handle complex boundary conditions such as buildings in addition we model thermal effects on the flow_field using the hybrid thermal mrtlbm our approach can also accommodate readings from various sensors distributed in the environment and adapt the simulation accordingly we accelerate the computation and efficiently render many buildings with small textures on the gpu we render streamlines and the contaminant smoke with self-shadowing composited with the textured buildings
many of us working in visualization have our own list of our top 5 or 10 unresolved problems in visualization we have assembled a group of panelists to debate and perhaps reach concensus on the top problems in visualization that still need to be explored we include panelists from both the information_and_scientific_visualization domains after our presentations we encourage interaction with the audience to see if we can further formulate and perhaps finalize our list of top unresolved problems in visualization
the evolution of computational science over the last decade has resulted in a dramatic increase in raw problem_solving capabilities this growth has given rise to advances in scientific and engineering simulations that have put a high demand on tools for high-performance large-scale_data_exploration and analysis these simulations have the potential to generate large amounts of data humans however are relatively poor at gaining insight from raw numerical data and as a result have used visualization as a tool for understanding interpreting and exploring data of all types and sizes allowing for efficient visual_explorations of data however requires that the ratio of knowledge gained versus the cost of the visualization be maximized this in turn mandates the integration of principles from human perception understanding perception as it relates to visualization requires that we understand not only the biology of the human_visual_system but principles from vision theory and perceptual psychology as well this panel is the result of bringing together practioners and researchers from a broad spectrum of interests relating to the ability to maximize the amount of information that is effectively perceived from a given visualization position statements will be given by researchers interested in perceptual psychology and the perception of natural images integrating art and design principles non-photorealistic_rendering techniques and the use of global_illumination methods to provide benefical perceptual cues
advances in graphics_hardware and rendering methods are shaping the future of visualization for example programmable graphics processors are redefining the traditional visualization cycle in some cases it is now possible to run the computational simulation and associated visualization side-by-side on the same chip moreover global_illumination and non-photorealistic effects promise to deliver imagery which enables greater insight into high resolution multivariate and higher-dimensional data the panelists will offer distinct viewpoints on the direction of future graphics_hardware and its potential impact on visualization and on the nature of advanced visualizationrelated tools and techniques presentation of these viewpoints will be followed by audience participation in the form of a question and answer period moderated by the panel organizer
scientific_visualization scivis has evolved past the point where one undergraduate course can cover all of the necessary topics so the question becomes "how do we teach scivis to this generation of students" some examples of current courses are a graduate computer science cs course that prepares the next generation of scivis researchers an undergraduate cs course that prepares the future software architects/developers of packages such as vtk vis5d and avs a class that teaches students how to do scivis with existing software packages and how to deal with the lack of interoperability between those packages via either a cs service course or a supercomputing center training course an inter-disciplinary course designed to prepare computer scientists to work with the "real" scientists via either a cs or computational science course in this panel we will discuss these types of courses and the advantages and disadvantages of each we will also talk about some issues that you have probably encountered at your university how do we keep the graphics/vis-oriented students from going to industry how does scivis fit in with evolving computational science programs is scivis destined to be a service course at most universities how do we deal with the diverse backgrounds of students that need scivis
multimedia objects are often described by high-dimensional feature vectors which can be used for retrieval and clustering tasks we have built an interactive retrieval system for 3d model databases that implements a variety of different feature transforms recently we have enhanced the functionality of our system by integrating a som-based visualization module in this poster demo we show how 2d maps can be used to improve the effectiveness of retrieval clustering and over-viewing tasks in a 3d multimedia system
the automotive industry demands visual support for the verification of the quality of their products from the design phase to the manufacturing phase this implies the need of tools for measurement planning programming measuring devices managing measurement data and the visual_exploration of the measurement results to improve the quality control throughout the whole process chain an integration of such tools in a platform independent framework is crucial we present emma enhanced measure management application a client/server system integrating measurement planning data_management and straightforward as well as sophisticated visual_exploration tools in a single framework
modern object-oriented programs are hierarchical systems with many thousands of interrelated subsystems visualization helps developers to better comprehend these large and complex systems this work presents a three-dimensional visualization_technique that represents the static structure of object-oriented software using distributions of three-dimensional objects on a two-dimensional plane the visual complexity is reduced by adjusting the transparency of object surfaces to the distance of the viewpoint an approach called hierarchical net is proposed for a clear representation of the relationships between the subsystems
the purpose of visualization is not just to depict data but to gain or present insight into the domain represented in data however in visualization_systems this link between features in the data and the meaning of those features is often missing or implicit it is assumed that the user through looking at the output will close the loop between representation and insight an alternative is to view visualization tools as interfaces between data and insight and to enrich this interface with capabilities linked to users conceptual_models of the data preliminary work has been carried out to develop such an interface as a modular component that can be installed in a pipelined architecture this poster expands the motivation for this work and describes the initialimplementation carried out within the visualization toolkit vtk
recent activity within the uk national e-science programme has identified a need to establish an ontology for visualization motivation for this includes defining web and grid services for visualization the semantic grid supporting collaborative work curation and underpinning visualization research and education at a preliminary meeting members of the uk visualization community identified a skeleton for the ontology we have started to build on this by identifying how existing work might be related and utilized we believe that the greatest challenge is reaching a consensus within the visualization community itself this poster is intended as one step in this process setting out the perceived needs for the ontology and sketching initial directions it is hoped that this will lead to debate feedback and involvement across the community
this poster abstract presents a scalable information_visualization system for mobile devices and desktop systems it is designed to support the operation and the workflow of wastewater systems the regarded information data includes general information about buildings and units process data occupational safety regulations work directions and first aid instructions in case of an accident technically the presented framework combines visualization with agent technology in order to automatically scale various visualization types to fit on different platforms like pdas personal digital assistants or tablet pcs theimplementation is based on but not limited to sql jsp html and vrml
this paper describes a data_management system for multimedial information_visualization called dami it is possible to create 2d or 3d model based on data out of standard databases and additional metainformation dami is a generic system guaranteeing an optimal reusability and compatibility
computer_graphics has be successfully applied to architecture design there is more demand to new applications one of them to be addressed in this work is the code checking and visualization of the checking results
although there is a remarkable pace in the advance of computational resources and storage for real-time_visualization the immensity of the input data continues to outstrip any advances the task for interactively visualizing such a massive terrain is to render a triangulated mesh using a view-dependent error tolerance thus intelligently and perceptually managing the scenes geometric complexity at any particular instance in time ie displayed frame this level-of-detail lod terrain surface consists of a mesh composed of hundreds of thousands of dynamically selected triangles the triangles are selected using the current time-steps view parameters and the view-dependent error tolerance massive terrain data easily exceeds main memory storage capacity such that out-of-core rendering must be performed this further complicates the triangle selection and terrain_rendering owing to tertiary storages relatively poor performance
we present a simple yet effective method for modeling of object decomposition under combustion a separate simulation models the flame production and generates heat from a combustion process which is used to trigger pyrolysis of the solid object the decomposition is modeled using level_set_methods and can handle complex topological changes even with a very simple flame model on a coarse grid we can achieve a plausible decomposition of the burning object
we present a novel scheme to interactively visualize time-varying scalar_fields defined on a structured grid the underlying approach is to maximize the use of current graphics_hardware by using 3d texture_mapping this approach commonly suffers from an expensive voxelization of each time-step as well as from large size of the voxel array approximating each step hence in our scheme instead of explicitly voxelizing each scalar field we directly store each time-step as a three dimensional texture in its native form we create the function that warps a voxel grid into the given structured grid at rendering time we reconstruct the function at each pixel using hardware-based trilinear_interpolation the resulting coordinates allow us to compute the scalar value at this pixel using a second texture lookup for fixed grids the function remains constant across time-steps and only the scalar field table needs to be re-loaded as a texture our new approach achieves excellent performance with relatively low texture memory requirements and low approximation error
we present the current state of vol-a-tile an interactive tool for exploring large_volumetric_data on scalable tiled_displays vol-a-tile presents a variety of features employed by scientists at the scripps institution of oceanography on data collected from the anatomy of a ridge-axis discontinuity seismic experiment hardware texture_mapping and level-of-detail_techniques provide interactivity a high-performance network protocol is used to connect remote data sources over high-bandwidth photonic networks
researchers in computational condensed matter physics deal with complex data sets consisting oftime_varying 3d tensor vector and scalar quantities particularly in the research of topological defects in nematic_liquid_crystals lc displaying the results of the computer simulation of molecular_dynamics presents a challenge combining existing immersive and interactive_visualization methods we developed new methods that attempt to provide a clear efficient and intuitive way to visualize and explore lc data in addition the visualization of the data has presented us with a novel method of obtaining the locations of the topological defects present in a liquid crystal system
in this paper we offer methods for visualization of the formation of nanoparticles in turbulent flows we present the use of pointillism as a technique to convey the distribution of nanoparticle sizes as texture in an area we also demonstrate a method of producing and packing spot glyphs representative of the distribution of nanoparticle sizes at every point in the flow to produce an intuitive and extensible framework for the visualization
the visualization of any vector field is dependent on the relative velocity of the observer in experimentally generated vector_fields the average value of the streamwise component of the global vector field is typically calculated and subtracted from each vector we demonstrate that the resulting image critical_points and vector field features are greatly influenced by the magnitude of the value subtracted from the streamwise velocity
noise reduction is an important preprocessing step for many visualization_techniques that make use of feature_extraction we propose a method for denoising 2-d vector_fields that are corrupted by additive noise the method is based on the vector wavelet transform and wavelet coefficient thresholding we compare our wavelet-based denoising method with gaussian filtering and test the effect of these methods on the signal-to-noise ratio snr of the vector_fields before and after denoising we also study the effect on relevant details for visualization such as vortex measures the results show that for low snr gaussian filtering with large kernels has a somewhat higher performance than the wavelet-based method in terms of snr for larger snr the wavelet-based method outperforms gaussian filtering this is mostly due to the fact that gaussian filtering tends to remove small details which are preserved by the wavelet-based method
we describe a method for processing large amounts of volumetric_data collected from a knife edge scanning microscope kesm the neuronal data that we acquire consists of thin branching structures extending over very large regions that prior volumetric representations have difficulty dealing with efficiently since the full volume data set can be extremely large on-the-fly processing of the data is necessary
coronary heart disease chd is the number one killer in the united states although it is well known that chd mainly occurs due to blocked arteries there are contradictory results from studies designed to identify basic causes for this common disease is to find out more about the true reason for chd virtual models can be employed to better understand the way the heart functions with such a model scientists and surgeons are able to analyze the effects of different treatment options and ultimately find more suited ways to prevent coronary heart diseases to investigate a given model appropriate navigation methods are required including suitable input devices for the visualization graphics cards originally designed for gaming applications are used so it is a just natural transition to adapt gaming input devices to a visualization system for controlling of the navigation these devices are usually well designed with respect to ergonomics and durability yielding more degrees of freedom in steering than two-dimensional input devices such as desktop mice this poster describes a visualization system that provides the user with advanced control devices for navigation enabling interactive_exploration of the model force-feedback and sound effects provide additional cues
we present a novel method to encode four data channels in a volumetric_data set and render it at interactive frame rates with maximum intensity projection mip using textured polygons the first three channels are stored in the volume textures red green and blue components the fourth channel is stored in the alpha channel to achieve real-time rendering speed we are using a pixel shader
the name voyager a web based visualization of historical trends in baby naming has proven remarkably popular this paper discusses the interaction techniques it uses for smooth visual_exploration of thousands of time_series we also describe design decisions behind the application and lessons learned in creating an application that makes do-it-yourself data_mining popular the prime lesson it is hypothesized is that an information_visualization tool may be fruitfully viewed not as a tool but as part of an online social environment in other words to design a successful exploratory_data_analysis tool one good strategy is to create a system that enables "social" data_analysis
it has long been known that ancient temples were frequently oriented along the cardinal directions or to certain points along the horizon where sun or moon rise or set on special days of the year in the last decades archaeologists have found evidence of even older building structures buried in the soil with doorways that also appear to have distinct orientations this paper presents a novel diagram combining archaeological maps with a folded-apart flattened view of the whole sky showing the local horizon and the daily paths of sun moon and brighter stars by use of this diagram interesting groupings of astronomical orientation directions eg to certain sunrise and sunset points could be identified which were evidently used to mark certain days of the year orientations to a few significant stars very likely indicated the beginning of the agricultural year in the middle neolithic period
the general problem of visualizing "family_trees" or genealogical graphs in 2d is considered a graph theoretic analysis is given which identifies why genealogical graphs can be difficult to draw this motivates some novel graphical representations including one based on a dual tree a subgraph formed by the union of two trees dual trees can be drawn in various styles including an indented outline style and allow users to browse general multitrees in addition to genealogical graphs by transitioning between different dual tree views a software prototype for such browsing is described that supports smoothly animated transitions automatic camera framing rotation of subtrees and a novel interaction technique for expanding or collapsing subtrees to any depth with a single mouse drag
we present the visual code navigator a set of three interrelated visual tools that we developed for exploring large source code software projects from three different perspectives or views the syntactic view shows the syntactic constructs in the source code the symbol view shows the objects a file makes available after compilation such as function signatures variables and namespaces the evolution view looks at different versions in a project lifetime of a number of selected source files the views share one code model which combines hierarchical syntax based and line based information from multiple source files versions we render this code model using a visual model that extends the pixel-filling space partitioning properties of shaded cushion treemaps with novel techniques we discuss how our views allow users to interactively answer complex questions on various code elements by simple mouse clicks we validate the efficiency and effectiveness of our toolset by an informal user_study on the source code of vtk a large industry-size c++ code base
recent years have witnessed the dramatic popularity of online social networking services in which millions of members publicly articulate mutual "friendship" relations guided by ethnographic research of these online communities we have designed and implemented a visualization system for playful end-user exploration and navigation of large scale online social_networks our design builds upon familiar node link network_layouts to contribute customized techniques for exploring connectivity in large graph structures supporting visual_search and analysis and automatically identifying and visualizing community structures both public installation and controlled studies of the system provide evidence of the system's usability capacity for facilitating discovery and potential for fun and engaged social activity
we present prisad the first generic rendering infrastructure for information_visualization_applications that use the accordion drawing technique rubber sheet navigation with guaranteed visibility for marked areas of interest our new rendering algorithms are based on the partitioning of screen space which allows us to handle dense dataset regions correctly the algorithms in previous work led to incorrect visual representations because of overculling and to inefficiencies due to overdrawing multiple items in the same region our pixel based drawing infrastructure guarantees correctness by eliminating overculling and improves rendering performance with tight bounds on overdrawing pritree and priseq are applications built on prisad with the feature sets of treejuxtaposer and sequencejuxtaposer respectively we describe our pritree and priseq dataset_traversal algorithms which are used for efficient rendering culling and layout of datasets within the prisad framework we also discuss pritree node marking techniques which offer order-of-magnitude improvements to both memory and time performance versus previous range storage and retrieval techniques our pritreeimplementation features a five fold increase in rendering speed for nontrivial tree structures and also reduces memory requirements in some real world datasets by up to eight times so we are able to handle trees of several million nodes priseq renders fifteen times faster and handles datasets twenty times larger than previous work
treemaps are a well known method for the visualization of attributed hierarchical_data previously proposed treemap_layout_algorithms are limited to rectangular shapes which cause problems with the aspect_ratio of the rectangles as well as with identifying the visualized hierarchical structure the approach of voronoi_treemaps presented in this paper eliminates these problems through enabling subdivisions of and in polygons additionally this allows for creating treemap visualizations within areas of arbitrary shape such as triangles and circles thereby enabling a more flexible adaptation of treemaps for a wider range of applications
we investigate the use of elastic_hierarchies for representing trees where a single graphical depiction uses a hybrid mixture or "interleaving" of more basic forms at different nodes of the tree in particular we explore combinations of node link and treemap forms to combine the space efficiency of treemaps with the structural clarity of node link diagrams a taxonomy is developed to characterize the design space of such hybrid combinations a software prototype is described which we used to explore various techniques for visualizing browsing and interacting with elastic_hierarchies such as side by side overview_and_detail views highlighting and rubber banding across views visualization of multiple foci and smooth animations across transitions the paper concludes with a discussion of the characteristics of elastic_hierarchies and suggestions for research on their properties and uses
we describe a new method for visualization of directed graphs the method combines constraint programming techniques with a high performance force directed placement fdp algorithm so that the directed nature of the graph is highlighted while useful properties of fdp - such as emphasis of symmetries and preservation of proximity relations - are retained our algorithm automatically identifies those parts of the digraph that contain hierarchical information and draws them accordingly additionally those parts that do not contain hierarchy are drawn at the same quality expected from a nonhierarchical undirected layout_algorithm an interesting application of our algorithm is directional multidimensional scaling dmds dmds deals with low dimensional embedding of multivariate data where we want to emphasize the overall flow in the data eg chronological progress along one of the axes
the paper describes a novel technique to visualize graphs with extended node and link labels the lengths of these labels range from a short phrase to a full sentence to an entire paragraph and beyond our solution is different from all the existing approaches that almost always rely on intensive computational effort to optimize the label_placement problem instead we share the visualization resources with the graph and present the label information in static interactive and dynamic modes without the requirement for tackling the intractability issues this allows us to reallocate the computational resources for dynamic presentation of real time information the paper includes a user_study to evaluate the effectiveness and efficiency of the visualization_technique
space-filling_visualizations such as the treemap are well suited for displaying the properties of nodes in hierarchies to browse the contents of the hierarchy the primary mode of interaction is by drilling down through many successive layers in this paper we introduce a distortion algorithm based on fisheye and continuous zooming techniques for browsing data in the treemap representation the motivation behind the distortion approach is for assisting users to rapidly browse information displayed in the treemap without opening successive layers of the hierarchy two experiments were conducted to evaluate the new approach in the first experiment n=20 the distortion approach is compared to the drill down method results show that subjects are quicker and more accurate in locating targets of interest using the distortion method the second experiment n=12 evaluates the effectiveness of the two approaches in a task requiring context we define as the context browsing task the results show that subjects are quicker and more accurate in locating targets with the distortion technique in the context browsing task
many visual_analysis tools operate on a fixed set of data however professional information analysts follow issues over a period of time and need to be able to easily add new documents to an ongoing exploration some analysts handle documents in a moving window of time with new documents constantly added and old ones aging out this paper describes both the user_interaction and the technicalimplementation approach for a visual_analysis system designed to support constantly evolving text collections
we present a method for visual_summary of bilateral conflict structures embodied in event data such data consists of actors linked by time stamped events and may be extracted from various sources such as news reports and dossiers when analyzing political events it is of particular importance to be able to recognize conflicts and actors involved in them by projecting actors into a conflict space we are able to highlight the main opponents in a series of tens of thousands of events and provide a graphic overview of the conflict structure moreover our method allows for smooth animation of the dynamics of a conflict
existing system level taxonomies of visualization tasks are geared more towards the design of particular representations than the facilitation of user analytic_activity we present a set of ten low level analysis tasks that largely capture people's activities while employing information_visualization tools for understanding data to help develop these tasks we collected nearly 200 sample questions from students about how they would analyze five particular data sets from different domains the questions while not being totally comprehensive illustrated the sheer variety of analytic questions typically posed by users when employing information_visualization_systems we hope that the presented set of tasks is useful for information_visualization system designers as a kind of common substrate to discuss the relative analytic capabilities of the systems further the tasks may provide a form of checklist for system designers
we present an effort to evaluate the possible utility of a new type of 3d_glyphs intended for visualizations of multivariate spatial_data they are based on results from vision research suggesting that our perception of metric 3d structure is distorted and imprecise relative to the actual scene before us eg "metric 3d structure in visualizations" by m lind et al 2003 only a class of qualitative properties of the scene is perceived withaccuracy these properties are best characterized as being invariant over affine but not euclidean transformations they are related but not identical to the non-accidental properties naps described by lowe in "perceptual_organization and visual recognition" 1984 on which the notion of geons is based in "recognition by components - a theory of image understanding" by i biederman 1987 a large number of possible 3d_glyphs for the visualization of spatial_data can be constructed using such properties one group is based on the local sign of surface curvature we investigated these properties in a visualization experiment the results are promising and the implications for visualization are discussed
in order to gain insight into multivariate data complex structures must be analysed and understood parallel_coordinates is an excellent tool for visualizing this type of data but has its limitations this paper deals with one of its main limitations - how to visualize a large number of data items without hiding the inherent structure they constitute we solve this problem by constructing clusters and using high precision textures to represent them we also use transfer_functions that operate on the high precision textures in order to highlight different aspects of the cluster characteristics providing predefined transfer_functions as well as the support to draw customized transfer_functions makes it possible to extract different aspects of the data we also show how feature_animation can be used as guidance when simultaneously analysing several clusters this technique makes it possible to visually represent statistical information about clusters and thus guides the user making the analysis process more efficient
the discrete nature of categorical_data makes it a particular challenge for visualization methods that work very well for continuous data are often hardly usable with categorical dimensions only few methods deal properly with such data mostly because of the discrete nature of categorical_data which does not translate well into the continuous domains of space and color parallel_sets is a new visualization method that adopts the layout of parallel_coordinates but substitutes the individual data points by a frequency based representation this abstracted view combined with a set of carefully designed interactions supports visual_data_analysis of large and complex data sets the technique allows efficient work with meta data which is particularly important when dealing with categorical_datasets by creating new dimensions from existing ones for example the user can filter the data according to his or her current needs we also present the results from an interactive analysis of crm data using parallel_sets we demonstrate how the flexible layout eases the process of knowledge crystallization especially when combined with a sophisticated interaction scheme
aggregating items can simplify the display of huge quantities of data values at the cost of losing information about the attribute values of the individual items we propose a distribution glyph in both two- and three-dimensional forms which specifically addresses the concept of how the aggregated_data is distributed over the possible range of values it is capable of displaying distribution variability and extent information for up to four attributes at a time of multivariate clustered_data user studies validate the concept showing that both glyphs are just as good as raw data and the 3d glyph is better for answering some questions
parallel_coordinates are a powerful method for visualizing multidimensional data but when applied to large_data sets they become cluttered and difficult to read star_glyphs on the other hand can be used to display either the attributes of a data item or the values across all items for a single attribute star_glyphs may readily provide a quick impression however since the full data set require multiple glyphs overall readings are more difficult we present parallel glyphs an interactive integration of the visual representations of parallel_coordinates and star_glyphs that utilizes the advantages of both representations to offset the disadvantages they have separately we discuss the role of uniform and stepped colour scales in the visual_comparison of non-adjacent items and star_glyphs parallel glyphs provide capabilities for focus-in-context exploration using two types of lenses and interactions specific to the 3d space
we introduce tukey and tukey scagnostics and develop graph-theoretic methods for implementing their procedure on large_datasets
exploratory_visualization_environments allow users to build and browse coordinated multiview visualizations interactively as the number of views and amount of coordination increases conceptualizing coordination structure becomes more and more important for successful data_exploration integrated meta visualization is exploratory_visualization of coordination and other interactive structure directly inside a visualization's own user interface this paper presents a model of integrated meta visualization describes the problem of capturing dynamic interface structure as visualizable data and outlines three general approaches to integration meta visualization has been implemented in improvise using views lenses and embedding to reveal the dynamic structure of its own highly coordinated visualizations
a new pseudo_coloring technique for large scale one-dimensional datasets is proposed for visualization of a large scale dataset user_interaction is indispensable for selecting focus areas in the dataset however excessive switching of the visualized image makes it difficult for the user to recognize overview/ detail and detail/ detail relationships the goal of this research is to develop techniques for visualizing details as precisely as possible in overview display in this paper visualization of a one-dimensional but very large_dataset is considered the proposed method is based on pseudo_coloring however each scalar value corresponds to two discrete colors by painting with two colors at each value users can read out the value precisely this method has many advantages it requires little image space for visualization both the overview_and_details of the dataset are visible in one image without distortion andimplementation is very simple several application examples such as meteorological observation data and train convenience evaluation data show the effectiveness of the method
a recent line of treemap research has focused on layout_algorithms that optimize properties such as stability preservation of ordering information and aspect_ratio of rectangles no ideal treemap_layout_algorithm has been found and so it is natural to explore layouts that produce nonrectangular regions this note describes a connection between space-filling_visualizations and the mathematics of space-filling curves and uses that connection to characterize a family of layout_algorithms which produce nonrectangular regions but enjoy geometric continuity under changes to the data and legibility even for highly unbalanced trees
we are building an intelligent multimodal conversation system to aid users in exploring large and complex data sets to tailor to diverse user queries introduced during a conversation we automate the generation of system responses including both spoken and visual outputs in this paper we focus on the problem of visual_context_management a process that dynamically updates an existing visual display to effectively incorporate new information requested by subsequent user queries specifically we develop an optimization based approach to visual_context_management compared to existing approaches which normally handle predictable visual context updates our work offers two unique contributions first we provide a general computational framework that can effectively manage a visual context for diverse unanticipated situations encountered in a user system conversation moreover we optimize the satisfaction of both semantic and visual constraints which otherwise are difficult to balance using simple heuristics second we present an extensible representation model that uses feature_based metrics to uniformly define all constraints we have applied our work to two different applications and our evaluation has shown the promise of this work
the usability of knowledge domain visualization kdviz tools can be assessed at several levels cognitive_walkthrough cw is a well known usability inspection method that focuses on how easily users can learn software through exploration typical applications of cw follow structured tasks where user goals and action sequences that lead to achievement of the goals are well defined kdviz and other information_visualization tools however are typically designed for users to explore data and user goals and actions are less well understood in this paper we describe how the traditional cw method may be adapted for assessing the usability of these systems we apply the adapted version of cw to citespace a kdviz tool that uses bibliometric analyses to create visualizations of scientific literatures we describe usability issues identified by the adapted cw and discuss how citespace supported the completion of tasks such as identifying research fronts and the achievement of goals finally we discuss improvements to the adapted cw and issues to be addressed before applying it to a wider range of kdviz tools
time_series are an important type of data with applications in virtually every aspect of the real world often a large number of time_series have to be monitored and analyzed in parallel_sets of time_series may show intrinsic hierarchical relationships and varying degrees of importance among the individual time_series effective techniques for visually analyzing large sets of time_series should encode the relative importance and hierarchical ordering of the time_series_data by size and position and should also provide a high degree of regularity in order to support comparability by the analyst in this paper we present a framework for visualizing large sets of time_series based on the notion of inter time_series importance relationships we define a set of objective functions that space-filling layout schemes for time_series_data should obey we develop an efficient algorithm addressing the identified problems by generating layouts that reflect hierarchy and importance based relationships in a regular layout with favorable aspect_ratios we apply our technique to a number of real world data sets including sales and stock data and we compare our technique with an aspect_ratio aware variant of the well known treemap algorithm the examples show the advantages and practical usefulness of our layout_algorithm
partitioning of geo-spatial_data for efficient allocation of resources such as schools and emergency health care services is driven by a need to provide better and more effective services partitioning of spatial_data is a complex process that depends on numerousfactors such as population costs incurred in deploying or utilizing resources and target capacity of a resource moreover complex data such as population distributions are dynamic ie they may change over time simple animation may not effectively show temporal changes in spatial_data we propose the use of three temporal_visualization_techniques -wedges rings and time slices - to display the nature of change in temporal_data in a single view along with maximizing resource utilization and minimizing utilization costs a partition should also ensure the long term effectiveness of the plan we use multi-attribute_visualization_techniques to highlight the strengths and identify the weaknesses of a partition comparative_visualization_techniques allow multiple partitions to be viewed simultaneously users can make informed decisions about how to partition geo spatial_data by using a combination of our techniques for multi-attribute_visualization temporal_visualization and comparative_visualization
cartographers have long used flow_maps to show the movement of objects from one location to another such as the number of people in a migration the amount of goods being traded or the number of packets in a network the advantage of flow_maps is that they reduce visual_clutter by merging edges most flow_maps are drawn by hand and there are few computer algorithms available we present a method for generating flow_maps using hierarchical_clustering given a set of nodes positions and flow data between the nodes our techniques are inspired by graph_layout_algorithms that minimize edge crossings and distort node positions while maintaining their relative position to one another we demonstrate our technique by producing flow_maps for network traffic census data and trade data
the most common approach to support analysis of graphs with associated time_series_data include overlay of data on graph vertices for one timepoint at a time by manipulating a visual property eg color of the vertex along with sliders or some such mechanism to animate the graph for other timepoints alternatively data from all the timepoints can be overlaid simultaneously by embedding small charts into graph vertices these graph_visualizations may also be linked to other visualizations eg parallel co-ordinates using brushing_and_linking this paper describes a study performed to evaluate and rank graph+timeseries visualization options based on users' performance time andaccuracy of responses on predefined tasks the results suggest that overlaying data on graph vertices one timepoint at a time may lead to more accurate performance for tasks involving analysis of a graph at a single timepoint and comparisons between graph vertices for two distinct timepoints overlaying data simultaneously for all the timepoints on graph vertices may lead to more accurate and faster performance for tasks involving searching for outlier vertices displaying different behavior than the rest of the graph vertices for all timepoints single views have advantage over multiple_views on tasks that require topological information also the number of attributes displayed on nodes has a non trivial influence onaccuracy of responses whereas the number of visualizations affect the performance time
we present a system that allows users to interactively explore complex flow scenarios represented as sankey_diagrams our system provides an overview of the flow graph and allows users to zoom in and explore details on demand the support for quantitative flow tracing across the flow graph as well as representations at different levels of detail facilitate the understanding of complex flow situations the energy flow in a city serves as a sample scenario for our system different forms of energy are distributed within the city and they are transformed into heat electricity or other forms of energy these processes are visualized and interactively explored in addition our system can be used as a planning tool for the exploration of alternative scenarios by interactively manipulating different parameters in the energy flow network
as information_visualization matures as an academic research field commercial spinoffs are proliferating but success stories are harder to find this is the normal process of emergence for new technologies but the panel organizers believe that there are certain strategies that facilitate success to teach these lessons we have invited several key figures who are seeking to commercialize information_visualization tools the panelists make short presentations engage in a moderated discussion and respond to audience questions
artificial_neural_networks are computer software or hardware models inspired by the structure and behavior of neurons in the human nervous system as a powerful learning tool increasingly neural_networks have been adopted by many large-scale information processing applications but there is no a set of well defined criteria for choosing a neural network the user mostly treats a neural network as a black box and cannot explain how learning from input data was done nor how performance can be consistently ensured we have experimented with several information_visualization designs aiming to open the black box to possibly uncover underlying dependencies between the input data and the output data of a neural network in this paper we present our designs and show that the visualizations not only help us design more efficient neural_networks but also assist us in the process of using neural_networks for problem_solving such as performing a classification task
analysis of phenomena that simultaneously occur on different spatial and temporal scales requires adaptive hierarchical schemes to reduce computational and storage demands adaptive_mesh_refinement amr schemes support both refinement in space that results in a time-dependent grid topology as well as refinement in time that results in updates at higher rates for refined levels visualization of amr data requires generating data for absent refinement levels at specific time steps we describe a solution starting from a given set of "key frames" with potentially different grid topologies the presented work was developed in a project involving several research institutes that collaborate in the field of cosmology and numerical relativity amr data results from simulations that are run on dedicated compute machines and is thus stored centrally whereas the analysis of the data is performed on the local computers of the scientists we built a distributed solution using remote procedure calls rpc to keep the application responsive we split the bulk data transfer from the rpc response and deliver it asynchronously as a binary stream the number of network round-trips is minimized by using high level operations in summary we provide an application for exploratory_visualization of remotely stored amr data
diffusion_tensor_imaging is a magnetic_resonance_imaging method which has gained increasing importance in neuroscience and especially in neurosurgery it acquires diffusion properties represented by a symmetric 2nd order tensor for each voxel in the gathered dataset from the medical point of view the data is of special interest due lo different diffusion characteristics of varying brain tissue allowing conclusions about the underlying structures such as while matter tracts an obvious way to visualize this data is to focus on the anisotropic areas using the major eigenvector for tractography and rendering lines for visualization of the simulation results our approach extends this technique to avoid line representation since lines lead 10 very complex illustrations and furthermore are mistakable instead we generate surfaces wrapping bundles of lines thereby a more intuitive representation of different tracts is achieved
we propose a distributed data_management scheme for large_data_visualization that emphasizes efficient data sharing and access to minimize data access time and support users with a variety of local computing capabilities we introduce an adaptive data selection method based on an "enhanced time-space partitioning" etsp tree that assists with effective visibility_culling as well as multiresolution data selection by traversing the tree our data_management algorithm can quickly identify the visible regions of data and for each region adaptively choose the lowest resolution satisfying user-specified error tolerances only necessary data elements are accessed and sent to the visualization pipeline to further address the issue of sharing large-scale_data among geographically distributed collaborative teams we have designed an infrastructure for integrating our data_management technique with a distributed data storage system provided by logistical_networking lon data sets at different resolutions are generated and uploaded to lon for wide-area access we describe a parallel volume_rendering system that verifies the effectiveness of our data storage selection and access scheme
sort-last parallel_rendering is an efficient technique to visualize huge datasets on cots clusters the dataset is subdivided and distributed across the cluster nodes for every frame each node renders a full resolution image of its data using its local gpu and the images are composited together using a parallel image_compositing algorithm in this paper we present a performance evaluation of standard sort-last parallel_rendering methods and of the different improvements proposed in the literature this evaluation is based on a detailed analysis of the different hardware and software components we present a newimplementation of sort-last_rendering that fully overlaps cpus gpu and network usage all along the algorithm we present experiments on a 3 years old 32-node pc cluster and on a 15 years old 5-node pc cluster both with gigabit interconnect showing volume_rendering at respectively 13 and 31 frames per second and polygon rendering at respectively 8 and 17 frames per second on a 1024 x 768 render area and we show that ourimplementation outperforms or equals many otherimplementations and specialized visualization clusters
in this paper we present two novel texture-based techniques to visualize uncertainty in time-dependent 2d flow_fields both methods use semi-lagrangian texture_advection to show flow direction by streaklines and convey uncertainty by blurring these streaklines the first approach applies a cross advection perpendicular to the flow direction the second method employs isotropic diffusion that can be implemented by gaussian filtering both methods are derived from a generic filtering process that is incorporated into the traditional texture_advection pipeline our visualization methods allow for a continuous change of the density of flow representation by adapting the density of particle injection all methods can be mapped to efficient gpuimplementations therefore the user can interactively control all important characteristics of the system like particle density error influence or dye injection to create meaningful illustrations of the underlying uncertainty even though there are many sources of uncertainties we focus on uncertainty that occurs during data acquisition we demonstrate the usefulness of our methods for the example of real-world fluid flow data measured with the particle image velocimetry piv technique furthermore we compare these techniques with an adapted multi-frequency noise approach
real-time rendering of massively textured 3d scenes usually involves two major problems large numbers of texture switches are a well-known performance bottleneck and the set of simultaneously visible textures is limited by the graphics memory this paper presents a level-of-detail texturing technique that overcomes both problems in a preprocessing step the technique creates a hierarchical_data structure for all textures used by scene objects and it derives texture atlases at different resolutions at runtime our texturing technique requires only a small set of these texture atlases which represent scene textures in an appropriate size depending on the current camera position and screen resolution independent of the number and total size of all simultaneously visible textures the achieved frame rates are similar to that of rendering the scene without any texture switches since the approach includes dynamic texture loading the total size of the textures is only limited by the hard disk capacity the technique is applicable for any 3d scenes whose scene objects are primarily distributed in a plane such as in the case of 3d city models or outdoor scenes in computer games our approach has been successfully applied to massively textured large-scale 3d city models
in this paper we introduce gpu particle_tracing for the visualization of 3d diffusion tensor_fields for about half a million particles reconstruction of diffusion directions from the tensor_field time integration and rendering can be done at interactive rates different visualization options like oriented particles of diffusion-dependent shape stream lines or stream tubes facilitate the use of particle_tracing for diffusion_tensor_visualization the proposed methods provide efficient and intuitive means to show the dynamics in diffusion tensor_fields and they accommodate the exploration of the diffusion properties of biological tissue
we present a robust method for 3d_reconstruction of closed surfaces from sparsely sampled parallel contours a solution to this problem is especially important for medical segmentation where manual contouring of 2d imaging scans is still extensively used our proposed method is based on a morphing process applied to neighboring contours that sweeps out a 3d surface our method is guaranteed to produce closed surfaces that exactly pass through the input contours regardless of the topology of the reconstruction our general approach consecutively morphs between sets of input contours using an eulerian formulation ie fixed grid augmented with lagrangian particles ie interface tracking this is numerically accomplished by propagating the input contours as 2d level_sets with carefully constructed continuous speed functions specifically this involves particle advection to estimate distances between the contours monotonicity constrained spline interpolation to compute continuous speed functions without overshooting and state-of-the-art numerical techniques for solving the level set equations we demonstrate the robustness of our method on a variety of medical topographic and synthetic_data sets
visit is a richly featured visualization tool that is used to visualize some of the largest simulations ever run the scale of these simulations requires that optimizations are incorporated into every operation visit performs but the set of applicable optimizations that visit can perform is dependent on the types of operations being done complicating the issue visit has a plugin capability that allows new unforeseen components to be added making it even harder to determine which optimizations can be applied we introduce the concept of a contract to the standard data flow network design this contract enables each component of the data flow network to modify the set of optimizations used in addition the contract allows for new components to be accommodated gracefully within visit's data flow network system
gpu-based raycasting offers an interesting alternative to conventional slice-based volume_rendering due to the inherent flexibility and the high_quality of the generated images recent advances in graphics_hardware allow for the ray traversal and volume sampling to be executed on a per-fragment level completely on the gpu leading to interactive framerates in this work we present optimization techniques that improve the performance and quality of gpu-based volume_raycasting we apply a hybrid image/object space approach to accelerate the ray traversal in animation sequences that works for both isosurface rendering and semi-transparent volume_rendering an empty-space-leaping technique that exploits the spatial coherence between consecutively rendered images is used to estimate the optimal initial ray sampling point for each image pixel these can double the rendering performance for typical volumetric_data sets without sacrificing image quality the achieved speed-up allows for further improvements of image quality we demonstrate an object space antialiasing technique based on selective super-sampling at sharp creases and silhouette edges which also benefits from exploiting frame-to-frame coherence
simulations often generate large amounts of data that require use of scivis techniques for effective exploration of simulation results in some cases like 1d theory of fluid dynamics conventional scivis techniques are not very useful one such example is a simulation of injection_systems that is becoming more and more important due to an increasingly restrictive emission regulations there are many parameters and correlations among them that influence the simulation results we describe how basic information_visualization_techniques can help in visualizing understanding and analyzing this kind of data the com vis tool is developed and used to analyze and explore the data com vis supports multiple linked_views and common information_visualization displays such as 2d and 3d scatter-plot histogram parallel_coordinates pie-chart etc a diesel common rail injector with 2/2 way valve is used for a case_study data sets were generated using a commercially available avl hydsim simulation tool for dynamic analysis of hydraulic and hydro-mechanical systems with the main application area in the simulation of fuel injection_systems
understanding and analyzing complex volumetrically varying data is a difficult problem many computational visualization_techniques have had only limited success in succinctly portraying the structure of three-dimensional turbulent flow motivated by both the extensive history and success of illustration and photographic flow_visualization_techniques we have developed a new interactive_volume_rendering and visualization system for flows and volumes that simulates and enhances traditional illustration experimental advection and photographic flow_visualization_techniques our system uses a combination of varying focal and contextual illustrative styles new advanced two-dimensional transfer_functions enhanced schlieren and shadowgraphy shaders and novel oriented structure enhancement techniques to allow interactive_visualization exploration and comparative_analysis of scalar vector and time-varying volume datasets both traditional illustration techniques and photographic flow_visualization_techniques effectively reduce visual_clutter by using compact oriented structure information to convey three-dimensional structures therefore a key to the effectiveness of our system is using one-dimensional schlieren and shadowgraphy and two-dimensional silhouette oriented structural information to reduce visual_clutter while still providing enough three-dimensional structural information for the user's visual system to understand complex three-dimensional flow data by combining these oriented feature visualization_techniques with flexible transfer_function controls we can visualize scalar and vector data allow comparative_visualization of flow properties in a succinct informative manner and provide continuity for visualizing time-varying_datasets
quality surface meshes for molecular models are desirable in the studies of protein shapes and functionalities however there is still no robust software that is capable to generate such meshes with good quality in this paper we present a delaunay-based surface triangulation algorithm generating quality surface meshes for the molecular skin model we expand the restricted union of balls along the surface and generate an ε-sampling of the skin surface incrementally at the same time a quality surface mesh is extracted from the delaunay_triangulation of the sample points the algorithm supports robust and efficientimplementation and guarantees the mesh quality and topology as well our results facilitate molecular_visualization and have made a contribution towards generating quality volumetric tetrahedral_meshes for the macromolecules
recent years have seen an immense increase in the complexity of geometric data sets today's gigabyte-sized polygon models can no longer be completely loaded into the main memory of common desktop pcs unfortunately current mesh formats which were designed years ago when meshes were orders of magnitudes smaller do not account for this using such formats to store large meshes is inefficient and complicates all subsequent processing we describe a streaming format for polygon_meshes that is simple enough to replace current offline mesh formats and is more suitable for representing large_data sets furthermore it is an ideal input and output format for i/o-efficient out-of-core_algorithms that process meshes in a streaming possibly pipelined fashion this paper chiefly concerns the underlying theory and the practical aspects of creating and working with this new representation in particular we describe desirable qualities for streaming meshes and methods for converting meshes from a traditional to a streaming format a central theme of this paper is the issue of coherent and compatible layouts of the mesh vertices and polygons we present metrics and diagrams that characterize the coherence of a mesh layout and suggest appropriate strategies for improving its "streamability" to this end we outline several out-of-core_algorithms for reordering meshes with poor coherence and present results for a menagerie of well known and generally incoherent surface meshes
in this case_study a data-oriented approach is used to visualize a complex digital signal_processing pipeline the pipeline implements a frequency modulated fm software-defined radio sdr sdr is an emerging technology where portions of the radio hardware such as filtering and modulation are replaced by software components we discuss how an sdrimplementation is instrumented to illustrate the processes involved in fm transmission and reception by using audio-encoded images we illustrate the processes involved in radio such as how filters are used to reduce noise the nature of a carrier wave and how frequency modulation acts on a signal the visualization approach used in this work is very effective in demonstrating advanced topics in digital signal_processing and is a useful tool for experimenting with the software radio design
this paper presents a strategy for seeding streamlines in 3d flow_fields its main goal is to capture the essential flow patterns and to provide sufficient coverage in the field while reducing clutter first critical_points of the flow_field are extracted to identify regions with important flow patterns that need to be presented different seeding templates are then used around the vicinity of the different critical_points because there is significant variability in the flow pattern even for the same type of critical point our template can change shape depending on how far the critical point is from transitioning into another type of critical point to accomplish this we introduce the α-β map of 3d critical_points next we use poisson seeding to populate the empty regions finally we filter the streamlines based on their geometric and spatial properties altogether this multi-step strategy reduces clutter and yet captures the important 3d flow features
we propose a hybrid particle and texture based approach for the visualization of time-dependent_vector_fields the underlying space-time framework builds a dense vector field representation in a two-step process 1 particle-based forward integration of trajectories in spacetime for temporal_coherence and 2 texture-based convolution along another set of paths through the spacetime for spatially correlated patterns particle density is controlled by stochastically injecting and removing particles taking into account the divergence of the vector field alternatively a uniform density can be maintained by placing exactly one particle in each cell of a uniform grid which leads to particle-in-cell forward advection moreover we discuss strategies of previous visualization methods for unsteady_flow and show how they address issues of spatiotemporal_coherence and dense visual representations we demonstrate how our framework is capable of realizing several of these strategies finally we present an efficient gpuimplementation that facilitates an interactive_visualization of unsteady 2d flow on shader model 3 compliant graphics_hardware
fiber_tracking is a standard approach for the visualization of the results of diffusion_tensor_imaging dti if fibers are reconstructed and visualized individually through the complete white_matter the display gets easily cluttered making it difficult to get insight in the data various clustering techniques have been proposed to automatically obtain bundles that should represent anatomical structures but it is unclear which clustering methods and parameter settings give the best results we propose a framework to validate clustering methods for white-matter fibers clusters are compared with a manual classification which is used as a ground truth for the quantitative_evaluation of the methods we developed a new measure to assess the difference between the ground truth and the clusterings the measure was validated and calibrated by presenting different clusterings to physicians and asking them for their judgement we found that the values of our new measure for different clusterings match well with the opinions of physicians using this framework we have evaluated different clustering algorithms including shared nearest neighbor clustering which has not been used before for this purpose we found that the use of hierarchical_clustering using single-link and a fiber similarity measure based on the mean distance between fibers gave the best results
stars form in dense clouds of interstellar gas and dust the residual dust surrounding a young star scatters and diffuses its light making the star's "cocoon" of dust observable from earth the resulting structures called reflection nebulae are commonly very colorful in appearance due to wavelength-dependent effects in the scattering and extinction of light the intricate interplay of scattering and extinction cause the color hues brightness distributions and the apparent shapes of such nebulae to vary greatly with viewpoint we describe an interactive_visualization tool for realistically rendering the appearance of arbitrary 3d dust distributions surrounding one or more illuminating stars our rendering algorithm is based on the physical models used in astrophysics research the tool can be used to create virtual fly-throughs of reflection nebulae for interactive desktop visualizations or to produce scientifically accurate animations for educational purposes eg in planetarium shows the algorithm is also applicable to investigate on-the-fly the visual effects of physical parameter variations exploiting visualization technology to help gain a deeper and more intuitive understanding of the complex interaction of light and dust in real astrophysical settings
tensor_topology is useful in providing a simplified and yet detailed representation of a tensor_field recently the field of 3d tensor_topology is advanced by the discovery that degenerate_tensors usually form lines in their most basic configurations these lines form the backbone for further topological_analysis a number of ways for extracting and tracing the degenerate tensor_lines have also been proposed in this paper we complete the previous work by studying the behavior and extracting the separating_surfaces emanating from these degenerate lines first we show that analysis of eigenvectors around a 3d degenerate tensor can be reduced to 2d that is in most instances the 3d separating_surfaces are just the trajectory of the individual 2d separatrices which includes trisectors and wedges but the proof is by no means trivial since it is closely related to perturbation theory around a pair of singular slate such analysis naturally breaks down at the tangential points where the degenerate lines pass through the plane spanned by the eigenvectors associated with the repeated eigenvalues second we show that the separatrices along a degenerate line may switch types eg trisectors to wedges exactly at the points where the eigenplane is tangential to the degenerate curve this property leads to interesting and yet complicated configuration of surfaces around such transition points finally we apply the technique to several common data sets to verify its correctness
we propose a novel algorithm for placement of streamlines from two-dimensional steady vector or direction fields our method consists of placing one streamline at a time by numerical integration starting at the furthest away from all previously placed streamlines such a farthest point seeding_strategy leads to high_quality placements by favoring long streamlines while retaining uniformity with the increasing density our greedy approach generates placements of comparable quality with respect to the optimization approach from turk and banks while being 200 times faster simplicity robustness as well as efficiency is achieved through the use of a delaunay_triangulation to model the streamlines address proximity queries and determine the biggest voids by exploiting the empty circle property our method handles variable_density and extends to multiresolution
the field of visualization is getting mature many problems have been solved and new directions are sought for in order to make good choices an understanding of the purpose and meaning of visualization is needed especially it would be nice if we could assess what a good visualization is in this paper an attempt is made to determine the value of visualization a technological viewpoint is adopted where the value of visualization is measured based on effectiveness and efficiency an economic model of visualization is presented and benefits and costs are established next consequences brand limitations of visualization are discussed including the use of alternative methods high initial costs subjective/less and the role of interaction as well as examples of the use of the model for the judgement of existing classes of methods and understanding why they are or are not used in practice furthermore two alternative views on visualization are presented and discussed viewing visualization as an art or as a scientific discipline implications and future directions are identified
this paper presents a novel approach for surface_reconstruction from point clouds the proposed technique is general in the sense that it naturally handles both manifold and non-manifold_surfaces providing a consistent way for reconstructing closed surfaces as well as surfaces with boundaries it is also robust in the presence of noise irregular sampling and surface gaps furthermore it is fast parallelizable and easy to implement because it is based on simple local operations in this approach surface_reconstruction consists of three major steps first the space containing the point cloud is subdivided creating a voxel representation then a voxel surface is computed using gap filling and topological thinning operations finally the resulting voxel surface is converted into a polygonal mesh we demonstrate the effectiveness of our approach by reconstructing polygonal models from range scans of real objects as well as from synthetic_data
traditionally time-varying_data has been visualized using snapshots of the individual time steps or an animation of the snapshots shown in a sequential manner for larger datasets with many time-varying features animation can be limited in its use as an observer can only track a limited number of features over the last few frames visually inspecting each snapshot is not practical either for a large number of time-steps we propose new techniques inspired from the illustration literature to convey change over time more effectively in a time-varying_dataset speedlines are used extensively by cartoonists to convey motion speed or change over different panels flow ribbons are another technique used by cartoonists to depict motion in a single frame strobe silhouettes are used to depict previous positions of an object to convey the previous positions of the object to the user these illustration-inspired techniques can be used in conjunction with animation to convey change over time
we present a system for three-dimensional visualization of complex liquid chromatography-mass_spectrometry lcms data every lcms data point has three attributes time mass and intensity instead of the traditional visualization of two-dimensional subsets of the data we visualize it as a height field or terrain in 3d unlike traditional terrains lcms data has non-linear sampling and consists mainly of tall needle-like features we adapt the level-of-detail_techniques of geometry clipmaps for hardware-accelerated rendering of lcms data the data is cached in video memory as a set of nested rectilinear grids centered about the view frustum we introduce a simple compression scheme and dynamically stream data from the cpu to the gpu as the viewpoint moves our system allows interactive investigation of complex lcms data with close to one billion data points at up to 130 frames per second depending on the view conditions
visualization users are increasingly in need of techniques for assessing quantitative uncertainty and error in the images produced statistical segmentation algorithms compute these quantitative results yet volume_rendering tools typically produce only qualitative imagery via transfer_function-based classification this paper presents a visualization_technique that allows users to interactively explore the uncertainty risk and probabilistic decision of surface boundaries our approach makes it possible to directly visualize the combined "fuzzy" classification results from multiple segmentations by combining these data into a unified probabilistic data space we represent this unified space the combination of scalar volumes from numerous segmentations using a novel graph-based dimensionality_reduction scheme the scheme both dramatically reduces the dataset size and is suitable for efficient high_quality quantitative visualization lastly we show that the statistical risk arising from overlapping segmentations is a robust measure for visualizing features and assigning optical properties
a new technique is presented to increase the performance of volume_splatting by using hardware accelerated point sprites this allows creating screen aligned elliptical splats for high_quality volume_splatting at very low cost on the gpu only one vertex per splat is stored on the graphics card gpu generated point sprite texture coordinates are used for computing splats and per-fragment 3d-texture coordinates on the fly thus only 6 bytes per splat are stored on the gpu and vertex shader load is 25% in comparison to applying textured quads for eight predefined viewing directions depth-sorting of the splats is performed in a pre-processing step where the resulting indices are stored on the gpu thereby there is no data transfer between cpu and gpu during rendering post-classificative two dimensional transfer_functions with lighting for scalar_data and tagged volumes were implemented thereby we focused on the visualization of neurovascular structures where typically no more than 2% of the voxels contribute to the resulting 3d-representation a comparison with a 3d-texture-based slicing algorithm showed frame rates up to 11 times higher for the presented approach on current cpus the presented technique was evaluated with a broad medical database and its value for highly sparse volume_visualization is shown
we have developed a real-time experiment-control and data-display system for a novel microscope the 3d-force microscope 3dfm which is designed for nanometer-scale and nanonewton-force biophysical experiments the 3dfm software suite synthesizes the several data sources from the 3dfm into a coherent view and provides control over data collection and specimen manipulation herein we describe the system architecture designed to handle the several feedback loops and data flows present in the microscope and its control system we describe the visualization_techniques used in the 3dfm software suite where used and on which types of data we present feedback from our scientist-users regarding the usefulness of these techniques and we also present lessons learned from our successiveimplementations
we present a higher-order approach to the extraction of isosurfaces from unstructured_meshes existing methods use linear_interpolation along each mesh edge to find isosurface intersections in contrast our method determines intersections by performing barycentric interpolation over diamonds formed by the tetrahedra incident to each edge our method produces smoother more accurate isosurfaces additionally interpolating over diamonds rather than linearly interpolating edge endpoints enables us to identify up to two isosurface intersections per edge this paper details how our new technique extracts isopoints and presents a simple connection strategy for forming a triangle mesh isosurface
the evacuation of buildings in the event of a fire requires careful planning of ventilation and evacuation routes during early architectural design stages different designs are evaluated by simulating smoke propagation using computational_fluid_dynamics_(cfd) visibility plays a decisive role in finding the nearest fire exit this paper presents real-time volume_rendering of transient smoke propagation conforming to standardized visibility distances we visualize time dependent smoke particle concentration on unstructured tetrahedral_meshes using a direct_volume_rendering approach due to the linear transfer_function of the optical model commonly used in fire protection engineering accurate pre-integration of diffuse color across tetrahedra can be carried out with a single 2d texture lookup we reduce rounding errors during frame buffer blending by applying randomized dithering if highaccuracy frame buffers are unavailable on the target platform a simple absorption-based lighting model is evaluated in a preprocessing step using the same rendering approach back-illuminated exit signs are commonly used to indicate the escape route as light emitting objects are visible further than reflective objects the transfer_function in front of illuminated exit signs must be adjusted with a deferred rendering pass
a new close range virtual_reality system is introduced that allows intuitive and immersive user_interaction with computer generated objects a projector with a special spherical lens is combined with a flexible tracked rear projection screen that users hold in their hands unlike normal projectors the spherical lens allows for a 180 degree field of view and nearly infinite depth of focus this allows the user to move the screen around the environment and use it as a virtual "slice" to examine the interior of 3d volumes this provides a concrete correspondence between the virtual representation of the 3d volume and how that volume would actually appear if its real counterpart was sliced open the screen can also be used as a "magic window" to view the mesh of the volume from different angles prior to taking cross sections of it real time rendering of the desired 3d volume or mesh is accomplished using current graphics_hardware additional applications of the system are also discussed
little is known about the cognitive abilities which influence the comprehension of scientific and information_visualizations and what properties of the visualization affect comprehension our goal in this paper is to understand what makes visualizations difficult we address this goal by examining the spatial_ability differences in a diverse population selected for spatial_ability variance for example how is spatial_ability related to visualization comprehension what makes a particular visualization difficult or time intensive for specific groups of subjects in this paper we present the results of an experiment designed to answer these questions fifty-six subjects were tested on a basic visualization task and given standard paper tests of spatial abilities an equal number of males and females were recruited in this study in order to increase spatial_ability variance our results show that high spatial_ability is correlated withaccuracy on our three-dimensional visualization test but not with time high spatial_ability subjects also had less difficulty with object complexity and the hidden properties of an object
existing parallel or remote rendering solutions rely on communicating pixels opengl commands scene-graph changes or application-specific data we propose an intermediate solution based on a set of independent graphics primitives that use hardware shaders to specify their visual appearance compared to an opengl based approach it reduces the complexity of the model by eliminating most fixed function parameters while giving access to the latest functionalities of graphics cards it also suppresses the opengl state machine that creates data dependencies making primitive re-scheduling difficult using a retained-modecommunication protocol transmitting changes between each frame combined with the possibility to use shaders to implement interactive data processing operations instead of sending final colors and geometry we are able to optimize the network load high level information such as bounding volumes is used to setup advanced schemes where primitives are issued in parallel routed according to their visibility merged and re-ordered when received for rendering different optimization algorithms can be efficiently implemented saving network bandwidth or reducing texture switches for instance we present performance results based on two vtk applications a parallel iso-surface_extraction and a parallel volume renderer we compare our approach with chromium results show that our approach leads to significantly better performance and scalability while offering easy access to hardware accelerated rendering algorithms
in this paper a novel high-quality reconstruction scheme is presented although our method is mainly proposed to reconstruct volumetric_data sampled on an optimal body-centered cubic bcc grid it can be easily adapted lo the conventional regular rectilinear grid as well the reconstruction process is decomposed into two steps the first step which is considered to be a preprocessing is a discrete gaussian deconvolution performed only once in the frequency domain afterwards the second step is a spatial-domain convolution with a truncated gaussian kernel which is used to interpolate arbitrary samples for ray casting since the preprocessing is actually a discrete prefiltering we call our technique prefiltered gaussian reconstruction pgr it is shown that the impulse response of pgr well approximates the ideal reconstruction kernel therefore the quality of pgr is much higher than that of previous reconstruction techniques proposed for optimally sampled data which are based on linear and cubic box_splines adapted to the bcc grid concerning the performance pgr is slower than linear box-spline reconstruction but significantly faster than cubic box-spline reconstruction
optimal viewpoint_selection is an important task because it considerably influences the amount of information contained in the 2d projected images of 3d objects and thus dominates their first impressions from a psychological point of view although several methods have been proposed that calculate the optimal positions of viewpoints especially for 3d surface meshes none has been done for solid objects such as volumes this paper presents a new method of locating such optimal viewpoints when visualizing volumes using direct_volume_rendering the major idea behind our method is to decompose an entire volume into a set of feature components and then find a globally optimal viewpoint by finding a compromise between locally optimal viewpoints for the components as the feature components the method employs interval_volumes and their combinations that characterize the topological transitions of isosurfaces according to the scalar field furthermore opacity transfer_functions are also utilized to assign different weights to the decomposed components so that users can emphasize features of specific interest in the volumes several examples of volume datasets together with their optimal positions of viewpoints are exhibited in order to demonstrate that the method can effectively guide naive users to find optimal projections of volumes
as standard volume_rendering is based on an integral in physical space or "coordinate space" it is inherently dependent on the scaling of this space although this dependency is appropriate for the realistic rendering of semitransparent volumetric objects it has several unpleasant consequences for volume_visualization in order to overcome these disadvantages a new variant of the volume_rendering integral is proposed which is defined in data space instead of physical space apart from achieving scale invariance this new method supports the rendering of isosurfaces of uniform opacity and color independently of the local gradient or" the visualized scalar field moreover it reveals certain structures in scalar_fields even with constant transfer_functions furthermore it can be defined as the limit of infinitely many semitransparent isosurfaces and is therefore based on an intuitive and at the same time precise definition in addition to the discussion of these features of scale-invariant volume_rendering efficient adaptations of existing volume_rendering algorithms and extensions for silhouette_enhancement and local_illumination by transmitted light are presented
we present build-by-number a technique for quickly designing architectural structures that can be rendered photorealistically at interactive rates we combine image-based capturing and rendering with procedural modeling techniques to allow the creation of novel structures in the style of real-world structures starting with a simple model recovered from a sparse image set the model is divided into feature regions such as doorways windows and brick these feature regions essentially comprise a mapping from model space to image space and can be recombined to texture a novel model procedural rules for the growth and reorganization of the model are automatically derived to allow for very fast editing and design further the redundancies marked by the feature labeling can be used to perform automatic occlusion replacement and color equalization in the finished scene which is rendered using view-dependent texture_mapping on standard graphics_hardware results using four captured scenes show that a great variety of novel structures can be created very quickly once a captured scene is available and rendered with a degree of realism comparable to the original scene
we describe a new dynamic level-of-detail lod technique that allows real-time rendering of large tetrahedral_meshes unlike approaches that require hierarchies of tetrahedra our approach uses a subset of the faces that compose the mesh no connectivity is used for these faces so our technique eliminates the need for topological information and hierarchical_data structures by operating on a simple set of triangular faces our algorithm allows a robust and straightforward graphics_hardware gpuimplementation because the subset of faces processed can be constrained to arbitrary size interactive rendering is possible for a wide range of data sets and hardware configurations
we introduce an approach to tracking vortex core lines in time-dependent 3d flow_fields which are defined by the parallel vectors approach they build surface_structures in the 4d space-time domain to extract them we introduce two 4d vector_fields which act as feature flow_fields ie their integration gives the vortex core structures as part of this approach we extract and classify local bifurcations of vortex core lines in space-time based on a 4d stream surface integration we provide an algorithm to extract the complete vortex core structure we apply our technique to a number of test data sets
we study the problem of visualizing large networks and develop techniques for effectively abstracting a network and reducing the size to a level that can be clearly viewed our size reduction techniques are based on sampling where only a sample instead of the full network is visualized we propose a randomized notion of "focus" that specifies a part of the network and the degree to which it needs to be magnified visualizing a sample allows our method to overcome the scalability_issues inherent in visualizing massive networks we report some characteristics that frequently occur in large networks and the conditions under which they are preserved when sampling from a network this can be useful in selecting a proper sampling scheme that yields a sample with similar characteristics as the original network our method is built on top of a relational_database thus it can be easily and efficiently implemented using any off-the-shelf database software as a proof of concept we implement our methods and report some of our experiments over the movie database and the connectivity graph of the web
this paper describes an experimental study of three perceptual properties of motion flicker direction and velocity our goal is to understand how to apply these properties to represent data in a visualization environment results from our experiments show that all three properties can encode multiple data values but that minimum visual differences are needed to ensure rapid and accurate target detection flicker must be coherent and must have a cycle length of 120 milliseconds or greater direction must differ by at least 20° and velocity must differ by at least 043° of subtended visual angle we conclude with an overview of how we are applying our results to real-world data and then discuss future work we plan to pursue
traditionally sort-middle is a technique that has been difficult to attain on clusters because of the tight coupling of geometry and rasterization processes on commodity graphics_hardware in this paper we describe theimplementation of a new sort-middle approach for performing immediate-mode_rendering in chromium the chromium rendering system is used extensively to drive multi-projector displays on pc clusters with inexpensive commodity graphics components by default chromium uses a sort-first approach to distribute rendering work to individual nodes in a pc cluster while this sort-first approach works effectively in retained-mode rendering it suffers from various network bottlenecks when rendering in immediate-mode current techniques avoid these bottlenecks by sorting vertex data as a pre-processing step and grouping vertices into specific bounding boxes using chromium's bounding box extension these steps may be expensive especially if the dataset is dynamic in our approach we utilize standard programmable_graphics_hardware and extend standard apis to achieve a separation in the rendering pipeline the pre-processing of vertex data or the grouping of vertices into bounding boxes are not required additionally the amount of opengl state commands transmitted through the network are reduced our results indicate that the approach can attain twice the frame rates as compared to chromium's sort-first approach when rendering in immediate-mode
differential protein expression analysis is one of the main challenges in proteomics it denotes the search for proteins whose encoding genes are differentially expressed under a given experimental setup an important task in this context is to identify the differentially expressed proteins or more generally all proteins present in the sample one of the most promising and recently widely used approaches for protein identification is to cleave proteins into peptides separate the peptides using liquid chromatography and determine the masses of the separated peptides using mass_spectrometry the resulting data needs to be analyzed and matched against protein sequence databases the analysis step is typically done by searching for intensity peaks in a large number of 2d graphs we present an interactive_visualization tool for the exploration of liquid-chromatography/mass-spectrometry data in a 3d space which allows for the understanding of the data in its entirety and a detailed analysis of regions of interest we compute differential expression over the liquid-chromatography/mass-spectrometry domain and embed it visually in our system our exploration tool can treat single liquid-chromatography/mass-spectrometry data sets as well as data acquired using multi-dimensional protein identification technology for efficiency purposes we perform a peak-preserving data resampling and multiresolution hierarchy generation prior to visualization
with the growing size of captured 3d models it has become increasingly important to provide basic efficient processing methods for large unorganized raw surface-sample point data sets in this paper we introduce a novel stream-based and out-of-core point_processing framework the proposed approach processes points in an orderly sequential way by sorting them and sweeping along a spatial dimension the major advantages of this new concept are 1 support of extensible and concatenate local operators called stream operators 2 low main-memory usage and 3 applicability to process very large_data sets out-of-core
the problem of perceptually optimizing complex visualizations is a difficult one involving perceptual as well as aesthetic issues in our experience controlled experiments are quite limited in their ability to uncover interrelationships among visualization parameters and thus may not be the most useful way to develop rules-of-thumb or theory to guide the production of high-quality visualizations in this paper we propose a new experimental approach to optimizing visualization quality that integrates some of the strong points of controlled experiments with methods more suited to investigating complex highly-coupled phenomena we use human-in-the-loop experiments to search through visualization parameter space generating large_databases of rated visualization solutions this is followed by data_mining to extract results such as exemplar visualizations guidelines for producing visualizations and hypotheses about strategies leading to strong visualizations the approach can easily address both perceptual and aesthetic concerns and can handle complex parameter interactions we suggest a genetic_algorithm as a valuable way of guiding the human-in-the-loop search through visualization parameter space we describe our methods for using clustering histogramming principal_component_analysis and neural_networks for data_mining the experimental approach is illustrated with a study of the problem of optimal texturing for viewing layered_surfaces so that both surfaces are maximally observable
diffusion_tensor_imaging dti is an mri-based technique for quantifying water diffusion in living tissue in the white_matter of the brain water diffuses more rapidly along the neuronal axons than in the perpendicular direction by exploiting this phenomenon dti can be used to determine trajectories of fiber bundles or neuronal connections between regions in the brain the resulting bundles can be visualized however the resulting visualizations can be complex and difficult to interpret an effective approach is to pre-determine trajectories from a large number of positions throughout the white_matter full brain fiber_tracking and to offer facilities to aid the user in selecting fiber bundles of interest twofactors are crucial for the use and acceptance of this technique in clinical studies firstly the selection of the bundles by brain experts should be interactive supported by real-time_visualization of the trajectories registered with anatomical mri scans secondly the fiber selections should be reproducible so that different experts will achieve the same results in this paper we present a practical technique for the interactive selection of fiber-bundles using multiple convex objects that is an order of magnitude faster than similar techniques published earlier we also present the results of a clinical study with ten subjects that show that our selection approach is highly reproducible for fractional anisotropy fa calculated over the selected fiber bundles
topological concepts and techniques have been broadly applied in computer_graphics and geometric modeling however the homotopy type of a mapping between two surfaces has not been addressed before in this paper we present a novel solution to the problem of computing continuous maps with different homotopy types between two arbitrary triangle meshes with the same topology inspired by the rich theory of topology as well as the existing body of work on surface mapping our newly-developed mapping techniques are both fundamental and unique offering many attractive advantages first our method allows the user to change the homotopy type or global structure of the mapping with minimal intervention moreover to locally affect shape correspondence we articulate a new technique that robustly satisfies hard feature constraints without the use of heuristics to ensure validity in addition to acting as a useful tool for computer_graphics applications our method can be used as a rigorous and practical mechanism for the visualization of abstract topological concepts such as homotopy type of surface mappings homology basis fundamental domain and universal covering space at the core of our algorithm is a procedure for computing the canonical homology basis and using it as a common cut graph for any surface with the same topology we demonstrate our results by applying our algorithm to shape_morphing in this paper
illustrations play a major role in the education process whether used to teach a surgical or radiologic procedure to illustrate normal or aberrant anatomy or to explain the functioning of a technical device illustration significantly impacts learning although many specimens are readily available as volumetric_data sets particularly in medicine illustrations are commonly produced manually as static images in a time-consuming process our goal is to create a fully dynamic three-dimensional illustration environment which directly operates on volume data single images have the aesthetic appeal of traditional illustrations but can be interactively altered and explored in this paper we present methods to realize such a system which combines artistic visual styles and expressive visualization_techniques we introduce a novel concept for direct multi-object volume_visualization which allows control of the appearance of inter-penetrating objects via two-dimensional transfer_functions furthermore a unifying approach to efficiently integrate many non-photorealistic_rendering models is presented we discuss several illustrative concepts which can be realized by combining cutaways ghosting and selective deformation finally we also propose a simple interface to specify objects of interest through three-dimensional volumetric painting all presented methods are integrated into volumeshop an interactive hardware-accelerated application for direct volume_illustration
we present a new method for guiding virtual colonoscopic navigation and registration by using teniae coli as anatomical landmarks as most existing protocols require a patient to be scanned in both supine and prone positions to increase sensitivity in detecting colonic polyps reference and registration between scans are necessary however the conventional centerline approach generating only the longitudinal distance along the colon lacks the necessary orientation information to synchronize the virtual navigation cameras in both scanned positions in this paper we describe a semi-automatic method to detect teniae coli from a colonic surface model reconstructed from ct_colonography teniae coli are three bands of longitudinal smooth muscle on the surface of the colon they form a triple helix structure from the appendix to the sigmoid colon and are ideal references for virtual navigation our method was applied to 3 patients resulting in 6 data sets supine and prone scans the detected teniae coli matched well with our visual inspection in addition we demonstrate that polyps visible on both scans can be located and matched more efficiently with the aid of a teniae coli guided navigationimplementation
high resolution volumes require high precision compositing to preserve detailed structures this is even more desirable for volumes with high_dynamic_range values after the high precision intermediate image has been computed simply rounding up pixel values to regular display scales loses the computed details in this paper we present a novel high_dynamic_range volume_visualization method for rendering volume data with both high spatial and intensity resolutions our method performs high precision volume_rendering followed by dynamic tone_mapping to preserve details on regular display devices by leveraging available high_dynamic_range image display_algorithms this dynamic tone_mapping can be automatically adjusted to enhance selected features for the final display we also present a novel transfer_function_design interface with nonlinear magnification of the density range and logarithmic scaling of the color/opacity range to facilitate high_dynamic_range volume_visualization by leveraging modern commodity graphics_hardware and out-of-core acceleration our system can produce an effective visualization of huge volume data
the genus of a knot or link can be defined via seifert_surfaces a seifert surface of a knot or link is an oriented surface whose boundary coincides with that knot or link schematic images of these surfaces are shown in every text book on knot_theory but from these it is hard to understand their shape and structure in this paper the visualization of such surfaces is discussed a method is presented to produce different styles of surfaces for knots and links starting from the so-called braid representation also it is shown how closed oriented surfaces can be generated in which the knot is embedded such that the knot subdivides the surface into two parts these closed surfaces provide a direct visualization of the genus of a knot
displays combining both 2d and 3d views have been shown to support higher performance on certain visualization tasks however it is not clear how best to arrange a combination of 2d and 3d views spatially in a display in this study we analyzed the eyegaze strategies of participants using two arrangements of 2d and 3d views to estimate the relative position of objects in a 3d scene our results show that the 3d view was used significantly more often than individual 2d views in both displays indicating the importance of the 3d view for successful task completion however viewing patterns were significantly different between the two displays transitions through centrally-placed views were always more frequent and users avoided saccades between views that were far apart although the change in viewing strategy did not result in significant performance differences error analysis indicates that a 3d overview in the center may reduce the number of serious errors compared to a 3d overview placed off to the side
in this paper we describe a methodology andimplementation for interactive dataset_traversal using motion-controlled transfer_functions dataset_traversal here refers lo the process of translating a transfer_function along a specific path in scientific_visualization it is often necessary to manipulate transfer_functions in order to visualize datasets more effectively this manipulation of transfer_functions is usually performed globally ie a new transfer_function is applied to the entire dataset our approach allows one to locally manipulate transfer_functions while controling its movement along a traversal path the method we propose allows the user to select a traversal path within the dataset based on the shape of the volumetric model and manipulate a transfer_function along this path examples of dataset_traversal include the animation of transfer_functions along a pre-defined path the simulation of flow in vascular structures and the visualization of convoluted shapes for example this type of traversal is often used in medical illustration to highlight flow in blood_vessels we present an interactiveimplementation of our method using graphics_hardware based on the decomposition of the volume we show examples of our approach using a variety of volumetric_datasets and we also demonstrate that with our novel decomposition the rendering process is faster
curve-skeletons are a 1d subset of the medial_surface of a 3d object and are useful for many visualization tasks including virtual navigation reduced-model formulation visualization improvement mesh repair animation etc there are many algorithms in the literature describing extraction methodologies for different applications however it is unclear how general and robust they are in this paper we provide an overview of many curve-skeleton applications and compile a set of desired properties of such representations we also give a taxonomy of methods and analyze the advantages and drawbacks of each class of algorithms
the size and resolution of volume datasets in science and medicine are increasing at a rate much greater than the resolution of the screens used to view them this limits the amount of data that can be viewed simultaneously potentially leading to a loss of overall context of the data when the user views or zooms into a particular area of interest we propose a focus+context framework that uses various standard and advanced magnification lens rendering techniques to magnify the features of interest while compressing the remaining volume regions without clipping them away completely some of these lenses can be interactively configured by the user to specify the desired magnification patterns while others are feature-adaptive all our lenses are accelerated on the gpu they allow the user to interactively manage the available screen area dedicating more area to the more resolution-important features
techniques in numerical simulation such as the finite_element_method depend on basis_functions for approximating the geometry and variation of the solution over discrete regions of a domain existing visualization_systems can visualize these basis_functions if they are linear or for a small set of simple non-linear bases however newer numerical approaches often use basis_functions of elevated and mixed order or complex form hence existing visualization_systems cannot directly process them in this paper we describe an approach that supports automatic adaptive tessellation of general basis_functions using a flexible and extensible software architecture in conjunction with an on demand edge-based recursive subdivision algorithm the framework supports the use of functions implemented in external simulation packages eliminating the need to reimplement the bases within the visualization system we demonstrate our method on several examples and have implemented the framework in the open-source visualization system vtk
line primitives are a very powerful visual attribute used for scientific_visualization and in particular for 3d vector-field visualization we extend the basic line primitives with additional visual attributes including color line width texture and orientation to implement the visual attributes we represent the stylized line primitives as generalized cylinders one important contribution of our work is an efficient rendering algorithm for stylized lines which is hybrid in the sense that it uses both cpu and gpu based rendering we improve the depth perception with a shadow algorithm we present several applications for the visualization with stylized lines among which are the visualizations of 3d vector_fields and molecular structures
indirect_volume_rendering is a widespread method for the display of volume datasets it is based on the extraction of polygonal iso-surfaces from volumetric_data which are then rendered using conventional rasterization methods whereas this rendering approach is fast and relatively easy to implement it cannot easily provide an understandable display of structures occluded by the directly visible iso-surface simple approaches like alpha-blending for transparency when drawing the iso-surface often generate a visually complex output which is difficult to interpret moreover such methods can significantly increase the computational complexity of the rendering process in this paper we therefore propose a new approach for the illustrative indirect rendering of volume data in real-time this algorithm emphasizes the silhouette of objects represented by the iso-surface additionally shading intensities on objects are reproduced with a monochrome hatching technique using a specially designed two-pass rendering process structures behind the front layer of the iso-surface are automatically extracted with a depth peeling method the shapes of these hidden structures are also displayed as silhouette outlines as an additional option the geometry of explicitly specified inner objects can be displayed with constant translucency although these inner objects always remain visible a specific shading and depth attenuation method is used to convey the depth relationships we describe theimplementation of the algorithm which exploits the programmability of state-of-the-art graphics processing units gpus the algorithm described in this paper does not require any preprocessing of the input data or a manual definition of inner structures since the presented method works on iso-surfaces which are stored as polygonal datasets it can also be applied to other types of polygonal models
in this work we present a hardware-accelerated direct_volume_rendering system for visualizing multivariate wave functions in semiconducting quantum dot qd simulations the simulation data contains the probability density values of multiple electron orbitals for up to tens of millions of atoms computed by the nemo3-d quantum device simulator software run on large-scale cluster architectures these atoms form two interpenetrating crystalline face centered cubic lattices fcc where each fcc cell comprises the eight corners of a cubic cell and six additional face centers we have developed compact representation techniques for the fcc lattice within pc graphics_hardware texture memory hardware-accelerated linear and cubic reconstruction schemes and new multi-field rendering techniques utilizing logarithmic scale transfer_functions our system also enables the user to drill down through the simulation data and execute statistical queries using general-purpose computing on the gpu gpgpu
doppler_radars are useful facilities for weather forecasting the data sampled by using doppler_radars are used to measure the distributions and densities of rain drops snow crystals hail stones or even insects in the atmosphere in this paper we propose to build up a graphics-based software system for visualizing doppler_radar data in the system the reflectivity data gathered by using doppler_radars are post-processed to generate virtual cloud images which reveal the densities of precipitation in the air an optical_flow based method is adopted to compute the velocities of clouds advected by winds therefore the movement of clouds is depicted the cloud velocities are also used to interpolate reflectivities for arbitrary time steps therefore the reflectivities at any time can be produced our system composes of three stages at the first stage the raw radar data are re-sampled and filtered to create a multiple resolution data structure based on a pyramid structure at the second stage a numeric method is employed to compute cloud velocities in the air and to interpolate radar reflectivity data at given time steps the radar reflectivity data and cloud velocities are displayed at the last stage the reflectivities are rendered by using splatting_methods to produce semi-transparent cloud images two kinds of media are created for analyzing the reflectivity data the first kind media consists of a group of still images of clouds which displays the distribution and density of water in the air the second type media is a short animation of cloud images to show the formation and movement of the clouds to show the advection of clouds the cloud velocities are displayed by using two dimensional images in these images the velocities are represented by arrows and superimposed on cloud images to enhance image quality gradients and diffusion of the radar data are computed and used in the rendering process therefore the cloud structures are better portrayed in order to achieve interactive_visualization our system is also comprised with a view-dependent_visualization module the radar data at far distance are rendered in lower resolutions while the data closer to the eye position is rendered in details
we develop the first approach tor interactive_volume_visualization based on a sophisticated rendering method of shear-warp type wavelet data encoding techniques and a trivariate spline model which has been introduced recently as a first step of our algorithm we apply standard wavelet expansions to represent and decimate the given gridded three-dimensional data based on this data encoding we give a sophisticated version of the shear-warp based volume_rendering method our new algorithm visits each voxel only once taking advantage of the particular data organization of octrees in addition the hierarchies of the data guide the local reconstruction of the quadratic super-spline models which we apply as a pure visualization tool the low total degree of the polynomial pieces allows to numerically approximate the volume_rendering integral efficiently since the coefficients of the splines are almost immediately available from the given data bernstein-bezier techniques can be fully employed in our algorithms in this way we demonstrate that these models can be successfully applied to full volume_rendering of hierarchically organized data our computational results show that even when hierarchical approximations are used the new approach leads to almost artifact-free visualizations of high_quality for complicated and noise-contaminated volume data sets while the computational effort is considerable low ie our currentimplementation yields 1-2 frames per second for parallel perspective rendering a 2563 volume data set using simple opacity transfer_functions in a 5122 view-port
we present a visual_analysis and exploration of fluid flow through a cooling_jacket engineers invest a large amount of time and serious effort to optimize the flow through this engine component because of its important role in transferring heat away from the engine block in this study we examine the design goals that engineers apply in order to construct an ideal-as-possible cooling_jacket geometry and use a broad range of visualization tools in order to analyze explore and present the results we systematically employ direct geometric and texture-based flow_visualization_techniques as well as automatic feature_extraction and interactive feature-based methodology and we discuss the relative advantages and disadvantages of these approaches as well as the challenges both technical and perceptual with this application the result is a feature-rich state-of-the-art flow_visualization analysis applied to an important and complex data set from real-world computational fluid dynamics simulations
we describe opengl multipipe sdk mpk a toolkit for scalable parallel_rendering based on opengl mpk provides a uniform application programming interface api to manage scalable graphics applications across many different graphics subsystems mpk-based applications run seamlessly from single-processor single-pipe desktop systems to large multi-processor multipipe scalable graphics systems the application is oblivious of the system configuration which can be specified through a configuration file at run time to scale application performance mpk uses a decomposition system that supports different modes for task partitioning and implements optimized cpu-based composition algorithms mpk also provides a customizable image composition interface which can be used to apply post-processing algorithms on raw pixel data obtained from executing sub-tasks on multiple graphics pipes in parallel this can be used to implement parallel versions of any cpu-based algorithm not necessarily used for rendering in this paper we motivate the need for a scalable graphics api and discuss the architecture of mpk we present mpk's graphics configuration interface introduce the notion of compound-based decomposition schemes and describe ourimplementation we present some results from our work on a couple of target system architectures and conclude with future directions of research in this area
we present a new particle_tracing approach for the simulation of mid- and high-frequency sound inspired by the photorealism obtained by methods like photon_mapping we develop a similar method for the physical_simulation of sound within rooms for given source and listener positions our method computes a finite-response filter accounting for the different reflections at various surfaces with frequency-dependent absorption coefficients convoluting this filter with an anechoic input signal reproduces a realistic aural impression of the simulated room we do not consider diffraction effects due to low frequencies since these can be better computed by finite_elements our method allows the visualization of a wave front propagation using color-coded blobs traversing the paths of individual phonons
we introduce a technique to visualize the gradual evolutionary change of the shapes of living things as a morph between known three-dimensional shapes given geometric computer models of anatomical shapes for some collection of specimens - here the skulls of the some of the extant members of a family of monkeys - an evolutionary tree for the group implies a hypothesis about the way in which the shape changed through time we use a statistical model which expresses the value of some continuous variable at an internal point in the tree as a weighted average of the values at the leaves the framework of geometric morphometrics can then be used to define a shape-space based on the correspondences of landmark points on the surfaces within which these weighted averages can be realized as actual surfaces our software provides tools for performing and visualizing such an analysis in three dimensions beginning with laser range scans of crania we use our landmark editor to interactively place landmark points on the surface we use these to compute a "tree-morph" that smoothly interpolates the shapes across the tree each intermediate shape in the morph is a linear combination of all of the input surfaces we create a surface model for an intermediate shape by warping all the input meshes towards the correct shape and then blending them together to do the blending we compute a weighted average of their associated trivariate distance functions and then extract a surface from the resulting function we implement this idea using the squared distance function rather than the usual signed distance function in a novel way
we present the application of hardware accelerated volume_rendering algorithms to the simulation of radiographs as an aid to scientists designing experiments validating simulation codes and understanding experimental data the techniques presented take advantage of 32-bit floating point texture capabilities to obtain solutions to the radiative transport equation for x-rays the hardware accelerated solutions are accurate enough to enable scientists to explore the experimental design space with greater efficiency than the methods currently in use an unsorted hexahedron projection algorithm is presented for curvilinear hexahedral meshes that produces simulated radiographs in the absorption-only regime a sorted tetrahedral projection algorithm is presented that simulates radiographs of emissive materials we apply the tetrahedral projection algorithm to the simulation of experimental diagnostics for inertial confinement fusion experiments on a laser at the university of rochester
this paper presents a novel method for computing simulated x-ray images or drrs digitally_reconstructed_radiographs of tetrahedral_meshes with higher-order attenuation functions drrs are commonly used in computer assisted surgery cas with the attenuation function consisting of a voxelized ct study which is viewed from different directions our application of drrs is in intra-operative "2d-3d" registration ie finding the pose of the ct dataset given a small number of patient radiographs we register 2d patient images with a statistical tetrahedral model which encodes the ct intensity numbers as bernstein polynomials and includes knowledge about typical shape variation modes the unstructured grid is more suitable for applying deformations than a rectilinear grid and the higher-order polynomials provide a better approximation of the actual density than constant or linear models the infra-operative environment demands a fast method for creating the drrs which we present here we demonstrate this application through the creation and use of a deformable atlas of human pelvis bones compared with other works on rendering unstructured_grids the main contributions of this work are 1 simple and perspective-correct interpolation of the thickness of a tetrahedral cell 2 simple and perspective-correct interpolation of front and back barycentric_coordinates with respect to the cell 3 computing line integrals of higher-order functions 4 capability of applying shape deformations and variations in the attenuation function without significant performance loss the method does not depend on for pre-integration and does not require depth-sorting of the visualized cells we present imaging and timing results of implementing the algorithm and discuss the impact of using higher-order functions on the quality of the result and the performance
this paper describes the adaptation and evaluation of existing nested-surface visualization_techniques for the problem of displaying intersecting_surfaces for this work we collaborated with a neurosurgeon who is comparing multiple tumor segmentations with the goal of increasing the segmentationaccuracy and reliability a second collaborator a physicist aims to validate geometric models of specimens against atomic-force microscope images of actual specimens these collaborators are interested in comparing both surface shape and inter-surface distances many commonly employed techniques for visually comparing multiple surfaces side-by-side wireframe colormaps uniform translucence do not simultaneously convey inter-surface distance and the shapes of two or more surfaces this paper describes a simple geometric partitioning of intersecting_surfaces that enables the application of existing nested-surface techniques such as texture-modulated translucent rendering of exteriors to a broader range of visualization problems three user studies investigate the performance of existing techniques and a new shadow-casting glyph technique the results of the first user_study show that texture glyphs on partitioned intersecting_surfaces can convey inter-surface distance better than directly mapping distance to a red-gray-blue color scale on a single surface the results of the second study show similar results for conveying local surface orientation the results of the third user_study show that adding cast shadows to texture glyphs can increase the understanding of inter-surface distance in static images but can be overpowered by the shape cues from a simple rocking motion
in this paper we present a volume_roaming system dedicated to oil_and_gas_exploration our system combines probe-based volume_rendering with data processing and computing the daily oil production and the estimation of the world proven-reserves directly affect the barrel price and have a strong impact on the economy among others production and correct estimation are linked to theaccuracy of the sub-surface model used for predicting oil reservoirs shape and size geoscientists build this model from the interpretation of seismic_data ie 3d images of the subsurface obtained from geophysical surveys our system couples visualization and data processing for the interpretation of seismic_data it is based on volume_roaming along with efficient volume paging to manipulate the multi-gigabyte data sets commonly acquired during seismic surveys our volume_rendering lenses implement high_quality pre-integrated volume_rendering with accurate lighting they use a generic multi-modal volume_rendering system that blends several volumes in the spirit of the "stencil" paradigm used in 2d painting programs in addition our system can interactively display non-polygonal isosurfaces painted with an attribute beside the visualization algorithms automatic extraction of local features of the subsurface model also take full advantage of the volume paging
we present a multimodal paradigm for exploring topological surfaces embedded in four dimensions we exploit haptic methods in particular to overcome the intrinsic limitations of 3d graphics images and 3d physical models the basic problem is that just as 2d shadows of 3d curves lose structure where lines cross 3d graphics projections of smooth 4d topological surfaces are interrupted where one surface intersects another furthermore if one attempts to trace real knotted ropes or a plastic models of self-intersecting_surfaces with a fingertip one inevitably collides with parts of the physical artifact in this work we exploit the free motion of a computer-based haptic probe to support a continuous motion that follows the local continuity of the object being explored for our principal test case of 4d-embedded surfaces projected to 3d this permits us to follow the full local continuity of the surface as though in fact we were touching an actual 4d object we exploit additional sensory cues to provide supplementary or redundant information for example we can use audio tags to note the relative 4d depth of illusory 3d surface intersections produced by projection from 4d as well as providing automated refinement of the tactile exploration path to eliminate jitter and snagging resulting in a much cleaner exploratory motion than a bare uncorrected motion visual enhancements provide still further improvement to the feedback by opening a view-direction-defined cutaway into the interior of the 3d surface projection we allow the viewer to keep the haptic probe continuously in view as it traverses any touchable part of the object finally we extend the static tactile exploration framework using a dynamic mode that links each stylus motion to a change in orientation that creates at each instant a maximal-area screen projection of a neighborhood of the current point of interest this minimizes 4d distortion and permits true metric sizes to be deduced locally at any point all these methods combine to reveal the full richness of the complex spatial relationships of the target shapes and to overcome many expected perceptual limitations in 4d visualization
vistrails is a new system that enables interactive multiple-view visualizations by simplifying the creation and maintenance of visualization pipelines and by optimizing their execution it provides a general infrastructure that can be combined with existing visualization_systems and libraries a key component of vistrails is the visualization trail vistrail a formal specification of a pipeline unlike existing dataflow-based systems in vistrails there is a clear separation between the specification of a pipeline and its execution instances this separation enables powerful scripting capabilities and provides a scalable mechanism for generating a large number of visualizations vistrails also leverages the vistrail specification to identify and avoid redundant operations this optimization is especially useful while exploring multiple visualizations when variations of the same pipeline need to be executed substantial speedups can be obtained by caching the results of overlapping subsequences of the pipelines in this paper we describe the design andimplementation of vistrails and show its effectiveness in different application scenarios
the study of physical models for knots has recently received much interest in the mathematics community in this paper we consider the ropelength model which considers knots tied in an idealized rope this model is interesting in pure mathematics and has been applied to the study of a variety of problems in the natural sciences as well modeling and visualizing the tightening of knots in this idealized rope poses some interesting challenges in computer_graphics in particular self-contact in a deformable rope model is a difficult problem which cannot be handled by standard techniques in this paper we describe a solution based on reformulating the contact problem and using constrained-gradient techniques from nonlinear_optimization the resulting animations reveal new properties of the tightening flow and provide new insights into the geometric structure of tight_knots and links
in a visualization of a three-dimensional dataset the insights gained are dependent on what is occluded and what is not suggestion of interesting viewpoints can improve both the speed and efficiency of data understanding this paper presents a view selection method designed for volume_rendering it can be used to find informative views for a given scene or to find a minimal set of representative views which capture the entire scene it becomes particularly useful when the visualization process is non-interactive - for example when visualizing large_datasets or time-varying sequences we introduce a viewpoint "goodness" measure based on the formulation of entropy from information_theory the measure takes into account the transfer_function the data distribution and the visibility of the voxels combined with viewpoint properties like view-likelihood and view-stability this technique can be used as a guide which suggests "interesting" viewpoints for further exploration domain knowledge is incorporated into the algorithm via an importance transfer_function or volume this allows users to obtain view selection behaviors tailored to their specific situations we generate a view_space_partitioning and select one representative view for each partition together this set of views encapsulates the "interesting" and distinct views of the data viewpoints in this set can be used as starting points for interactive_exploration of the data thus reducing the human effort in visualization in non-interactive situations such a set can be used as a representative visualization of the dataset from all directions
the multi triangulation framework mt is a very general approach for managing adaptive resolution in triangle meshes the key idea is arranging mesh fragments at different resolution in a directed_acyclic_graph dag which encodes the dependencies between fragments thereby encompassing a wide class of multiresolution approaches that use hierarchies or dags with predefined topology on current architectures the classic mt is however unfit for real-time rendering since dag traversal costs vastly dominate raw rendering costs in this paper we redesign the mt framework in a gpu friendly fashion moving its granularity from triangles to precomputed optimized triangle patches the patches can be conveniently tri-stripped and stored in secondary memory to be loaded on demand ready to be sent to the gpu using preferential paths in this manner central memory only contains the dag structure and cpu workload becomes negligible the major contributions of this work are a new out-of-core multiresolution framework that just like the mt encompasses a wide class of multiresolution structures a robust and elegant way to build a well conditioned mt dag by introducing the concept of v-partitions that can encompass various state of the art multiresolution algorithms an efficient multithreaded rendering engine and a general subsystem for the external memory processing and simplification of huge meshes
scientific_illustrations use accepted conventions and methodologies to effectively convey object properties and improve our understanding we present a method to illustrate volume datasets by emulating example illustrations as with technical illustrations our volume_illustrations more clearly delineate objects enrich details and artistically visualize volume datasets for both color and scalar 3d volumes we have developed an automatic color_transfer method based on the clustering and similarities in the example illustrations and volume sources as an extension to 2d wang tiles we provide a new general texture_synthesis method for wang_cubes that solves the edge discontinuity problem we have developed a 2d illustrative slice viewer and a gpu-based direct_volume_rendering system that uses these non-periodic 3d textures to generate illustrative results similar to the 2d examples both applications simulate scientific_illustrations to provide more information than the original data and visualize objects more effectively while only requiring simple user_interaction
it is a challenging task to visualize the behavior of time-dependent 3d vector_fields most of the time an overview of unsteady fields is provided via animations but unfortunately animations provide only transient impressions of momentary flow in this paper we present two approaches to visualizetime_varying fields with fixed geometry path_lines and streak lines represent such a steady visualization of unsteady_vector_fields but because of occlusion and visual_clutter it is useless to draw them all over the spatial domain a selection is needed we show how bundles of streak lines and path_lines running at different times through one point in space like through an eyelet yield an insightful visualization of flow structure "eyelet lines" to provide a more intuitive and appealing visualization we also explain how to construct a surface from these lines as second approach we use a simple measurement of local changes of a field over time to determine regions with strong changes we visualize these regions with isosurfaces to give an overview of the activity in the dataset finally we use the regions as a guide for placing eyelets
this paper presents an approach to extracting and classifying higher order critical_points of 3d vector_fields to do so we place a closed convex surface s around the area of interest then we show that the complete 3d classification of a critical point into areas of different flow behavior is equivalent to extracting the topological skeleton of an appropriate 2d vector field on s if each critical point is equipped with an additional bit of information out of this skeleton we create an icon which replaces the complete topological structure inside s for the visualization we apply our method to find a simplified visual representation of clusters of critical_points leading to expressive visualizations of topologically complex 3d vector_fields
analysis of degenerate_tensors is a fundamental step in finding the topological structures and separatrices in tensor_fields previous work in this area have been limited to analyzing symmetric second order tensor_fields in this paper we extend the topological_analysis to 2d general asymmetric second order tensor_fields we show that it is not sufficient to define degeneracies based on eigenvalues alone but one must also include the eigenvectors in the analysis we also study the behavior of these eigenvectors as they cross from one topological region into another
in this article we describe stress nets a technique for exploring 2d tensor_fields our method allows a user to examine simultaneously the tensors' eigenvectors both major and minor as well as scalar-valued tensor_invariants by avoiding noise-advection techniques we are able to display both principal directions of the tensor_field as well as the derived scalars without cluttering the display we present a cpu-onlyimplementation of stress nets as well as a hybrid cpu/gpu approach and discuss the relative strengths and weaknesses of each stress nets have been used as part of an investigation into crack_propagation they were used to display the directions of maximum shear in a slab of material under tension as well as the magnitude of the shear forces acting on each point our methods allowed users to find new features in the data that were not visible on standard plots of tensor_invariants these features disagree with commonly accepted analytical crack_propagation solutions and have sparked renewed investigation though developed for a materials mechanics problem our method applies equally well to any 2d tensor_field having unique characteristic directions
for the rendering of vector and tensor_fields several texture-based volumetric rendering methods were presented in recent years while they have indisputable merits the classical vertex-based rendering of integral curves has the advantage of better zooming capabilities as it is not bound to a fixed resolution it has been shown that lighting can improve spatial_perception of lines significantly especially if lines appear in bundles although opengl does not directly support lighting of lines fast rendering of illuminated lines can be achieved by using basic texture_mapping this existing technique is based on a maximum principle which gives a good approximation of specular reflection diffuse reflection however is essentially limited to bidirectional lights at infinity we show how the realism can be further increased by improving diffuse reflection we present simplified expressions for the phong/blinn lighting of infinitesimally thin cylindrical tubes based on these we propose a fast rendering technique with diffuse and specular reflection for orthographic and perspective views and for multiple local and infinite lights the method requires commonly available programmable vertex and fragment shaders and only two-dimensional lookup textures field_lines graphics_hardware illumination texture_mapping vector_field_visualization
tensors occur in many areas of science and engineering especially they are used to describe charge mass and energy transport ie electrical conductivity tensor diffusion tensor thermal conduction tensor resp if the locale transport pattern is complicated usual second order tensor representation is not sufficient so far there are no appropriate visualization methods for this case we point out similarities of symmetric higher order tensors and spherical_harmonics a spherical harmonic representation is used to improve tensor_glyphs this paper unites the definition of streamlines and tensor_lines and generalizes tensor_lines to those applications where second order tensors representations fail the algorithm is tested on the tractography problem in diffusion tensor magnetic_resonance_imaging dt-mri and improved for this special application
the study of stress and strains in soils and structures solids help us gain a better understanding of events such as failure of bridges dams and buildings or accumulated stresses and strains in geological subduction zones that could trigger earthquakes and subsequently tsunamis in such domains the key feature of interest is the location and orientation of maximal shearing planes this paper describes a method that highlights this feature in stress_tensor_fields it uses a plane-in-a-box glyph which provides a global perspective of shearing planes based on local analysis of tensors the analysis can be performed over the entire domain or the user can interactively specify where to introduce these glyphs alternatively they can also be placed depending on the threshold level of several physical relevant parameters such as double_couple and compensated_linear_vector_dipole both methods are tested on stress_tensor_fields from geomechanics
in this paper we present a topological approach for simplifying continuous functions defined on volumetric domains we introduce two atomic operations that remove pairs of critical_points of the function and design a combinatorial algorithm that simplifies the morse-smale_complex by repeated application of these operations the morse-smale_complex is a topological data structure that provides a compact representation of gradient flow between critical_points of a function critical_points paired by the morse-smale_complex identify topological features and their importance the simplification procedure leaves important critical_points untouched and is therefore useful for extracting desirable features we also present a visualization of the simplified topology
intelligence_analysis often involves the task of gathering information about an organization knowledge about individuals in an organization and their relationships often represented as a hierarchical organization chart is crucial for understanding the organization however it is difficult for intelligence analysts to follow all individuals in an organization existing hierarchy_visualizations have largely focused on the visualization of fixed structures and can not effectively depict the evolution of a hierarchy over time we introduce timetree a novel visualization tool designed to enable exploration of a changing hierarchy timetree enables analysts to navigate the history of an organization identify events associated with a specific entity visualized on a timeslider and explore an aggregate view of an individual's career path a careertree we demonstrate the utility of timetree by investigating a set of scenarios developed by an expert intelligence analyst the scenarios are evaluated using a real dataset composed of eighteen thousand career events from more than eight thousand individuals insights gained from this analysis are presented doi tree timetree organizational_chart timeseries data tree_visualization visual_analytics
spatio-temporal relationships among features extracted from temporally-varying scientific datasets can provide useful information about the evolution of an individual feature and its interactions with other features however extracting such useful relationships without user guidance is cumbersome and often an error prone process in this paper we present a visual_analysis system that interactively discovers such relationships from the trajectories of derived features we describe analysis algorithms to derive various spatial and spatio-temporal relationships a visual interface is presented using which the user can interactively select spatial and temporal extents to guide the knowledge_discovery process we show the usefulness of our proposed algorithms on datasets originating from computational fluid dynamics we also demonstrate how the derived relationships can help in explaining the occurrence of critical_events like merging and bifurcation of the vortices d design tools and techniques user_interfaces feature_extraction h database applications data_mining h database applications scientific databases knowledge_discovery scientific_analytics spatio-temporal_predicates trajectory_analysis visual_analytics
decade scale oceanic phenomena like el_nino are correlated with weather anomalies all over the globe only by understanding the events that produced the climatic conditions in the past will it be possible to forecast abrupt climate changes and prevent disastrous consequences for human beings and their environment paleoceanography research is a collaborative effort that requires the analysis of paleo time-series which are obtained from a number of independent techniques and instruments and produced by a variety of different researchers and/or laboratories the complexity of these phenomena that consist of massive dynamic and often conflicting data can only be faced by means of analytical reasoning supported by a highly interactive visual interface this paper presents an interactive_visual_analysis environment for paleoceanography that permits to gain insight into the paleodata and allow the control and steering of the analytical methods involved in the reconstruction of the climatic conditions of the past h user/machine_systems human information processingÂ¿visual_analytics infovis j physical sciences and engineering earth and atmospheric sciencesÂ¿applications exploratory_analysis multiple linked_views parallel_coordinates
geotime and nspace are new analysis tools that provide innovative visual analytic capabilities this paper uses an epidemiology analysis scenario to illustrate and discuss these new investigative methods and techniques in addition this case_study is an exploration and demonstration of the analytical synergy achieved by combining geotime's geo-temporal_analysis capabilities with the rapid information triage scanning and sense-making provided by nspace a fictional analyst works through the scenario from the initial brainstorming through to a final collaboration and report with the efficient knowledge acquisition and insights into large amounts of documents there is more time for the analyst to reason about the problem and imagine ways to mitigate threats the use of both nspace and geotime initiated a synergistic exchange of ideas where hypotheses generated in either software tool could be cross-referenced refuted and supported by the other tool geo-spatial information systems human information interaction information_visualization sense making temporal analysis user centered design visual_analytics
understanding the space and time characteristics of human interaction in complex social_networks is a critical component of visual tools for intelligence_analysis consumer behavior analysis and human geography visual identification and comparison of patterns of recurring events is an essential feature of such tools in this paper we describe a tool for exploring hotel visitation patterns in and around rebersburg pennsylvania from 1898-1900 the tool uses a wrapping spreadsheet technique called reruns to display cyclic patterns of geographic events in multiple overlapping natural and artificial calendars implemented as an improvise visualization the tool is in active development through a iterative process of data collection hypothesis design discovery and evaluation in close collaboration with historical geographers several discoveries have inspired ongoing data collection and plans to expand exploration to include historic weather records and railroad schedules distributed online evaluations of usability and usefulness have resulted in numerous feature and design recommendations d software_engineering design tools and techniquesÂ¿user_interfaces geo visualization h information systems information interfaces and presentationÂ¿user_interfaces coordinated multiple_views exploratory_visualization historical_geography travel pattern analysis
visualizing and analyzing social_networks is a challenging problem that has been receiving growing attention an important first step before analysis can begin is ensuring that the data is accurate a common data quality problem is that the data may inadvertently contain several distinct references to the same underlying entity the process of reconciling these references is called entity-resolution d-dupe is an interactive tool that combines data_mining algorithms for entity resolution with a task-specific network_visualization users cope with complexity of cleaning large networks by focusing on a small subnetwork containing a potential duplicate pair the subnetwork highlights relationships in the social network making the common relationships easy to visually identify d-dupe users resolve ambiguities either by merging nodes or by marking them distinct the entity resolution process is iterative as pairs of nodes are resolved additional duplicates may be revealed therefore resolution decisions are often chained together we give examples of how users can flexibly apply sequences of actions to produce a high_quality entity resolution result we illustrate and evaluate the benefits of d-dupe on three bibliographic collections two of the datasets had already been cleaned and therefore should not have contained duplicates despite this fact many duplicates were rapidly identified using d-dupe's unique combination of entity resolution algorithms within a task-specific visual interface data_cleaning_and_integration h information systems database applicationsÂ¿data_mining h information interfaces and presentation user_interfacesÂ¿user-centered_design user_interfaces visual_analytics visual_data_mining
a visual investigation involves both the examination of existing information and the synthesis of new analytic knowledge this is a progressive process in which newly synthesized knowledge becomes the foundation for future discovery in this paper we present a novel system supporting interactive progressive synthesis of analytic knowledge here we use the term "analytic knowledge" to refer to concepts that a user derives from existing data along with the evidence supporting such concepts unlike existing visual analytic-tools which typically support only exploration of existing information our system offers two unique features first we support user-system cooperative visual synthesis of analytic knowledge from existing data specifically users can visually define new concepts by annotating existing information and refine partially formed concepts by linking additional evidence or manipulating related concepts in response to user actions our system can automatically manage the evolving corpus of synthesized knowledge and its corresponding evidence second we support progressive visual_analysis of synthesized knowledge this feature allows analysts to visually explore both existing knowledge and synthesized knowledge dynamically incorporating earlier analytic conclusions into the ensuing discovery process we have applied our system to two complex but very different analytic applications our preliminary evaluation shows the promise of our work h information systems models and principlesÂ¿user/machine_systems h information systems intelligence_analysis problem-solving solving environments visual_analytics visual_knowledge_discovery
understanding the nature and dynamics of conflicting opinions is a profound and challenging issue in this paper we address several aspects of the issue through a study of more than 3000 amazon customer reviews of the controversial bestseller the da vinci code including 1738 positive and 918 negative reviews the study is motivated by critical questions such as what are the differences between positive and negative reviews what is the origin of a particular opinion how do these opinions change over time to what extent can differentiating features be identified from unstructured text how accurately can these features predict the category of a review we first analyze terminology variations in these reviews in terms of syntactic semantic and statistic associations identified by termwatch and use term variation patterns to depict underlying topics we then select the most predictive terms based on log likelihood tests and demonstrate that this small set of terms classifies over 70% of the conflicting reviews correctly this feature_selection process reduces the dimensionality of the feature space from more than 20000 dimensions to a couple of hundreds we utilize automatically generated decision_trees to facilitate the understanding of conflicting opinions in terms of these highly predictive terms this study also uses a number of visualization and modeling tools to identify not only what positive and negative reviews have in common but also they differ and evolve over time visual_analytics conflicting opinions decision tree predictive text_analysis sense making terminology variation
a semantic graph is a network of heterogeneous nodes and links annotated with a domain ontology in intelligence_analysis investigators use semantic graphs to organize concepts and relationships as graph nodes and links in hopes of discovering key trends patterns and insights however as new information continues to arrive from a multitude of sources the size and complexity of the semantic graphs will soon overwhelm an investigator's cognitive capacity to carry out significant analyses we introduce a powerful visual_analytics framework designed to enhance investigators' natural analytical capabilities to comprehend and analyze large semantic graphs the paper describes the overall framework design presents major development accomplishments to date and discusses future directions of a new visual_analytics system known as have green visualization - information_visualization,visualization_systems and software,visualization_techniques methodologies graph and network_visualization information_analytics information_visualization visual_analytics
in this paper we have developed a novel visualization_framework to enable more effective visual_analysis of large-scale news videos where keyframes and keywords are automatically extracted from news video clips and visually represented according to their interestingness measurement to help audiences rind news stories of interest at first glance a computational approach is also developed to quantify the interestingness measurement of video clips our experimental results have shown that our techniques for intelligent news video analysis have the capacity to enable more effective visualization of large-scale news videos our news video_visualization system is very useful for security applications and for general audiences to quickly find news topics of interest from among many channels  artificial intelligence learningÂ¿concept learning  computer_graphics methodology and techniquesÂ¿interaction techniques news_visualization semantic_video_classification
mobile devices are rapidly gaining popularity due to their small size and their wide range of functionality with the constant improvement in wireless network access they are an attractive option not only for day to day use but also for in-field analytics by first responders in widespread areas however their limited processing display graphics and power resources pose a major challenge in developing effective applications nevertheless they are vital for rapid decision_making in emergencies when combined with appropriate analysis tools in this paper we present an efficient interactive visual analytic system using a pda to visualize network information from purdue's ross-ade stadium during football games as an example of in-held data analytics combined with text and video analysis with our system we can monitor the distribution of attendees with mobile devices throughout the stadium through their access of information and association/disassociation from wireless access points enabling the detection of crowd movement and event activity through correlative visualization and analysis of synchronized video instant replay video and text information play statistics with the network activity we can provide insightful information to network_monitoring personnel safety personnel and analysts this work provides a demonstration and testbed for mobile sensor_analytics that will help to improve network performance and provide safety personnel with information for better emergency planning and guidance i computer_graphics graphics systemsÂ¿network graphics i computer_graphics applicationsÂ¿visual_analytics mobile_visualization network_visualization visual_analytics
networks have remained a challenge for information_retrieval and visualization because of the rich set of tasks that users want to accomplish this paper offers an abstract content-actor_network_data model a classification of tasks and a tool to support them the netlens interface was designed around the abstract content-actor_network_data model to allow users to pose a series of elementary queries and iteratively refine visual overviews and sorted lists this enables the support of complex queries that are traditionally hard to specify netlens is general and scalable in that it applies to any dataset that can be represented with our abstract data model this paper describes netlens applying a subset of the acm digital_library consisting of about 4000 papers from the cm i conference written by about 6000 authors in addition we are now working on a collection of half a million emails and a dataset of legal cases human-computer_interaction content-actor_network_data digital_library incremental_data_exploration information_visualization iterative query refinement network_visualization piccolo user_interfaces
wormhole_attacks in wireless_networks can severely deteriorate the network performance and compromise the security through spoiling the routing protocols and weakening the security enhancements this paper develops an approach interactive_visualization of wormholes ivow to monitor and detect such attacks in large scale wireless_networks in real time we characterize the topology features of a network under wormhole_attacks through the node position changes and visualize the information at dynamically adjusted scales we integrate an automatic detection algorithm with appropriate user_interactions to handle complicated scenarios that include a large number of moving nodes and multiple worm-hole attackers various visual forms have been adopted to assist the understanding and analysis of the reconstructed network topology and improve the detectionaccuracy extended simulation has demonstrated that the proposed approach can effectively locate the fake neighbor connections without introducing many false alarms ivow does not require the wireless nodes to be equipped with any special hardware thus avoiding any additional cost the proposed approach demonstrates that interactive_visualization can be successfully combined with network_security mechanisms to greatly improve the intrusion detection capabilities c computer-communication networks generalÂ¿security and protection h information systems information interfaces and presentationÂ¿user_interfaces interactive detection topology_visualization visualization_on_network_security wireless_networks wormhole_attacks
this paper presents a network traffic analysis system that couples visual_analysis with a declarative knowledge_representation the system supports multiple iterations of the sense-making loop of analytic_reasoning by allowing users to save discoveries as they are found and to reuse them in future iterations we show how the knowledge_representation can be used to improve both the visual representations and the basic analytical tasks of filtering and changing level of detail we describe how the system can be used to produce models of network patterns and show results from classifying one day of network traffic in our laboratory network traffic visualization visual_analysis
realizing operational analytics solutions where large and complex data must be analyzed in a time-critical fashion entails integrating many different types of technology this paper focuses on an interdisciplinary combination of scientific_data_management and visualization/analysis technologies targeted at reducing the time required for data_filtering querying hypothesis_testing and knowledge_discovery in the domain of network connection data_analysis we show that use of compressed bitmap_indexing can quickly answer queries in an interactive visual_data_analysis application and compare its performance with two alternatives for serial and parallel filtering/querying on 25 billion records' worth of network connection data collected over a period of 42 weeks our approach to visual network connection data_exploration centers on two primaryfactors interactive ad-hoc and multiresolution query formulation and execution over n dimensions and visual display of the n-dimensional histogram results this combination is applied in a case_study to detect a distributed network scan and to then identify the set of remote hosts participating in the attack our approach is sufficiently general to be applied to a diverse set of data understanding problems as well as used in conjunction with a diverse set of analysis and visualization tools data_mining network_security query-driven_visualization visual_analytics
extensive spread of malicious code on the internet and also within intranets has risen the user's concern about what kind of data is transferred between her or his computer and other hosts on the network visual_analysis of this kind of information is a challenging task due to the complexity and volume of the data type considered and requires special design of appropriate visualization_techniques in this paper we present a scalable_visualization toolkit for analyzing network activity of computer hosts on a network the visualization combines network packet volume and type distribution information with geographic_information enabling the analyst to use geographic distortion techniques such as the histomap technique to become aware of the traffic components in the course of the analysis the presented analysis tool is especially useful to compare important network load characteristics in a geographically aware display to relatecommunication partners and to identify the type of network traffic occurring the results of the analysis are helpful in understanding typical networkcommunication activities and in anticipating potential performance bottlenecks or problems it is suited for both off-line analysis of historic data and via animation for on-line monitoring of packet-based network traffic in real time geography-based solutions information_visualization network_traffic_monitoring visual_analytics
we describe a framework for the display of complex multidimensional data designed to facilitate exploration analysis and collaboration among multiple analysts this framework aims to support human collaboration by making it easier to share representations to translate from one point of view to another to explain arguments to update conclusions when underlying assumptions change and to justify or account for decisions or actions multidimensional visualization_techniques are used with interactive context-sensitive and tunable graphs visual representations are flexibly generated using a knowledge_representation scheme based on annotated logic this enables not only tracking and fusing different viewpoints but also unpacking them fusing representations supports the creation of multidimensional meta-displays as well as the translation or mapping from one point of view to another at the same time analysts also need to be able to unpack one another's complex chains of reasoning especially if they have reached different conclusions and to determine the implications if any when underlying assumptions or evidence turn out to be false the framework enables us to support a variety of scenarios as well as to systematically generate and test experimental hypotheses about the impact of different kinds of visual representations upon interactive collaboration by teams of distributed analysts collaborative_and_distributed_visualization data_management_and_knowledge_representation visual_analytics visual_knowledge_discovery
we introduce a visual_analytics environment for the support of remote-collaborative sense-making activities team members use their individual_graphical interfaces to collect organize and comprehend task-relevant information relative to their areas of expertise a system of computational agents infers possible relationships among information items through the analysis of the spatial and temporal organization and collaborative use of information the computational agents support the exchange of information among team members to converge their individual contributions our system allows users to navigate vast amounts of shared information effectively and remotely dispersed team members to work independently without diverting from common objectives as well as to minimize the necessary amount of verbalcommunication agents h information search and retrieval information filtering, relevance feedback, selection process h office automation groupware h group and organization interfaces collaborative computing, computer-supported_cooperative_work indirect_collaboration indirect human computer interaction sense-making spatial_information_organization visual_analytics
a new field of research visual_analytics has been introduced this has been defined as "the science of analytical reasoning facilitated by interactive visual interfaces" thomas and cook 2005 visual analytic environments therefore support analytical reasoning using visual representations and interactions with data representations and transformation capabilities to support production presentation and dissemination as researchers begin to develop visual analytic environments it is advantageous to develop metrics and methodologies to help researchers measure the progress of their work and understand the impact their work has on the users who work in such environments this paper presents five areas or aspects of visual analytic environments that should be considered as metrics and methodologies for evaluation are developed evaluation aspects need to include usability but it is necessary to go beyond basic usability the areas of situation_awareness collaboration interaction creativity and utility are proposed as the five evaluation areas for initial consideration the steps that need to be undertaken to develop systematic evaluation methodologies and metrics for visual analytic environments are outlined analytic environments metrics visualization
we have built a visualization system and analysis portal for evaluating the performance of computational linguistics algorithms our system focuses on algorithms that classify and cluster documents by assigning weights to words and scoring each document against high dimensional reference concept vectors the visualization and algorithm analysis techniques include confusion matrices roc_curves document_visualizations showing word importance and interactive reports one of the unique aspects of our system is that the visualizations are thin-client web-based components built using svg visualization components ajax roc_curves svg confusion matrices document_categorization thin-client
a great deal of analytical work is done in the context of reading in digesting the semantics of the material the identification of important entities and capturing the relationship between entities visual analytic environments therefore must encompass reading tools that enable the rapid digestion of large amount of reading material other than plain text search subject indexes and basic highlighting tools are needed for rapid foraging of text in this paper we describe a technique that presents an enhanced subject index for a book by conceptually reorganizing it to suit particular expressed user information needs users first enter information needs via keywords describing the concepts they are trying to retrieve and comprehend then our system called scentindex computes what index entries are conceptually related and reorganizes and displays these index entries on a single page we also provide a number of navigational cues to help users peruse over this list of index entries and find relevant passages quickly compared to regular reading of a paper book our study showed that users are more efficient and more accurate in finding comparing and comprehending material in our system book_index information_scent contextualization ebooks personalized information access
finding patterns of events over time is important in searching patient histories web logs news stories and criminal activities this paper presents patternfinder an integrated interface for query and result-set_visualization for search and discovery of temporal patterns within multivariate and categorical_data sets we define temporal patterns as sequences of events with inter-event time spans patternfinder allows users to specify the attributes of events and time spans to produce powerful pattern queries that are difficult to express with other formalisms we characterize the range of queries patternfinder supports as users vary the specificity at which events and time spans are defined pattern finder's query capabilities together with coupled ball-and-chain and tabular visualizations enable users to effectively query explore and analyze event patterns both within and across data entities eg patient histories terrorist groups web logs etc temporal_query information_visualization user interface
a variety of user_interfaces have been developed to support the querying of hierarchical multi-dimensional_data in an olap setting such as pivot tables and polaris they are used to regularly check portions of a dataset and to explore a new dataset for the first time in this paper we establish criteria for olap user interface capabilities to facilitate comparison two criteria are the number of displayed dimensions along which comparisons can be made and the number of dimensions that are viewable at once - visual_comparison depth and width we argue that interfaces with greater visual_comparison depth support regular checking of known data by users that know roughly where to look while interfaces with greater comparison width support exploration of new data by users that have no a priori starting point and need to scan all dimensions pivot tables and polaris are examples of the former the main contribution of this paper is to introduce a new scalable interface that uses parallel dimension axis which supports the latter greater visual_comparison width we compare our approach to both table based and parallel coordinate based interfaces we present animplementation of our interface sgviewer user scenarios and provide an evaluation that supports the usability of our interface data_exploration olap parallel_coordinates visualization
real-world data is known to be imperfect suffering from various forms of defects such as sensor variability estimation errors uncertainty human errors in data entry and gaps in data gathering analysis conducted on variable quality data can lead to inaccurate or incorrect results an effective visualization system must make users aware of the quality of their data by explicitly conveying not only the actual data content but also its quality attributes while some research has been conducted on visualizing uncertainty in spatio-temporal_data and univariate data little work has been reported on extending this capability into multivariate data visualization in this paper we describe our approach to the problem of visually exploring multivariate data with variable quality as a foundation we propose a general approach to defining quality measures for tabular_data in which data may experience quality problems at three granularities individual data values complete records and specific dimensions we then present two approaches to visual_mapping of quality information into display space in particular one solution embeds the quality measures as explicit values into the original dataset by regarding value quality and record quality as new data dimensions the other solution is to superimpose the quality information within the data visualizations using additional visual_variables we also report on user studies conducted to assess alternate mappings of quality attributes to visual_variables for the second method in addition we describe case studies that expose some of the advantages and disadvantages of these two approaches h information interfaces and presentation user_interfacesÂ¿graphical user_interfaces uncertainty_visualization data quality multivariate visualization
browsing and retrieving images from large image collections are becoming common and important activities semantic image_analysis techniques which automatically detect high level semantic contents of images for annotation are promising solutions toward this problem however few efforts have been made to convey the annotation results to users in an intuitive manner to enable effective image browsing and retrieval there is also a lack of methods to monitor and evaluate the automatic image_analysis algorithms due to the high dimensional nature of image data features and contents in this paper we propose a novel scalable semantic image browser by applying existing information_visualization_techniques to semantic image_analysis this browser not only allows users to effectively browse and search in large image databases according to the semantic content of images but also allows analysts to evaluate their annotation process through interactive_visual_exploration the major visualization components of this browser are multi-dimensional_scaling mds based image_layout the value and relation var display that allows effective high dimensional visualization without dimension reduction and a rich set of interaction tools such as search by sample images and content relationship detection our preliminary user_study showed that the browser was easy to use and understand and effective in supporting image browsing and retrieval tasks  image_processing and computer vision scene analysisÂ¿object recognition h information storage and retrieval information search and retrievalÂ¿search process h information interfaces and presentation user_interfacesÂ¿graphical user_interfaces image retrieval image_layout multi-dimensional_visualization semantic_image_classification visual_analytics
during the last two decades a wide variety of advanced methods for the visual_exploration of large_data sets have been proposed for most of these techniques user_interaction has become a crucial element since there are many situations in which a user or an analyst has to select the right parameter settings from among many or select a subset of the available attribute space for the visualization process in order to construct valuable visualizations that provide insight into the data and reveal interesting patterns the right choice of input parameters is often essential since suboptimal parameter settings or the investigation of irrelevant data dimensions make the exploration process more time consuming and may result in wrong conclusions in this paper we propose a novel method for automatically determining meaningful parameter- and attribute settings based on the information content of the resulting visualizations our technique called pixnostics in analogy to scagnostics wilkinson et al 2005 automatically analyses pixel images resulting from diverse parameter mappings and ranks them according to the potential value for the user this allows a more effective and more efficient visual_data_analysis process since the attribute/parameter space is reduced to meaningful selections and thus the analyst obtains faster insight into the data real world applications are provided to show the benefit of the proposed approach visualization information_visualizationÂ¿visualization_techniques and methodologies visual_analytics visual_data_exploration visualization_technique
visual_analytics experts realize that one effective way to push the field forward and to develop metrics for measuring the performance of various visual_analytics components is to hold an annual competition the first visual_analytics science and technology vast contest was held in conjunction with the 2006 ieee vast symposium the competition entailed the identification of possible political shenanigans in the fictitious town of alderwood a synthetic_data set was made available as well as tasks we summarize how we prepared and advertised the contest developed some initial metrics for evaluation and selected the winners the winners were invited to participate at an additional live competition at the symposium to provide them with feedback from senior analysts h information interfaces & presentations user_interfaces - graphical user_interfaces (gui) contest evaluation human information interaction metrics sense making visual_analytics
these pre-pages to the issue contain a table of contents a list of supporting organizations a message from the editor-in-chief the preface committee and reviewer listings 2005visualization awards and the keynote and capstone addressess for vis and infovis
we describe ask-graphview a node-link-based graph_visualization system that allows clustering and interactive navigation of large graphs ranging in size up to 16 million edges the system uses a scalable architecture and a series of increasingly sophisticated clustering algorithms to construct a hierarchy on an arbitrary weighted undirected input graph by lowering the interactivity requirements we can scale to substantially bigger graphs the user is allowed to navigate this hierarchy in a top down manner by interactively expanding individual clusters ask-graphview also provides facilities for filtering and coloring annotation and cluster labeling graph_clustering graph_visualization information_visualization
matrixexplorer is a network_visualization system that uses two representations node-link_diagrams and matrices its design comes from a list of requirements formalized after several interviews and a participatory design session conducted with social science researchers although matrices are commonly used in social_networks analysis very few systems support the matrix-based_representations to visualize and analyze networks matrixexplorer provides several novel features to support the exploration of social_networks with a matrix-based representation in addition to the standard interactive filtering and clustering functions it provides tools to reorder layout matrices to annotate and compare findings across different layouts and find consensus among several clusterings matrixexplorer also supports node-link diagram views which are familiar to most users and remain a convenient way to publish or communicate exploration results matrix and node-link representations are kept synchronized at all stages of the exploration process consensus exploratory_process interactive_clustering matrix_ordering matrix-based_representations node-link_diagrams social_networks_visualization
we present a new approach for the visual_analysis of state transition graphs we deal with multivariate graphs where a number of attributes are associated with every node our method provides an interactive attribute-based clustering facility clustering results in metric hierarchical and relational_data represented in a single visualization to visualize hierarchically structured quantitative data we introduce a novel technique the bar tree we combine this with a node-link diagram to visualize the hierarchy and an arc diagram to visualize relational_data our method enables the user to gain significant insight into large state transition graphs containing tens of thousands of nodes we illustrate the effectiveness of our approach by applying it to a real-world use case the graph we consider models the behavior of an industrial wafer stepper and contains 55 043 nodes and 289 443 edges graph_visualization finite_state_machines interactive_clustering multivariate visualization state_spaces transition_systems
social_network_analysis sna has emerged as a powerful method for understanding the importance of relationships in networks however interactive_exploration of networks is currently challenging because 1 it is difficult to find patterns and comprehend the structure of networks with many nodes and links and 2 current systems are often a medley of statistical methods and overwhelming visual output which leaves many analysts uncertain about how to explore in an orderly manner this results in exploration that is largely opportunistic our contributions are techniques to help structural analysts understand social_networks more effectively we present socialaction a system that uses attribute_ranking and coordinated_views to help users systematically examine numerous sna measures users can 1 flexibly iterate through visualizations of measures to gain an overview filter nodes and findoutliers 2 aggregate networks using link structure find cohesive subgroups and focus on communities of interest and 3 untangle networks by viewing different link types separately or find patterns across different link types using a matrix overview for each operation a stable node layout is maintained in the network_visualization so users can make comparisons socialaction offers analysts a strategy beyond opportunism as it provides systematic yet flexible techniques for exploring social_networks social_networks attribute_ranking coordinated_views exploratory_data_analysis interactive_graph_visualization
in his text visualizing data william cleveland demonstrates how the aspect_ratio of a line chart can affect an analyst's perception of trends in the data cleveland proposes an optimization technique for computing the aspect_ratio such that the average absolute orientation of line segments in the chart is equal to 45 degrees this technique called banking to 45deg is designed to maximize the discriminability of the orientations of the line segments in the chart in this paper we revisit this classic result and describe two new extensions first we propose alternate optimization criteria designed to further improve the visual perception of line segment orientations second we develop multi-scale banking a technique that combines spectral analysis with banking to 45deg our technique automatically identifies trends at various frequency scales and then generates a banked chart for each of these scales we demonstrate the utility of our techniques in a range of visualization tools and analysis examples information_visualization banking to  degrees graphical_perception line_charts sparklines time-series
data abstraction techniques are widely used in multiresolution_visualization_systems to reduce visual_clutter and facilitate analysis from overview to detail however analysts are usually unaware of how well the abstracted data represent the original dataset which can impact the reliability of results gleaned from the abstractions in this paper we define two data abstraction quality measures for computing the degree to which the abstraction conveys the original dataset the histogram difference measure and the nearest neighbor measure they have been integrated within xmdvtool a public-domain multiresolution_visualization system for multivariate data_analysis that supports sampling as well as clustering to simplify data several interactive operations are provided including adjusting the data abstraction level changing selected regions and setting the acceptable data abstraction quality level conducting these operations analysts can select an optimal data abstraction level also analysts can compare different abstraction methods using the measures to see how well relative data_density andoutliers are maintained and then select an abstraction method that meets the requirement of their analytic tasks clustering metrics multiresolution_visualization authors  sampling
we have previously shown that random_sampling is an effective clutter_reduction technique and that a sampling lens can facilitate focus+context viewing of particular regions this demands an efficient method of estimating the overlap or occlusion of large numbers of intersecting lines in order to automatically adjust the sampling rate within the lens this paper proposes several ways for measuring occlusion in parallel coordinate plots an empirical_study into theaccuracy and efficiency of the occlusion measures show that a probabilistic approach combined with a 'binning' technique is very fast and yet approaches theaccuracy of the more expensive 'true' complete measurement sampling clutter density_reduction information visualisation lens occlusion overplotting parallel_coordinates random_sampling
we propose a new metaphor for the visualization of prefixes propagation in the internet such a metaphor is based on the concept of topographic map and allows to put in evidence the relative importance of the internet service providers isps involved in the routing of the prefix based on the new metaphor we propose an algorithm for computing layouts and experiment with such algorithm on a test suite taken from the real internet the paper extends the visualization approach of the bgplay service which is an internet routing monitoring tool widely used by isp operators graph_drawing interdomain_routing internet_visualization spring_embedder
networks have remained a challenge for information_visualization designers because of the complex issues of node and link layout coupled with the rich set of tasks that users present this paper offers a strategy based on two principles 1 layouts are based on user-defined semantic_substrates which are non-overlapping regions in which node placement is based on node attributes 2 users interactively adjust sliders to control link visibility to limit clutter and thus ensure comprehensibility of source and destination scalability is further facilitated by user control of which nodes are visible we illustrate our semantic_substrates approach as implemented in nvss 10 with legal precedent data for up to 1122 court cases in three regions with 7645 legal citations network_visualization graphical user_interfaces information_visualization semantic_substrate
a compound graph is a frequently encountered type of data set relations are given between items and a hierarchy is defined on the items as well we present a new method for visualizing such compound graphs our approach is based on visually bundling the adjacency edges ie non-hierarchical edges together we realize this as follows we assume that the hierarchy is shown via a standard tree_visualization method next we bend each adjacency edge modeled as a b-spline curve toward the polyline defined by the path via the inclusion edges from one node to another this hierarchical bundling reduces visual_clutter and also visualizes implicit adjacency edges between parent nodes that are the result of explicit adjacency edges between their respective child nodes furthermore hierarchical edge_bundling is a generic method which can be used in conjunction with existing tree_visualization_techniques we illustrate our technique by providing example visualizations and discuss the results based on an informal evaluation provided by potential users of such visualizations network_visualization curves edge_aggregation edge_bundling edge_concentration graph_visualization hierarchies node-link_diagrams tree_visualization treemaps
in many applications data is collected and indexed by geo-spatial location discovering interesting patterns through visualization is an important way of gaining insight about such data a previously proposed approach is to apply local placement functions such as pixelmaps that transform the input data set into a solution set that preserves certain constraints while making interesting patterns more obvious and avoid data loss from overplotting in experience this family of spatial transformations can reveal fine structures in large point sets but it is sometimes difficult to relate those structures to basic geographic features such as cities and regional boundaries recent information_visualization research has addressed other types of transformation functions that make spatially-transformed maps with recognizable shapes these types of spatial-transformation are called global shape functions in particular cartogram-based map distortion has been studied on the other hand cartogram-based distortion does not handle point sets readily in this study we present a framework that allows the user to specify a global shape function and a local placement function we combine cartogram-based layout global shape with pixelmaps local placement obtaining some of the benefits of each toward improved exploration of dense geo-spatial_data sets cartogram geo-spatial_data pixel placement shape transformation
this paper describes the worldmapper project which makes use of novel visualization_techniques to represent a broad variety of social and economic data about the countries of the world the goal of the project is to use the map projections known as cartograms to depict comparisons and relations between different territories and its execution raises many interesting design challenges that were not all apparent at the outset we discuss the approaches taken towards these challenges some of which may have considerably broad application we conclude by commenting on the positive initial response to the worldmapper images published on the web which we believe is due at least in part to the particular effectiveness of the cartogram as a tool for communicating quantitative geographic data cartogram computer_graphics data visualization geographic_visualization social_visualization worldmapper
people in different places talk about different things this interest distribution is reflected by the newspaper articles circulated in a particular area we use data from our large-scale newspaper analysis system lydia to make entity datamaps a spatial visualization of the interest in a given named entity our goal is to identify entities which display regional biases we develop a model of estimating the frequency of reference of an entity in any given city from the reference frequency centered in surrounding cities and techniques for evaluating the spatial significance of this distribution gis geographic_visualization information_analytics newspapers spidering text_and_document_visualization www data visualization
we address the problem of filtering selecting and placing labels on a dynamic map which is characterized by continuous zooming and panning capabilities this consists of two interrelated issues the first is to avoid label popping and other artifacts that cause confusion and interrupt navigation and the second is to label at interactive speed in most formulations the static map_labeling problem is np-hard and a fast approximation might have on log n complexity even this is too slow during interaction when the number of labels shown can be several orders of magnitude less than the number in the map in this paper we introduce a set of desiderata for "consistent" dynamic map_labeling which has qualities desirable for navigation we develop a new framework for dynamic_labeling that achieves the desiderata and allows for fast interactive_display by moving all of the selection and placement decisions into the preprocessing phase this framework is general enough to accommodate a variety of selection and placement algorithms it does not appear possible to achieve our desiderata using previous frameworks prior to this paper there were no formal models of dynamic_maps or of dynamic labels our paper introduces both we formulate a general optimization problem for dynamic map_labeling and give a solution to a simple version of the problem the simple version is based on label priorities and a versatile and intuitive class of dynamic label_placements we call "invariant point placements" despite these restrictions our approach gives a useful and practical solution ourimplementation is incorporated into the g-vis system which is a full-detail dynamic map of the continental usa this demo is available through any browser gis hci map_labeling computational_cartography dynamic_maps human-computer_interface label consistency label_filtering label_placement label_selection preprocessing realtime
dynamical models that explain the formation of spatial structures of rna molecules have reached a complexity that requires novel visualization methods that help to analyze the validity of these models we focus on the visualization of so-called folding landscapes of a growing rna molecule folding landscapes describe the energy of a molecule as a function of its spatial configuration thus they are huge and high dimensional their most salient features however are encapsulated by their so-called barrier_tree that reflects the local minima and their connecting saddle points for each length of the growing rna chain there exists a folding landscape we visualize the sequence of folding landscapes by an animation of the corresponding barrier_trees to generate the animation we adapt the foresight layout with tolerance algorithm for general dynamic_graph_layout problems since it is very general we give a detailed description of each phase constructing a supergraph for the trees layout of that supergraph using a modified dot algorithm and presentation techniques for the final animation graph_drawing rna_folding barrier_tree dynamic graph energy_landscape fitness_landscape
business data is often presented using simple business graphics these familiar visualizations are effective for providing overviews but fall short for the presentation of large amounts of detailed information treemaps can provide such detail but are often not easy to understand we present how standard treemap algorithms can be adapted such that the results mimic familiar business graphics specifically we present the use of different layout_algorithms per level a number of variations of the squarified algorithm the use of variable borders and the use of non-rectangular shapes the combined use of these leads to histograms pie charts and a variety of other styles information_visualization business graphics hierarchical_data treemap
the dominant paradigm for searching and browsing large_data stores is text-based presenting a scrollable list of search results in response to textual search term input while this works well for the web there is opportunity for improvement in the domain of personal information stores which tend to have more heterogeneous data and richer metadata in this paper we introduce facetmap an interactive query-driven_visualization generalizable to a wide range of metadata-rich data stores facetmap uses a visual metaphor for both input selection of metadata facets as filters and output results of a user_study provide insight into tradeoffs between facetmap's graphical approach and the traditional text-oriented approach graphical visualization faceted_metadata interactive_information_retrieval
quasi-trees namely graphs with tree-like structure appear in many application domains including bioinformatics and computer networks our new spf approach exploits the structure of these graphs with a two-level approach to drawing where the graph is decomposed into a tree of biconnected components the low-level biconnected components are drawn with a force-directed approach that uses a spanning tree skeleton as a starting point for the layout the higher-level structure of the graph is a true tree with meta-nodes of variable size that contain each biconnected component that tree is drawn with a new area-aware variant of a tree drawing algorithm that handles high-degree nodes gracefully at the cost of allowing edge-node overlaps spf performs an order of magnitude faster than the best previous approaches while producing drawings of commensurate or improved quality graph and network_visualization financial_data_visualization hierarchy_visualization time_series_data
quasi-trees namely graphs with tree-like structure appear in many application domains including bioinformatics and computer networks our new spf approach exploits the structure of these graphs with a two-level approach to drawing where the graph is decomposed into a tree of biconnected components the low-level biconnected components are drawn with a force-directed approach that uses a spanning tree skeleton as a starting point for the layout the higher-level structure of the graph is a true tree with meta-nodes of variable size that contain each biconnected component that tree is drawn with a new area-aware variant of a tree drawing algorithm that handles high-degree nodes gracefully at the cost of allowing edge-node overlaps spf performs an order of magnitude faster than the best previous approaches while producing drawings of commensurate or improved quality graph and network_visualization quasi-tree
existing information-visualization_techniques that target small_screens are usually limited to exploring a few hundred items in this article we present a scatterplot tool for personal digital assistants that allows the handling of many thousands of items the application's scalability is achieved by incorporating two alternative interaction techniques a geometric-semantic_zoom that provides smooth transition between overview_and_detail and a fisheye distortion that displays the focus_and_context regions of the scatterplot in a single view a user_study with 24 participants was conducted to compare the usability and efficiency of both techniques when searching a book database containing 7500 items the study was run on a pen-driven wacom board simulating a pda interface while the results showed no significant difference in task-completion times a clear majority of 20 users preferred the fisheye_view over the zoom interaction in addition other dependent variables such as user_satisfaction and subjective rating of orientation and navigation support revealed a preference for the fisheye distortion these findings partly contradict related research and indicate that when using a small_screen users place higher value on the ability to preserve navigational context than they do on the ease of use of a simplistic metaphor-based interaction style graph_drawing constraints force directed algorithms layout multidimensional scaling stress_majorization
larger higher resolution displays can be used to increase the scalability of information_visualizations but just how much can scalability increase using larger displays before hitting human perceptual or cognitive limits are the same visualization_techniques that are good on a single monitor also the techniques that are best when they are scaled up using large high-resolution displays to answer these questions we performed a controlled experiment on user performance timeaccuracy and subjective workload when scaling up data quantity with different space-time-attribute visualizations using a large tiled display twelve college students used small_multiples embedded bar matrices and embedded time-series graphs either on a 2 megapixel mp display or with data scaled up using a 32 mp tiled display participants performed various overview_and_detail tasks on geospatially-referenced multidimensional time-series_data results showed that current designs are perceptually scalable because they result in a decrease in task completion time when normalized per number of data attributes along with no decrease inaccuracy it appears that for the visualizations selected for this study the relative comparison between designs is generally consistent between display sizes however results also suggest that encoding is more important on a smaller display while spatial grouping is more important on a larger display some suggestions for designers are provided based on our experience designing visualizations for large displays pda small_screen fisheye focus+context scatter plot zoom
larger higher resolution displays can be used to increase the scalability of information_visualizations but just how much can scalability increase using larger displays before hitting human perceptual or cognitive limits are the same visualization_techniques that are good on a single monitor also the techniques that are best when they are scaled up using large high-resolution displays to answer these questions we performed a controlled experiment on user performance timeaccuracy and subjective workload when scaling up data quantity with different space-time-attribute visualizations using a large tiled display twelve college students used small_multiples embedded bar matrices and embedded time-series graphs either on a 2 megapixel mp display or with data scaled up using a 32 mp tiled display participants performed various overview_and_detail tasks on geospatially-referenced multidimensional time-series_data results showed that current designs are perceptually scalable because they result in a decrease in task completion time when normalized per number of data attributes along with no decrease inaccuracy it appears that for the visualizations selected for this study the relative comparison between designs is generally consistent between display sizes however results also suggest that encoding is more important on a smaller display while spatial grouping is more important on a larger display some suggestions for designers are provided based on our experience designing visualizations for large displays information_visualization empirical_evaluation large displays
commonly known detail in context techniques for the two-dimensional euclidean space enlarge details and shrink their context using mapping functions that introduce geometrical compression this makes it difficult or even impossible to recognize shapes for large differences in magnificationfactors in this paper we propose to use the complex_logarithm and the complex root functions to show very small details even in very large contexts these mappings are conformal which means they only locally rotate and scale thus keeping shapes intact and recognizable they allow showing details that are orders of magnitude smaller than their surroundings in combination with their context in one seamless visualization we address the utilization of this universal technique for the interaction with complex two-dimensional data considering the exploration of large graphs and other examples detail in context analytic_functions complex_logarithm conformal_mappings interaction
despite a diversity of software architectures supporting information_visualization it is often difficult to identify evaluate and re-apply the design solutions implemented within such frameworks one popular and effective approach for addressing such difficulties is to capture successful solutions in design_patterns abstract descriptions of interacting software components that can be customized to solve design problems within a particular context based upon a review of existing frameworks and our own experiences building visualization software we present a series of design_patterns for the domain of information_visualization we discuss the structure context_of_use and interrelations of patterns spanning data representation graphics and interaction by representing design knowledge in a reusable form these patterns can be used to facilitate software designimplementation and evaluation and improve developer education andcommunication design_patterns information_visualization object-oriented_programming software_engineering
we present a novel pipeline for computer-aided detection cad of colonic polyps by integrating texture and shape analysis with volume_rendering and conformal colon_flattening using our automatic method the 3d polyp_detection problem is converted into a 2d pattern_recognition problem the colon surface is first segmented and extracted from the ct data set of the patient's abdomen which is then mapped to a 2d rectangle using conformal_mapping this flattened image is rendered using a direct_volume_rendering technique with a translucent electronic biopsy transfer_function the polyps are detected by a 2d clustering method on the flattened image the false positives are further reduced by analyzing the volumetric shape and texture features compared with shape based methods our method is much more efficient without the need of computing curvature and other shape parameters for the whole colon surface the final detection results are stored in the 2d image which can be easily incorporated into a virtual_colonoscopy vc system to highlight the polyp locations the extracted colon surface mesh can be used to accelerate the volumetric ray casting algorithm used to generate the vc endoscopic view the proposed automatic cad pipeline is incorporated into an interactive vc system with a goal of helping radiologists detect polyps faster and with higheraccuracy computer_aided_detection texture_analysis virtual_colonoscopy volume_rendering
this paper presents a procedure for virtual autopsies based on interactive 3d_visualizations of large scale high resolution data from ct-scans of human cadavers the procedure is described using examples from forensic medicine and the added value and future potential of virtual autopsies is shown from a medical and forensic perspective based on the technical demands of the procedure state-of-the-art volume_rendering techniques are applied and refined to enable real-time full body virtual autopsies involving gigabyte sized data on standard gpus the techniques applied include transfer_function based data reduction using level-of-detail selection and multi-resolution rendering techniques the paper also describes a data_management component for large out-of-core data sets and an extension to the gpu-based raycaster for efficient dual tf rendering detailed benchmarks of the pipeline are presented using data sets from forensic cases forensics autopsies large scale data medical visualization volume_rendering
we present real-time vascular_visualization methods which extend on illustrative_rendering techniques to particularly accentuate spatial depth and to improve the perceptive separation of important vascular properties such as branching level and supply area the resulting visualization can and has already been used for direct projection on a patient's organ in the operation theater where the varying absorption and reflection characteristics of the surface limit the use of color the important contributions of our work are a gpu-based hatching algorithm for complex tubular structures that emphasizes shape and depth as well as gpu-accelerated shadow-like depth indicators which enable reliable comparisons of depth distances in a static monoscopic 3d_visualization in addition we verify the expressiveness of our illustration methods in a large quantitative study with 160 subjects vessel_visualization_evaluation functional realism illustrative_rendering spatial_perception
computer-aided diagnosis cad is a helpful addition to laborious visual inspection for preselection of suspected colonic polyps in virtual_colonoscopy most of the previous work on automatic polyp_detection makes use of indicators based on the scalar curvature of the colon wall and can result in many false-positive detections our work tries to reduce the number of false-positive detections in the preselection of polyp candidates polyp surface shape can be characterized and visualized using lines of curvature in this paper we describe techniques for generating and rendering lines of curvature on surfaces and we show that these lines can be used as part of a polyp_detection approach we have adapted existing approaches on explicit triangular surface meshes and developed a new algorithm on implicit_surfaces embedded in 3d volume data the visualization of shaded colonic surfaces can be enhanced by rendering the derived lines of curvature on these surfaces features strongly correlated with true-positive detections were calculated on lines of curvature and used for the polyp candidate selection we studied the performance of these features on 5 data sets that included 331 pre-detected candidates of which 50 sites were true polyps the winding angle had a significant discriminating power for true-positive detections which was demonstrated by a wilcoxon rank sum test with p<0001 the median winding angle and inter-quartile range iqr for true polyps were 7817 and 6770-9288 compared to 2954 and 1995-3749 for false-positive detections medical visualization implicit surface line_of_curvature polyp_detection virtual_colonoscopy
focus+context_visualization integrates a visually accentuated representation of selected data items in focus more details more opacity etc with a visually deemphasized representation of the rest of the data ie the context the role of context_visualization is to provide an overview of the data for improved user orientation and improved navigation a good overview comprises the representation of bothoutliers and trends up to now however context_visualization not really treatedoutliers sufficiently in this paper we present a new approach to focus+context_visualization in parallel_coordinates which is truthful tooutliers in the sense that small-scale features are detected before visualization and then treated specially during context_visualization generally we present a solution which enables context_visualization at several levels of abstraction both for the representation ofoutliers and trends we introduce outlier detection and context generation to parallel_coordinates on the basis of a binned data representation this leads to an output-oriented visualization approach which means that only those parts of the visualization process are executed which actually affect the final rendering accordingly the performance of this solution is much more dependent on the visualization size than on the data size which makes it especially interesting for large_datasets previous approaches are outperformed the new solution was successfully applied to datasets with up to 3 million data records and up to 50 dimensions parallel_coordinates focus+context_visualization large_data_visualizationoutliers & trends
we present the first scalable algorithm that supports the composition of successive rectilinear deformations earlier systems that provided stretch and squish navigation could only handle small datasets more recent work featuring rubber sheet navigation for large_datasets has focused on rendering and on application-specific issues however no algorithm has yet been presented for carrying out such navigation methods our paper addresses this problem for maximum flexibility with large_datasets a stretch and squish navigation algorithm should allow for millions of potentially deformable regions however typical usage only changes the extents of a small subset k of these n regions at a time the challenge is to avoid computations that are linear in n because a single deformation can affect the absolute screen-space location of every deformable region we provide an oklogn algorithm that supports any application that can lay out a dataset on a generic grid and show animplementation that allows navigation of trees and gene sequences with millions of items in sub-millisecond time focus+context information_visualization navigation real time rendering
time-varying multi-variate and comparative data sets are not easily visualized due to the amount of data that is presented to the user at once by combining several volumes together with different operators into one visualized volume the user is able to compare values from different data sets in space over time run or field without having to mentally switch between different renderings of individual data sets in this paper we propose using a volume shader where the user is given the ability to easily select and operate on many data volumes to create comparison relationships the user specifies an expression with set and numerical operations and her data to see relationships between data fields furthermore we render the contextual information of the volume shader by converting it to a volume tree we visualize the different levels and nodes of the volume tree so that the user can see the results of suboperations this gives the user a deeper understanding of the final visualization by seeing how the parts of the whole are operationally constructed comparative focus + context multi-variate time-varying
we present an approach to visualizing correlations in 3d multifield scalar_data the core of our approach is the computation of correlation fields which are scalar_fields containing the local correlations of subsets of the multiple fields while the visualization of the correlation fields can be done using standard 3d volume_visualization_techniques their huge number makes selection and handling a challenge we introduce the multifield-graph to give an overview of which multiple fields correlate and to show the strength of their correlation this information guides the selection of informative correlation fields for visualization we use our approach to visually analyze a number of real and synthetic multifield datasets visualization correlation multifield
recent research in visual saliency has established a computational measure of perceptual importance in this paper we present a visual-saliency-based operator to enhance selected regions of a volume we show how we use such an operator on a user-specified saliency field to compute an emphasis field we further discuss how the emphasis field can be integrated into the visualization pipeline through its modifications of regional luminance and chrominance finally we validate our work using an eye-tracking-based user_study and show that our new saliency enhancement operator is more effective at eliciting viewer attention than the traditional gaussian enhancement operator saliency non-photorealistic_rendering perceptual_enhancement visual_attention volume_rendering
this paper introduces a concept for automatic focusing on features within a volumetric_data set the user selects a focus ie object of interest from a set of pre-defined features our system automatically determines the most expressive view on this feature a characteristic viewpoint is estimated by a novel information-theoretic framework which is based on the mutual information measure viewpoints change smoothly by switching the focus from one feature to another one this mechanism is controlled by changes in the importance distribution among features in the volume the highest importance is assigned to the feature in focus apart from viewpoint_selection the focusing mechanism also steers visual emphasis by assigning a visually more prominent representation to allow a clear view on features that are normally occluded by other parts of the volume the focusing for example incorporates cut-away views illustrative_visualization characteristic_viewpoint_estimation focus+context techniques interacting_with_volumetric_datasets volume_visualization
volume rendered imagery often includes a barrage of 3d information like shape appearance and topology of complex structures and it thus quickly overwhelms the user in particular when focusing on a specific region a user cannot observe the relationship between various structures unless he has a mental picture of the entire data in this paper we present clearview a gpu-based interactive framework for texture-based volume ray-casting that allows users which do not have the visualization skills for this mental exercise to quickly obtain a picture of the data in a very intuitive and user-friendly way clearview is designed to enable the user to focus on particular areas in the data while preserving context information without visual_clutter clearview does not require additional feature volumes as it derives any features in the data from image information only a simple point-and-click interface enables the user to interactively highlight structures in the data clearview provides an easy to use interface to complex volumetric_data as it only uses transparency in combination with a few specific shaders to convey focus_and_context information gpu_rendering termsÂ¿focus & context volume_raycasting
vortices are undesirable in many applications while indispensable in others it is therefore of common interest to understand their mechanisms of creation this paper aims at analyzing the transport of vorticity inside incompressible flow the analysis is based on the vorticity equation and is performed along pathlines which are typically started in upstream direction from vortex_regions different methods for the quantitative and explorative_analysis of vorticity_transport are presented and applied to cfd simulations of water turbines simulation quality is accounted for by including the errors of meshing and convergence into analysis and visualization the obtained results are discussed and interpretations with respect to engineering questions are given flow_visualization linked_views unsteady_flow vorticity_transport
in order to understand complex vortical flows in large_data sets we must be able to detect and visualize vortices in an automated fashion in this paper we present a feature-based vortex_detection and visualization_technique that is appropriate for large computational fluid dynamics data sets computed on unstructured_meshes in particular we focus on the application of this technique to visualization of the flow over a serrated wing and the flow_field around a spinning missile with dithering canards we have developed a core line extraction technique based on the observation that vortex cores coincide with local extrema in certain scalar_fields we also have developed a novel technique to handle complex vortex topology that is based on k-means clustering these techniques facilitate visualization of vortices in simulation data that may not be optimally resolved or sampled results are included that highlight the strengths and weaknesses of our approach we conclude by describing how our approach can be improved to enhance robustness and expand its range of applicability vortex_detection feature mining vortex_visualization
this paper presents an advanced evenly-spaced streamline_placement algorithm for fast high-quality and robust layout of flow lines a fourth-order runge-kutta integrator with adaptive step size and error control is employed for rapid accurate streamline advection cubic hermite polynomial interpolation with large sample-spacing is adopted to create fewer evenly-spaced samples along each streamline to reduce the amount of distance checking we propose two methods to enhance placement quality double queues are used to prioritize topological seeding and to favor long streamlines to minimize discontinuities adaptive distance control based on the local flow variance is explored to reduce cavities furthermore we propose a universal effective fast and robust loop detection strategy to address closed and spiraling streamlines our algorithm is an order-of-magnitude faster than jobard and lefer's algorithm with better placement quality and over 5 times faster than mebarki et al's algorithm with comparable placement quality but with a more robust solution to loop detection flow_visualization closed_streamlines evenly-spaced streamlines seeding_strategy streamline_placement
the pipeline_model in visualization has evolved from a conceptual_model of data processing into a widely used architecture for implementing visualization_systems in the process a number of capabilities have been introduced including streaming of data in chunks distributed pipelines and demand-driven processing visualization_systems have invariably built on stateful programming technologies and these capabilities have had to be implemented explicitly within the lower layers of a complex hierarchy of services the good news for developers is that applications built on top of this hierarchy can access these capabilities without concern for how they are implemented the bad news is that by freezing capabilities into low-level services expressive power and flexibility is lost in this paper we express visualization_systems in a programming language that more naturally supports this kind of processing model lazy functional languages support fine-grained demand-driven processing a natural form of streaming and pipeline-like function composition for assembling applications the technology thus appears well suited to visualization_applications using surface_extraction algorithms as illustrative examples and the lazy functional language haskell we argue the benefits of clear and concise expression combined with fine-grained demand-driven computation just as visualization provides insight into data functional abstraction provides new insight into visualization pipeline_model functional_programming laziness
this paper presents an interactive_visualization system named websearchviz for visualizing_the_web_search_results and facilitating users' navigation and exploration the metaphor in our model is the solar system with its planets and asteroids revolving around the sun location color movement and spatial distance of objects in the visual space are used to represent the semantic relationships between a query and relevant web pages especially the movement of objects and their speeds add a new dimension to the visual space illustrating the degree of relevance among a query and web_search_results in the context_of_users' subjects of interest by interacting with the visual space users are able to observe the semantic relevance between a query and a resulting web page with respect to their subjects of interest context information or concern users' subjects of interest can be dynamically changed redefined added or deleted from the visual space visualization model web_search_results movement speed
in the past decade a lot of research work has been conducted to support collaborative_visualization among remote users over the networks allowing them to visualize and manipulate shared data for problem_solving there are many applications of collaborative_visualization such as oceanography meteorology and medical science to facilitate user_interaction a critical system requirement for collaborative_visualization is to ensure that remote users would perceive a synchronized view of the shared data failing this requirement the user's ability in performing the desirable collaborative tasks would be affected in this paper we propose a synchronization method to support collaborative_visualization it considers how interaction with dynamic objects is perceived by application participants under the existence of network_latency and remedies the motion trajectory of the dynamic objects it also handles the false positive and false negative collision_detection problems the new method is particularly well designed for handling content changes due to unpredictable user interventions or object collisions we demonstrate the effectiveness of our method through a number of experiments collaborative_visualization distributed_synchronization motion synchronization network_latency
we describe a concurrent_visualization pipeline designed for operation in a production supercomputing environment the facility was initially developed on the nasa ames "columbia" supercomputer for a massively parallel forecast model geos4 during the 2005 atlantic hurricane season geos4 was run 4 times a day under tight time constraints so that its output could be included in an ensemble prediction that was made available to forecasters at the national hurricane center given this time-critical context we designed a configurable concurrent pipeline to visualize multiple global fields without significantly affecting the runtime model performance or reliability we use mpeg compression of the accruing images to facilitate live low-bandwidth distribution of multiple visualization streams to remote sites we also describe the use of our concurrent_visualization_framework with a global ocean circulation model which provides a 864-fold increase in the temporal resolution of practically achievable animations in both the atmospheric and oceanic circulation models the application scientists gained new insights into their model dynamics due to the high temporal resolution animations attainable ecco geos global climate model supercomputing concurrent_visualization high_temporal_resolution_visualization hurricane_visualization interactive_visual_computing ocean_modeling time-varying_data
navigating through large-scale virtual environments such as simulations of the astrophysical universe is difficult the huge spatial range of astronomical models and the dominance of empty space make it hard for users to travel across cosmological scales effectively and the problem of wayfinding further impedes the user's ability to acquire reliable spatial knowledge of astronomical contexts we introduce a new technique called the scalable world-in-miniature_(wim) map as a unifying interface to facilitate travel and wayfinding in a virtual environment spanning gigantic spatial scales power-law spatial seating enables rapid and accurate transitions among widely separated regions logarithmically mapped miniature spaces offer a global overview mode when the full context is too large 3d landmarks represented in the wim are enhanced by scale positional and directional cues to augment spatial_context awareness a series of navigation models are incorporated into the scalable wim to improve the performance of travel tasks posed by the unique characteristics of virtual cosmic exploration the scalable wim user interface supports an improved physical navigation experience and assists pragmatic cognitive understanding of a visualization context that incorporates the features of large-scale astronomy astrophysical_visualization interaction techniques large-scale_exploration world-in-miniature (wim)
this paper describes a set of visual_cues of contact designed to improve the interactive_manipulation of virtual objects in industrial assembly/maintenance_simulations these visual_cues display information of proximity contact and effort between virtual objects when the user manipulates a part inside a digital mock-up the set of visual_cues encloses the apparition of glyphs arrow disk or sphere when the manipulated object is close or in contact with another part of the virtual environment light sources can also be added at the level of contact points a filtering technique is proposed to decrease the number of glyphs displayed at the same time various effects - such as change in color change in size and deformation of shape - can be applied to the glyphs as a function of proximity with other objects or amplitude of the contact forces a preliminary evaluation was conducted to gather the subjective preference of a group of participants during the simulation of an automotive assembly operation the collected questionnaires showed that participants globally appreciated our visual_cues of contact the changes in color appeared to be preferred concerning the display of distances and proximity information size changes and deformation effects appeared to be preferred in terms of perception of contact forces between the parts last light sources were selected to focus the attention of the user on the contact areas assembly/maintenance_simulation contact force glyph light proximity virtual_prototyping visual_cues
many sophisticated techniques for the visualization of volumetric_data such as medical data have been published while existing techniques are mature from a technical point of view managing the complexity of visual parameters is still difficult for nonexpert users to this end this paper presents new ideas to facilitate the specification of optical properties for direct_volume_rendering we introduce an additional level of abstraction for parametric models of transfer_functions the proposed framework allows visualization experts to design high-level transfer_function models which can intuitively be used by non-expert users the results are user_interfaces which provide semantic information for specialized visualization problems the proposed method is based on principal_component_analysis as well as on concepts borrowed from computer animation volume_rendering semantic_models transfer_function_design
in multiresolution volume_visualization a visual representation of level-of-detail lod quality is important for us to examine compare and validate different lod selection algorithms while traditional methods rely on ultimate images for quality measurement we introduce the lod map - an alternative representation of lod quality and a visual interface for navigating multiresolution data_exploration our measure for lod quality is based on the formulation of entropy from information_theory the measure takes into account the distortion and contribution of multiresolution data blocks a lod map is generated through the mapping of key lod ingredients to a treemap representation the ordered treemap layout is used for relative stable update of the lod map when the view or lod changes this visual interface not only indicates the quality of lods in an intuitive way but also provides immediate suggestions for possible lod improvement through visually-striking features it also allows us to compare different views and perform rendering budget control a set of interactive techniques is proposed to make the lod adjustment a simple and easy task we demonstrate the effectiveness and efficiency of our approach on large scientific and medical data sets lod map knowledge_representation large_volume_visualization author  multiresolution_rendering perceptual_reasoning
a method for the semi-automatic detection and visualization of defects in models of nematic_liquid_crystals nlcs is introduced this method is suitable for unstructured models a previously unsolved problem the detected defects - also known as disclinations - are regions were the alignment of the liquid crystal rapidly changes over space these defects play a large role in the physical behavior of the nlc substrate defect detection is based upon a measure of total angular change of crystal orientation the director over a node neighborhood via the use of a nearest neighbor path visualizations based upon the detection algorithm clearly identify complete defect regions as opposed to incomplete visual descriptions provided by cutting-plane and isosurface approaches the introduced techniques are currently in use by scientists studying the dynamics of defect change defects disclination feature_extraction nematic_liquid_crystal scientific_visualization unstructured grid
when a heavy fluid is placed above a light fluid tiny vertical perturbations in the interface create a characteristic structure of rising bubbles and falling spikes known as rayleigh-taylor instability rayleigh-taylor instabilities have received much attention over the past half-century because of their importance in understanding many natural and man-made phenomena ranging from the rate of formation of heavy elements in supernovae to the design of capsules for inertial confinement fusion we present a new approach to analyze rayleigh-taylor instabilities in which we extract a hierarchical segmentation of the mixing envelope surface to identify bubbles and analyze analogous segmentations of fields on the original interface plane we compute meaningful statistical information that reveals the evolution of topological features and corroborates the observations made by scientists we also use geometric tracking to follow the evolution of single bubbles and highlight merge/split events leading to the formation of the large and complex structures characteristic of the later stages in particular we i provide a formal definition of a bubble ii segment the envelope surface to identify bubbles iii provide a multi-scale analysis technique to produce statistical measures of bubble growth iv correlate bubble measurements with analysis of fields on the interface plane v track the evolution of individual bubbles over time our approach is based on the rigorous mathematical foundations of morse_theory and can be applied to a more general class of applications morse_theory multi-resolution topology
the network for computational nanotechnology ncn has developed a science gateway at nanohuborg for nanotechnology education and research remote users can browse through online seminars and courses and launch sophisticated nanotechnology simulation tools all within their web browser simulations are supported by a middleware that can route complex jobs to grid supercomputing resources but what is truly unique about the middleware is the way that it uses hardware accelerated graphics to support both problem setup and result visualization this paper describes the design and integration of a remote_visualization_framework into the nanohub for interactive visual_analytics of nanotechnology simulations our services flexibly handle a variety of nanoscience simulations render them utilizing graphics_hardware_acceleration in a scalable manner and deliver them seamlessly through the middleware to the user rendering is done only on-demand as needed so each graphics_hardware unit can simultaneously support many user sessions additionally a novel node distribution scheme further improves our system's scalability our approach is not only efficient but also cost-effective only half-dozen render nodes are anticipated to support hundreds of active tool sessions on the nanohub moreover this architecture and visual_analytics environment provides capabilities that can serve many areas of scientific simulation and analysis beyond nanotechnology with its ability to interactively analyze and visualize multivariate scalar and vector_fields flow_visualization graphics_hardware nanotechnology simulation remote_visualization volume_visualization
in this paper we describe a gpu-based technique for creating illustrative_visualization through interactive_manipulation of volumetric models it is partly inspired by medical illustrations where it is common to depict cuts and deformation in order to provide a better understanding of anatomical and biological structures or surgical processes and partly motivated by the need for a real-time solution that supports the specification and visualization of such illustrative_manipulation we propose two new feature aligned techniques namely surface alignment and segment alignment and compare them with the axis-aligned techniques which were reported in previous work on volume_manipulation we also present a mechanism for defining features using texture volumes and methods for computing correct normals for the deformed volume in respect to different alignments we describe a gpu-basedimplementation to achieve real-time performance of the techniques and a collection of manipulation operators including peelers retractors pliers and dilators which are adaptations of the metaphors and tools used in surgical procedures and medical illustrations our approach is directly applicable in medical and biological illustration and we demonstrate how it works as an interactive tool for focus+context_visualization as well as a generic technique for volume graphics gpu_computing illustrative_manipulation illustrative_visualization computer-assisted medical illustration volume_deformation volume_rendering
exploded_views are an illustration technique where an object is partitioned into several segments these segments are displaced to reveal otherwise hidden detail in this paper we apply the concept of exploded_views to volumetric_data in order to solve the general problem of occlusion in many cases an object of interest is occluded by other structures while transparency or cutaways can be used to reveal a focus object these techniques remove parts of the context information exploded_views on the other hand do not suffer from this drawback our approach employs a force-based model the volume is divided into a part configuration controlled by a number of forces and constraints the focus object exerts an explosion force causing the parts to arrange according to the given constraints we show that this novel and flexible approach allows for a wide variety of explosion-based visualizations including view-dependent explosions furthermore we present a high-quality gpu-based volume ray casting algorithm for exploded_views which allows rendering and interaction at several frames per second exploded_views illustrative_visualization volume_rendering
caricatures are pieces of art depicting persons or sociological conditions in a non-veridical way in both cases caricatures are referring to a reference model the deviations from the reference model are the characteristic features of the depicted subject good caricatures exaggerate the characteristics of a subject in order to accent them the concept of caricaturistic visualization is based on the caricature metaphor the aim of caricaturistic visualization is an illustrative depiction of characteristics of a given dataset by exaggerating deviations from the reference model we present the general concept of caricaturistic visualization as well as a variety of examples we investigate different visual representations for the depiction of caricatures further we present the caricature matrix a technique to make differences between datasets easily identifiable exploded_views illustrative_visualization volume_rendering
video_visualization is a computation process that extracts meaningful information from original video data sets and conveys the extracted information to users in appropriate visual representations this paper presents a broad treatment of the subject following a typical research pipeline involving concept formulation system development a path-finding user_study and a field trial with real application data in particular we have conducted a fundamental study on the visualization of motion events in videos we have for the first time deployed flow_visualization_techniques in video_visualization we have compared the effectiveness of different abstract visual representations of videos we have conducted a user_study to examine whether users are able to learn to recognize visual_signatures of motions and to assist in the evaluation of different visualization_techniques we have applied our understanding and the developed techniques to a set of application video clips our study has demonstrated that video_visualization is both technically feasible and cost-effective it has provided the first set of evidence confirming that ordinary users can be accustomed to the visual_features depicted in video_visualizations and can learn to recognize visual_signatures of a variety of motion events gpu_rendering video_visualization flow_visualization human_factors optical_flow user_study video_processing visual_signatures volume_visualization
centralized techniques have been used until now when automatically calibrating both geometrically and photometrically large high-resolution displays created by tiling multiple projectors in a 2d array a centralized server managed all the projectors and also the cameras used to calibrate the display in this paper we propose an asynchronous distributed calibration methodology via a display unit called the plug-and-play projector ppp the ppp consists of a projector camera computation andcommunication unit thus creating a self-sufficient module that enables an asynchronous distributed architecture for multi-projector displays we present a single-program-multiple-data spmd calibration algorithm that runs on each ppp and achieves a truly scalable and reconfigurable display without any input from the user it instruments novel capabilities like adding/removing ppps from the display dynamically detecting faults and reshaping the display to a reasonable rectangular shape to react to the addition/removal/faults to the best of our knowledge this is the first attempt to realize a completely asynchronous and distributed calibration architecture and methodology for multi-projector displays multi-projector displays distributed_algorithms geometric_and_color_calibration projector-camera_systems
animation is an effective way to show how time-varying phenomena evolve over time a key issue of generating a good animation is to select ideal views through which the user can perceive the maximum amount of information from the time-varying_dataset in this paper we first propose an improved view selection method for static data the method measures the quality of a static view by analyzing the opacity color and curvature distributions of the corresponding volume_rendering images from the given view our view selection metric prefers an even opacity distribution with a larger projection area a larger area of salient features' colors with an even distribution among the salient features and more perceived curvatures we use this static_view_selection method and a dynamic programming approach to select time-varying views the time-varying view selection maximizes the information perceived from the time-varying_dataset based on the constraints that the time-varying view should show smooth changes of direction and near-constant speed we also introduce a method that allows the user to generate a smooth transition between any two views in a given time step with the perceived information maximized as well by combining the static and dynamic_view_selection methods the users are able to generate a time-varying view that shows the maximum amount of information from a time-varying_data set dynamic_view_selection image_based_method information_entropy optimization static_view_selection
we present empirical studies that consider the effects of stereopsis and simulated aerial perspective on depth perception in translucent volumes we consider a purely absorptive lighting model in which light is not scattered or reflected but is simply absorbed as it passes through the volume a purely absorptive lighting model is used for example when rendering digitally_reconstructed_radiographs drrs which are synthetic x-ray images reconstructed from ct volumes surgeons make use of drrs in planning and performing operations so an improvement of depth perception in drrs may help diagnosis and surgical planning radiograph stereo stereopsis volume_rendering x--ray
this paper is a contribution to the literature on perceptually optimal_visualizations of layered three-dimensional surfaces specifically we develop guidelines for generating texture patterns which when tiled on two overlapped surfaces minimize confusion in depth-discrimination and maximize the ability to localize distinct features we design a parameterized texture space and explore this texture space using a "human in the loop" experimental approach subjects are asked to rate their ability to identify gaussian bumps on both upper and lower surfaces of noisy terrain fields their ratings direct a genetic_algorithm which selectively searches the texture parameter space to find fruitful areas data collected from these experiments are analyzed to determine what combinations of parameters work well and to develop texture generation guidelines data_analysis methods include anova linear_discriminant_analysis decision_trees and parallel_coordinates to confirm the guidelines we conduct a post-analysis experiment where subjects rate textures following our guidelines against textures violating the guidelines across all subjects textures following the guidelines consistently produce high rated textures on an absolute scale and are rated higher than those that did not follow the guidelines data_mining decision_trees genetic_algorithm human-in-the-loop layered_surfaces linear_discriminant_analysis optimal_visualization parallel_coordinates perception
we present an evaluation of a parameterized set of 2d icon-based visualization methods where we quantified how perceptual_interactions among visual elements affect effective data_exploration during the experiment subjects quantified three different design_factors for each method the spatial resolution it could represent the number of data values it could display at each point and the degree to which it is visually linear the class of visualization methods includes poisson-disk distributed icons where icon size icon spacing and icon brightness can be set to a constant or coupled to data values from a 2d scalar field by only coupling one of those visual components to data we measured filtering interference for all three design_factors filtering interference characterizes how different levels of the constant visual elements affect the evaluation of the data-coupled element our novel experimental_methodology allowed us to generalize this perceptual information gathered using ad-hoc artificial datasets onto quantitative rules for visualizing real scientific datasets this work also provides a framework for evaluating visualizations of multi-valued data that incorporate additional visual_cues such as icon orientation or color d visualization methods perception_models perceptual_interactions visual_design visualization_evaluation
this paper presents a method for occlusion-free_animation of geographical landmarks and its application to a new type of car navigation system in which driving routes of interest are always visible this is achieved by animating a nonperspective image where geographical landmarks such as mountain tops and roads are rendered as if they are seen from different viewpoints the technical contribution of this paper lies in formulating the nonperspective terrain navigation as an inverse problem of continuously deforming a 3d terrain surface from the 2d screen arrangement of its associated geographical landmarks the present approach provides a perceptually reasonable compromise between the navigation clarity and visual realism where the corresponding nonperspective view is fully augmented by assigning appropriate textures and shading effects to the terrain surface according to its geometry an eye_tracking experiment is conducted to prove that the present approach actually exhibits visually-pleasing navigation frames while users can clearly recognize the shape of the driving route without occlusion together with the spatial configuration of geographical landmarks in its neighborhood car_navigation_systems nonperspective_projection occlusion-free_animation temporal_coherence visual perception
we present gyve an interactive_visualization tool for understanding structure in sparse three-dimensional 3d point data the scientific goal driving the tool's development is to determine the presence of filaments and voids as defined by inferred 3d galaxy positions within the horologium-reticulum supercluster hrs gyve provides visualization_techniques tailored to examine structures defined by the intercluster galaxies specific techniques include interactive user control to move between a global overview and local viewpoints labelled axes and curved drop lines to indicate positions in the astronomical ra-dec-cz coordinate system torsional rocking and stereo to enhance 3d perception and geometrically distinct glyphs to show potential correlation between intercluster galaxies and known clusters we discuss the rationale for each design decision and review the success of the techniques in accomplishing the scientific goals in practice gyve has been useful for gaining intuition about structures that were difficult to perceive with 2d_projection techniques alone for example during their initial session with gyve our collaborators quickly confirmed scientific conclusions regarding the large-scale structure of the hrs previously obtained over months of study with 2d_projections and statistical techniques further use of gyve revealed the spherical shape of voids and showed that a presumed filament was actually two disconnected structures sparse point visualization astronomy cosmology
meteorological research involves the analysis of multi-field multi-scale and multi-source data sets unfortunately traditional atmospheric visualization_systems only provide tools to view a limited number of variables and small segments of the data these tools are often restricted to 2d contour or vector plots or 3d isosurfaces the meteorologist must mentally synthesize the data from multiple plots to glean the information needed to produce a coherent picture of the weather phenomenon of interest in order to provide better tools to meteorologists and reduce system limitations we have designed an integrated atmospheric visual_analysis and exploration system for interactive analysis of weather data sets our system allows for the integrated visualization of 1d 2d and 3d atmospheric data sets in common meteorological grid structures and utilizes a variety of rendering techniques these tools provide meteorologists with new abilities to analyze their data and answer questions on regions of interest ranging from physics-based atmospheric rendering to illustrative_rendering containing particles and glyphs in this paper we discuss the use and performance of our visual_analysis for two important meteorological applications the first application is warm rain formation in small cumulus clouds in this our three-dimensional interactive_visualization of modeled drop trajectories within spatially correlated fields from a cloud simulation has provided researchers with new insight our second application is improving and validating severe storm models specifically the weather research and forecasting wrf model this is done through correlative visualization of wrf model and experimental doppler storm data glyph_rendering grid structures transfer_function volume_rendering volume_visualization warm_rain_entrainment_process weather_visualization
thread-like structures are becoming more common in modern volumetric_data sets as our ability to image vascular and neural tissue at higher resolutions improves the thread-like structures of neurons and micro-vessels pose a unique problem in visualization since they tend to be densely packed in small volumes of tissue this makes it difficult for an observer to interpret useful patterns from the data or trace individual fibers in this paper we describe several methods for dealing with large amounts of thread-like data such as data sets collected using knife-edge scanning microscopy kesm and serial block-face scanning electron microscopy sbf-sem these methods allow us to collect volumetric_data from embedded samples of whole-brain tissue the neuronal and microvascular data that we acquire consists of thin branching structures extending over very large regions traditional visualization schemes are not sufficient to make sense of the large dense complex structures encountered in this paper we address three methods to allow a user to explore a fiber network effectively we describe interactive techniques for rendering large sets of neurons using self-orienting surfaces implemented on the gpu we also present techniques for rendering fiber networks in a way that provides useful information about flow and orientation third a global_illumination framework is used to create high-quality visualizations that emphasize the underlying fiber structureimplementation details performance and advantages and disadvantages of each approach are discussed gpu_acceleration global_illumination neuron visualization orientation_filtering
we present a comparative_visualization of the acoustic_simulation results obtained by two different approaches that were combined into a single simulation algorithm the first method solves the wave equation on a volume grid based on finite_elements the second method phonon_tracing is a geometric approach that we have previously developed for interactive simulation visualization and modeling of room_acoustics geometric approaches of this kind are more efficient than fem in the high and medium frequency range for low frequencies they fail to represent diffraction which on the other hand can be simulated properly by means of fem when combining both methods we need to calibrate them properly and estimate in which frequency range they provide comparable results for this purpose we use an acoustic metric called gain and display the resulting error furthermore we visualize interference patterns since these depend not only on diffraction but also exhibit phase-dependent amplification and neutralization effects acoustic_simulation comparative_visualization nite element method phonon_map ray tracing
diffusion_tensor_imaging is of high value in neurosurgery providing information about the location of white_matter_tracts in the human brain for their reconstruction streamline techniques commonly referred to as fiber_tracking model the underlying fiber structures and have therefore gained interest to meet the requirements of surgical planning and to overcome the visual limitations of line representations a new real-time_visualization approach of high visual quality is introduced for this purpose textured triangle strips and point sprites are combined in a hybrid strategy employing gpu_programming the triangle strips follow the fiber streamlines and are textured to obtain a tube-like appearance a vertex program is used to orient the triangle strips towards the camera in order to avoid triangle flipping in case of fiber segments where the viewing and segment direction are parallel a correct visual representation is achieved in these areas by chains of point sprites as a result high_quality visualization similar to tubes is provided allowing for interactive multimodal inspection overall the presented approach is faster than existing techniques of similar visualization quality and at the same time allows for real-time rendering of dense bundles encompassing a high number of fibers which is of high importance for diagnosis and surgical planning diffusion tensor data fiber_tracking streamline_visualization
topological methods are often used to describe flow structures in fluid dynamics and topological flow_field_analysis usually relies on the invariants of the associated tensor_fields a visual impression of the local properties of tensor_fields is often complex and the search of a suitable technique for achieving this is an ongoing topic in visualization this paper introduces and assesses a method of representing the topological properties of tensor_fields and their respective flow patterns with the use of colors first a tensor norm is introduced which preserves the properties of the tensor and assigns the tensor_invariants to values of the rgb color space secondly the rgb colors of the tensor_invariants are transferred to corresponding hue values as an alternative color representation the vectorial tensor_invariants field is reduced to a scalar hue field and visualization of iso-surfaces of this hue value field allows us to identify locations with equivalent flow topology additionally highlighting by the maximum of the eigenvalue difference field reflects the magnitude of the structural change of the flow the method is applied on a vortex_breakdown flow structure inside a cylinder with a rotating lid flow_visualization invariants tensor_field_topology
a glyph-based method for visualizing the nematic_liquid_crystal alignment tensor is introduced unlike previous approaches the glyph is based upon physically-linked metrics not offsets of the eigenvalues these metrics combined with a set of superellipsoid shapes communicate both the strength of the crystal's uniaxial alignment and the amount of biaxiality with small modifications our approach can visualize any real symmetric traceless tensor nematic_liquid_crystals scientific_visualization symmetric traceless tensor tensor_visualization
isosurfaces are ubiquitous in many fields including visualization graphics and vision they are often the main computational component of important processing pipelines eg surface_reconstruction and are heavily used in practice the classical approach to compute isosurfaces is to apply the marching_cubes algorithm which although robust and simple to implement generates surfaces that require additional processing steps to improve triangle quality and mesh size an important issue is that in some cases the surfaces generated by marching_cubes are irreparably damaged and important details are lost which can not be recovered by subsequent processing the main motivation of this work is to develop a technique capable of constructing high-quality and high-fidelity isosurfaces we propose a new advancing_front technique that is capable of creating high-quality isosurfaces from regular and irregular volumetric_datasets our work extends the guidance field framework of schreiner et al to implicit_surfaces and improves it in significant ways in particular we describe a set of sampling conditions that guarantee that surface features will be captured by the algorithm we also describe an efficient technique to compute a minimal guidance field which greatly improves performance our experimental results show that our technique can generate high-quality meshes from complex datasets advancing_front curvature isosurface_extraction
current computer architectures employ caching to improve the performance of a wide variety of applications one of the main characteristics of such cache schemes is the use of block fetching whenever an uncached data element is accessed to maximize the benefit of the block fetching mechanism we present novel cache-aware_and_cache-oblivious_layouts of surface and volume meshes that improve the performance of interactive_visualization and geometric processing algorithms based on a general i/o model we derive new cache-aware and cache-oblivious metrics that have high correlations with the number of cache misses when accessing a mesh in addition to guiding the layout process our metrics can be used to quantify the quality of a layout eg for comparing different layouts of the same mesh and for determining whether a given layout is amenable to significant improvement we show that layouts of unstructured_meshes optimized for our metrics result in improvements over conventional layouts in the performance of visualization_applications such as isosurface_extraction and view-dependent rendering moreover we improve upon recent cache-oblivious mesh layouts in terms of performance applicability andaccuracy mesh_and_graph_layouts cache-aware_and_cache-oblivious_layouts data locality metrics_for_cache_coherence
we propose an out-of-core method for creating semi-regular surface_representations from large input surface meshes our approach is based on a streamingimplementation of the maps remesher of lee et al our remeshing procedure consists of two stages first a simplification process is used to obtain the base domain during simplification we maintain the mapping information between the input and the simplified meshes the second stage of remeshing uses the mapping information to produce samples of the output semi-regular mesh the out-of-core operation of our method is enabled by the synchronous streaming of a simplified mesh and the mapping information stored at the original vertices the synchronicity of two streaming buffers is maintained using a specially designed write strategy for each buffer experimental results demonstrate the remeshing performance of the proposed method as well as other applications that use the created mapping between the simplified and the original surface_representations out-of-core_algorithm semi-regular_remeshing shape compression
computational simulations frequently generate solutions defined over very large tetrahedral volume meshes containing many millions of elements furthermore such solutions may often be expressed using non-linear basis_functions certain solution techniques such as discontinuous_galerkin_methods may even produce non-conforming meshes such data is difficult to visualize interactively as it is far too large to fit in memory and many common data reduction techniques such as mesh simplification cannot be applied to non-conforming meshes we introduce a point-based_visualization system for interactive rendering of large potentially non-conforming tetrahedral_meshes we propose methods for adaptively sampling points from non-linear solution data and for decimating points at run time to fit gpu memory limits because these are streaming processes memory consumption is independent of the input size we also present an order-independent point rendering method that can efficiently render volumes on the order of 20 million tetrahedra at interactive rates interactive_large_higher-order_tetrahedral_volume_visualization point-based_visualization
large scale scientific simulation codes typically run on a cluster of cpus that write/read time steps to/from a single file system as data sets are constantly growing in size this increasingly leads to i/o bottlenecks when the rate at which data is produced exceeds the available i/o bandwidth the simulation stalls and the cpus are idle data_compression can alleviate this problem by using some cpu cycles to reduce the amount of data needed to be transfered most compression schemes however are designed to operate offline and seek to maximize compression not throughput furthermore they often require quantizing floating-point values onto a uniform integer grid which disqualifies their use in applications where exact values must be retained we propose a simple scheme for lossless online compression of floating-point data that transparently integrates into the i/o of many applications a plug-in scheme for data-dependent prediction makes our scheme applicable to a wide variety of data used in visualization such as unstructured_meshes point sets images and voxel grids we achieve state-of-the-art compression rates and speeds the latter in part due to an improved entropy coder we demonstrate that this significantly accelerates i/o throughput in real simulation runs unlike previous schemes our method also adapts well to variable-precision floating-point and integer data high_throughput fast_entropy_coding file compaction for i/o efficiency large scale simulation and visualization lossless_compression predictive_coding range_coder
in this paper we propose an approach in which interactive_visualization and analysis are combined with batch tools for the processing of large_data collections large and heterogeneous data collections are difficult to analyze and pose specific problems to interactive_visualization_application of the traditional interactive_processing and visualization approaches as well as batch processing encounter considerable drawbacks for such large and heterogeneous data collections due to the amount and type of data computing resources are not sufficient for interactive_exploration of the data and automated analysis has the disadvantage that the user has only limited control and feedback on the analysis process in our approach an analysis procedure with features and attributes of interest for the analysis is defined interactively this procedure is used for offline processing of large collections of data sets the results of the batch process along with "visual summaries" are used for further analysis visualization is not only used for the presentation of the result but also as a tool to monitor the validity and quality of the operations performed during the batch process operations such as feature_extraction and attribute calculation of the collected data sets are validated by visual inspection this approach is illustrated by an extensive case_study in which a collection of confocal_microscopy data sets is analyzed biomedical_visualization features_in_volume_data_sets large_data_set_visualization author
in this paper we show that histograms represent spatial function distributions with a nearest neighbour interpolation we confirm that this results in systematic underrepresentation of transitional features of the data and provide new insight why this occurs we further show that isosurface statistics which use higher quality interpolation give better representations of the function distribution we also use our experimentally collected isosurface statistics to resolve some questions as to the formal complexity of isosurfaces histograms isosurface statistics isosurfaces
we present an efficient point-based isosurface exploration system with high_quality rendering our system incorporates two point-based isosurface_extraction and visualization methods edge splatting and the edge kernel method in a volume two neighboring voxels define an edge the intersection points between the active edges and the isosurface are used for exact isosurface_representation the point generation is incorporated in the gpu-based hardware-accelerated rendering thus avoiding any overhead when changing the isovalue in the exploration we call this method edge splatting in order to generate high_quality isosurface rendering regardless of the volume resolution and the view we introduce an edge kernel method the edge kernel upsamples the isosurface by subdividing every active cell of the volume data enough sample points are generated to preserve the exact shape of the isosurface defined by the trilinear_interpolation of the volume data by employing these two methods we can achieve interactive isosurface exploration with high_quality rendering gpu_acceleration isosurface hardware_acceleration isosurface_extraction point-based_visualization
we present a novel approach to out-of-core time-varying isosurface visualization we attempt to interactively visualize time-varying_datasets which are too large to fit into main memory using a technique which is dramatically different from existing algorithms inspired by video encoding techniques we examine the data differences between time steps to extract isosurface information we exploit span_space extraction techniques to retrieve operations necessary to update isosurface geometry from neighboring time steps because only the changes between time steps need to be retrieved from disk i/o bandwidth requirements are minimized we apply temporal compression to further reduce disk access and employ a point-based previewing technique that is refined in idle interaction cycles our experiments on computational simulation data indicate that this method is an extremely viable solution to large time-varying isosurface visualization our work advances the state-of-the-art by enabling all isosurfaces to be represented by a compact set of operations isosurface out-of-core point-based_rendering span_space time-varying
we propose a novel persistent octree pot indexing structure for accelerating isosurface_extraction and spatial filtering from volumetric_data this data structure efficiently handles a wide range of visualization problems such as the generation of view-dependent isosurfaces ray tracing and isocontour slicing for high dimensional data pot can be viewed as a hybrid data structure between the interval tree and the branch-on-need octree bono in the sense that it achieves the asymptotic bound of the interval tree for identifying the active cells corresponding to an isosurface and is more efficient than bono for handling spatial queries we encode a compact octree for each isovalue each such octree contains only the corresponding active cells in such a way that the combined structure has linear space the inherent hierarchical structure associated with the active cells enables very fast filtering of the active cells based on spatial constraints we demonstrate the effectiveness of our approach by performing view-dependent isosurfacing on a wide variety of volumetric_data sets and 4d isocontour slicing on the time-varying richtmyer-meshkov instability dataset indexing isosurface_extraction scientific_visualization
volumetric_datasets with multiple variables on each voxel over multiple time steps are often complex especially when considering the exponentially large attribute space formed by the variables in combination with the spatial and temporal dimensions it is intuitive practical and thus often desirable to interactively select a subset of the data from within that high-dimensional value space for efficient visualization this approach is straightforward to implement if the dataset is small enough to be stored entirely in-core however to handle datasets sized at hundreds of gigabytes and beyond this simplistic approach becomes infeasible and thus more sophisticated solutions are needed in this work we developed a system that supports efficient visualization of an arbitrary subset selected by range-queries of a large multivariate time-varying_dataset by employing specialized data structures and schemes of data distribution our system can leverage a large number of networked computers as parallel data servers and guarantees a near optimal load-balance we demonstrate our system of scalable data servers using two large time-varying simulation datasets parallel and distributed volume_visualization large_data_set_visualization multi-variate_visualization volume_visualization
we present a cluster-based volume_rendering system for roaming very large_volumes this system allows to move a gigabyte-sized probe inside a total volume of several tens or hundreds of gigabytes in real-time while the size of the probe is limited by the total amount of texture memory on the cluster the size of the total data set has no theoretical limit the cluster is used as a distributed graphics processing unit that both aggregates graphics power and graphics memory a hardware-accelerated volume renderer runs in parallel on the cluster nodes and the final image_compositing is implemented using a pipelined sort-last_rendering algorithm meanwhile volume_bricking and volume paging allow efficient data caching on each rendering node a distributed hierarchical cache system implements a global software-based distributed_shared_memory on the cluster in case of a cache miss this system first checks page residency on the other cluster nodes instead of directly accessing local disks using two gigabit ethernet network interfaces per node we accelerate data fetching by a factor of 4 compared to directly accessing local disks the system also implements asynchronous disk access and texture loading which makes it possible to overlap data loading volume slicing and rendering for optimal volume_roaming large_volumes distributed_shared_memory graphics_cluster graphics_hardware hardware-accelerated_volume_visualization hierarchical caching out-of-core parallel_rendering volume_roaming
we describe a new progressive technique that allows real-time rendering of extremely large tetrahedral_meshes our approach uses a client-server architecture to incrementally stream portions of the mesh from a server to a client which refines the quality of the approximate rendering until it converges to a full quality rendering the results of previous steps are re-used in each subsequent refinement thus leading to an efficient rendering our novel approach keeps very little geometry on the client and works by refining a set of rendered images at each step our interactive representation of the dataset is efficient light-weight and high_quality we present a framework for the exploration of large_datasets stored on a remote server with a thin client that is capable of rendering and managing full quality volume_visualizations client-server large unstructured_grids level-of-detail progressive_rendering volume_rendering
accurately representing higher-order singularities of vector_fields defined on piecewise linear surfaces is a non-trivial problem in this work we introduce a concise yet complete interpolation scheme of vector_fields on arbitrary triangulated surfaces the scheme enables arbitrary singularities to be represented at vertices the representation can be considered as a facet-based "encoding" of vector_fields on piecewise linear surfaces the vector field is described in polar coordinates over each facet with a facet edge being chosen as the reference to define the angle an integer called the period jump is associated to each edge of the triangulation to remove the ambiguity when interpolating the direction of the vector field between two facets that share an edge to interpolate the vector field we first linearly interpolate the angle of rotation of the vectors along the edges of the facet graph then we use a variant of nielson's side-vertex scheme to interpolate the vector field over the entire surface with our representation we remove the bound imposed on the complexity of singularities that a vertex can represent by its connectivity this bound is a limitation generally exists in vertex-based linear schemes furthermore using our data structure the index of a vertex of a vector field can be combinatorily determined gpu higher-order singularities line_integral_convolution vector_field_visualization
a common goal of multivariate visualization is to enable data inspection at discrete points while also illustrating larger-scale continuous structures in diffusion_tensor_visualization glyphs are typically used to meet the first goal and methods such as texture_synthesis or fiber_tractography can address the second we adapt particle_systems originally developed for surface_modeling and anisotropic mesh generation to enhance the utility of glyph-based tensor_visualizations by carefully distributing glyphs throughout the field either on a slice or in the volume into a dense packing using potential energy profiles shaped by the local tensor value we remove undue visual emphasis of the regular sampling grid of the data and the underlying continuous features become more apparent the method is demonstrated on a dt-mri scan of a patient with a brain tumor diffusion tensor anisotropic_sampling fiber_tractography glyphs particle_systems
recent advances in algorithms and graphics_hardware have opened the possibility to render tetrahedral_grids at interactive rates on commodity pcs this paper extends on this work in that it presents a direct_volume_rendering method for such grids which supports both current and upcoming graphics_hardware architectures large and deformable grids as well as different rendering options at the core of our method is the idea to perform the sampling of tetrahedral elements along the view rays entirely in local barycentric_coordinates then sampling requires minimum gpu memory and texture access operations and it maps efficiently onto a feed-forward pipeline of multiple stages performing computation and geometry construction we propose to spawn rendered elements from one single vertex this makes the method amenable to upcoming direct3d 10 graphics_hardware which allows to create geometry on the gpu by only modifying the algorithm slightly it can be used to render per-pixel iso-surfaces and to perform tetrahedral cell_projection as our method neither requires any pre-processing nor an intermediate grid representation it can efficiently deal with dynamic and large 3d meshes direct_volume_rendering programmable_graphics_hardware unstructured_grids
in this paper we investigate the effects of function composition in the form gfx = hx by means of a spectral analysis of h we decompose the spectral description of hx into a scalar product of the spectral description of gx and a term that solely depends on fx and that is independent of gx we then use the method of stationary phase to derive the essential maximum frequency of gfx bounding the main portion of the energy of its spectrum this limit is the product of the maximum frequency of gx and the maximum derivative of fx this leads to a proper sampling of the composition h of the two functions g and f we apply our theoretical results to a fundamental open problem in volume_rendering - the proper sampling of the rendering integral after the application of a transfer_function in particular we demonstrate how the sampling criterion can be incorporated in adaptive ray integration visualization with multi-dimensional_transfer_functions and pre-integrated volume_rendering fourier_transform adaptive sampling signal_processing transfer_function volume_rendering
the internet has become a wild place malicious code is spread on personal computers across the world deploying botnets ready to attack the network infrastructure the vast number of security incidents and other anomalies overwhelms attempts at manual analysis especially when monitoring service provider backbone links we present an approach to interactive_visualization with a case_study indicating that interactive_visualization can be applied to gain more insight into these large_data sets we superimpose a hierarchy on ip address space and study the suitability of treemap variants for each hierarchy level because viewing the whole ip hierarchy at once is not practical for most tasks we evaluate layout stability when eliding large parts of the hierarchy while maintaining the visibility and ordering of the data of interest information_visualization network_monitoring network_security treemap
online pick'em games such as the recent ncaa college basketball march madness tournament form a large and rapidly growing industry in these games players make predictions on a tournament bracket that defines which competitors play each other and how they proceed toward a single champion throughout the course of the tournament players monitor the brackets to track progress and to compare predictions made by multiple players this is often a complex sense making task the classic bracket visualization was designed for use on paper and utilizes an incrementally additive system in which the winner of each match-up is rewritten in the next round as the tournament progresses unfortunately this representation requires a significant amount of space and makes it relatively difficult to get a quick overview of the tournament state since competitors take arbitrary paths through the static bracket in this paper we present adaptivitree a novel visualization that adaptively deforms the representation of the tree and uses its shape to convey outcome information adaptivitree not only provides a more compact and understandable representation but also allows overlays that display predictions as well as other statistics we describe results from a lab study we conducted to explore the efficacy of adaptivitree as well as from a deployment of the system in a recent real-world sports tournament online_fantasy_sports adaptive_tree_visualization bracket picks tournament
we describe the design and deployment of many eyes a public web site where users may upload data create interactive_visualizations and carry on discussions the goal of the site is to support collaboration around visualizations at a large scale by fostering a social style of data_analysis in which visualizations not only serve as a discovery tool for individuals but also as a medium to spur discussion among users to support this goal the site includes novel mechanisms for end-user creation of visualizations and asynchronous_collaboration around those visualizations in addition to describing these technologies we provide a preliminary report on the activity of our userscommunication-minded_visualization social_data_analysis social_software_visualization world_wide_web
this paper presents scented widgets graphical user interface controls enhanced with embedded visualizations that facilitate navigation ininformation_spaces we describe design_guidelines for adding visual_cues to common user interface widgets such as radio buttons sliders and combo boxes and contribute a general software framework for applying scented widgets within applications with minimal modifications to existing source code we provide a number of example applications and describe a controlled experiment which finds that users exploring unfamiliar data make up to twice as many unique discoveries using widgets imbued with social_navigation data however these differences equalize as familiarity with the data increases information_visualization information_foraging social_data_analysis social_navigation user_interface_toolkits
this paper describes show me an integrated set of user interface commands and defaults that incorporate automatic_presentation into a commercial visual_analysis system called tableau a key aspect of tableau is vizql a language for specifying views which is used by show me to extend automatic_presentation to the generation of tables of views commonly called small multiple displays a key research issue for the commercial application of automatic_presentation is the user_experience which must support the flow of visual_analysis user_experience has not been the focus of previous research on automatic_presentation the show me user_experience includes the automatic selection of mark types a command to add a single field to a view and a pair of commands to build views for multiple fields although the use of these defaults and commands is optional user interface logs indicate that show me is used by commercial users automatic_presentation best_practices data visualization graphic_design small_multiples visual_analysis
information_visualization has often focused on providing deep insight for expert user populations and on techniques for amplifying cognition through complicated interactive visual models this paper proposes a new subdomain for infovis research that complements the focus on analytic tasks and expert use instead of work-related and analytically driven infovis we propose casual_information_visualization or casual infovis as a complement to more traditional infovis domains traditional infovis systems techniques and methods do not easily lend themselves to the broad range of user populations from expert to novices or from work tasks to more everyday situations we propose definitions perspectives and research directions for further investigations of this emerging subfield these perspectives build from ambient information_visualization skog et al 2003 social_visualization and also from artistic work that visualizes information viegas and wattenberg 2007 we seek to provide a perspective on infovis that integrates these research agendas under a coherent vocabulary and framework for design we enumerate the following contributions first we demonstrate how blurry the boundary of infovis is by examining systems that exhibit many of the putative properties of infovis systems but perhaps would not be considered so second we explore the notion of insight and how instead of a monolithic definition of insight there may be multiple types each with particular characteristics third we discuss design challenges for systems intended for casual audiences finally we conclude with challenges for system evaluation in this emerging subfield casual_information_visualization ambient_infovis design editorial evaluation social_infovis
the technology available to building designers now makes it possible to monitor buildings on a very large scale video cameras and motion sensors are commonplace in practically every office space and are slowly making their way into living spaces the application of such technologies in particular video cameras while improving security also violates privacy on the other hand motion sensors while being privacy-conscious typically do not provide enough information for a human operator to maintain the same degree of awareness about the space that can be achieved by using video cameras we propose a novel approach in which we use a large number of simple motion sensors and a small set of video cameras to monitor a large office space in our system we deployed 215 motion sensors and six video cameras to monitor the 3000-square-meter office space occupied by 80 people for a period of about one year the main problem in operating such systems is finding a way to present this highly multidimensional data which includes both spatial and temporal components to a human operator to allow browsing and searching recorded data in an efficient and intuitive way in this paper we present our experiences and the solutions that we have developed in the course of our work on the system we consider this work to be the first step in helping designers and managers of building systems gain access to information about occupants' behavior in the context of an entire building in a way that is only minimally intrusive to the occupants' privacy sensor_networks spatio-temporal_visualization surveillance timeline user_interfaces
numerous systems have been developed to display large collections of data for urban contexts however most have focused on layering of single dimensions of data and manual calculations to understand relationships within the urban environment furthermore these systems often limit the user's perspectives on the data thereby diminishing the user's spatial understanding of the viewing region in this paper we introduce a highly interactive urban visualization tool that provides intuitive understanding of the urban_data our system utilizes an aggregation method that combines buildings and city_blocks into legible clusters thus providing continuous levels of abstraction while preserving the user's mental_model of the city in conjunction with a 3d view of the urban model a separate but integrated information_visualization view displays multiple disparate dimensions of the urban_data allowing the user to understand the urban environment both spatially and cognitively in one glance for our evaluation expert users from various backgrounds viewed a real city model with census data and confirmed that our system allowed them to gain more intuitive and deeper understanding of the urban model from different perspectives and levels of abstraction than existing commercial urban visualization_systems information_visualization multi-resolution urban_models
exploratory visual_analysis is useful for the preliminary investigation of large structured multifaceted spatio-temporal_datasets this process requires the selection and aggregation of records by time space and attribute the ability to transform data and the flexibility to apply appropriate visual_encodings and interactions we propose an approach inspired by geographical 'mashups' in which freely-available functionality and data are loosely but flexibly combined using de facto exchange standards our case_study combines mysql php and the landserf gis to allow google earth to be used for visual synthesis and interaction with encodings described in kml this approach is applied to the exploration of a log of 142 million requests made of a mobile directory service novel combinations of interaction and visual_encoding are developed including spatial 'tag_clouds' 'tag maps' 'data dials' and multi-scale density surfaces four aspects of the approach are informally evaluated the visual_encodings employed their success in the visual_exploration of the dataset the specific tools used and the 'mashup' approach preliminary findings will be beneficial to others considering using mashups for visualization the specific techniques developed may be more widely applied to offer insights into the structure of multifarious spatio-temporal_data of the type explored here large_dataset_visualization_applications_of_infovis geographic_visualization multiresolution_visualization text_and_document_visualization
understanding how people use online maps allows data acquisition teams to concentrate their efforts on the portions of the map that are most seen by users online maps represent vast databases and so it is insufficient to simply look at a list of the most-accessed urls hotmap takes advantage of the design of a mapping system's imagery pyramid to superpose a heatmap of the log files over the original maps users' behavior within the system can be observed and interpreted this paper discusses the imagery acquisition task that motivated hotmap and presents several examples of information that hotmap makes visible we discuss the design choices behind hotmap including logarithmic color schemes low-saturation background images and tuning images to explore both infrequently-viewed and frequently-viewed spaces gis geographical visualization heatmap online_mapping_systems server_log_analysis social_navigation
we present vislink a method by which visualizations and the relationships between them can be interactively explored vislink readily generalizes to support multiple visualizations empowers inter-representational queries and enables the reuse of the spatial variables thus supporting efficient information encoding and providing for powerful visualization bridging our approach uses multiple 2d layouts drawing each one in its own plane these planes can then be placed and re-positioned in 3d space side by side in parallel or in chosen placements that provide favoured views relationships connections and patterns between visualizations can be revealed and explored using a variety of interaction techniques including spreading activation and search filters d visualization graph_visualization edge_aggregation hierarchies node-link_diagrams structural_comparison
both the resource description framework rdf used in the semantic web and maya viz u-forms represent data as a graph of objects connected by labeled edges existing systems for flexible visualization of this kind of data require manual specification of the possible visualization roles for each data attribute when the schema is large and unfamiliar this requirement inhibits exploratory_visualization by requiring a costly up-front data_integration step to eliminate this step we propose an automatic technique for mapping data attributes to visualization attributes we formulate this as a schema matching problem finding appropriate paths in the data model for each required visualization attribute in a visualization template data_integration rdf attribute_inference
documents and other categorical valued time_series are often characterized by the frequencies of short range sequential patterns such as n-grams this representation converts sequential data of varying lengths to high dimensional histogram vectors which are easily modeled by standard statistical_models unfortunately the histogram representation ignores most of the medium and long range sequential dependencies making it unsuitable for visualizing sequential data we present a novel framework for sequential visualization of discrete categorical time_series based on the idea of local statistical modeling the framework embeds categorical time_series as smooth curves in the multinomial simplex summarizing the progression of sequential trends we discuss several visualization_techniques based on the above framework and demonstrate their usefulness for document_visualization document_visualization local_fitting multi-resolution_analysis
information visualisation is about gaining insight into data through a visual representation this data is often multivariate and increasingly the datasets are very large to help us explore all this data numerous visualisation applications both commercial and research prototypes have been designed using a variety of techniques and algorithms whether they are dedicated to geo-spatial_data or skewed hierarchical_data most of the visualisations need to adopt strategies for dealing with overcrowded displays brought about by too much data to fit in too small a display space this paper analyses a large number of these clutter_reduction methods classifying them both in terms of how they deal with clutter_reduction and more importantly in terms of the benefits and losses the aim of the resulting taxonomy is to act as a guide to match techniques to problems where different criteria may have different importance and more importantly as a means to critique and hence develop existing and new techniques clutter_reduction information visualisation large_datasets occlusion taxonomy
even though interaction is an important part of information_visualization infovis it has garnered a relatively low level of attention from the infovis community a few frameworks and taxonomies of infovis interaction techniques exist but they typically focus on low-level operations and do not address the variety of benefits interaction provides after conducting an extensive review of infovis systems and their interactive capabilities we propose seven general categories of interaction techniques widely used in infovis 1 select 2 explore 3 reconfigure 4 encode 5 abstract/elaborate 6 filter and 7 connect these categories are organized around a user's intent while interacting with a system rather than the low-level interaction techniques provided by a system the categories can act as a framework to help discuss and evaluate interaction techniques and hopefully lay an initial foundation toward a deeper understanding and a science_of_interaction information_visualization interaction interaction techniques taxonomy visual_analytics
in many domains increased collaboration has lead to more innovation by fostering the sharing of knowledge skills and ideas shared analysis of information_visualizations does not only lead to increased information processing power but team members can also share negotiate and discuss their views and interpretations on a dataset and contribute unique perspectives on a given problem designing technologies to support collaboration around information_visualizations poses special challenges and relatively few systems have been designed we focus on supporting small groups collaborating around information_visualizations in a co-located setting using a shared interactive tabletop display we introduce an analysis of challenges and requirements for the design of co-located collaborative information_visualization_systems we then present a new system that facilitates hierarchical_data comparison tasks for this type of collaborative work our system supports multi-user input shared and individual views on the hierarchical_data visualization flexible use of representations and flexible workspace organization to facilitate group_work around visualizations information_visualization co-located_work collaboration hierarchical_data comparison
treemaps provide an interesting solution for representing hierarchical_data however most studies have mainly focused on layout_algorithms and paid limited attention to the interaction with treemaps this makes it difficult to explore large_data sets and to get access to details especially to those related to the leaves of the trees we propose the notion of zoomable_treemaps ztms an hybridization between treemaps and zoomable user_interfaces that facilitates the navigation in large hierarchical_data sets by providing a consistent set of interaction techniques ztms make it possible for users to browse through very large_data sets eg 700000 nodes dispatched amongst 13 levels these techniques use the structure of the displayed data to guide the interaction and provide a way to improve interactive navigation in treemaps information_visualization multi-scale_interaction structure-aware navigation zoomable_treemaps
michotte's theory of ampliation suggests that causal relationships are perceived by objects animated under appropriate spatiotemporal conditions we extend the theory of ampliation and propose that the immediate perception of complex causal relations is also dependent on a set of structural and temporal rules we designed animated representations based on michotte's rules for showing complex causal relationships or causal semantics in this paper we describe a set of animations for showing semantics such as causal amplification causal strength causal dampening and causal multiplicity in a two part study we compared the effectiveness of both the static and animated representations the first study n=44 asked participants to recall passages that were previously displayed using both types of representations participants were 8% more accurate in recalling causal semantics when they were presented using animations instead of static graphs in the second study n=112 we evaluated the intuitiveness of the representations our results showed that while users were as accurate with the static graphs as with the animations they were 9% faster in matching the correct causal statements in the animated condition overall our results show that animated diagrams that are designed based on perceptual rules such as those proposed by michotte have the potential to facilitate comprehension of complex causal relations causality animated_graphs graph_semantics perception semantics visualization visualizing cause and effect
spatializations represent non-spatial_data using a spatial_layout similar to a map we present an experiment comparing different visual representations of spatialized data to determine which representations are best for a non-trivial search and point estimation task primarily we compare point-based displays to 2d and 3d information_landscapes we also compare a colour hue scale to a grey lightness scale for the task we studied point-based spatializations were far superior to landscapes and 2d landscapes were superior to 3d landscapes little or no benefit was found for redundantly encoding data using colour or greyscale combined with landscape height 3d landscapes with no colour scale height-only were particularly slow and inaccurate a colour scale was found to be better than a greyscale for all display types but a greyscale was helpful compared to height-only these results suggest that point-based spatializations should be chosen over landscape representations at least for tasks involving only point data itself rather than derived information about the data space d d colour greyscale information_landscape numerosity points spatialization surface user_study
in many applications it is important to understand the individual values of and relationships between multiple related scalar variables defined across a common domain several approaches have been proposed for representing data in these situations in this paper we focus on strategies for the visualization of multivariate data that rely on color mixing in particular through a series of controlled observer experiments we seek to establish a fundamental understanding of the information-carrying capacities of two alternative methods for encoding multivariate information using color color_blending and color_weaving we begin with a baseline experiment in which we assess participants' abilities to accurately read numerical data encoded in six different basic color scales defined in the l*a*b* color space we then assess participants' abilities to read combinations of 2 3 4 and 6 different data values represented in a common region of the domain encoded using either color_blending or color_weaving in color_blending a single mixed color is formed via linear combination of the individual values in l*a*b* space and in color_weaving the original individual colors are displayed side-by-side in a high frequency texture that fills the region a third experiment was conducted to clarify some of the trends regarding the color contrast and its effect on the magnitude of the error that was observed in the second experiment the results indicate that when the component colors are represented side-by-side in a high frequency texture most participants' abilities to infer the values of individual components are significantly improved relative to when the colors are blended participants' performance was significantly better with color_weaving particularly when more than 2 colors were used and even when the individual colors subtended only 3 minutes of visual angle in the texture however the information-carrying capacity of the color_weaving approach has its limits - - we found that participants' abilities to accurately interpret each of the individual components in a high frequency color texture typically falls off as the number of components increases from 4 to 6 we found no significant advantages in either color_blending or color_weaving to using color scales based on component hues thatare more widely separated in the l*a*b* color space furthermore we found some indications that extra difficulties may arise when opponent hues are employed color color_blending color_weaving perception visualization
in interfaces that provide multiple visual information resolutions vir low-vir overviews typically sacrifice visual details for display capacity with the assumption that users can select regions of interest to examine at higher vi rs designers can create low virs based on multi-level structure inherent in the data but have little guidance with single-level data to better guide design tradeoff between display capacity and visual target perceivability we looked at overview_use in two multiple-vir interfaces with high-vir displays either embedded within or separate from the overviews we studied two visual requirements for effective overview and found that participants would reliably use the low-vir overviews only when the visual targets were simple and had small visual spans otherwise at least 20% chose to use the high-vir view exclusively surprisingly neither of the multiple-vir interfaces provided performance benefits when compared to using the high-vir view alone however we did observe benefits in providing side-by-side comparisons for target matching we conjecture that the high cognitive load of multiple-vir interface interactions whether real or perceived is a more considerable barrier to their effective use than was previously considered multiple resolutions overview_use user_study
while the treemap is a popular method for visualizing hierarchical_data it is often difficult for users to track layout and attribute changes when the data evolve over time when viewing the treemaps side by side or back and forth there exist several problems that can prevent viewers from performing effective comparisons those problems include abrupt layout changes a lack of prominent visual patterns to represent layouts and a lack of direct contrast to highlight differences in this paper we present strategies to visualize changes of hierarchical_data using treemaps a new treemap_layout_algorithm is presented to reduce abrupt layout changes and produce consistent visual patterns techniques are proposed to effectively visualize the difference and contrast between two treemap snapshots in terms of the map items' colors sizes and positions experimental data show that our algorithm can achieve a good balance in maintaining a treemap's stability continuity readability and average aspect_ratio a software tool is created to compare treemaps and generate the visualizations user studies show that the users can better understand the changes in the hierarchy and layout and more quickly notice the color and size differences using our method treemap tree_comparison treemap_layout_algorithm visualize changes
we present a directed_acyclic_graph visualisation designed to allow interaction with a set of multiple classification trees specifically to find overlaps and differences between groups of trees and individual trees the work is motivated by the need to find a representation for multiple trees that has the space-saving property of a general graph representation and the intuitive parent-child direction cues present in individual representation of trees using example taxonomic data sets we describe augmentations to the common barycenter dag layout method that reveal shared sets of child nodes between common parents in a clearer manner other interactions such as displaying the multiple ancestor paths of a node when it occurs in several trees and revealing intersecting sibling sets within the context of a single dag representation are also discussed directed_acyclic_graph multiple trees
the need to visualize large social_networks is growing as hardware capabilities make analyzing large networks feasible and many new data sets become available unfortunately the visualizations in existing systems do not satisfactorily resolve the basic dilemma of being readable both for the global structure of the network and also for detailed analysis of local communities to address this problem we present nodetrix a hybrid representation for networks that combines the advantages of two traditional representations node-link_diagrams are used to show the global structure of a network while arbitrary portions of the network can be shown as adjacency matrices to better support the analysis of communities a key contribution is a set of interaction techniques these allow analysts to create a nodetrix visualization by dragging selections to and from node-link and matrix forms and to flexibly manipulate the nodetrix representation to explore the dataset and create meaningful summary visualizations of their findings finally we present a case_study applying nodetrix to the analysis of the infovis 2004 coauthorship dataset to illustrate the capabilities of nodetrix as both an exploration tool and an effective means of communicating results aggregation hybrid_visualization interaction matrix_visualization network_visualization
this paper presents a new algorithm for force directed graph_layout on the gpu the algorithm whose goal is to compute layouts accurately and quickly has two contributions the first contribution is proposing a general multi-level scheme which is based on spectral partitioning the second contribution is computing the layout on the gpu since the gpu requires a data parallel programming model the challenge is devising a mapping of a naturally unstructured graph into a well-partitioned structured one this is done by computing a balanced partitioning of a general graph this algorithm provides a general multi-level scheme which has the potential to be used not only for computation on the gpu but also on emerging multi-core architectures the algorithm manages to compute high_quality layouts of large graphs in a fraction of the time required by existing algorithms of similar quality an application for visualization of the topologies of isp internet service provider networks is presented gpu graph_layout graph partitioning
much of the visualization research has focused on improving the rendering quality and speed and enhancing the perceptibility of features in the data recently significant emphasis has been placed on focus+context f+c techniques eg fisheye_views and magnification lens for data_exploration in addition to viewing transformation and hierarchical navigation however most of the existing data_exploration techniques rely on the manipulation of viewing attributes of the rendering system or optical attributes of the data objects with users being passive viewers in this paper we propose a more active approach to data_exploration which attempts to mimic how we would explore data if we were able to hold it and interact with it in our hands this involves allowing the users to physically or actively manipulate the geometry of a data object while this approach has been traditionally used in applications such as surgical simulation where the original geometry of the data objects is well understood by the users there are several challenges when this approach is generalized for applications such as flow and information_visualization where there is no common perception as to the normal or natural geometry of a data object we introduce a taxonomy and a set of transformations especially for illustrative deformation of general data_exploration we present combined geometric or optical illustration operators for focus+context_visualization and examine the best means for preventing the deformed context from being misperceived we demonstrated the feasibility of this generalization with examples of flow information and video_visualization volume_deformation focus+context_visualization interaction techniques
conveying shape using feature lines is an important visualization tool in visual computing the existing feature lines eg ridges valleys silhouettes suggestive contours etc are solely determined by local geometry properties eg normals and curvatures as well as the view position this paper is strongly inspired by the observation in human vision and perception that a sudden change in the luminance plays a critical role to faithfully represent and recover the 3d information in particular we adopt the edge detection techniques in image_processing for 3d shape visualization and present photic_extremum_lines_(pels) which emphasize significant variations of illumination over 3d surfaces comparing with the existing feature lines pels are more flexible and offer users more freedom to achieve desirable visualization effects in addition the user can easily control the shape visualization by changing the light position the number of light sources and choosing various light models we compare pels with the existing approaches and demonstrate that pel is a flexible and effective tool to illustrate 3d surface and volume for visual computing surface_and_volume_illustration digital_geometry_processing illumination photic extremum lines (pels) ridges_and_valleys silhouettes suggestive contours
direct_volume_rendering techniques map volumetric attributes eg density gradient magnitude etc to visual styles commonly this mapping is specified by a transfer_function the specification of transfer_functions is a complex task and requires expert knowledge about the underlying rendering technique in the case of multiple volumetric attributes and multiple visual styles the specification of the multi-dimensional transfer_function becomes more challenging and non-intuitive we present a novel methodology for the specification of a mapping from several volumetric attributes to multiple illustrative visual styles we introduce semantic layers that allow a domain expert to specify the mapping in the natural language of the domain a semantic layer defines the mapping of volumetric attributes to one visual style volumetric attributes and visual styles are represented as fuzzy sets the mapping is specified by rules that are evaluated with fuzzy_logic arithmetics the user specifies the fuzzy sets and the rules without special knowledge about the underlying rendering technique semantic layers allow for a linguistic specification of the mapping from attributes to visual styles replacing the traditional transfer_function specification focus+context techniques illustrative_visualization volume_visualization
volumetric_data commonly has high depth complexity which makes it difficult to judge spatial relationships accurately there are many different ways to enhance depth perception such as shading contours and shadows artists and illustrators frequently employ halos for this purpose in this technique regions surrounding the edges of certain structures are darkened or brightened which makes it easier to judge occlusion based on this concept we present a flexible method for enhancing and highlighting structures of interest using gpu-based direct_volume_rendering our approach uses an interactively defined halo transfer_function to classify structures of interest based on data value direction and position a feature-preserving spreading algorithm is applied to distribute seed values to neighboring locations generating a controllably smooth field of halo intensities these halo intensities are then mapped to colors and opacities using a halo profile function our method can be used to annotate features at interactive frame rates volume_rendering halos illustrative_visualization
today's pcs incorporate multiple cpus and gpus and are easily arranged in clusters for high-performance interactive_graphics we present an approach based on hierarchical screen-space tiles to parallelizing rendering with level of detail adapt tiles render tiles and machine tiles are associated with cpus gpus and pcs respectively to efficiently parallelize the workload with good resource utilization adaptive tile sizes provide load balancing while our level of detail system allows total and independent management of the load on cpus and gpus we demonstrate our approach on parallel configurations consisting of both single pcs and a cluster of pcs geometric calibration photometric_calibration tiled_displays
we present a general framework for the modeling and optimization of scalable multi-projector displays based on this framework we derive algorithms that can robustly optimize the visual quality of an arbitrary combination of projectors without manual adjustment when the projectors are tiled we show that our framework automatically produces blending maps that outperform state-of-the-art projector blending methods when all the projectors are superimposed the framework can produce high-resolution images beyond the nyquist resolution limits of component projectors when a combination of tiled and superimposed projectors are deployed the same framework harnesses the best features of both tiled and superimposed multi-projector projection paradigms the framework creates for the first time a new unified paradigm that is agnostic to a particular configuration of projectors yet robustly optimizes for the brightness contrast and resolution of that configuration in addition we demonstrate that our algorithms support high resolution video at real-time interactive frame rates achieved on commodity graphics platforms this work allows for inexpensive compelling flexible and robust large scale visualization_systems to be built and deployed very efficiently multi-projector displays automatic_geometric_alignment blending large_format_displays photometric_correction stitching super-resolution superimposed_projection tiled_displays
multi-projector displays today are automatically registered both geometrically and photometrically using cameras existing registration techniques assume pre-calibrated projectors and cameras that are devoid of imperfections such as lens_distortion in practice however these devices are usually imperfect and uncalibrated registration of each of these devices is often more challenging than the multi-projector display registration itself to make tiled projection-based displays accessible to a layman user we should allow the use of uncalibrated inexpensive devices that are prone to imperfections in this paper we make two important advances in this direction first we present a new geometric registration technique that can achieve geometric alignment in the presence of severe projector lens_distortion using a relatively inexpensive low-resolution camera this is achieved via a closed-form model that relates the projectors to cameras in planar multi-projector displays using rational bezier patches this enables us to geometrically calibrate a 3000 times 2500 resolution planar multi-projector display made of 3 times 3 array of nine severely distorted projectors using a low resolution 640 times 480 vga camera second we present a photometric self-calibration technique for a projector-camera pair this allows us to photometrically calibrate the same display made of nine projectors using a photometrically uncalibrated camera to the best of our knowledge this is the first work that allows geometrically imperfect projectors and photometrically uncalibrated cameras in calibrating multi-projector displays geometric calibration photometric_calibration tiled_displays
pipeline architectures provide a versatile and efficient mechanism for constructing visualizations and they have been implemented in numerous libraries and applications over the past two decades in addition to allowing developers and users to freely combine algorithms visualization pipelines have proven to work well when streaming data and scale well on parallel distributed- memory computers however current pipeline visualization_frameworks have a critical flaw they are unable to managetime_varying data as data flows through the pipeline each algorithm has access to only a single snapshot in time of the data this prevents theimplementation of algorithms that do any temporal processing such as particle_tracing plotting over time or interpolation fitting or smoothing of time_series_data as data acquisition technology improves as simulation time-integration techniques become more complex and as simulations save less frequently and regularly the ability to analyze the time-behavior of data becomes more important this paper describes a modification to the traditional pipeline architecture that allows it to accommodate temporal algorithms furthermore the architecture allows temporal algorithms to be used in conjunction with algorithms expecting a single time snapshot thus simplifying software design and allowing adoption into existing pipeline frameworks our architecture also continues to work well in parallel distributed-memory environments we demonstrate our architecture by modifying the popular vtk framework and exposing the functionality to the paraview application we use this framework to apply time-dependent algorithms on large_data with a parallel cluster computer and thereby exercise a functionality that previously did not exist data-parallel visualization pipeline time-varying_data
modern unsteady multi-field_visualizations require an effective reduction of the data to be displayed from a huge amount of information the most informative parts have to be extracted instead of the fuzzy application dependent notion of feature a new approach based on information theoretic concepts is introduced in this paper to detect important regions this is accomplished by extending the concept of local_statistical_complexity from finite state cellular automata to discretized multi-fields thus informative parts of the data can be highlighted in an application-independent purely mathematical sense the new measure can be applied to unsteady multifields on regular grids in any application domain the ability to detect and visualize important parts is demonstrated using diffusion flow and weather simulations cluster_detection_analysis evolution_graph_view glyph visualization molecular_dynamics_visualization out-of-core_techniques time-dependent_scattered_data
perfusion data are dynamic medical image data which characterize the regional blood flow in human tissue these data bear a great potential in medical diagnosis since diseases can be better distinguished and detected at an earlier stage compared to static image data the wide-spread use of perfusion data is hampered by the lack of efficient evaluation methods for each voxel a time-intensity curve characterizes the enhancement of a contrast agent parameters derived from these curves characterize the perfusion and have to be integrated for diagnosis the diagnostic evaluation of this multi-field data is challenging and time-consuming due to its complexity for the visual_analysis of such datasets feature-based approaches allow to reduce the amount of data and direct the user to suspicious areas we present an interactive_visual_analysis approach for the evaluation of perfusion data for this purpose we integrate statistical methods and interactive feature specification correlation_analysis and principal_component_analysis pca are applied for dimension reduction and to achieve a better understanding of the inter-parameter relations multiple linked_views facilitate the definition of features by brushing multiple dimensions the specification result is linked to all views establishing a focus+context style of visualization in 3d we discuss our approach with respect to clinical datasets from the three major application areas ischemic stroke diagnosis breast tumor diagnosis as well as the diagnosis of the coronary heart disease chd it turns out that the significance of perfusion parameters strongly depends on the individual patient scanning parameters and data pre-processing integrating infovis/scivis multi-field_visualization time-varying volume data visual_data_mining
our ability to generate ever-larger increasingly-complex data has established the need for scalable methods that identify and provide insight into important variable trends and interactions query-driven methods are among the small subset of techniques that are able to address both large and highly complex datasets this paper presents a new method that increases the utility of query-driven techniques by visually conveying statistical information about the trends that exist between variables in a query in this method correlation fields created between pairs of variables are used with the cumulative distribution functions of variables expressed in a users query this integrated use of cumulative distribution functions and correlation fields visually reveals with respect to the solution space of the query statistically important interactions between any three variables and allows for trends between these variables to be readily identified we demonstrate our method by analyzing interactions between variables in two flame-front simulations multivariate data query-driven_visualization
we present a comprehensive system for weather data visualization weather data are multivariate and contain vector_fields formed by wind speed and direction several well-established visualization_techniques such as parallel_coordinates and polar_systems are integrated into our system we also develop various novel methods including circular pixel bar_charts embedded into polar_systems enhanced parallel_coordinates with s-shape axis and weighted complete graphs our system was used to analyze the air_pollution problem in hong kong and some interesting patterns have been found weather data visualization air_pollution parallel_coordinates polar_system visual_analytics
scientific_visualization and illustration tools are designed to help people understand the structure and complexity of scientific data with images that are as informative and intuitive as possible in this context the use of metaphors plays an important role since they make complex information easily accessible by using commonly known concepts in this paper we propose a new metaphor called "topological landscapes" which facilitates understanding the topological structure of scalar functions the basic idea is to construct a terrain with the same topology as a given dataset and to display the terrain as an easily understood representation of the actual input data in this projection from an n-dimensional scalar function to a two-dimensional 2d model we preserve function values of critical_points the persistence function span of topological features and one possible additional metric property in our examples volume by displaying this topologically equivalent landscape together with the original data we harness the natural human proficiency in understanding terrain topography and make complex topological information easily accessible contour_tree feature_detection (primary keyword) soar terrain topology user_interfaces visual_analytics
topology has been an important tool for analyzing scalar_data and flow_fields in visualization in this work we analyze the topology of multivariate image and volume data sets with discontinuities in order to create an efficient raster-based representation we call istar specifically the topology information is used to create a dual structure that contains nodes and connectivity information for every segmentable region in the original data set this graph structure along with a sampled representation of the segmented_data set is embedded into a standard raster image which can then be substantially downsampled and compressed during rendering the raster image is upsampled and the dual_graph is used to reconstruct the original function unlike traditional raster approaches our representation can preserve sharp discontinuities at any level of magnification much like scalable_vector_graphics however because our representation is raster-based it is well suited to the real-time rendering pipeline we demonstrate this by reconstructing our data sets on graphics_hardware at real-time rates compression image representation multi-field_visualization topology
analysis of the results obtained from material simulations is important in the physical sciences our research was motivated by the need to investigate the properties of a simulated porous_solid as it is hit by a projectile this paper describes two techniques for the generation of distance_fields containing a minimal number of topological features and we use them to identify features of the material we focus on distance_fields defined on a volumetric domain considering the distance to a given surface embedded within the domain topological features of the field are characterized by its critical_points our first method begins with a distance_field that is computed using a standard approach and simplifies this field using ideas from morse_theory we present a procedure for identifying and extracting a feature set through analysis of the ms complex and apply it to find the invariants in the clean distance_field our second method proceeds by advancing a front beginning at the surface and locally controlling the creation of new critical_points we demonstrate the value of topologically clean distance_fields for the analysis of filament structures in porous_solids our methods produce a curved skeleton representation of the filaments that helps material scientists to perform a detailed qualitative and quantitative analysis of pores and hence infer important material properties furthermore we provide a set of criteria for finding the "difference" between two skeletal structures and use this to examine how the structure of the porous_solid changes over several timesteps in the simulation of the particle impact morse_theory morse-smale_complex critical point distance_field material_science porous_solid topological_simplification wavefront
the morse-smale_complex is an efficient representation of the gradient behavior of a scalar function and critical_points paired by the complex identify topological features and their importance we present an algorithm that constructs the morse-smale_complex in a series of sweeps through the data identifying various components of the complex in a consistent manner all components of the complex both geometric and topological are computed providing a complete decomposition of the domain efficiency is maintained by representing the geometry of the complex in terms of point sets d scalar_fields morse_theory morse-smale_complexes computational_topology feature_detection multiresolution simplification
most streamline_generation algorithms either provide a particular density of streamlines across the domain or explicitly detect features such as critical_points and follow customized rules to emphasize those features however the former generally includes many redundant streamlines and the latter requires boolean decisions on which points are features and may thus suffer from robustness problems for real-world data we take a new approach to adaptive streamline_placement for steady vector_fields in 2d and 3d we define a metric for local similarity among streamlines and use this metric to grow streamlines from a dense set of candidate seed points the metric considers not only euclidean distance but also a simple statistical measure of shape and directional similarity without explicit feature_detection our method produces streamlines that naturally accentuate regions of geometric interest in conjunction with this method we also propose a quantitative error metric for evaluating a streamline representation based on how well it preserves the information from the original vector field this error metric reconstructs a vector field from points on the streamline representation and computes a difference of the reconstruction from the original vector field adaptive_streamlines shape_matching vector field reconstruction
this paper presents a method for filtered ridge_extraction based on adaptive_mesh_refinement it is applicable in situations where the underlying scalar field can be refined during ridge_extraction this requirement is met by the concept of lagrangian coherent_structures which is based on trajectories started at arbitrary sampling grids that are independent of the underlying vector field the lagrangian coherent_structures are extracted as ridges in finite lyapunov exponent fields computed from these grids of trajectories the method is applied to several variants of finite lyapunov exponents one of which is newly introduced high computation time due to the high number of required trajectories is a main drawback when computing lyapunov exponents of 3-dimensional vector_fields the presented method allows a substantial speed-up by avoiding the seeding of trajectories in regions where no ridges are present or do not satisfy the prescribed filter criteria such as a minimum finite lyapunov exponent coherent_structures flow_visualization ridge_extraction unsteady_vector_fields vector_field_topology
the recently introduced notion of finite-time_lyapunov_exponent to characterize coherent lagrangian structures provides a powerful framework for the visualization and analysis of complex technical flows its definition is simple and intuitive and it has a deep theoretical foundation while the application of this approach seems straightforward in theory the associated computational cost is essentially prohibitive due to the lagrangian nature of this technique a huge number of particle paths must be computed to fill the space-time flow domain in this paper we propose a novel scheme for the adaptive computation of ftle fields in two and three dimensions that significantly reduces the number of required particle paths furthermore for three-dimensional flows we show on several examples that meaningful results can be obtained by restricting the analysis to a well-chosen plane intersecting the flow domain finally we examine some of the visualization aspects of ftle-based methods and introduce several new variations that help in the analysis of specific aspects of a flow d vector_field_visualization feature_detection flow_visualization
analyzing visualizing and illustrating changes within time-varying volumetric_data is challenging due to the dynamic changes occurring between timesteps the changes and variations in computational fluid dynamic volumes and atmospheric 3d datasets do not follow any particular transformation features within the data move at different speeds and directions making the tracking and visualization of these features a difficult task we introduce a texture-based feature_tracking technique to overcome some of the current limitations found in the illustration and visualization of dynamic changes within time-varying volumetric_data our texture-based technique tracks various features individually and then uses the tracked objects to better visualize structural changes we show the effectiveness of our texture-based tracking technique with both synthetic and real world time-varying_data furthermore we highlight the specific visualization annotation registration and feature isolation benefits of our technique for instance we show how our texture-based tracking can lead to insightful visualizations of time-varying_data such visualizations more than traditional visualization_techniques can assist domain scientists to explore and understand dynamic changes feature_tracking flow_visualization texture-based analysis time-varying_data_visualization
in this paper we present a method to compute and visualize volumetric white_matter connectivity in diffusion tensor magnetic_resonance_imaging dt-mri using a hamilton-jacobi h-j solver on the gpu graphics processing unit paths through the volume are assigned costs that are lower if they are consistent with the preferred diffusion directions the proposed method finds a set of voxels in the dti volume that contain paths between two regions whose costs are within a threshold of the optimal path the result is a volumetric optimal path analysis which is driven by clinical and scientific questions relating to the connectivity between various known anatomical regions of the brain to solve the minimal path problem quickly we introduce a novel numerical algorithm for solving h-j equations which we call the fast iterative method fim this algorithm is well-adapted to parallel architectures and we present a gpu-basedimplementation which runs roughly 50-100 times faster than traditional cpu-based solvers for anisotropic h-j equations the proposed system allows users to freely change the endpoints of interesting pathways and to visualize the optimal volumetric path between them at an interactive rate we demonstrate the proposed method on some synthetic and real dt-mri datasets and compare the performance with existing methods diffusion_tensor_visualization graphics_hardware interactivity
diffusion_tensor_imaging dti of the human brain coupled with tractography techniques enable the extraction of large- collections of three-dimensional tract pathways per subject these pathways and pathway bundles represent the connectivity between different brain regions and are critical for the understanding of brain related diseases a flexible and efficient gpu-based rendering technique for dti tractography data is presented that addresses common performance bottlenecks and image-quality issues allowing interactive render rates to be achieved on commodity hardware an occlusion query-based pathway lod management system for streamlines/streamtubes/tuboids is introduced that optimizes input geometry vertex processing and fragment processing loads and helps reduce overdraw the tuboid a fully-shaded streamtube impostor constructed entirely on the gpu from streamline vertices is also introduced unlike full streamtubes and other impostor constructs tuboids require little to no preprocessing or extra space over the original streamline_data the supported fragment processing levels of detail range from texture-based draft shading to full raycast normal computation phong shading environment mapping and curvature-correct text labeling the presented text labeling technique for tuboids provides adaptive aesthetically pleasing labels that appear attached to the surface of the tubes furthermore an occlusion query aggregating and scheduling scheme for tuboids is described that reduces the query overhead results for a tractography dataset are presented and demonstrate that lod-managed tuboids offer benefits over traditional streamtubes both in performance and appearance tuboids interactive_gpu-centric_rendering neuronal_pathways streamtubes
topological methods give concise and expressive visual representations of flow_fields the present work suggests a comparable method for the visualization of human brain diffusion mri data we explore existing techniques for the topological_analysis of generic tensor_fields but find them inappropriate for diffusion mri data thus we propose a novel approach that considers the asymptotic behavior of a probabilistic_fiber_tracking method and define analogs of the basic concepts of flow topology like critical_points basins and faces with interpretations in terms of brain anatomy the resulting features are fuzzy reflecting the uncertainty inherent in any connectivity estimate from diffusion imaging we describe an algorithm to extract the new type of features demonstrate its robustness under noise and present results for two regions in a diffusion mri dataset to illustrate that the method allows a meaningful visual_analysis of probabilistic_fiber_tracking results diffusion tensor probabilistic_fiber_tracking tensor_topology uncertainty_visualization
we present a method for stochastic fiber tract mapping from diffusion_tensor_mri dt-mri implemented on graphics_hardware from the simulated fibers we compute a connectivity map that gives an indication of the probability that two points in the dataset are connected by a neuronal fiber path a bayesian formulation of the fiber model is given and it is shown that the inversion method can be used to construct plausible connectivity animplementation of this fiber model on the graphics processing unit gpu is presented since the fiber paths can be stochastically generated independently of one another the algorithm is highly parallelizable this allows us to exploit the data-parallel nature of the gpu fragment processors we also present a framework for the connectivity computation on the gpu ourimplementation allows the user to interactively select regions of interest and observe the evolving connectivity results during computation results are presented from the stochastic generation of over 250000 fiber steps per iteration at interactive frame rates on consumer-grade graphics_hardware diffusion tensor magnetic_resonance_imaging stochastic_tractography
we propose a novel geometrically adaptive method for surface_reconstruction from noisy and sparse point clouds without orientation information the method employs a fast convection algorithm to attract the evolving surface towards the data points the force field in which the surface is convected is based on generalized coulomb potentials evaluated on an adaptive_grid ie an octree using a fast hierarchical algorithm formulating reconstruction as a convection problem in a velocity field generated by coulomb potentials offers a number of advantages unlike methods which compute the distance from the data set to the implicit surface which are sensitive to noise due to the very reliance on the distance transform our method is highly resilient to shot noise since global generalized coulomb potentials can be used to disregard the presence ofoutliers due to noise coulomb potentials represent long-range interactions that consider all data points at once and thus they convey global information which is crucial in the fitting process both the spatial and temporal complexities of our spatially-adaptive method are proportional to the size of the reconstructed object which makes our method compare favorably with respect to previous approaches in terms of speed and flexibility experiments with sparse as well as noisy data sets show that the method is capable of delivering crisp and detailed yet smooth_surfaces generalized coulomb potentials implicit_surfaces octrees polygonization surface_reconstruction
this paper describes a novel method for creating surface models of multi-material_components using dual energy computed_tomography dect the application scenario is metrology and dimensional_measurement in industrial high resolution 3d_x-ray_computed_tomography 3dct based on the dual source / dual exposure technology this method employs 3dct scans of a high precision micro-focus and a high energy macro-focus x-ray source the presented work makes use of the advantages of dual x-ray exposure technology in order to facilitate dimensional_measurements of multi-material_components with high density material within low density material we propose a workflow which uses image_fusion and local_surface_extraction techniques a prefiltering step reduces noise inherent in the data for image_fusion the datasets have to be registered in the fusion step the benefits of both scans are combined the structure of the specimen is taken from the low precision blurry high energy dataset while the sharp edges are adopted and fused into the resulting image from the high precision crisp low energy dataset in the final step a reliable surface model is extracted from the fused dataset using a local adaptive technique the major contribution of this paper is the development of a specific workflow for dimensional_measurements of multi-material industrial components which takes two x-ray ct datasets with complementary strengths and weaknesses into account the performance of the workflow is discussed using a test specimen as well as two real world industrial parts as result a significant improvement in overall measurement precision surface geometry and mean deviation to reference measurement compared to single exposure scans was facilitated dect_image_fusion dual_energy_ct dimensional_measurement local_surface_extraction metrology variance_comparison
we present a method for extracting boundary surfaces from segmented cross-section image data we use a constrained potts model to interpolate an arbitrary number of region boundaries between segmented images this produces a segmented volume from which we extract a triangulated boundary surface using well-known marching tetrahedra methods this surface contains staircase-like artifacts and an abundance of unnecessary triangles we describe an approach that addresses these problems with a voxel-accurate simplification algorithm that reduces surface complexity by an order of magnitude our boundary interpolation and simplification methods are novel contributions to the study of surface_extraction from segmented cross-sections we have applied our method to construct polycrystal grain boundary surfaces from micrographs of a sample of the metal tantalum life sciences and engineering polygonal_meshes surface_extraction visualization_in_physical_sciences
with the exponential growth in size of geometric data it is becoming increasingly important to make effective use of multilevel caches limited disk storage and bandwidth as a result recent work in the visualization community has focused either on designing sequential access compression schemes or on producing cache-coherent_layouts of uncompressed meshes for random_access unfortunately combining these two strategies is challenging as they fundamentally assume conflicting modes of data access in this paper we propose a novel order-preserving compression method that supports transparent random_access to compressed triangle meshes our decompression method selectively fetches from disk decodes and caches in memory requested parts of a mesh we also provide a general mesh access api for seamless mesh traversal and incidence queries while the method imposes no particular mesh layout it is especially suitable for cache-oblivious layouts which minimize the number of decompression i/o requests and provide high cache utilization during access to decompressed in-memory portions of the mesh moreover the transparency of our scheme enables improved performance without the need for application code changes we achieve compression rates on the order of 201 and significantly improved i/o performance due to reduced data transfer to demonstrate the benefits of our method we implement two common applications as benchmarks by using cache-oblivious layouts for the input models we observe 2-6 times overall speedup compared to using uncompressed meshes mesh_compression cache-coherent_layouts external_memory_algorithms mesh_data_structures random_access
although real-time interactive_volume_rendering is available even for very large_data sets this visualization method is used quite rarely in the clinical practice we suspect this is because it is very complicated and time consuming to adjust the parameters to achieve meaningful results the clinician has to take care of the appropriate viewpoint zooming transfer_function setup clipping planes and other parameters because of this most often only 2d slices of the data set are examined our work introduces livesync a new concept to synchronize 2d slice views and volumetric views of medical data sets through intuitive picking actions on the slice the users define the anatomical structures they are interested in the 3d volumetric view is updated automatically with the goal that the users are provided with expressive result images to achieve this live synchronization we use a minimal set of derived information without the need for segmented_data sets or data-specific pre-computations the components we consider are the picked point slice view zoom patient orientation viewpoint history local object shape and visibility we introduce deformed viewing spheres which encode the viewpoint quality for the components a combination of these deformed viewing spheres is used to estimate a good viewpoint our system provides the physician with synchronized views which help to gain deeper insight into the medical data with minimal user_interaction navigation interaction linked_views medical visualization viewpoint_selection
new product development involves people with different backgrounds designers engineers and consumers all have different design criteria and these criteria interact early concepts evolve in this kind of collaborative context and there is a need for dynamic_visualization of the interaction between design shape and other shape-related design criteria in this paper a morphable_model is defined from simplified representations of suitably chosen real cars providing a continuous shape_space to navigate manipulate and visualize physical properties and consumer-provided scores for the real cars such as 'weight' and 'sportiness' are estimated for new designs across the shape_space this coupling allows one to manipulate the shape directly while reviewing the impact on estimated criteria or conversely to manipulate the criterial values of the current design to produce a new shape with more desirable attributes morphable_model barycentric_coordinates design space shape_space
while there have been advances in visualization_systems particularly in multi-view visualizations and visual_exploration the process of building visualizations remains a major bottleneck in data_exploration we show that provenance metadata collected during the creation of pipelines can be reused to suggest similar content in related visualizations and guide semi-automated changes we introduce the idea of query-by-example in the context of an ensemble of visualizations and the use of analogies as first-class operations in a system to guide scalable interactions we describe animplementation of these techniques in vistrails a publicly-available open-source system analogy query-by-example visualization_systems
multiple spatially-related videos are increasingly used in securitycommunication and other applications since it can be difficult to understand the spatial relationships between multiple videos in complex environments eg to predict a person's path through a building some visualization_techniques such as video texture projection have been used to aid spatial understanding in this paper we identify and begin to characterize an overall class of visualization_techniques that combine video with 3d spatial_context this set of techniques which we call contextualized videos forms a design palette which must be well understood so that designers can select and use appropriate techniques that address the requirements of particular spatial video tasks in this paper we first identify user tasks in video surveillance that are likely to benefit from contextualized videos and discuss the video model and navigation related dimensions of the contextualized video design space we then describe our contextualized video testbed which allows us to explore this design space and compose various video_visualizations for evaluation finally we describe the results of our process to identify promising design_patterns through user selection of visualization features from the design space followed by user interviews design space situational awareness testbed_design_and_evaluation videos virtual_environment_models
we describe a novel volumetric global_illumination framework based on the face-centered cubic fcc lattice an fcc lattice has important advantages over a cartesian lattice it has higher packing density in the frequency domain which translates to better sampling efficiency furthermore it has the maximal possible kissing number equivalent to the number of nearest neighbors of each site which provides optimal 3d angular discretization among all lattices we employ a new two-pass illumination and rendering global_illumination scheme on an fcc lattice this scheme exploits the angular discretization to greatly simplify the computation in multiple scattering and to minimize illumination information storage the gpu has been utilized to further accelerate the rendering stage we demonstrate our new framework with participating_media and volume_rendering with multiple scattering where both are significantly faster than traditional techniques with comparable quality fcc lattice gpu volume_visualization lattice multiple scattering participating_media sampling volume_rendering
we present a powerful framework for 3d-texture-based_rendering of multiple arbitrarily intersecting volumetric_datasets each volume is represented by a multi-resolution octree-based structure and we use out-of-core_techniques to support extremely large_volumes users define a set of convex polyhedral volume lenses which may be associated with one or more volumetric_datasets the volumes or the lenses can be interactively moved around while the region inside each lens is rendered using interactively defined multi-volume shaders our rendering pipeline splits each lens into multiple convex regions such that each region is homogenous and contains a fixed number of volumes each such region is further split by the brick boundaries of the associated octree representations the resulting puzzle of lens fragments is sorted in front-to-back or back-to-front order using a combination of a view-dependent octree traversal and a gpu-based depth peeling technique our currentimplementation uses slice-based volume_rendering and allows interactive roaming through multiple intersecting multi-gigabyte volumes multi-volume_visualization constructive_solid_geometry display_algorithms shading
this paper presents a scalable framework for real-time raycasting of large unstructured volumes that employs a hybrid bricking approach it adaptively combines original unstructured bricks in important focus regions with structured bricks that are resampled on demand in less important context regions the basis of this focus+context approach is interactive specification of a scalar degree_of_interest doi function thus rendering always considers two volumes simultaneously a scalar_data volume and the current doi volume the crucial problem of visibility sorting is solved by raycasting individual bricks and compositing in visibility order from front to back in order to minimize visual errors at the grid boundary it is always rendered accurately even for resampled bricks a variety of different rendering modes can be combined including contour enhancement a very important property of our approach is that it supports a variety of cell types natively ie it is not constrained to tetrahedral_grids even when interpolation within cells is used moreover our framework can handle multi-variate_data eg multiple scalar channels such as temperature or pressure as well as time-dependent_data the combination of unstructured and structured bricks with different quality characteristics such as the type of interpolation or resampling resolution in conjunction with custom texture memory management yields a very scalable system focus+context techniques hardware-assisted_volume_rendering volume_rendering_of_unstructured_grids
hardware-accelerated_volume_rendering using the gpu is now the standard approach for real-time volume_rendering although limited graphics memory can present a problem when rendering large volume data sets volumetric compression in which the decompression is coupled to rendering has been shown to be an effective solution to this problem however most existing techniques were developed in the context of software volume_rendering and all but the simplest approaches are prohibitive in a real-time hardware-accelerated_volume_rendering context in this paper we present a novel block-based transform_coding scheme designed specifically with real-time volume_rendering in mind such that the decompression is fast without sacrificing compression quality this is made possible by consolidating the inverse transform with dequantization in such a way as to allow most of the reprojection to be precomputed furthermore we take advantage of the freedom afforded by offline compression in order to optimize the encoding as much as possible while hiding this complexity from the decoder in this context we develop a new block classification scheme which allows us to preserve perceptually important features in the compression the result of this work is an asymmetric transform_coding scheme that allows very large_volumes to be compressed and then decompressed in real-time while rendering on the gpu compressed volume_rendering hardware-accelerated_volume_rendering transform_coding volume_compression
in this paper we introduce a visualization_technique that provides an abstracted view of the shape and spatio-physico-chemical properties of complex molecules unlike existing molecular viewing methods our approach suppresses small details to facilitate rapid comprehension yet marks the location of significant features so they remain visible our approach uses a combination of filters and mesh restructuring to generate a simplified representation that conveys the overall shape and spatio-physico-chemical properties eg electrostatic charge surface markings are then used in the place of important removed details as well as to supply additional information these simplified representations are amenable to display using stylized_rendering algorithms to further enhance comprehension our initial experience suggests that our approach is particularly useful in browsing collections of large molecules and in readily making comparisons between them cartographic_labeling molecular surfaces molecular_visualization surfaces textures
proteins are highly flexible and large amplitude deformations of their structure also called slow dynamics are often decisive to their function we present a two-level rendering approach that enables visualization of slow dynamics of large protein assemblies our approach is aligned with a hierarchical model of large scale molecules instead of constantly updating positions of large amounts of atoms we update the position and rotation of residues ie higher level building blocks of a protein residues are represented by one vertex only indicating its position and additional information defining the rotation the atoms in the residues are generated on-the-fly on the gpu exploiting the new graphics_hardware geometry shader capabilities moreover we represent the atoms by billboards instead of tessellated spheres our representation is then significantly faster and pixel precise we demonstrate the usefulness of our new approach in the context of our collaborative bioinformatics project molecular_visualization hardware_acceleration protein_dynamics
a current research topic in molecular thermodynamics is the condensation of vapor to liquid and the investigation of this process at the molecular level condensation is found in many physical phenomena eg the formation of atmospheric clouds or the processes inside steam turbines where a detailed knowledge of the dynamics of condensation processes will help to optimize energy efficiency and avoid problems with droplets of macroscopic size the key properties of these processes are the nucleation rate and the critical cluster size for the calculation of these properties it is essential to make use of a meaningful definition of molecular clusters which currently is a not completely resolved issue in this paper a framework capable of interactively visualizing molecular datasets of such nucleation simulations is presented with an emphasis on the detected molecular clusters to check the quality of the results of the cluster detection our framework introduces the concept of flow groups to highlight potential cluster evolution over time which is not detected by the employed algorithm to confirm the findings of the visual_analysis we coupled the rendering view with a schematic view of the clusters' evolution this allows to rapidly assess the quality of the molecular cluster detection algorithm and to identify locations in the simulation data in space as well as in time where the cluster detection fails thus thermodynamics researchers can eliminate weaknesses in their cluster detection algorithms several examples for the effective and efficient usage of our tool are presented cluster_detection_analysis evolution_graph_view glyph visualization molecular_dynamics_visualization out-of-core_techniques time-dependent_scattered_data
we present novel comprehensive visualization_techniques for the diagnosis of patients with coronary artery disease using segmented cardiac_mri data we extent an accepted medical visualization_technique called the bull's eye plot by removing discontinuities preserving the volumetric nature of the left ventricular wall and adding anatomical context the resulting volumetric bull's eye plot can be used for the assessment of transmurality we link these visualizations to a 3d view that presents viability information in a detailed anatomical context we combine multiple mri scans whole heart anatomical data late_enhancement data and multiple segmentations polygonal heart model late_enhancement contours coronary artery tree by selectively combining different rendering techniques we obtain comprehensive yet intuitive visualizations of the various data sources cardiac_mri bull's eye plot late_enhancement viability
visualization of uncertainty or error in astrophysical data is seldom available in simulations of astronomical phenomena and yet almost all rendered attributes possess some degree of uncertainty due to observational error uncertainties associated with spatial location typically vary significantly with scale and thus introduce further complexity in the interpretation of a given visualization this paper introduces effective techniques for visualizing uncertainty in large-scale virtual astrophysical environments building upon our previous transparently scalable_visualization architecture we develop tools that enhance the perception and comprehension of uncertainty across wide scale ranges our methods include a unified color-coding scheme for representing log-scale distances and percentage errors an ellipsoid model to represent positional uncertainty an ellipsoid envelope model to expose trajectory uncertainty and a magic-glass design supporting the selection of ranges of log-scale distance and uncertainty parameters as well as an overview mode and a scalable wim tool for exposing the magnitudes of spatial_context and uncertainty uncertainty_visualization astronomy interstellar data large_spatial_scale
direct_volume_rendering has proved to be an effective visualization method for medical data sets and has reached wide-spread clinical use the diagnostic exploration in essence corresponds to a tissue classification task which is often complex and time-consuming moreover a major problem is the lack of information on the uncertainty of the classification which can have dramatic consequences for the diagnosis in this paper this problem is addressed by proposing animation methods to convey uncertainty in the rendering the foundation is a probabilistic transfer_function model which allows for direct user_interaction with the classification the rendering is animated by sampling the probability domain over time which results in varying appearance for uncertain regions a particularly promising application of this technique is a "sensitivity lens" applied to focus regions in the data set the methods have been evaluated by radiologists in a study simulating the clinical task of stenosis assessment in which the animation technique is shown to outperform traditional rendering in terms of assessmentaccuracy uncertainty medical visualization probability transfer_function volume_rendering
we present the results of two controlled studies comparing layered surface visualizations under various texture conditions the task was to estimate surface normals measured byaccuracy of a hand-set surface normal probe a single surface visualization was compared with the two-surfaces case under conditions of no texture and with projected grid textures variations in relative texture spacing on top and bottom surfaces were compared as well as opacity of the top surface significant improvements are found for the textured cases over non-textured surfaces either larger or thinner top-surface textures and lower top surface opacities are shown to give less bottom surface error top surface error appears to be highly resilient to changes in texture given the results we also present an example of how appropriate textures might be useful in volume_visualization perception layered_surfaces optimal_visualization texturing
visualization algorithms can have a large number of parameters making the space of possible rendering results rather high-dimensional only a systematic analysis of the perceived quality can truly reveal the optimal setting for each such parameter however an exhaustive search in which all possible parameter permutations are presented to each user within a study group would be infeasible to conduct additional complications may result from possible parameter co-dependencies here we will introduce an efficient user_study design and analysis strategy that is geared to cope with this problem the user feedback is fast and easy to obtain and does not require exhaustive parameter testing to enable such a framework we have modified a preference measuring methodology conjoint_analysis that originated in psychology and is now also widely used in market_research we demonstrate our framework by a study that measures the perceived quality in volume_rendering within the context of large parameter spaces conjoint_analysis parameterized_algorithms volume_visualization
we present a new approach for real-time sound rendering in complex virtual scenes with dynamic sources and objects our approach combines the efficiency of interactive ray tracing with theaccuracy of tracing a volumetric representation we use a four-sided convex frustum and perform clipping and intersection tests using ray packet tracing a simple and efficient formulation is used to compute secondary frusta and perform hierarchical traversal we demonstrate the performance of our algorithm in an interactive_system for complex environments and architectural models with tens or hundreds of thousands of triangles our algorithm can perform real-time simulation and rendering on a high-end pc acoustic propagation ray tracing
just as we can work with two-dimensional floor plans to communicate 3d architectural design we can exploit reduced- dimension shadows to manipulate the higher-dimensional objects generating the shadows in particular by taking advantage of physically reactive 3d shadow-space controllers we can transform the task of interacting with 4d objects to a new level of physical reality we begin with a teaching tool that uses 2d knot diagrams to manipulate the geometry of 3d mathematical knots via their projections our unique 2d haptic interface allows the user to become familiar with sketching editing exploration and manipulation of 3d knots rendered as projected images on a 2d shadow space by combining graphics and collision-sensing haptics we can enhance the 2d shadow-driven editing protocol to successfully leverage 2d pen-and-paper or blackboard skills building on the reduced-dimension 2d editing tool for manipulating 3d shapes we develop the natural analogy to produce a reduced-dimension 3d tool for manipulating 4d shapes by physically modeling the correct properties of 4d surfaces their bending forces and their collisions in the 3d haptic controller interface we can support full-featured physical exploration of 4d mathematical objects in a manner that is otherwise far beyond the experience accessible to human beings as far as we are aware this paper reports the first interactive_system with force-feedback that provides "4d haptic visualization" permitting the user to model and interact with 4d cloth-like objects haptics knot_theory visualization
surgical approaches tailored to an individual patient's anatomy and pathology have become standard in neurosurgery precise preoperative planning of these procedures however is necessary to achieve an optimal therapeutic effect therefore multiple radiological imaging modalities are used prior to surgery to delineate the patient's anatomy neurological function and metabolic processes developing a three-dimensional perception of the surgical approach however is traditionally still done by mentally fusing multiple modalities concurrent 3d_visualization of these datasets can therefore improve the planning process significantly in this paper we introduce an application for planning of individual neurosurgical approaches with high-quality interactive multimodal_volume_rendering the application consists of three main modules which allow to 1 plan the optimal skin incision and opening of the skull tailored to the underlying pathology 2 visualize superficial brain anatomy function and metabolism and 3 plan the patient-specific approach for surgery of deep-seated lesions the visualization is based on direct multi-volume_raycasting on graphics_hardware where multiple volumes from different modalities can be displayed concurrently at interactive frame rates graphics memory limitations are avoided by performing raycasting on bricked volumes for preprocessing tasks such as registration or segmentation the visualization modules are integrated into a larger framework thus supporting the entire workflow of preoperative planning hardware_assisted_raycasting multimodal_volume_rendering surgery planning
this paper describes a method for constructing isosurface triangulations of sampled volumetric three-dimensional scalar_fields the resulting meshes consist of triangles that are of consistently high_quality making them well suited for accurate interpolation of scalar and vector-valued quantities as required for numerous applications in visualization and numerical simulation the proposed method does not rely on a local construction or adjustment of triangles as is done for instance in advancing wavefront or adaptive_refinement methods instead a system of dynamic particles optimally samples an implicit function such that the particles' relative positions can produce a topologically correct delaunay_triangulation thus the proposed method relies on a global placement of triangle vertices the main contributions of the paper are the integration of dynamic particles systems with surface sampling theory and pde-based methods for controlling the local variability of particle densities as well as detailing a practical method that accommodates delaunay sampling requirements to generate sparse sets of points for the production of high-quality tessellations delaunay_triangulation isosurface_extraction particle_systems
we describe our visualization process for a particle-based simulation of the formation of the first stars and their impact on cosmic history the dataset consists of several hundred time-steps of point simulation data with each time-step containing approximately two million point particles for each time-step we interpolate the point data onto a regular grid using a method taken from the radiance estimate of photon_mapping [21] we import the resulting regular grid representation into paraview [24] with which we extract isosurfaces across multiple variables our images provide insights into the evolution of the early universe tracing the cosmic transition from an initially homogeneous state to one of increasing complexity specifically our visualizations capture the build-up of regions of ionized gas around the first stars their evolution and their complex interactions with the surrounding matter these observations will guide the upcoming james webb space telescope the key astronomy mission of the next decade astronomy cosmology interpolation isosurface
we have combined methods from volume_visualization and data_analysis to support better diagnosis and treatment of human retinal diseases many diseases can be identified by abnormalities in the thicknesses of various retinal layers captured using optical_coherence_tomography oct we used a support_vector_machine svm to perform semi-automatic segmentation of retinal layers for subsequent analysis including a comparison of layer thicknesses to known healthy parameters we have extended and generalized an older svm approach to support better performance in a clinical setting through performance enhancements and graceful handling of inherent noise in oct data by considering statistical characteristics at multiple levels of resolution the addition of the multi-resolution hierarchy extends the svm to have "global awareness" a feature such as a retinal layer can therefore be modeled within the svm as a combination of statistical characteristics across all levels thus capturing high- and low-frequency information we have compared our semi-automatically generated segmentations to manually segmented layers for verification purposes our main goals were to provide a tool that could i be used in a clinical setting ii operate on noisy oct data and iii isolate individual or multiple retinal layers in both healthy and disease cases that contain structural deformities image_analysis image_processing optical_coherence_tomography retinal segmentation support_vector_machine volume_visualization
we describe a system for interactively rendering isosurfaces of tetrahedral finite-element scalar_fields using coherent ray tracing techniques on the cpu by employing state-of-the art methods in polygonal ray tracing namely aggressive packet/frustum traversal of a bounding volume hierarchy we can accommodate large and time-varying unstructured_data in conjunction with this efficiency structure we introduce a novel technique for intersecting ray packets with tetrahedral primitives ray tracing is flexible allowing for dynamic changes in isovalue and time step visualization of multiple isosurfaces shadows and depth-peeling transparency effects the resulting system offers the intuitive simplicity of isosurfacing guaranteed-correct visual results and ultimately a scalable dynamic and consistently interactive solution for visualizing unstructured volumes isosurfaces ray tracing scalar_fields tetrahedra time-varying_data unstructured_meshes
we present a method to extract and visualize vortices that originate from bounding walls of three-dimensional time- dependent flows these vortices can be detected using their footprint on the boundary which consists of critical_points in the wall shear stress vector field in order to follow these critical_points and detect their transformations affected regions of the surface are parameterized thus an existing singularity_tracking algorithm devised for planar settings can be applied the trajectories of the singularities are used as a basis for seeding particles this leads to a new type of streak line visualization in which particles are released from a moving source these generalized_streak_lines visualize the particles that are ejected from the wall we demonstrate the usefulness of our method on several transient fluid flow datasets from computational fluid dynamics simulations skin friction flow_visualization generalized_streak_line singularity_tracking time-dependent_vector_fields vortex
we present a novel approach for analyzing two-dimensional 2d flow_field data based on the idea of invariant moments moment invariants have traditionally been used in computer vision applications and we have adapted them for the purpose of interactive_exploration of flow_field data the new class of moment invariants we have developed allows us to extract and visualize 2d flow patterns invariant under translation scaling and rotation with our approach one can study arbitrary flow patterns by searching a given 2d flow data set for any type of pattern as specified by a user further our approach supports the computation of moments at multiple scales facilitating fast pattern_extraction and recognition this can be done for critical point classification but also for patterns with greater complexity this multi-scale moment representation is also valuable for the comparative_visualization of flow_field data the specific novel contributions of the work presented are the mathematical derivation of the new class of moment invariants their analysis regarding critical point features the efficient computation of a novel feature space representation and based upon this the development of a fast pattern_recognition algorithm for complex flow structures feature_detection flow_visualization image_processing pattern_recognition
physics-based flow_visualization_techniques seek to mimic laboratory flow_visualization methods with virtual analogues in this work we describe the rendering of a virtual rheoscopic fluid to produce images with results strikingly similar to laboratory experiments with real-world rheoscopic fluids using products such as kalliroscope these fluid additives consist of microscopic anisotropic particles which when suspended in the flow align with both the flow velocity and the local shear to produce high-quality depictions of complex flow structures our virtual rheoscopic fluid is produced by defining a closed-form formula for the orientation of shear layers in the flow and using this orientation to volume render the flow as a material with anisotropic reflectance and transparency examples are presented for natural convection thermocapillary convection and taylor-couette flow simulations the latter agree well with photographs of experimental results of taylor-couette flows from the literature flow_visualization rheoscopic fluids
in nature and in flow experiments particles form patterns of swirling motion in certain locations existing approaches identify these structures by considering the behavior of stream lines however in unsteady_flows particle_motion is described by path_lines which generally gives different swirling patterns than stream lines we introduce a novel mathematical characterization of swirling motion cores in unsteady_flows by generalizing the approach of sujudi/haimes to path_lines the cores of swirling particle_motion are lines sweeping over time ie surfaces in the space-time domain they occur at locations where three derived 4d vectors become coplanar to extract them we show how to re-formulate the problem using the parallel vectors operator we apply our method to a number of unsteady_flow_fields feature_extraction particle_motion unsteady_flow_visualization
how we as insiders see and understand infovis is quite different from how it is seen by most people in the world out there most people get only glimpses of what we do and those glimpses rarely tell the story clearly think about the view of infovis that has been created in 2007 through marketing blogs and articles this view is peppered with misperception in this presentation i'll take you on a tour of infovis' exposure in 2007 the highlights and the failures that have shaped the world's perception of our beloved and important work the world needs what we do but this need remains largely unsatisfied
in this paper we present a system to analyze activities and detect anomalies in a surveillance application which exploits the intuition and experience of security and surveillance experts through an easy- to-use visual feedback loop the multi-scale and location specific nature of behavior patterns in space and time is captured using a wavelet-based feature descriptor the system learns the fundamental descriptions of the behavior patterns in a semi-supervised fashion by the higher order singular value decomposition of the space described by the training data this training process is guided and refined by the users in an intuitive fashion anomalies are detected by projecting the test data into this multi-linear space and are visualized by the system to direct the attention of the user to potential problem spots we tested our system on real-world surveillance data and it satisfied the security concerns of the environment hosvd anomaly_detection surveillance trajectory wavelets
an architecture for visualizing information extracted from text documents is proposed in conformance with this architecture a toolkit femarepviz has been implemented to extract and visualize temporal geospatial and summarized information from fema national update reports preliminary tests have shown satisfactoryaccuracy for femarepviz a central component of the architecture is an entity extractor that extracts named entities like person names location names temporal references etc femarepviz is based on factxtractor an entity-extractor that works on text documents the information extracted using factxtractor is processed using geotagger a geographical name disambiguation tool based on a novel clustering-based disambiguation algorithm to extract relationships among entities we propose a machine-learning based algorithm that uses a novel stripped dependency tree kernel we illustrate and evaluate the usefulness of our system on the fema national situation updates daily reports are fetched by femarepviz from the fema website segmented into coherent sections and each section is classified into one of several known incident types we use concept vista google maps and google earth to visualize the events extracted from the text reports and allow the user to interactively filter the topics locations and time-periods of interest to create a visual_analytics toolkit that is useful for rapid analysis of events reported in a large set of text documents geo-temporal_visualization geospatial_analytics knowledge_discovery text_processing visual_analytics
a story is a powerful abstraction used by intelligence analysts to conceptualize threats and understand patterns as part of the analytical process this paper demonstrates a system that detects geo-temporal patterns and integrates story narration to increase analytic sense-making cohesion in geotime the geotime geo-temporal event visualization tool was augmented with a story system that uses narratives hypertext linked visualizations visual annotations and pattern detection to create an environment for analytic exploration andcommunication thereby assisting the analyst in identifying extracting arranging and presenting stories within the data the story system lets analysts operate at the story level with higher-level abstractions of data such as behaviors and events while staying connected to the evidence the story system was developed and evaluated in collaboration with analysts human information interaction narrative pattern detection sense-making story_making story telling visual_analytics
coordinated animal-human health monitoring can provide an early warning system with fewer false alarms for naturally occurring disease outbreaks as well as biological chemical and environmental incidents this monitoring requires the integration and analysis of multi-field multi-scale and multi-source data sets in order to better understand these data sets models and measurements at different resolutions must be analyzed to facilitate these investigations we have created an application to provide a visual_analytics framework for analyzing both human emergency room data and veterinary hospital data our integrated visual analytic tool links temporally varying geospatial visualization of animal and human patient health information with advanced statistical_analysis of these multi-source data various statistical_analysis techniques have been applied in conjunction with a spatio-temporal viewing window such an application provides researchers with the ability to visually search the data for clusters in both a statistical model view and a spatio-temporal view our interface provides a factor specification/filtering component to allow exploration of causalfactors and spread patterns in this paper we will discuss the application of our linked animal-human visual_analytics lahva tool to two specific case studies the first case_study is the effect of seasonal influenza and its correlation with different companion animals eg cats dogs syndromes here we use data from the indiana network for patient care inpc and banfield pet hospitals in an attempt to determine if there are correlations between respiratory syndromes representing the onset of seasonal influenza in humans and general respiratory syndromes in cats and dogs our second case_study examines the effect of the release of industrial wastewater in a community through companion animal surveillance
using mobile devices for visualization provides a ubiquitous environment for accessing information and effective decision_making these visualizations are critical in satisfying the knowledge needs of operators in areas as diverse as education business law_enforcement protective services medical services scientific discovery and homeland security in this paper we present an efficient and interactive mobile visual analytic system for increased situational awareness and decision_making in emergency_response and training situations our system provides visual_analytics with locational scene data within a simple interface tailored to mobile device capabilities in particular we focus on processing and displaying sensor network data for first responders to verify our system we have used simulated data of the station nightclub fire evacuation emergency_response mobile_visualization visual_analytics
application of the ideas of visual_analytics is a promising approach to supporting decision_making in particular where the problems have geographic or spatial and temporal aspects visual_analytics may be especially helpful in time-critical applications which pose hard challenges to decision support we have designed a suite of tools to support transportation-planning tasks such as emergency evacuation of people from a disaster- affected area the suite combines a tool for automated scheduling based on a genetic_algorithm with visual_analytics techniques allowing the user to evaluate tool results and direct its work a transportation schedule which is generated by the tool is a complex construct involving geographical space time and heterogeneous objects people and vehicles with states and positions varying in time we apply task-analytical approach to design techniques that could effectively support a human planner in the analysis of this complex information h 12 [user/machine_systems] human information processing - visual_analytics 169 [visualization] information_visualization geo visualization coordinated multiple_views task-centered_design transportation_planning vehicle scheduling
we have developed a web 20 thin client visualization_framework called geoboosttrade our framework focuses on geospatial visualization and using scalable_vector_graphics svg ajax rss and georss we have built a complete thin client component set our component set provides a rich user_experience that is completely browser based it includes maps standard business charts graphs and time-oriented components the components are live interactive linked and support real time collaboration javascript linked_view_visual_analytics scalable_vector_graphics visualization components web
this paper introduces a new visual_analysis tool named imas interactive multigenomic analysis system which combines common analysis tools such as glimmer blast and clustal-w into a unified visual analytic framework imas displays the primary dna_sequence being analyzed by the biologist in a highly interactive zoomable visual display the user may analyze the sequence in a number of ways and visualize these analyses in a coherent sequence aligned form with all related analysis products grouped together this enables the user to rapidly perform analyses of dna_sequences without the need for tedious and error-prone cutting and pasting of sequence data from text files to and from web-based databases and data_analysis services as is now common practice bioinformatics visual_analytics
designing a visualization system capable of processing managing and presenting massive data sets while maximizing the user's situational awareness sa is a challenging but important research question in visual_analytics traditional data_management and interactive retrieval approaches have often focused on solving the data overload problem at the expense of the user's sa this paper discusses various data_management strategies and the strengths and limitations of each approach in providing the user with sa a new data_management strategy coined smart_aggregation is presented as a powerful approach to overcome the challenges of both massive data sets and maintaining sa by combining automatic data_aggregation with user-defined controls on what how and when data should be aggregated we present a visualization system that can handle massive amounts of data while affording the user with the best possible sa this approach ensures that a system is always usable in terms of both system resources and human perceptual resources we have implemented our smart_aggregation approach in a visual_analytics system called viassist visual assistant for information assurance analysis to facilitate exploration discovery and sa in the domain of information assurance data_management data retrieval information_visualization situational awareness smart_aggregation visual_analytics
cluster analysis ca is a powerful strategy for the exploration of high-dimensional_data in the absence of a-priori hypotheses or data classification models and the results of ca can then be used to form such models but even though formal models and classification rules may not exist in these data_exploration scenarios domain scientists and experts generally have a vast amount of non-compiled knowledge and intuition that they can bring to bear in this effort in ca there are various popular mechanisms to generate the clusters however the results from their non- supervised deployment rarely fully agree with this expert knowledge and intuition to this end our paper describes a comprehensive and intuitive framework to aid scientists in the derivation of classification hierarchies in ca using k-means as the overall clustering engine but allowing them to tune its parameters interactively based on a non-distorted compact visual presentation of the inherent characteristics of the data in high- dimensional space these include cluster geometry composition spatial relations to neighbors and others in essence we provide all the tools necessary for a high-dimensional activity we call cluster sculpting and the evolving hierarchy can then be viewed in a space-efficient radial dendrogram we demonstrate our system in the context of the mining and classification of a large collection of millions of data items of aerosol mass spectra but our framework readily applies to any high-dimensional ca scenario high-dimensional_data space and environmental sciences visual_analytics visual_data_mining visualization_in_earth
visualization_systems traditionally focus on graphical representation of information they tend not to provide integrated analytical services that could aid users in tackling complex knowledge_discovery tasks users' exploration in such environments is usually impeded due to several problems 1 valuable information is hard to discover when too much data is visualized on the screen 2 users have to manage and organize their discoveries off line because no systematic discovery_management mechanism exists 3 their discoveries based on visual_exploration alone may lackaccuracy 4 and they have no convenient access to the important knowledge learned by other users to tackle these problems it has been recognized that analytical tools must be introduced into visualization_systems in this paper we present a novel analysis-guided exploration system called the nugget management system nms it leverages the collaborative effort of human comprehensibility and machine computations to facilitate users' visual_exploration processes specifically nms first extracts the valuable information nuggets hidden in datasets based on the interests of users given that similar nuggets may be re-discovered by different users nms consolidates the nugget candidate set by clustering based on their semantic similarity to solve the problem of inaccurate discoveries localized data_mining techniques are applied to refine the nuggets to best represent the captured patterns in datasets lastly the resulting well-organized nugget pool is used to guide users' exploration to evaluate the effectiveness of nms we integrated nms into xmd- vtool a freeware multivariate visualization system user studies were performed to compare the users' efficiency andaccuracy in finishing tasks on real datasets with and without the help of nms our user studies confirmed the effectiveness of nms analysis_guided_exploration discovery_management visual_analytics visual_knowledge_discovery
visualizations of large multi-dimensional_data_sets occurring in scientific and commercial applications often reveal interesting local patterns analysts want to identify the causes and impacts of these interesting areas and they also want to search for similar patterns occurring elsewhere in the data set in this paper we introduce the intelligent visual_analytics_query ivquery concept that combines visual_interaction with automated analytical methods to support analysts in discovering the special properties and relations of identified patterns the idea of ivquery is to interactively select focus areas in the visualization then according to the characteristics of the selected areas such as the data dimensions and records ivquery employs analytical methods to identify the relationships to other portions of the data set finally ivquery generates visual representations for analysts to view and refine the results ivquery has been applied successfully to different real-world data sets such as data warehouse performance product sales and sever performance_analysis and demonstrates the benefits of this technique over traditional filtering and zooming techniques the visual_analytics_query technique can be used with many different types of visual representation in this paper we show how to use ivquery with parallel_coordinates visual maps and scatter plots interactive_queries similarity queries visual_analytics_query
the task of building effective representations to visualize and explore collections with moderate to large number of documents is hard it depends on the evaluation of some distance measure among texts and also on the representation of such relationships in bi- dimensional spaces in this paper we introduce an alternative approach for building visual maps of documents based on their content similarity through reconstruction of phylogenetic_trees the tree is capable of representing relationships that allows the user to quickly recover information detected by the similarity metric for a variety of text collections of different natures we show that we can achieve improved exploration capability and more clear visualization of relationships amongst documents document_analysis document_visualization multidimensional visualization phylogenetic_trees text_analytics
in this paper we have developed a novel framework to enable more effective investigation of large-scale news video database via knowledge_visualization to relieve users from the burdensome exploration of well-known and uninteresting knowledge of news reports a novel interestingness measurement for video news reports is presented to enable users to find news stories of interest at first glance and capture the relevant knowledge in large-scale video news databases efficiently our framework takes advantage of both automatic semantic video analysis and human intelligence by integrating with visualization_techniques on semantic video retrieval systems our techniques on intelligent news video analysis and knowledge_discovery have the capacity to enable more effective visualization and exploration of large-scale news video collections in addition news video_visualization and exploration can provide valuable feedback to improve our techniques for intelligent news video analysis and knowledge_discovery knowledge_discovery knowledge_visualization semantic_video_classification
in computer-based literary analysis different types of features are used to characterize a text usually only a single feature value or vector is calculated for the whole text in this paper we combine automatic literature analysis methods with an effective visualization_technique to analyze the behavior of the feature values across the text for an interactive_visual_analysis we calculate a sequence of feature values per text and present them to the user as a characteristic fingerprint the feature values may be calculated on different hierarchy levels allowing the analysis to be done on different resolution levels a case_study shows several successful applications of our new method to known literature problems and demonstrates the advantage of our new visual literature fingerprinting visual_literature_analysis literature fingerprinting visual_analytics
in this paper we introduce newslab an exploratory_visualization approach for the analysis of large scale broadcast news video collections containing many thousands of news stories over extended periods of time a river metaphor is used to depict the thematic changes of the news over time an interactive lens metaphor allows the playback of fine-grained video segments selected through the river overview multi-resolution navigation is supported via a hierarchical time structure as well as a hierarchical theme structure themes can be explored hierarchically according to their thematic structure or in an unstructured fashion using various ranking criteria a rich set of interactions such as filtering drill-down/roll-up navigation history animation and keyword based search are also provided our case studies show how this set of tools can be used to find emerging topics in the news compare different broadcasters or mine the news for topics of interest large_data_exploration animation broadcast_video_analysis clustering comparative_analysis time_filtering
investigative analysts who work with collections of text documents connect embedded threads of evidence in order to formulate hypotheses about plans and activities of potential interest as the number of documents and the corresponding number of concepts and entities within the documents grow larger sense-making processes become more and more difficult for the analysts we have developed a visual analytic system called jigsaw that represents documents and their entities visually in order to help analysts examine reports more efficiently and develop theories about potential actions more quickly jigsaw provides multiple_coordinated_views of document entities with a special emphasis on visually illustrating connections between entities across the different documents visual_analytics information_visualization intelligence_analysis investigative_analysis multiple_views
this article presents spiralview a visualization tool for helping system administrators to assess network policies the tool is meant to be a complementary support to the routine activity of network_monitoring enabling a retrospective view on the alarms generated during and extended period of time the tool permits to reason about how alarms distribute over time and how they correlate with network resources eg users ips applications etc supporting the analysts in understanding how the network evolves and thus in devising new security policies for the future the spiral visualization plots alarms in time and coupled with interactive bar_charts and a users/applications graph view is used to present network data and perform queries the user is able to segment the data in meaningful subsets zoom on specific related information and inspect for relationships between alarms users and applications in designing the visualizations and their interaction and through tests with security experts several ameliorations over the standard techniques have been provided data_exploration intrusion detection network_security visualization
large-scale session log analysis typically includes statistical methods and detailed log examinations while both methods have merits statistical methods can miss previously unknown sub- populations in the data and detailed analyses may have selection biases we therefore built session viewer a visualization tool to facilitate and bridge between statistical and detailed analyses taking a multiple-coordinated view approach session viewer shows multiple session populations at the aggregate multiple and detail data levels to support different analysis styles to bridge between the statistical and the detailed analysis levels session viewer provides fluid traversal between data levels and side-by-side comparison at all data levels we describe an analysis of a large-scale web usage study to demonstrate the use of session viewer where we quantified the importance of grouping sessions based on task type web_session_log_analysis information_visualization visual_exploratory_data_analysis
large financial institutions such as bank of america handle hundreds of thousands of wire transactions per day although most transactions are legitimate these institutions have legal and financial obligations in discovering those that are suspicious with the methods of fraudulent activities ever changing searching on predefined patterns is often insufficient in detecting previously undiscovered methods in this paper we present a set of coordinated visualizations based on identifying specific keywords within the wire transactions the different views used in our system depict relationships among keywords and accounts over time furthermore we introduce a search-by-example technique which extracts accounts that show similar transaction patterns in collaboration with the anti-money laundering division at bank of america we demonstrate that using our tool investigators are able to detect accounts and transactions that exhibit suspicious behaviors fraud_detection categorial time-varying_data financial_data_visualization
wikipedia is a wiki-based encyclopedia that has become one of the most popular collaborative on-line knowledge systems as in any large collaborative system as wikipedia has grown conflicts and coordination costs have increased dramatically visual analytic tools provide a mechanism for addressing these issues by enabling users to more quickly and effectively make sense of the status of a collaborative environment in this paper we describe a model for identifying patterns of conflicts in wikipedia articles the model relies on users' editing history and the relationships between user edits especially revisions that void previous edits known as "reverts" based on this model we constructed revert graph a tool that visualizes the overall conflict patterns between groups of users it enables visual_analysis of opinion groups and rapid interactive_exploration of those relationships via detail drill- downs we present user patterns and case studies that show the effectiveness of these techniques and discuss how they could generalize to other systems wikipedia collaboration graph revert user_model visualization wiki
information_visualization leverages the human_visual_system to support the process of sensemaking in which information is collected organized and analyzed to generate knowledge and inform action though most research to date assumes a single-user focus on perceptual and cognitive processes in practice sensemaking is often a social process involving parallelization of effort discussion and consensus building this suggests that to fully support sensemaking interactive_visualization should also support social interaction however the most appropriate collaboration mechanisms for supporting this interaction are not immediately clear in this article we present design considerations for asynchronous_collaboration in visual_analysis environments highlighting issues of work parallelizationcommunication and social organization these considerations provide a guide for the design and evaluation of collaborative_visualization_systems analysis collaboration computer-supported_cooperative_work design visualization
wikipedia is a large and rapidly growing web-based collaborative authoring environment where anyone on the internet can create modify and delete pages about encyclopedic topics a remarkable property of some wikipedia pages is that they are written by up to thousands of authors who may have contradicting opinions in this paper we show that a visual_analysis of the "who revises whom"- network gives deep insight into controversies we propose a set of analysis and visualization_techniques that reveal the dominant authors of a page the roles they play and the alters they confront thereby we provide tools to understand how wikipedia authors collaborate in the presence of controversy wikipedia controversy social_network_analysis
supporting visual_analytics of multiple large-scale multidimensional datasets requires a high degree of interactivity and user control beyond the conventional challenges of visualizing such datasets we present the datameadow a visual canvas providing rich interaction for constructing visual_queries using graphical set representations called dataroses a datarose is essentially a starplot of selected columns in a dataset displayed as multivariate visualizations with dynamic_query sliders integrated into each axis the purpose of the datameadow is to allow users to create advanced visual_queries by iteratively selecting and filtering into the multidimensional data furthermore the canvas provides a clear history of the analysis that can be annotated to facilitate dissemination of analytical results to outsiders towards this end the datameadow has a direct_manipulation interface for selection filtering and creation of sets subsets and data dependencies using both simple and complex mouse gestures we have evaluated our system using a qualitative expert review involving two researchers working in the area results from this review are favorable for our new method multivariate data dynamic queries iterative_analysis parallel_coordinates small_multiples starplot visual_analytics
the investigation of the vast contest collection provided a valuable test for text mining techniques our group has focused on creating analytical tools to unveil relevant patterns and to aid with the content navigation in such text collections our results show how such an approach in combination with visualization_techniques can ease the discovery process especially when multiple tools founded on the same approach to data_mining are used in complement to and in concert with one another text mining digital libraries information_visualization knowledge_discovery visual_analytics
ats intelligent discovery analyzed the vast 2007 contest data set using two of its proprietary applications ndcore and reggae relationship generating graph analysis engine the paper describes these tools and how they were used to discover the contest's scenarios of wildlife law_enforcement endangered species issues and ecoterrorism data discovery text_analysis visual_analytics visualization
this article briefly introduces the jigsaw system and describes how we used it in analysis activities for the vast '07 contest jigsaw is a visual analytic system that provides multiple_coordinated_views to show connections between entities that are extracted from a collection of documents visual_analytics information_visualization intelligence_analysis investigative_analysis multiple_views
geotime and nspace are two interactive visual_analytics tools that support the process of analyzing massive and complex datasets the two tools were used to examine and interpret the 2007 vast contest dataset this poster paper describes how the capabilities of the tools were used to facilitate and expedite every stage of an analyst workflow geo-spatial information systems human information interaction sense making temporal analysis visual_analytics
texplorer is an integrated system for exploring and analyzing large amounts of text documents the data processing modules of texplorer consist of named entity extraction entity relation extraction hierarchical_clustering and text summarization tools using a timeline tool tree-view table-view and concept_maps texplorer provides an analytical interface for exploring a set of text documents from different perspectives and allows users to explore vast amount of text documents efficiently text vast contest visualization entity-relation extraction named-entity extraction
we present a new framework - vispad - to support the user to revisit the visual_exploration process and to synthesize and disseminate information it offers three integrated views the data view allows the user to interactively explore the data the navigation view captures the exploration process it enables the user to revisit any particular state and reuse it the knowledge view enables the user to record his/her findings and the relations between these findings information synthesis navigation presentation
c-group is a tool for analyzing dynamic group membership in social_networks over time unlike most network_visualization tools which show the group structure within an entire network or the group membership for a single actor c-group allows users to focus their analysis on a pair of individuals of interest and unlike most dynamic social network_visualization tools which focus on the addition and deletion of nodes actors and edges relationships over time c-group focuses on changing group memberships over time c-group provides users with a flexible interface for defining and redefining groups interactively and allows users to view the changing group memberships for the pair over time this helps to highlight the similarities and differences between the individuals and their evolving group memberships c-group allows users to dynamically select the time granularity of the temporal evolution and supports two novel visual representations of the evolving group memberships this flexibility gives users alternate views that are appropriate for different network sizes and provides users with different insights into the grouping behavior
we present a visualization tool to enhance situation_awareness for global argus a system that tracks and detects indications and warnings of biological events in near real time because global argus generates massive amounts of data daily its analysts often struggle to interpret the information to overcome this problem we have developed the global argus situation_awareness tool gasat using the inteleview/world wind geographical information system this tool allows users to visualize current and past events in a particular region and thus to understand how events evolve over time combined with the other tools that we are developing gasat will contribute to enhanced situation_awareness in the tracking and detection of biological events biological events biosurveillance situation_awareness visual_analytics visualization
the presence of highly tangled patterns in spectra and other serial data exacerbates the difficulty of performing visual_comparison between a test model for a particular pattern and the data the use of a simple map that plants peaks in the data directly onto their corresponding position in a residual plot with respect to a chosen test model not only retrieves the advantages of dynamic regression plotting but in practical cases also causes patterns in the data to congregate in meaningful ways with respect to more than one reference curve in the plane the technique is demonstrated on a polyphonic music signal i pattern_recognitionimplementationÂ¿interactive_systems
this paper presents a theory of analytical discourse and a formal model of the intentional structure of visual analytic_reasoning_process our model rests on the theory of collaborative discourse and allows for cooperative human-machinecommunication in visual interactive dialogues using a sample discourse from a crisis_management scenario we demonstrated the utility of our theory in characterizing the discourse context and collaboration in particular we view analytical discourse as plans consisting of complex mental attitude towards analytical tasks and issues under this view human reasoning and computational analysis become integral part of the collaborative plan that evolves through discourse analytical discourse human-computer collaboration science_of_interaction
computational and experimental sciences produce and collect ever- larger and complex datasets often in large-scale multi-institution projects the inability to gain insight into complex scientific phenomena using current software tools is a bottleneck facing virtually all endeavors of science in this paper we introduce sunfall a collaborative visual_analytics system developed for the nearby supernova factory an international astrophysics experiment and the largest data volume supernova search currently in operation sunfall utilizes novel interactive_visualization and analysis techniques to facilitate deeper scientific insight into complex noisy high-dimensional high-volume time-critical data the system combines novel image_processing algorithms statistical_analysis and machine_learning with highly interactive visual interfaces to enable collaborative user-driven scientific exploration of supernova image and spectral data sunfall is currently in operation at the nearby supernova factory it is the first visual_analytics system in production use at a major astrophysics project data_and_knowledge_visualization astrophysics scientific_visualization visual_analytics visual_exploration
many dynamic_networks have associated geological information here we present two complementing visual_analysis methods for such networks the first one provides an overview with summerized information while the second one presents a more detailed view the geological information is encoded in the network_layout which is designed to help maintain user's mental_map we also combined visualization with social_network_analysis to facilitate knowledge_discovery especially to understand network changes in the context overall evolution both methods are applied to the "history of the fifa world cup competition" data set centrality clustering dynamic network hierarchy network_visualization temporal network visual_analytics
this poster presents an exploratory field study of a vast 2007 contest entry we applied cognitive task analysis cta grounded theory gt and activity theory at to analysis of field notes and interviews from participants our results are described in the context of activity theory and sensemaking two theoretical perspectives that we have found to be particularly useful in understanding analytic tasks collaboration evaluation field methods meta-analysis theory building
visual_analytics has become a rapidly growing field of study it is also a field that is addressing very significant real world problems in homeland security business analytics emergency management genetics and bioinformatics investigative_analysis medical analytics and other areas for both these reasons it is attracting new funding and will continue to do so in the future visual_analytics has also become an international field with significant research efforts in canada europe and australia as well as the us there is significant new research funding in canada and germany with other efforts being discussed including a major program sponsored by the european union the contributors to this panel are some of the primary thought leaders providing research funding or involved in setting up the funding apparatus we have asked them to present their needs funding programs and expectations from the research community they all come from different perspectives different missions and different expectations they will present their views of the range of activity in both the us and internationally and discuss what is coming come learn about these programs initiatives and plans and how you can contribute
visual_analytics experts realize that one effective way to push the field forward and to develop metrics for measuring the performance of various visual_analytics components is to hold an annual competition the second visual_analytics science and technology vast contest was held in conjunction with the 2007 ieee vast symposium in this contest participants were to use visual analytic tools to explore a large heterogeneous data collection to construct a scenario and find evidence buried in the data of illegal and terrorist activities that were occurring a synthetic_data set was made available as well as tasks in this paper we describe some of the advances we have made from the first competition held in 2006 contest evaluation human information interaction metrics sense making visual_analytics
geotime and nspace are two interactive visual_analytics tools that support the process of analyzing massive and complex datasets the two tools were used to examine and interpret the 2007 vast contest dataset this paper describes how the capabilities of the tools were used to facilitate and expedite every stage of the analysis geo-spatial information systems human information interaction sense making temporal analysis visual_analytics
this article describes our use of the jigsaw system in working on the vast 2007 contest jigsaw provides multiple_views of a document collection and the individual entities within those documents with a particular focus on exposing connections between entities we describe how we refined the identified entities in order to better facilitate jigsaw's use and how the different views helped us to uncover key parts of the underlying plot visual_analytics information_visualization intelligence_analysis investigative_analysis multiple_views
the investigation of the vast contest collection provided a valuable test for text mining techniques our group has focused on creating analytical tools to unveil relevant patterns and to aid with the content navigation in such text collections our results show how such an approach in combination with visualization_techniques can ease the discovery process especially when multiple tools founded on the same approach to data_mining are used in complement to and in concert with one another text mining digital libraries information_visualization knowledge_discovery visual_analytics
the open source titan informatics toolkit project which extends the visualization toolkit vtk to include information_visualization capabilities is being developed by sandia national laboratories in collaboration with kitware the vast contest provided us with an opportunity to explore various ideas for constructing an analysis tool while allowing us to exercise our architecture in the solution of a complex problem as amateur analysts we found the experience both enlightening and fun information_visualization visual_analytics
texplorer is an integrated system for exploring and analyzing vast amount of text documents the data processing modules of texplorer consist of named entity extraction entity relation extraction hierarchical_clustering and text summarization tools using time line tool tree-view table-view and concept_maps texplorer provides visualizations from different aspects and allows analysts to explore vast amount of text documents efficiently text vast contest visualization
ats intelligent discovery analyzed the vast 2007 contest data set using two of its proprietary applications ndcore and reggae relationship generating graph analysis engine the paper describes these tools and how they were used to discover the contest's scenarios of wildlife law_enforcement endangered species issues and ecoterrorism data discovery text_analysis visual_analytics visualization
scatterplots remain one of the most popular and widely-used visual representations for multidimensional data due to their simplicity familiarity and visual clarity even if they lack some of the flexibility and visual expressiveness of newer multidimensional visualization_techniques this paper presents new interactive methods to explore multidimensional data using scatterplots this exploration is performed using a matrix of scatterplots that gives an overview of the possible configurations thumbnails of the scatterplots and support for interactive navigation in the multidimensional space transitions between scatterplots are performed as animated rotations in 3d space somewhat akin to rolling dice users can iteratively build queries using bounding volumes in the dataset sculpting the query from different viewpoints to become more and more refined furthermore the dimensions in the navigation space can be reordered manually or automatically to highlight salient correlations and differences among them an example scenario presents the interaction techniques supporting smooth and effortless visual_exploration of multidimensional datasets index terms&# interaction multivariate data navigation visual_analytics visual_exploration visual_queries
interaction cost is an important but poorly understood factor in visualization design we propose a framework of interaction costs inspired by normanpsilas seven stages of action to facilitate study from 484 papers we collected 61 interaction-related usability problems reported in 32 user studies and placed them into our framework of seven costs 1 decision costs to form goals 2 system-power costs to form system operations 3 multiple input mode costs to form physical sequences 4 physical-motion costs to execute sequences 5 visual-cluttering costs to perceive state 6 view-change costs to interpret perception 7 state-change costs to evaluate interpretation we also suggested ways to narrow the gulfs of execution 2-4 and evaluation 5-7 based on collected reports our framework suggests a need to consider decision costs 1 as the gulf of goal formation framework index terms&# information_visualization interaction interface_evaluation
the treemap is one of the most popular methods for visualizing hierarchical_data when a treemap contains a large number of items inspecting or comparing a few selected items in a greater level of detail becomes very challenging in this paper we present a seamless multi-focus_and_context technique called balloon focus that allows the user to smoothly enlarge multiple treemap items served as the foci while maintaining a stable treemap layout as the context our method has several desirable features first this method is quite general and can be used with different treemap_layout_algorithms second as the foci are enlarged the relative positions among all items are preserved third the foci are placed in a way that the remaining space is evenly distributed back to the non-focus treemap items when balloon focus enlarges the focus items to a maximum degree the above features ensure that the treemap will maintain a consistent appearance and avoid any abrupt layout changes in our algorithm a dag directed_acyclic_graph is used to maintain the positional constraints and an elastic model is employed to govern the placement of the treemap items we demonstrate a treemap visualization system that integrates data query manual focus selection and our novel multi-focus+context technique balloon focus together a user_study was conducted results show that with balloon focus users can better perform the tasks of comparing the values and the distribution of the foci index terms&# treemap fisheye focus+context magnification multi-focus multi-scale viewing visualizing_query_results
traditional geospatial information_visualizations often present views that restrict the user to a single perspective when zoomed out local trends and anomalies become suppressed and lost when zoomed in for local inspection spatial awareness and comparison between regions become limited in our model coordinated visualizations are integrated within individual probe interfaces which depict the local data in user-defined regions-of-interest our probe concept can be incorporated into a variety of geospatial visualizations to empower users with the ability to observe coordinate and compare data across multiple local regions it is especially useful when dealing with complex simulations or analyses where behavior in various localities differs from other localities and from the system as a whole we illustrate the effectiveness of our technique over traditional interfaces by incorporating it within three existing geospatial visualization_systems an agent-based social simulation a census data_exploration tool and an 3d gis environment for analyzing urban change over time in each case the probe-based interaction enhances spatial awareness improves inspection and comparison capabilities expands the range of scopes and facilitates collaboration among multiple users index terms&# multiple-view_techniques focus + context geospatial_analysis geospatial visualization probes
even though information_visualization infovis research has matured in recent years it is generally acknowledged that the field still lacks supporting encompassing theories in this paper we argue that the distributed_cognition framework can be used to substantiate the theoretical foundation of infovis we highlight fundamental assumptions and theoretical constructs of the distributed_cognition approach based on the cognitive science literature and a real life scenario we then discuss how the distributed_cognition framework can have an impact on the research directions and methodologies we take as infovis researchers our contributions are as follows first we highlight the view that cognition is more an emergent property of interaction than a property of the human mind second we argue that a reductionist approach to study the abstract properties of isolated human minds may not be useful in informing infovis design finally we propose to make cognition an explicit research agenda and discuss the implications on how we perform evaluation and theory building index terms&# information_visualization distributed_cognition interaction representation theory_and_methods
digital information displays are becoming more common in public_spaces such as museums galleries and libraries however the public nature of these locations requires special considerations concerning the design of information_visualization in terms of visual representations and interaction techniques we discuss the potential for and challenges of information_visualization in the museum context based on our practical experience with emdialog an interactive information presentation that was part of the emily carr exhibition at the glenbow museum in calgary emdialog visualizes the diverse and multi-faceted discourse about this canadian artist with the goal to both inform and provoke discussion it provides a visual_exploration environment that offers interplay between two integrated visualizations one for information access along temporal and the other along contextual dimensions we describe the results of an observational_study we conducted at the museum that revealed the different ways visitors approached and interacted with emdialog as well as how they perceived this form of information presentation in the museum context our results include the need to present information in a manner sufficiently attractive to draw attention and the importance of rewarding passive observation as well as both short- and longer term information exploration index terms&# artistic information_visualization interactive_information_visualization public_displays walk-up-and-use_interaction
interactive history tools ranging from basic undo and redo to branching timelines of user actions facilitate iterative forms of interaction in this paper we investigate the design of history mechanisms for information_visualization we present a design space analysis of both architectural and interface issues identifying design decisions and associated trade-offs based on this analysis we contribute a design_study of graphical history tools for tableau a database_visualization system these tools record and visualize interaction histories support data_analysis andcommunication of findings and contribute novel mechanisms for presenting managing and exporting histories furthermore we have analyzed aggregated collections of history sessions to evaluate tableau usage we describe additional tools for analyzing userspsila history logs and how they have been applied to study usage patterns in tableau index terms&#visualization analysis evaluation history presentation undo
surveys and opinion polls are extremely popular in the media especially in the months preceding a general election however the available tools for analyzing poll results often require specialized training hence data_analysis remains out of reach for many casual computer users moreover the visualizations used to communicate the results of surveys are typically limited to traditional statistical_graphics like bar graphs and pie charts both of which are fundamentally noninteractive we present a simple interactive_visualization that allows users to construct queries on large tabular_data sets and view the results in real time the results of two separate user studies suggest that our interface lowers the learning curve for naive users while still providing enough analytical power to discover interesting correlations in the data index terms&# visual_query_languages data_analysis human-computer_interaction radial_visualization
in common web-based search_interfaces it can be difficult to formulate queries that simultaneously combine temporal spatial and topical data filters we investigate how coordinated visualizations can enhance search and exploration of information on the world_wide_web by easing the formulation of these types of queries drawing from visual_information_seeking and exploratory_search we introduce visgets - interactive query visualizations of web-based information that operate with online information within a web browser visgets provide the information seeker with visual overviews of web resources and offer a way to visually filter the data our goal is to facilitate the construction of dynamic search queries that combine filters from more than one data dimension we present a prototype information exploration system featuring three linked visgets temporal spatial and topical and used it to visually explore news items from online rss feeds index terms&# information_visualization world_wide_web exploratory_search information_retrieval visual_information_seeking
wikipedia is an example of the collaborative semi-structured data sets emerging on the web these data sets have large non-uniform schema that require costly data_integration into structured tables before visualization can begin we present vispedia a web-based visualization system that reduces the cost of this data_integration users can browse wikipedia select an interesting data table then use a search interface to discover integrate and visualize additional columns of data drawn from multiple wikipedia articles this interaction is supported by a fast path search algorithm over dbpedia a semantic graph extracted from wikipedia's hyperlink structure vispedia can also export the augmented data tables produced for use in traditional visualization_systems we believe that these techniques begin to address the "long tail" of visualization by allowing a wider audience to visualize a broader class of data we evaluated this system in a first-use formative lab study study participants were able to quickly create effective visualizations for a diverse set of domains performing data_integration as needed index terms&# wikipedia data_integration information_visualization search interface semantic web
we introduce the word tree a new visualization and information-retrieval technique aimed at text documents a word tree is a graphical version of the traditional "keyword-in-context" method and enables rapid querying and exploration of bodies of text in this paper we describe the design of the technique along with some of the technical issues that arise in itsimplementation in addition we discuss the results of several months of public deployment of word trees on many eyes which provides a window onto the ways in which users obtain value from the visualization index terms&# many eyes text_visualization case_study concordance document_visualization information_retrieval search
point placement strategies aim at mapping data points represented in higher dimensions to bi-dimensional spaces and are frequently used to visualize relationships amongst data instances they have been valuable tools for analysis and exploration of data sets of various kinds many conventional techniques however do not behave well when the number of dimensions is high such as in the case of documents collections later approaches handle that shortcoming but may cause too much clutter to allow flexible exploration to take place in this work we present a novel hierarchical point placement technique that is capable of dealing with these problems while good grouping and separation of data with high similarity is maintained without increasing computation cost its hierarchical structure lends itself both to exploration in various levels of detail and to handling data in subsets improving analysis capability and also allowing manipulation of larger data sets index terms&# text_and_document_visualization hierarchical_multidimensional_visualization high-dimensional_data visual_knowledge_discovery
in many information_visualization_techniques labels are an essential part to communicate the visualized data to preserve the expressiveness of the visual representation a placed label should neither occlude other labels nor visual representatives eg icons lines that communicate crucial information optimal non-overlapping labeling is an np-hard problem thus only a few approaches achieve a fast non-overlapping labeling in highly interactive scenarios like information_visualization these approaches generally target the point-feature label_placement pflp problem_solving only label-label conflicts this paper presents a new fast solid and flexible 2d labeling approach for the pflp problem that additionally respects other visual elements and the visual extent of labeled features the results number of placed labels processing time of our particle-based method compare favorably to those of existing techniques although the esthetic quality of non-real-time approaches may not be achieved with our method it complies with practical demands and thus supports the interactive_exploration ofinformation_spaces in contrast to the known adjacent techniques the flexibility of our technique enables labeling of dense point clouds by the use of non-occluding distant labels our approach is independent of the underlying visualization_technique which enables us to demonstrate the application of our labeling method within different information_visualization scenarios index terms&# automatic_label_placement dynamic_labeling information_visualization interactive_labeling occlusion-free
in february 2008 the new york times published an unusual chart of box office revenues for 7500 movies over 21 years the chart was based on a similar visualization developed by the first author that displayed trends in music listening this paper describes the design decisions and algorithms behind these graphics and discusses the reaction on the web we suggest that this type of complex layered graph is effective for displaying large_data sets to a mass audience we provide a mathematical analysis of how this layered graph relates to traditional stacked graphs and to techniques such as themeriver showing how each method is optimizing a different ldquoenergy functionrdquo finally we discuss techniques for coloring and ordering the layers of such graphs throughout the paper we emphasize the interplay between considerations of aesthetics and legibility index terms&# streamgraph themeriver aestheticscommunication-minded_visualization lastfm listening_history time_series
systems biologists use interaction graphs to model the behavior of biological systems at the molecular level in an iterative process such biologists observe the reactions of living cells under various experimental conditions view the results in the context of the interaction graph and then propose changes to the graph model these graphs serve as a form of dynamic knowledge_representation of the biological system being studied and evolve as new insight is gained from the experimental data while numerous graph_layout and drawing packages are available these tools did not fully meet the needs of our immunologist collaborators in this paper we describe the data information display needs of these immunologists and translate them into design decisions these decisions led us to create cerebral a system that uses a biologically guided graph_layout and incorporates experimental data directly into the graph display small multiple_views of different experimental conditions and a data-driven parallel_coordinates view enable correlations between experimental conditions to be analyzed at the same time that the data is viewed in the graph context this combination of coordinated_views allows the biologist to view the data from many different perspectives simultaneously to illustrate the typical analysis tasks performed we analyze two datasets using cerebral based on feedback from our collaborators we conclude that cerebral is a valuable tool for analyzing experimental data in the context of an interaction graph model graph_layout index terms&# design_study small_multiples systems_biology_visualization
the nature of an information_visualization can be considered to lie in the visual metaphors it uses to structure information the process of understanding a visualization therefore involves an interaction between these external visual metaphors and the user's internal knowledge_representations to investigate this claim we conducted an experiment to test the effects of visual metaphor and verbal metaphor on the understanding of tree_visualizations participants answered simple data comprehension questions while viewing either a treemap or a node-link diagram questions were worded to reflect a verbal metaphor that was either compatible or incompatible with the visualization a participant was using the results suggest that the visual metaphor indeed affects how a user derives information from a visualization additionally we found that the degree to which a user is affected by the metaphor is strongly correlated with the user's ability to answer task questions correctly these findings are a first step towards illuminating how visual metaphors shape user understanding and have significant implications for the evaluation application and theory_of_visualization cognition index terms&# evaluation hierarchies metaphors visualization_theory
in the established procedural model of information_visualization the first operation is to transform raw data into data tables the transforms typically include abstractions that aggregate and segment relevant data and are usually defined by a human user or programmer the theme of this paper is that for video data transforms should be supported by low level computer vision high level reasoning still resides in the human analyst while part of the low level perception is handled by the computer to illustrate this approach we present viz-a-vis an overhead video capture and access system for activity analysis in natural settings over variable periods of time overhead video provides rich opportunities for long-term behavioral and occupancy analysis but it poses considerable challenges we present initial steps addressing two challenges first overhead video generates overwhelmingly large_volumes of video impractical to analyze manually second automatic video analysis remains an open problem for computer vision index terms&# spatiotemporal_visualization image/video_analytics sensor_analytics time_series_data video_visualization
graphs have been widely used to model relationships among data for large graphs excessive edge crossings make the display visually cluttered and thus difficult to explore in this paper we propose a novel geometry-based edge-clustering framework that can group edges into bundles to reduce the overall edge crossings our method uses a control mesh to guide the edge-clustering process edge bundles can be formed by forcing all edges to pass through some control points on the mesh the control mesh can be generated at different levels of detail either manually or automatically based on underlying graph patterns users can further interact with the edge-clustering results through several advanced visualization_techniques such as color and opacity enhancement compared with other edge-clustering methods our approach is intuitive flexible and efficient the experiments on some large graphs demonstrate the effectiveness of our method graph_visualization index terms&# edge_clustering mesh visual_clutter
this paper proposes novel methods for visualizing specifically the large power-law graphs that arise in sociology and the sciences in such cases a large portion of edges can be shown to be less important and removed while preserving component connectedness and other features eg cliques to more clearly reveal the networkpsilas underlying connection pathways this simplification approach deterministically filters instead of clustering the graph to retain important node and edge semantics and works both automatically and interactively the improved graph filtering and layout is combined with a novel computer_graphics anisotropic_shading of the dense crisscrossing array of edges to yield a full social network and scale-free graph_visualization system both quantitative analysis and visual results demonstrate the effectiveness of this approach index terms&# scale-free network anisotropic_shading betweenness_centrality edge_filtering
a standard approach to large network_visualization is to provide an overview of the network and a detailed view of a small component of the graph centred around a focal node the user explores the network by changing the focal node in the detailed view or by changing the level of detail of a node or cluster for scalability fast force-based layout_algorithms are used for the overview and the detailed view however using the same layout_algorithm in both views is problematic since layout for the detailed view has different requirements to that in the overview here we present a model in which constrained graph_layout_algorithms are used for layout in the detailed view this means the detailed view has high-quality layout including sophisticated edge routing and is customisable by the user who can add placement constraints on the layout scalability is still ensured since the slower layout techniques are only applied to the small subgraph shown in the detailed view the main technical innovations are techniques to ensure that the overview_and_detailed view remain synchronized and modifying constrained graph_layout_algorithms to support smooth stable layout the key innovation supporting stability are new dynamic_graph_layout_algorithms that preserve the topology or structure of the network when the user changes the focus node or the level of detail by in situ semantic_zooming we have built a prototype tool and demonstrate its use in two application domains uml class diagrams and biological_networks graph_drawing index terms&# constraints force directed algorithms multidimensional scaling stress_majorization
network data frequently arises in a wide variety of fields and node-link_diagrams are a very natural and intuitive representation of such data in order for a node-link diagram to be effective the nodes must be arranged well on the screen while many graph_layout_algorithms exist for this purpose they often have limitations such as high computational complexity or node colocation this paper proposes a new approach to graph_layout through the use of space filling curves which is very fast and guarantees that there will be no nodes that are colocated the resulting layout is also aesthetic and satisfies several criteria for graph_layout effectiveness graph_layout index terms&# information_visualization space filling curves
data_transformation the process of preparing raw data for effective visualization is one of the key challenges in information_visualization although researchers have developed many data_transformation techniques there is little empirical_study of the general impact of data_transformation on visualization without such study it is difficult to systematically decide when and which data_transformation techniques are needed we thus have designed and conducted a two-part empirical_study that examines how the use of common data_transformation techniques impacts visualization quality which in turn affects user task_performance our first experiment studies the impact of data_transformation on user performance in single-step typical visual analytic tasks the second experiment assesses the impact of data_transformation in multi-step analytic tasks our results quantify the benefits of data_transformation in both experiments more importantly our analyses reveal that 1 the benefits of data_transformation vary significantly by task and by visualization and 2 the use of data_transformation depends on a user's interaction context based on our findings we present a set of design recommendations that help guide the development and use of data_transformation techniques index terms&# data_cleaning data_transformation empirical_evaluation user studies
exploring communities is an important task in social_network_analysis such communities are currently identified using clustering methods to group actors this approach often leads to actors belonging to one and only one cluster whereas in real life a person can belong to several communities as a solution we propose duplicating actors in social_networks and discuss potential impact of such a move several visual duplication designs are discussed and a controlled experiment comparing network_visualization with and without duplication is performed using 6 tasks that are important for graph readability and visual interpretation of social_networks we show that in our experiment duplications significantly improve community-related tasks but sometimes interfere with other graph readability tasks finally we propose a set of guidelines for deciding when to duplicate actors and choosing candidates for duplication and alternative ways to render them in social network representations clustering graph_visualization index terms&# node_duplications social_networks
animation has been used to show trends in multi-dimensional_data this technique has recently gained new prominence for presentations most notably with gapminder trendalyzer in trendalyzer animation together with interesting data and an engaging presenter helps the audience understand the results of an analysis of the data it is less clear whether trend animation is effective for analysis this paper proposes two alternative trend_visualizations that use static depictions of trends one which shows traces of all trends overlaid simultaneously in one display and a second that uses a small_multiples display to show the trend traces side-by-side the paper evaluates the three visualizations for both analysis and presentation results indicate that trend animation can be challenging to use even for presentations while it is the fastest technique for presentation and participants find it enjoyable and exciting it does lead to many participant errors animation is the least effective form for analysis both static depictions of trends are significantly faster than animation and the small_multiples display is more accurate index terms&# information_visualization animation design experiment trends
many graph_layout_algorithms optimize visual characteristics to achieve useful representations implicitly their goal is to create visual representations that are more intuitive to human observers in this paper we asked users to explicitly manipulate nodes in a network diagram to create layouts that they felt best captured the relationships in the data this allowed us to measure organizational behavior directly allowing us to evaluate the perceptual importance of particular visual_features such as edge crossings and edge-lengths uniformity we also manipulated the interior structure of the node relationships by designing data sets that contained clusters that is sets of nodes that are strongly interconnected by varying the degree to which these clusters were ldquomaskedrdquo by extraneous edges we were able to measure observerspsila sensitivity to the existence of clusters and how they revealed them in the network diagram based on these measurements we found that observers are able to recover cluster structure that the distance between clusters is inversely related to the strength of the clustering and that users exhibit the tendency to use edges to visually delineate perceptual groups these results demonstrate the role of perceptual_organization in representing graph data and provide concrete recommendations for graph_layout_algorithms index terms&# network_layout_visualization graph_layout perceptual_organization user studies
while it is quite typical to deal with attributes of different data types in the visualization of heterogeneous and multivariate datasets most existing techniques still focus on the most usual data types such as numerical attributes or strings in this paper we present a new approach to the interactive_visual_exploration_and_analysis of data that contains attributes which are of set type a set-typed attribute of a data item - like one cell in a table - has a list of ngt=0 elements as its value we present the setpsilaopsilagram as a new visualization approach to represent data of set type and to enable interactive_visual_exploration_and_analysis we also demonstrate how this approach is capable to help in dealing with datasets that have a larger number of dimensions more than a dozen or more especially also in the context of categorical_data to illustrate the effectiveness of our approach we present the interactive_visual_analysis of a crm dataset with data from a questionnaire on the education and shopping habits of about 90000 people categorical_data_visualization focus+context_visualization index terms&# interactive_visual_analysis interactive_visualization multidimensional multivariate data visualization multiple_coordinated_views
existing treemap_layout_algorithms suffer to some extent from poor or inconsistent mappings between data order and visual ordering in their representation reducing their cognitive plausibility while attempts have been made to quantify this mismatch and algorithms proposed to minimize inconsistency solutions provided tend to concentrate on one-dimensional ordering we propose extensions to the existing squarified layout_algorithm that exploit the two-dimensional arrangement of treemap nodes more effectively our proposed spatial squarified layout_algorithm provides a more consistent arrangement of nodes while maintaining low aspect_ratios it is suitable for the arrangement of data with a geographic component and can be used to create tessellated cartograms for geo visualization locational consistency is measured and visualized and a number of layout_algorithms are compared cielab color space and displacement vector overlays are used to assess and emphasize the spatial_layout of treemap nodes a case_study involving locations of tagged photographs in the flickr database is described cielab cartograms geographic_information geo visualization index terms&# tree structures treemaps
ranking data which result from m raters ranking n items are difficult to visualize due to their discrete algebraic structure and the computational difficulties associated with them when n is large this problem becomes worse when raters provide tied rankings or not all items are ranked we develop an approach for the visualization of ranking data for large n which is intuitive easy to use and computationally efficient the approach overcomes the structural and computational difficulties by utilizing a natural measure of dissimilarity for raters and projecting the raters into a low dimensional vector space where they are viewed the visualization_techniques are demonstrated using voting data jokes and movie preferences index terms&# partial rankings incomplete_rankings multidimensional scaling
visualization of volumetric_data faces the difficult task of finding effective parameters for the transfer_functions those parameters can determine the effectiveness andaccuracy of the visualization frequently volumetric_data includes multiple structures and features that need to be differentiated however if those features have the same intensity and gradient values existing transfer_functions are limited at effectively illustrating those similar features with different rendering properties we introduce texture-based transfer_functions for direct_volume_rendering in our approach the voxelpsilas resulting opacity and color are based on local textural properties rather than individual intensity values for example if the intensity values of the vessels are similar to those on the boundary of the lungs our texture-based transfer_function will analyze the textural properties in those regions and color them differently even though they have the same intensity values in the volume the use of texture-based transfer_functions has several benefits first structures and features with the same intensity and gradient values can be automatically visualized with different rendering properties second segmentation or prior knowledge of the specific features within the volume is not required for classifying these features differently third textural metrics can be combined and/or maximized to capture and better differentiate similar structures we demonstrate our texture-based transfer_function for direct_volume_rendering with synthetic and real-world medical data to show the strength of our technique index terms&# data_variability medical imaging statistical_analysis visualization volume_rendering
the method of moving least squares mls is a popular framework for reconstructing continuous functions from scattered data due to its rich mathematical properties and well-understood theoretical foundations this paper applies mls to volume_rendering providing a unified mathematical framework for ray casting of scalar_data stored over regular as well as irregular grids we use the mls reconstruction to render smooth isosurfaces and to compute accurate derivatives for high-quality shading effects we also present a novel adaptive preintegration scheme to improve the efficiency of the ray casting algorithm by reducing the overall number of function evaluations and an efficientimplementation of our framework exploiting modern graphics_hardware the resulting system enables high-quality volume integration and shaded isosurface rendering for regular and irregular volume data adaptive_integration index terms&# moving least squares reconstruction unstructured_grids volume_visualization
the visualization of complex 3d images remains a challenge a fact that is magnified by the difficulty to classify or segment volume data in this paper we introduce size-based transfer_functions which map the local scale of features to color and opacity features in a data set with similar or identical scalar values can be classified based on their relative size we achieve this with the use of scale fields which are 3d fields that represent the relative size of the local feature at each voxel we present a mechanism for obtaining these scale fields at interactive rates through a continuous scale-space analysis and a set of detection filters through a number of examples we show that size-based transfer_functions can improve classification and enhance volume_rendering techniques such as maximum intensity projection the ability to classify objects based on local size at interactive rates proves to be a powerful method for complex data_exploration gpu_techniques index terms&# interactive_visualization scale_space transfer_functions volume_rendering
in this work we present basic methodology for interactive volume_editing on gpus and we demonstrate the use of these methods to achieve a number of different effects we present fast techniques to modify the appearance and structure of volumetric scalar_fields given on cartesian grids similar to 2d circular brushes as used in surface painting we present 3d spherical brushes for intuitive coloring of particular structures in such fields this paint metaphor is extended to allow the user to change the data itself and the use of this functionality for interactive structure isolation hole filling and artefact removal is demonstrated building on previous work in the field we introduce high-resolution selection volumes which can be seen as a resolution-based focus+context metaphor by utilizing such volumes we present a novel approach to interactive volume_editing at sub-voxelaccuracy finally we introduce a fast technique to paste textures onto iso-surfaces in a 3d scalar field since the texture resolution is independent of the volume resolution this technique allows structure-aligned textures containing appearance properties or textual information to be used for volume augmentation and annotation gpu index terms&# volume_editing annotations carving painting
smoke rendering is a standard technique for flow_visualization most approaches are based on a volumetric particle based or image based representation of the smoke this paper introduces an alternative representation of smoke structures as semi-transparent streak_surfaces in order to make streak surface integration fast enough for interactive applications we avoid expensive adaptive retriangulations by coupling the opacity of the triangles to their shapes this way the surface shows a smoke-like look even in rather turbulent areas furthermore we show modifications of the approach to mimic smoke nozzles wool tufts and time surfaces the technique is applied to a number of test data sets index terms&# unsteady_flow_visualization smoke_visualization streak_surfaces
we present a novel approach for the direct computation of integral_surfaces in time-dependent_vector_fields as opposed to previous work which we analyze in detail our approach is based on a separation of integral surface computation into two stages surface approximation and generation of a graphical representation this allows us to overcome several limitations of existing techniques we first describe an algorithm for surface integration that approximates a series of time lines using iterative refinement and computes a skeleton of the integral surface in a second step we generate a well-conditioned triangulation our approach allows a highly accurate treatment of very large time-varying vector_fields in an efficient streaming fashion we examine the properties of the presented methods on several example datasets and perform a numerical study of its correctness andaccuracy finally we investigate some visualization aspects of integral_surfaces d vector_field_visualization index terms&# flow_visualization surface_extraction time-varying and time-series visualization
particle deposition in the small bronchial tubes generations six through twelve is strongly influenced by the vortex-dominated secondary flows that are induced by axial curvature of the tubes in this paper we employ particle destination maps in conjunction with two-dimensional finite-time_lyapunov_exponent maps to illustrate how the trajectories of finite-mass particles are influenced by the presence of vortices we consider two three-generation bronchial tube models a planar asymmetric geometry and a non-planar asymmetric geometry our visualizations demonstrate that these techniques coupled with judiciously seeded particle trajectories are effective tools for studying particle/flow structure interactions ftle index terms&# bronchial tube particle_trajectory visualization
a stand-alone visualization_application has been developed by a multi-disciplinary collaborative team with the sole purpose of creating an interactive_exploration environment allowing turbulent flow researchers to experiment and validate hypotheses using visualization this system has specific optimizations made in data_management caching computations and visualization allowing for the interactive_exploration of datasets on the order of 1tb in size using this application the user co-author calo is able to interactively visualize and analyze all regions of a transitional_flow volume including the laminar transitional and fully turbulent regions the underlying goal of the visualizations produced from these transitional_flow simulations is to localize turbulent spots in the laminar region of the boundary layer determine under which conditions they form and follow their evolution the initiation of turbulent spots which ultimately lead to full turbulence was located via a proposed feature_detection condition and verified by experimental results the conditions under which these turbulent spots form and coalesce are validated and presented applications_of_visualization flow_visualization index terms&# transitional_flow turbulence
scatterplots are well established means of visualizing discrete data values with two data variables as a collection of discrete points we aim at generalizing the concept of scatterplots to the visualization of spatially continuous input data by a continuous and dense plot an example of a continuous input field is data defined on an n-d spatial grid with respective interpolation or reconstruction of in-between values we propose a rigorous accurate and generic mathematical model of continuous scatterplots that considers an arbitrary density defined on an input field on an n-d domain and that maps this density to m-d scatterplots special cases are derived from this generic model and discussed in detail scatterplots where the n-d spatial domain and the m-d data attribute domain have identical dimension 1-d scatterplots as a way to define continuous histograms and 2-d scatterplots of data on 3-d spatial grids we show how continuous histograms are related to traditional discrete histograms and to the histograms of isosurface statistics based on the mathematical model of continuous scatterplots respective visualization algorithms are derived in particular for 2-d scatterplots of data from 3-d tetrahedral_grids for several visualization tasks we show the applicability of continuous scatterplots since continuous scatterplots do not only sample data at grid points but interpolate data values within cells a dense and complete visualization of the data set is achieved that scales well with increasing data set size especially for irregular grids with varying cell size improved results are obtained when compared to conventional scatterplots therefore continuous scatterplots are a suitable extension of a statistics visualization_technique to be applied to typical data from scientific computation index terms&# scatterplot continuous frequency plot histogram interpolation
parallel coordinate plots pcps are commonly used in information_visualization to provide insight into multi-variate_data these plots help to spot correlations between variables pcps have been successfully applied to unstructured_datasets up to a few millions of points in this paper we present techniques to enhance the usability of pcps for the exploration of large multi-timepoint volumetric_data sets containing tens of millions of points per timestep the main difficulties that arise when applying pcps to large numbers of data points are visual_clutter and slow performance making interactive_exploration infeasible moreover the spatial_context of the volumetric_data is usually lost we describe techniques for preprocessing using data quantization and compression and for fast gpu-based rendering of pcps using joint density distributions for each pair of consecutive variables resulting in a smooth continuous visualization also fast brushing techniques are proposed for interactive data selection in multiple linked_views including a 3d spatial volume view these techniques have been successfully applied to three large_data sets hurricane isabel vis'04 contest the ionization front instability data set vis'08 design contest and data from a large-eddy simulation of cumulus clouds with these data we show how pcps can be extended to successfully visualize and interactively explore multi-timepoint volumetric_datasets with an order of magnitude more data points index terms&# parallel coordinate plots linked_related_views multi-field time-varying
radviz is a radial_visualization with dimensions assigned to points called dimensional anchors das placed on the circumference of a circle records are assigned locations within the circle as a function of its relative attraction to each of the das the das can be moved either interactively or algorithmically to reveal different meaningful patterns in the dataset in this paper we describe vectorized_radviz vrv which extends the number of dimensions through data flattening we show how vrv increases the power of radviz through these extra dimensions by enhancing the flexibility in the layout of the das we apply vrv to the problem of analyzing the results of multiple_clusterings of the same data set called multiple cluster sets or cluster_ensembles we show how features of vrv help discern patterns across the multiple cluster sets we use the iris data set to explain vrv and a newt gene microarray_data set used in studying limb regeneration to show its utility we then discuss further applications of vrv cluster_ensembles clustering flattening_datasets index terms&# multiple_clustering radviz vectorized_radviz visualization
in this work we develop a new alternative to conventional maps for visualization of relatively short paths as they are frequently encountered in hotels resorts or museums our approach is based on a warped rendering of a 3d model of the environment such that the visualized path appears to be straight even though it may contain several junctions this has the advantage that the beholder of the image gains a realistic impression of the surroundings along the way which makes it easy to retrace the route in practice we give an intuitive method for generation of such images and present results from user studies undertaken to evaluate the benefit of the warped images for orientation in unknown environments index terms&# maps route visualization space_deformation
the visualization and exploration of multivariate data is still a challenging task methods either try to visualize all variables simultaneously at each position using glyph-based approaches or use linked_views for the interaction between attribute space and physical domain such as brushing of scatterplots most visualizations of the attribute space are either difficult to understand or suffer from visual_clutter we propose a transformation of the high-dimensional_data in attribute space to 2d that results in a point cloud called attribute cloud such that points with similar multivariate attributes are located close to each other the transformation is based on ideas from multivariate density estimation and manifold_learning the resulting attribute cloud is an easy to understand visualization of multivariate data in two dimensions we explain several techniques to incorporate additional information into the attribute cloud that help the user get a better understanding of multivariate data using different examples from fluid dynamics and climate simulation we show how brushing can be used to explore the attribute cloud and find interesting structures in physical space index terms&# multivariate data brushing data_transformation linked_views manifold_learning
extracting and visualizing temporal patterns in large scientific data is an open problem in visualization research first there are few proven methods to flexibly and concisely define general temporal patterns for visualization second with large time-dependent_data sets as typical with todaypsilas large-scale simulations scalable and general solutions for handling the data are still not widely available in this work we have developed a textual pattern matching approach for specifying and identifying general temporal patterns besides defining the formalism of the language we also provide a workingimplementation with sufficient efficiency and scalability to handle large_data sets using recent large-scale simulation data from multiple application domains we demonstrate that our visualization approach is one of the first to empower a concept driven exploration of large-scale time-varying multivariate data index terms&# multivariate visualization time-varying uncertainty
understanding fluid flow data especially vortices is still a challenging task sophisticated visualization tools help to gain insight in this paper we present a novel approach for the interactive comparison of scalar_fields using isosurfaces and its application to fluid flow datasets features in two scalar_fields are defined by largest contour segmentation after topological_simplification these features are matched using a volumetric similarity measure based on spatial overlap of individual features the relationships defined by this similarity measure are ranked and presented in a thumbnail gallery of feature pairs and a graph representation showing all relationships between individual contours additionally linked_views of the contour_trees are provided to ease navigation the main render view shows the selected features overlapping each other thus by displaying individual features and their relationships in a structured fashion we enable exploratory_visualization of correlations between similar structures in two scalar_fields we demonstrate the utility of our approach by applying it to a number of complex fluid flow datasets where the emphasis is put on the comparison of vortex related scalar quantities index terms&# scalar_topology comparative_visualization contour_tree flow_visualization largest_contours
data sets resulting from physical_simulations typically contain a multitude of physical variables it is therefore desirable that visualization methods take into account the entire multi-field volume data rather than concentrating on one variable we present a visualization approach based on surface_extraction from multi-field particle volume data the surfaces segment the data with respect to the underlying multi-variate function decisions on segmentation properties are based on the analysis of the multi-dimensional feature space the feature space exploration is performed by an automated multi-dimensional hierarchical_clustering method whose resulting density clusters are shown in the form of density level_sets in a 3d star coordinate layout in the star coordinate layout the user can select clusters of interest a selected cluster in feature space corresponds to a segmenting surface in object space based on the segmentation property induced by the cluster membership we extract a surface from the volume data our driving applications are smoothed_particle_hydrodynamics sph simulations where each particle carries multiple properties the data sets are given in the form of unstructured point-based volume data we directly extract our surfaces from such data without prior resampling or grid generation the surface_extraction computes individual points on the surface which is supported by an efficient neighborhood computation the extracted surface points are rendered using point-based_rendering operations our approach combines methods in scientific_visualization for object-space operations with methods in information_visualization for feature-space operations index terms&# multi-field and multi-variate_visualization isosurfaces_and_surface_extraction particle_simulations point-based_visualization star_coordinates visualization_in_astrophysics
for difficult cases in endoscopic sinus_surgery a careful planning of the intervention is necessary due to the reduced field of view during the intervention the surgeons have less information about the surrounding structures in the working area compared to open surgery virtual_endoscopy enables the visualization of the operating field and additional information such as risk structures eg optical nerve and skull base and target structures to be removed eg mucosal swelling the sinus endoscopy system provides the functional range of a virtual endoscopic system with special focus on a realistic representation furthermore by using direct_volume_rendering we avoid time-consuming segmentation steps for the use of individual patient datasets however the image quality of the endoscopic view can be adjusted in a way that a standard computer with a modern standard graphics card achieves interactive frame rates with low cpu utilization thereby characteristics of the endoscopic view are systematically used for the optimization of the volume_rendering speed the system design was based on a careful analysis of the endoscopic sinus_surgery and the resulting needs for computer support as a small standalone application it can be instantly used for surgical planning and patient education first results of a clinical evaluation with ent surgeons were employed to fine-tune the user interface in particular to reduce the number of controls by using appropriate default values wherever possible the system was used for preoperative planning in 102 cases provides useful information for intervention planning eg anatomic variations of the rec frontalis and closely resembles the intraoperative situation index terms&# medical visualization operationplanning sinus_surgery virtual_endoscopy volume_rendering
myocardial perfusion imaging with single photon emission computed_tomography spect is an established method for the detection and evaluation of coronary artery disease cad state-of-the-art spect scanners yield a large number of regional parameters of the left-ventricular myocardium eg blood supply at rest and during stress wall thickness and wall thickening during heart contraction that all need to be assessed by the physician today the individual parameters of this multivariate data set are displayed as stacks of 2d slices bull's eye plots or more recently surfaces in 3d which depict the left-ventricular wall in all these visualizations the data sets are displayed side-by-side rather than in an integrated manner such that the multivariate data have to be examined sequentially and need to be fused mentally this is time consuming and error-prone in this paper we present an interactive 3d glyph visualization which enables an effective integrated visualization of the multivariate data results from semiotic theory are used to optimize the mapping of different variables to glyph properties this facilitates an improved perception of important information and thus an accelerated diagnosis the 3d_glyphs are linked to the established 2d views which permit a more detailed inspection and to relevant meta-information such as known stenoses of coronary vessels supplying the myocardial region our method has demonstrated its potential for clinical routine use in real application scenarios assessed by nuclear physicians index terms&# multivariate visualization spect glyph techniques myocardial perfusion imaging
this paper presents a novel method for interactive_exploration of industrial ct volumes such as cast metal parts with the goal of interactively detecting classifying and quantifying features using a visualization-driven approach the standard approach for defect detection builds on region_growing which requires manually tuning parameters such as target ranges for density and size variance as well as the specification of seed points if the results are not satisfactory region_growing must be performed again with different parameters in contrast our method allows interactive_exploration of the parameter space completely separated from region_growing in an unattended pre-processing stage the pre-computed feature volume tracks a feature size curve for each voxel over time which is identified with the main region_growing parameter such as variance a novel 3d transfer_function domain over density featuresize time allows for interactive_exploration of feature classes features and feature size curves can also be explored individually which helps with transfer_function specification and allows coloring individual features and disabling features resulting from ct artifacts based on the classification obtained through exploration the classified features can be quantified immediately index terms&# multi-dimensional_transfer_functions non-destructive_testing region_growing volume_rendering
ventricular assist devices vads support the heart in its vital task of maintaining circulation in the human body when the heart alone is not able to maintain a sufficient flow rate due to illness or degenerative diseases however the engineering of these devices is a highly demanding task advanced modeling methods and computer simulations allow the investigation of the fluid flow inside such a device and in particular of potential blood_damage in this paper we present a set of visualization methods which have been designed to specifically support the analysis of a tensor-based blood_damage prediction model this model is based on the tracing of particles through the vad for each of which the cumulative blood_damage can be computed the model's tensor output approximates a single blood cell's deformation in the flow_field the tensor and derived scalar_data are subsequently visualized using techniques based on icons particle visualization and function plotting all these techniques are accessible through a virtual_reality-based user interface which features not only stereoscopic rendering but also natural interaction with the complex three-dimensional data to illustrate the effectiveness of these visualization methods we present the results of an analysis session that was performed by domain experts for a specific data set for the micromed debakey vad index terms&# tensor_visualization blood_damage time-dependent_data ventricular assist device virtual_reality
we introduce and analyze an efficient reconstruction algorithm for fcc-sampled data the reconstruction is based on the 6-direction box spline that is naturally associated with the fcc lattice and shares the continuity and approximation order of the triquadratic b-spline we observe less aliasing for generic level_sets and derive special techniques to attain the higher evaluation efficiency promised by the lower degree and smaller stencil-size of the c1 6-direction box spline over the triquadratic b-spline face-centered_cubic_lattice index terms&# box spline volumetric_data_reconstruction
smooth surface_extraction using partial_differential_equations pdes is a well-known and widely used technique for visualizing volume data existing approaches operate on gridded data and mainly on regular structured grids when considering unstructured point-based volume data where sample points do not form regular patterns nor are they connected in any form one would typically resample the data over a grid prior to applying the known pde-based methods we propose an approach that directly extracts smooth_surfaces from unstructured point-based volume data without prior resampling or mesh generation when operating on unstructured_data one needs to quickly derive neighborhood information the respective information is retrieved by partitioning the 3d domain into cells using a fed-tree and operating on its cells we exploit neighborhood information to estimate gradients and mean curvature at every sample point using a four-dimensional least-squares fitting approach gradients and mean curvature are required for applying the chosen pde-based method that combines hyperbolic advection to an isovalue of a given scalar field and mean curvature flow since we are using an explicit time-integration scheme time steps and neighbor locations are bounded to ensure convergence of the process to avoid small global time steps one can use asynchronous local integration we extract a smooth surface by successively fitting a smooth auxiliary function to the data set this auxiliary function is initialized as a signed distance function for each sample and for every time step we compute the respective gradient the mean curvature and a stable time step with these informations the auxiliary function is manipulated using an explicit euler time integration the process successively continues with the next sample point in time if the norm of the auxiliary function gradient in a sample exceeds a given threshold at some time the auxiliary function is reinitialized to a signed dista- - nce function after convergence of the evolvution the resulting smooth surface is obtained by extracting the zero isosurface from the auxiliary function using direct isosurface_extraction from unstructured point-based volume data and rendering the extracted surface using point-based_rendering methods index terms&# pdes level_sets point-based_visualization surface_extraction
methods that faithfully and robustly capture the geometry of complex material interfaces in labeled volume data are important for generating realistic and accurate visualizations and simulations of real-world objects the generation of such multimaterial models from measured data poses two unique challenges first the surfaces must be well-sampled with regular efficient tessellations that are consistent across material boundaries and second the resulting meshes must respect the nonmanifold geometry of the multimaterial interfaces this paper proposes a strategy for sampling and meshing multimaterial volumes using dynamic particle_systems including a novel differentiable representation of the material junctions that allows the particle system to explicitly sample corners edges and surfaces of material intersections the distributions of particles are controlled by fundamental sampling constraints allowing delaunay-based meshing algorithms to reliably extract watertight meshes of consistently high-quality index terms&# sampling meshing visualizations
the ability to identify and present the most essential aspects of time-varying_data is critically important in many areas of science and engineering this paper introduces an importance-driven approach to time-varying volume data visualization for enhancing that ability by conducting a block-wise analysis of the data in the joint_feature-temporal_space we derive an importance curve for each data block based on the formulation of conditional entropy from information_theory each curve characterizes the local temporal behavior of the respective block and clustering the importance curves of all the volume blocks effectively classifies the underlying data based on different temporal trends exhibited by importance curves and their clustering results we suggest several interesting and effective visualization_techniques to reveal the important aspects of time-varying_data index terms&# time-varying_data_clustering conditional entropy highlighting joint_feature-temporal_space transfer_function
with recent advances in the measurement technology for allsky astrophysical imaging our view of the sky is no longer limited to the tiny visible spectral range over the 2d celestial sphere we now can access a third dimension corresponding to a broad electromagnetic spectrum with a wide range of allsky surveys these surveys span frequency bands including long long wavelength radio microwaves very short x-rays and gamma rays these advances motivate us to study and examine multiwavelength visualization_techniques to maximize our capabilities to visualize and exploit these informative image data sets in this work we begin with the processing of the data themselves uniformizing the representations and units of raw data obtained from varied detector sources then we apply tools to map convert color-code and format the multiwavelength_data in forms useful for applications we explore different visual representations for displaying the data including such methods as textured image stacks the horseshoe representation and gpu-based volume_visualization a family of visual tools and analysis methods are introduced to explore the data including interactive data mapping on the graphics processing unit gpu the mini-map explorer and gpu-based interactive feature analysis astrophysical_visualization index terms&# astronomy multiwavelength_data
visualization of general_relativity illustrates aspects of einstein's insights into the curved nature of space and time to the expert as well as the layperson one of the most interesting models which came up with einstein's theory was developed by kurt godel in 1949 the godel_universe is a valid solution of einstein's field equations making it a possible physical description of our universe it offers remarkable features like the existence of an optical horizon beyond which time_travel is possible although we know that our universe is not a godel_universe it is interesting to visualize physical aspects of a world model resulting from a theory which is highly confirmed in scientific history standard techniques to adopt an egocentric point of view in a relativistic world model have shortcomings with respect to the time needed to render an image as well as difficulties in applying a direct illumination model in this paper we want to face both issues to reduce the gap between common visualization standards and relativistic visualization we will introduce two techniques to speed up recalculation of images by means of preprocessing and lookup tables and to increase image quality through a special optimization applicable to the godel_universe the first technique allows the physicist to understand the different effects of general_relativity faster and better by generating images from existing datasets interactively by using the intrinsic symmetries of godel's spacetime which are expressed by the killing vector field we are able to reduce the necessary calculations to simple cases using the second technique this even makes it feasible to account for a direct illumination model during the rendering process although the presented methods are applied to godel's universe they can also be extended to other manifolds for example light propagation in moving dielectric media therefore other areas of research can benefit from these generic improvements gödel universe general_relativity index terms&# nonlinear ray tracing time_travel
we present a toolbox for quickly interpreting and illustrating 2d slices of seismic volumetric reflection data searching for oil and gas involves creating a structural overview of seismic reflection data to identify hydrocarbon reservoirs we improve the search of seismic structures by precalculating the horizon structures of the seismic_data prior to interpretation we improve the annotation of seismic structures by applying novel illustrative_rendering algorithms tailored to seismic_data such as deformed texturing and line and texture transfer_functions the illustrative_rendering results in multi-attribute and scale invariant visualizations where features are represented clearly in both highly zoomed in and zoomed out views thumbnail views in combination with interactive appearance control allows for a quick overview of the data before detailed interpretation takes place these techniques help reduce the work of seismic illustrators and interpreters illustrative_rendering index terms&# seismic_attributes seismic_interpretation top-down_interpretation
one of the most prominent topics in climate research is the investigation detection and allocation of climate change in this paper we aim at identifying regions in the atmosphere eg certain height layers which can act as sensitive and robust indicators for climate change we demonstrate how interactive visual_data_exploration of large amounts of multi-variate and time-dependent climate data enables the steered generation of promising hypotheses for subsequent statistical evaluation the use of new visualization and interaction technology-in the context of a coordinated multiple_views framework-allows not only to identify these promising hypotheses but also to efficiently narrow down parameters that are required in the process of computational data_analysis two datasets namely an echam5 climate model run and the era-40 reanalysis incorporating observational data are investigated higher-order information such as linear trends or signal-to-noise ratio is derived and interactively explored in order to detect and explore those regions which react most sensitively to climate change as one conclusion from this study we identify an excellent potential for usefully generalizing our approach to other similar application cases as well index terms&# interactive_visual_exploration_and_analysis interactive visual hypothesis generation visualization_for_climate_research
neurosurgical planning and image guided neurosurgery require the visualization of multimodal data obtained from various functional and structural image modalities such as magnetic_resonance_imaging mri computed_tomography ct functional mri single photon emission computed_tomography spect and so on in the case of epilepsy neurosurgery for example these images are used to identify brain regions to guide intracranial electrode implantation and resection generally such data is visualized using 2d slices and in some cases using a 3d volume_rendering along with the functional imaging results visualizing the activation region effectively by still preserving sufficient surrounding brain regions for context is exceedingly important to neurologists and surgeons we present novel interaction techniques for visualization of multimodal data to facilitate improved exploration and planning for neurosurgery we extended the line widget from vtk to allow surgeons to control the shape of the region of the brain that they can visually crop away during exploration and surgery we allow simple spherical cubical ellipsoidal and cylindrical probe aligned cuts for exploration purposes in addition we integrate the cropping tool with the image-guided navigation system used for epilepsy neurosurgery we are currently investigating the use of these new tools in surgical planning and based on further feedback from our neurosurgeons we will integrate them into the setup used for image-guided neurosurgery index terms&# irregular cropping multimodal visualization user_interaction
visually assessing the effect of the coronary artery anatomy on the perfusion of the heart muscle in patients with coronary artery disease remains a challenging task we explore the feasibility of visualizing this effect on perfusion using a numerical approach we perform a computational simulation of the way blood is perfused throughout the myocardium purely based on information from a three-dimensional anatomical tomographic scan the results are subsequently visualized using both three-dimensional visualizations and bullpsilas eye plots partially inspired by approaches currently common in medical practice our approach results in a comprehensive visualization of the coronary anatomy that compares well to visualizations commonly used for other scanning technologies we demonstrate techniques giving detailed insight in blood supply coronary territories and feeding coronary arteries of a selected region we demonstrate the advantages of our approach through visualizations that show information which commonly cannot be directly observed in scanning data such as a separate visualization of the supply from each coronary artery we thus show that the results of a computational simulation can be effectively visualized and facilitate visually correlating these results to for example perfusion data cardiac_visualization index terms&# coronary_artery_territories myocardial perfusion
the effective visualization of vascular structures is critical for diagnosis surgical planning as well as treatment evaluation in recent work we have developed an algorithm for vessel detection that examines the intensity profile around each voxel in an angiographic image and determines the likelihood that any given voxel belongs to a vessel we term this the "vesselness coefficient" of the voxel our results show that our algorithm works particularly well for visualizing branch points in vessels compared to standard hessian based techniques which are fine-tuned to identify long cylindrical structures our technique identifies branches and connections with other vessels using our computed vesselness coefficient we explore a set of techniques for visualizing vasculature visualizing vessels is particularly challenging because not only is their position in space important for clinicians but it is also important to be able to resolve their spatial relationship we applied visualization_techniques that provide shape cues as well as depth cues to allow the viewer to differentiate between vessels that are closer from those that are farther we use our computed vesselness coefficient to effectively visualize vasculature in both clinical neurovascular x-ray computed_tomography based angiography images as well as images from three different animal studies we conducted a formal user_evaluation of our visualization_techniques with the help of radiologists surgeons and other expert users results indicate that experts preferred distance color_blending and tone shading for conveying depth over standard visualization_techniques evaluation_of_visualization_techniques index terms&# vessel_identification vessel_visualization
understanding the structure of microvasculature structures and their relationship to cells in biological tissue is an important and complex problem brain microvasculature in particular is known to play an important role in chronic diseases however these networks are only visible at the microscopic level and can span large_volumes of tissue due to recent advances in microscopy large_volumes of data can be imaged at the resolution necessary to reconstruct these structures due to the dense and complex nature of microscopy data sets it is important to limit the amount of information displayed in this paper we describe methods for encoding the unique structure of microvascular data allowing researchers to selectively explore microvascular anatomy we also identify the queries most useful to researchers studying microvascular and cellular relationships by associating cellular structures with our microvascular framework we allow researchers to explore interesting anatomical relationships in dense and complex data sets index terms&# cells complex data fibers microscopy vascular
we introduce a versatile framework for characterizing and extracting salient structures in three-dimensional symmetric second-order tensor_fields the key insight is that degenerate lines in tensor_fields as defined by the standard topological approach are exactly crease ridge and valley lines of a particular tensor invariant called mode this reformulation allows us to apply well-studied approaches from scientific_visualization or computer vision to the extraction of topological_lines in tensor_fields more generally this main result suggests that other tensor_invariants such as anisotropy measures like fractional anisotropy fa can be used in the same framework in lieu of mode to identify important structural properties in tensor_fields ourimplementation addresses the specific challenge posed by the non-linearity of the considered scalar measures and by the smoothness requirement of the crease manifold computation we use a combination of smooth reconstruction kernels and adaptive_refinement strategy that automatically adjust the resolution of the analysis to the spatial variation of the considered quantities together these improvements allow for the robust application of existing ridge line extraction algorithms in the tensor context of our problem results are proposed for a diffusion_tensor_mri dataset and for a benchmark stress_tensor_field used in engineering research index terms&# tensor &# crease_extraction elds ridge_lines structural_analysis tensor_invariants topology
diffusion_weighted_magnetic_resonance_imaging is a unique tool for non-invasive investigation of major nerve fiber tracts since the popular diffusion tensor dt-mri model is limited to voxels with a single fiber direction a number of high angular resolution techniques have been proposed to provide information about more diverse fiber distributions two such approaches are q-ball imaging and spherical deconvolution which produce orientation distribution functions odfs on the sphere for analysis and visualization the maxima of these functions have been used as principal directions even though the results are known to be biased in case of crossing fiber tracts in this paper we present a more reliable technique for extracting discrete orientations from continuous odfs which is based on decomposing their higher-order_tensor representation into an isotropic component several rank-1 terms and a small residual comparing to ground truth in synthetic_data shows that the novel method reduces bias and reliably reconstructs crossing fibers which are not resolved as individual maxima in the odf we present results on both q-ball and spherical deconvolution data and demonstrate that the estimated directions allow for plausible fiber_tracking in a real data set dw-mri index terms&# q-ball fiber_tracking higher-order_tensor spherical deconvolution tensor_decomposition
this paper presents a novel and efficient surface_matching and visualization_framework through the geodesic distance-weighted shape vector image diffusion based on conformal geometry our approach can uniquely map a 3d surface to a canonical rectangular domain and encode the shape characteristics eg mean curvatures and conformalfactors of the surface in the 2d domain to construct a geodesic distance-weighted shape vector image where the distances between sampling pixels are not uniform but the actual geodesic distances on the manifold through the novel geodesic distance-weighted shape vector image diffusion presented in this paper we can create a multiscale diffusion space in which the cross-scale extrema can be detected as the robust geometric features for the matching and registration of surfaces therefore statistical_analysis and visualization of surface properties across subjects become readily available the experiments on scanned surface models show that our method is very robust for feature_extraction and surface_matching even under noise and resolution change we have also applied the framework on the real 3d human neocortical surfaces and demonstrated the excellent performance of our approach in statistical_analysis and integrated visualization of the multimodality volumetric_data over the shape vector image index terms&# multiscale diffusion shape vector image surface_matching visualization
marching_cubes is the most popular isosurface_extraction algorithm due to its simplicity efficiency and robustness it has been widely studied improved and extended while much early work was concerned with efficiency and correctness issues lately there has been a push to improve the quality of marching_cubes meshes so that they can be used in computational codes in this work we present a new classification of mc cases that we call edge groups which helps elucidate the issues that impact the triangle quality of the meshes that the method generates this formulation allows a more systematic way to bound the triangle quality and is general enough to extend to other polyhedral cell shapes used in other polygonization algorithms using this analysis we also discuss ways to improve the quality of the resulting triangle mesh including some that require only minor modifications of the original algorithm index terms&# isosurface_extraction marching_cubes
recent results have shown a link between geometric properties of isosurfaces and statistical properties of the underlying sampled data however this has two defects not all of the properties described converge to the same solution and the statistics computed are not always invariant under isosurface-preserving transformations we apply federer's coarea formula from geometric measure theory to explain these discrepancies we describe an improved substitute for histograms based on weighting with the inverse gradient magnitude develop a statistical model that is invariant under isosurface-preserving transformations and argue that this provides a consistent method for algorithm evaluation across multiple datasets based on histogram equalization we use our corrected formulation to reevaluate recent results on average isosurface complexity and show evidence that noise is one cause of the discrepancy between the expected figure and the observed one coarea formula histograms index terms&# isosurfaces
in this paper we present an algorithm that operates on a triangular_mesh and classifies each face of a triangle as either inside or outside we present three example applications of this core algorithm normal_orientation inside_removal and layer-based visualization the distinguishing feature of our algorithm is its robustness even if a difficult input model that includes holes coplanar triangles intersecting triangles and lost connectivity is given our algorithm works with the original triangles of the input model and uses sampling to construct a visibility graph that is then segmented using graph cut graph cut index terms&# inside_removal interior/exterior_classification layer_classification normal_orientation
in this paper we introduce a technique for applying textual labels to 3d surfaces an effective labeling must balance the conflicting goals of conveying the shape of the surface while being legible from a range of viewing directions shape can be conveyed by placing the text as a texture directly on the surface providing shape cues meaningful landmarks and minimally obstructing the rest of the model but rendering such surface text is problematic both in regions of high curvature where text would be warped and in highly occluded regions where it would be hidden our approach achieves both labeling goals by applying surface labels to a psilatext scaffoldpsila a surface explicitly constructed to hold the labels text scaffolds conform to the underlying surface whenever possible but can also float above problem regions allowing them to be smooth while still conveying the overall shape this paper provides methods for constructing scaffolds from a variety of input sources including meshes constructive_solid_geometry and scalar_fields these sources are first mapped into a distance transform which is then filtered and used to construct a new mesh on which labels are either manually or automatically placed in the latter case annotated regions of the input surface are associated with proximal regions on the new mesh and labels placed using cartographic principles index terms&# annotation computational_cartography surface_labeling text authoring
volume_exploration is an important issue in scientific_visualization research on volume_exploration has been focused on revealing hidden structures in volumetric_data while the information of individual structures or features is useful in practice spatial relations between structures are also important in many applications and can provide further insights into the data in this paper we systematically study the extraction representationexploration and visualization of spatial relations in volumetric_data and propose a novel relation-aware visualization pipeline for volume_exploration in our pipeline various relations in the volume are first defined and measured using region connection calculus rcc and then represented using a graph interface called relation graph with rcc and the relation graph relation query and interactive_exploration can be conducted in a comprehensive and intuitive way the visualization process is further assisted with relation-revealing viewpoint_selection and color and opacity enhancement we also introduce a quality assessment scheme which evaluates the perception of spatial relations in the rendered images experiments on various datasets demonstrate the practical use of our system in exploratory_visualization exploratory_visualization index terms&# relation-based_visualization visualization pipeline
building visualization and analysis pipelines is a large hurdle in the adoption of visualization and workflow systems by domain scientists in this paper we propose techniques to help users construct pipelines by consensus-automatically suggesting completions based on a database of previously created pipelines in particular we compute correspondences between existing pipeline subgraphs from the database and use these to predict sets of likely pipeline additions to a given partial pipeline by presenting these predictions in a carefully designed interface users can create visualizations and other data products more efficiently because they can augment their normal work patterns with the suggested completions we present animplementation of our technique in a publicly-available open-source scientific workflow system and demonstrate efficiency gains in real-world situations auto_completion index terms&# scientific_visualization scientific workflows
interactive steering with visualization has been a common goal of the visualization research community for twenty years but it is rarely ever realized in practice in this paper we describe a successful realization of a tightly coupled steering loop integrating new simulation technology and interactive_visual_analysis in a prototyping environment for automotive industry system design due to increasing pressure on car manufacturers to meet new emission regulations to improve efficiency and to reduce noise both simulation and visualization are pushed to their limits automotive system components such as the powertrain system or the injection_system have an increasing number of parameters and new design approaches are required it is no longer possible to optimize such a system solely based on experience or forward optimization by coupling interactive_visualization with the simulation back-end computational_steering it is now possible to quickly prototype a new system starting from a non-optimized initial prototype and the corresponding simulation model the prototyping continues through the refinement of the simulation model of the simulation parameters and through trial-and-error attempts to an optimized solution the ability to early see the first results from a multidimensional simulation space - thousands of simulations are run for a multidimensional variety of input parameters - and to quickly go back into the simulation and request more runs in particular parameter regions of interest significantly improves the prototyping process and provides a deeper understanding of the system behavior the excellent results which we achieved for the common rail injection_system strongly suggest that our approach has a great potential of being generalized to other similar scenarios index terms&# common rail injection_system interactive_computational_steering interactive_visual_analysis simulation
we present an interactive algorithm to compute sound propagation paths for transmission specular reflection and edge diffraction in complex scenes our formulation uses an adaptive frustum representation that is automatically sub-divided to accurately compute intersections with the scene primitives we describe a simple and fast algorithm to approximate the visible surface for each frustum and generate new frusta based on specular reflection and edge diffraction our approach is applicable to all triangulated models and we demonstrate its performance on architectural and outdoor models with tens or hundreds of thousands of triangles and moving objects in practice our algorithm can perform geometric sound propagation in complex scenes at 4-20 frames per second on a multi-core pc index terms&# sound propagation auralization interactive_system
the visualization and analysis of amr-based simulations is integral to the process of obtaining new insight in scientific research we present a new method for performing query-driven_visualization and analysis on amr data with specific emphasis on time-varying amr data our work introduces a new method that directly addresses the dynamic spatial and temporal properties of amr grids that challenge many existing visualization_techniques further we present the firstimplementation of query-driven_visualization on the gpu that uses a gpu-based indexing structure to both answer queries and efficiently utilize gpu memory we apply our method to two different science domains to demonstrate its broad applicability amr index terms&# multitemporal_visualization query-driven_visualization
large_datasets typically contain coarse features comprised of finer sub-features even if the shapes of the small structures are evident in a 3d display the aggregate shapes they suggest may not be easily inferred from previous studies in shape perception the evidence has not been clear whether physically-based illumination confers any advantage over local_illumination for understanding scenes that arise in visualization of large_data sets that contain features at two distinct scales in this paper we show that physically-based illumination can improve the perception for some static scenes of complex 3d geometry from flow_fields we perform human-subjects experiments to quantify the effect of physically-based illumination on participant performance for two tasks selecting the closer of two streamtubes from a field of tubes and identifying the shape of the domain of a flow_field over different densities of tubes we find that physically-based illumination influences participant performance as strongly as perspective projection suggesting that physically-based illumination is indeed a strong cue to the layout of complex scenes we also find that increasing the density of tubes for the shape identification task improved participant performance under physically-based illumination but not under the traditional hardware-accelerated illumination model d shape perception dt-mri index terms&# flow_visualization global_illumination local_illumination multi-scale_visualization physically-based illumination streamtubes user_study volume_completion white_matter_tractography
the need to examine and manipulate large surface models is commonly found in many science engineering and medical applications on a desktop monitor however seeing the whole model in detail is not possible in this paper we present a new interactive focus+context method for visualizing large surface models our method based on an energy optimization model allows the user to magnify an area of interest to see it in detail while deforming the rest of the area without perceivable distortion the rest of the surface area is essentially shrunk to use as little of the screen space as possible in order to keep the entire model displayed on screen we demonstrate the efficacy and robustness of our method with a variety of models focus+context_visualization index terms&# bounding_space magnification
professional designers and artists are quite cognizant of the rules that guide the design of effective color palettes from both aesthetic and attention-guiding points of view in the field of visualization however the use of systematic rules embracing these aspects has received less attention the situation is further complicated by the fact that visualization often uses semi-transparencies to reveal occluded objects in which case the resulting color mixing effects add additional constraints to the choice of the color palette color_design forms a crucial part in visual aesthetics thus the consideration of these issues can be of great value in the emerging field of illustrative_visualization we describe a knowledge-based system that captures established color_design rules into a comprehensive interactive framework aimed to aid users in the selection of colors for scene objects and incorporating individual preferences importance functions and overall scene composition our framework also offers new knowledge and solutions for the mixing ordering and choice of colors in the rendering of semi-transparent layers and surfaces all design rules are evaluated via user studies for which we extend the method of conjoint_analysis to task-based testing scenarios our framework's use of principles rooted in color_design with application for the illustration of features in pre-classified data distinguishes it from existing systems which target the exploration of continuous-range density data via perceptual color maps color_design index terms&# conjoint_analysis illustrative_visualization transparency user_study_evaluation volume_rendering
we present an efficient and automatic image-recoloring technique for dichromats that highlights important visual details that would otherwise be unnoticed by these individuals while previous techniques approach this problem by potentially changing all colors of the original image causing their results to look unnatural to color vision deficients our approach preserves as much as possible the image's original colors our approach is about three orders of magnitude faster than previous ones the results of a paired-comparison evaluation carried out with fourteen color-vision deficients cvds indicated the preference of our technique over the state-of-the-art automatic recoloring technique for dichromats when considering information_visualization examples the subjects tend to prefer our results over the original images an extension of our technique that exaggerates color contrast tends to be preferred when cvds compared pairs of scientific_visualization images these results provide valuable information for guiding the design of visualizations for color-vision deficients color-contrast enhancement color-vision_deficiency index terms&# information_and_scientific_visualization recoloring_algorithms
many interesting and promising prototypes for visualizing video data have been proposed including those that combine videos with their spatial_context contextualized videos however relatively little work has investigated the fundamental design_factors behind these prototypes in order to provide general design guidance focusing on real-time video data visualization we evaluated two important design_factors - video_placement method and spatial_context presentation method - through a user_study in addition we evaluated the effect of spatial knowledge of the environment participantspsila performance was measured through path_reconstruction tasks where the participants followed a target through simulated surveillance videos and marked the target paths on the environment model we found that embedding videos inside the model enabled realtime strategies and led to faster performance with the help of contextualized videos participants not familiar with the real environment achieved similar task_performance to participants that worked in that environment we discuss design implications and provide general design recommendations for traffic and security surveillance system interfaces index terms&# contextualized videos design_factors path_reconstruction spatial_context tracking user_study video_placement
summary form only given as practitioners and educators in the field of visual_analytics science and technology youpsilave seen the power of visual_analytics for scientific and technical applications including homeland security but visual_analytics is spreading to the general business population solving unexpected problems and challenges in this talk tableau software ceo christian chabot will highlight the areas of opportunity for visual_analytics and demonstrate real examples of practical problems being solved by visual_analytics hepsilall share his vision for the future of this industry - how everyday people can and are using visual_analytics to solve some of businesspsilas and societypsilas most challenging issues hepsilall also identify whatpsilas needed to bring visual_analytics to the forefront of main-stream data_analysis and how the industry is helping to deliver on those needs
visual-interactive cluster analysis provides valuable tools for effectively analyzing large and complex data sets due to desirable properties and an inherent predisposition for visualization the kohonen feature map or self-organizing map or som algorithm is among the most popular and widely used visual_clustering techniques however the unsupervised nature of the algorithm may be disadvantageous in certain applications depending on initialization and data characteristics cluster maps cluster layouts may emerge that do not comply with user preferences expectations or the application context considering som-based analysis of trajectory data we propose a comprehensive visual-interactive monitoring and control framework extending the basic som algorithm the framework implements the general visual_analytics idea to effectively combine automatic data_analysis with human expert supervision it provides simple yet effective facilities for visually monitoring and interactively controlling the trajectory clustering process at arbitrary levels of detail the approach allows the user to leverage existing domain knowledge and user preferences arriving at improved cluster maps we apply the framework on a trajectory clustering problem demonstrating its potential in combining both unsupervised machine and supervised human expert processing in producing appropriate cluster results h information systems information systems applications i computing methodologies computer_graphics—methodology and techniques
uspex is a crystal structure predictor based on an evolutionary algorithm every uspex run produces hundreds or thousands of crystal structures some of which may be identical to ease the extraction of unique and potentially interesting structures we applied usual high-dimensional classification concepts to the unusual field of crystallography we experimented with various crystal structure descriptors distinct distance measures and tried different clustering methods to identify groups of similar structures these methods are already applied in combinatorial chemistry to organic molecules for a different goal and in somewhat different forms but are not widely used for crystal structures classification we adopted a visual_design and validation method in the development of a library crystalfp and an end-user application to select and validate method choices to gain userspsila acceptance and to tap into their domain expertise the use of the classifier has already accelerated the analysis of uspex output by at least one order of magnitude promoting some new crystallographic insight and discovery furthermore the visual display of key algorithm indicators has led to diverse unexpected discoveries that will improve the uspex algorithms i pattern_recognition design methodology—classifier design and evaluation j physical sciences and engineering chemistry
we describe a visual_analytics va infrastructure rooted on techniques in machine_learning and logic-based deductive reasoning that will assist analysts to make sense of large complex data sets by facilitating the generation and validation of models representing relationships in the data we use logic programming lp as the underlying computing machinery to encode the relations as rules and facts and compute with them a unique aspect of our approach is that the lp rules are automatically learned using inductive logic programming from examples of data that the analyst deems interesting when viewing the data in the high-dimensional visualization_interface using this system analysts will be able to construct models of arbitrary relationships in the data explore the data for scenarios that fit the model refine the model if necessary and query the model to automatically analyze incoming future data exhibiting the encoded relationships in other words it will support both model-driven data_exploration as well as data-driven model evolution more importantly by basing the construction of models on techniques from machine_learning and logic-based deduction the va process will be both flexible in terms of modeling arbitrary user-driven relationships in the data as well as readily scale across different data domains grand_tour h information interfaces and presentation user_interfaces—graphical user_interfaces high-dimensional_data i artificial intelligence learning—concept learning i pattern_recognition clustering—similarity measures knowledge_discovery machine_learning network_security visual_analytics visual_clustering
we present a novel collaborative visual_analytics application for cognitively overloaded users in the astrophysics domain the system was developed for scientists needing to analyze heterogeneous complex data under time pressure and then make predictions and time-critical decisions rapidly and correctly under a constant influx of changing data the sunfall data taking system utilizes several novel visualization and analysis techniques to enable a team of geographically distributed domain specialists to effectively and remotely maneuver a custom-built instrument under challenging operational conditions sunfall data taking has been in use for over eighteen months by a major international astrophysics collaboration the largest data volume supernova search currently in operation and has substantially improved the operational efficiency of its users we describe the system design process by an interdisciplinary team the system architecture and the results of an informal usability evaluation of the production system by domain experts in the context of endsleypsilas three levels of situation_awareness data_and_knowledge_visualization astrophysics scientific_visualization situation_awareness visual_analytics
when analyzing syndromic surveillance data health care officials look for areas with unusually high cases of syndromes unfortunately many outbreaks are difficult to detect because their signal is obscured by the statistical noise consequently many detection algorithms have a high false positive rate while many false alerts can be easily filtered by trained epidemiologists others require health officials to drill down into the data analyzing specific segments of the population and historical trends over time and space furthermore the ability to accurately recognize meaningful patterns in the data becomes more challenging as these data sources increase in volume and complexity to facilitate more accurate and efficient event_detection we have created a visual_analytics tool that provides analysts with linked geo-spatiotemporal and statistical analytic views we model syndromic hotspots by applying a kernel density estimation on the population sample when an analyst selects a syndromic hotspot temporal statistical graphs of the hotspot are created similarly regions in the statistical plots may be selected to generate geospatial features specific to the current time period demographic filtering can then be combined to determine if certain populations are more affected than others these tools allow analysts to perform real-time hypothesis_testing and evaluation
social network graphs concept_maps and process charts are examples of diagrammatic representations employed by intelligence analysts to understand complex systems unfortunately these 2d representations currently do not easily convey the flow sequence tempo and other important dynamic behaviors within these systems in this paper we present configurable spaces a novel analytical method for visualizing patterns of activity over time in complex diagrammatically- represented systems configurable spaces extends geotime's x y t coordinate workspace space for temporal analysis to any arbitrary diagrammatic work space by replacing a geographic map with a diagram this paper traces progress from concept to prototype and discusses how diagrams can be created transformed and leveraged for analysis including generating diagrams from knowledge bases visualizing temporal concept_maps and the use of linked diagrams for exploring complex multi-dimensional sequences of events an evaluation of the prototype by the national institute of standards and technology showed intelligence analysts believed they were able to attain an increased level of insight were able to explore data more efficiently and that configurable spaces would help them work faster concept_maps geo-temporal_analysis graph_visualization human information interaction visual_analytics
data about movements of various objects are collected in growing amounts by means of current tracking technologies traditional approaches to visualization and interactive_exploration of movement_data cannot cope with data of such sizes in this research paper we investigate the ways of using aggregation for visual_analysis of movement_data we define aggregation methods suitable for movement_data and find visualization and interaction techniques to represent results of aggregations and enable comprehensive exploration of the data we consider two possible views of movement traffic-oriented and trajectory-oriented each view requires different methods of analysis and of data_aggregation we illustrate our argument with example data resulting from tracking multiple cars in milan and example analysis tasks from the domain of city traffic management movement_data_aggregation geo visualization scalable_visualization spatio-temporal_data
visual analytic tools allow analysts to generate large collections of useful analytical results we anticipate that analysts in most real world situations will draw from these collections when working together to solve complicated problems this indicates a need to understand how users synthesize multiple collections of results this paper reports the results of collaborative synthesis experiments conducted with expert geographers and disease biologists ten participants were worked in pairs to complete a simulated real-world synthesis task using artifacts printed on cards on a large paper-covered workspace experiment results indicate that groups use a number of different approaches to collaborative synthesis and that they employ a variety of organizational metaphors to structure their information it is further evident that establishing common ground and role assignment are critical aspects of collaborative synthesis we conclude with a set of general design_guidelines for collaborative synthesis support tools h information interfaces and presentation group and organization interfaces synthesis collaboration user-centered_design visual_analytics
thanks to the web-related and other advanced technologies textual information is increasingly being stored in digital form and posted online automatic methods to analyze such textual information are becoming inevitable many of those methods are based on quantitative text features analysts face the challenge to choose the most appropriate features for their tasks this requires effective approaches for evaluation and feature-engineering i pattern_recognition design methodology—feature evaluation and selection i document and text_processing document capture—document_analysis
it has been widely accepted that interactive_visualization_techniques enable users to more effectively form hypotheses and identify areas for more detailed investigation there have been numerous empirical user studies testing the effectiveness of specific visual analytical tools however there has been limited effort in connecting a userpsilas interaction with his reasoning for the purpose of extracting the relationship between the two in this paper we present an approach for capturing and analyzing user_interactions in a financial visual analytical tool and describe an exploratory user_study that examines these interaction strategies to achieve this goal we created two visual tools to analyze raw interaction data captured during the user session the results of this study demonstrate one possible strategy for understanding the relationship between interaction and reasoning both operationally and strategically h information interfaces and presentation (eg, hci) user_interfaces—evaluation/methodology h information interfaces and presentation (eg, hci) user_interfaces—graphical user_interfaces (gui)
as the information being visualized and the process of understanding that information both become increasingly complex it is necessary to develop new visualization approaches that facilitate the flow of human reasoning in this paper we endeavor to push visualization design a step beyond current user_models by discussing a modeling framework of human ldquohigher cognitionrdquo based on this cognition model we present design_guidelines for the development of visual interfaces designed to maximize the complementary cognitive strengths of both human and computer some of these principles are already being reflected in the better visual_analytics designs while others have not yet been applied or fully applied but none of the guidelines have explained the deeper rationale that the model provides lastly we discuss and assess these visual_analytics guidelines through the evaluation of several visualization examples cognition and perception_theory embodied_cognition visual_analytics visualization_taxonomies_and_models
software tools that make it easier for analysts to collaborate as a natural part of their work will lead to better analysis that is informed by more perspectives we are interested to know if software tools can be designed that support collaboration even as they allow analysts to find documents and organize information including evidence schemas and hypotheses we have modified the entity workspace system described previously to test such designs we have evaluated the resulting design in both a laboratory study and a study where it is situated with an analysis team in both cases effects on collaboration appear to be positive key aspects of the design include an evidence notebook optimized for organizing entities rather than text characters information structures that can be collapsed and expanded visualization of evidence that emphasizes events and documents rather than emphasizing the entity graph and a notification system that finds entities of mutual interest to multiple analysts h information search and retrieval information filtering h information systems applications hm miscellaneous h user_interfaces graphical user_interfaces (gui) h group and organization interfaces collaborative computing, computer-supported_cooperative_work, web-based interaction argumentation_marshalling collaboration collective_intelligence entity-based exploratory_search information_foraging information_workspace intelligence_analysis semantic_notebook sensemaking visual_analytics visualization
this paper introduces the application of visual_analytics techniques as a novel approach for improving economic decision_making particularly we focus on two known problems where subjectspsila behavior consistently deviates from the optimal the winnerpsilas and loserpsilas curse according to economists subjects fail to recognize the profit-maximizing decision strategy in both the winnerpsilas and loserpsilas curse because they are unable to properly consider all the available information as such we have created a visual_analytics tool to aid subjects in decision_making under the acquiring a company framework common in many economic experiments we demonstrate the added value of visual_analytics in the decision_making process through a series of user studies comparing standard visualization methods with interactive visual_analytics techniques our work presents not only a basis for development and evaluation of economic visual analytic research but also empirical evidence demonstrating the added value of applying visual_analytics to general decision_making tasks
analyzing unstructured text streams can be challenging one popular approach is to isolate specific themes in the text and to visualize the connections between them some existing systems like themeriver provide a temporal view of changes in themes other systems like in-spire use clustering techniques to help an analyst identify the themes at a single point in time narratives combines both of these techniques it uses a temporal axis to visualize ways that concepts have changed over time and introduces several methods to explore how those concepts relate to each other narratives is designed to help the user place news stories in their historical and social context by understanding how the major topics associated with them have changed over time users can relate articles through time by examining the topical keywords that summarize a specific news event by tracking the attention to a news article in the form of references in social media such as weblogs a user discovers both important events and measures the social relevance of these stories i computer_graphics applications im document and text_processing miscellaneous blogs events time_series topic detection and tracking trends
insight_provenance - a historical record of the process and rationale by which an insight is derived - is an essential requirement in many visual_analytics applications while work in this area has relied on either manually recorded provenance eg user notes or automatically recorded event-based insight_provenance eg clicks drags and key-presses both approaches have fundamental limitations our aim is to develop a new approach that combines the benefits of both approaches while avoiding their deficiencies toward this goal we characterize userspsila visual analytic_activity at multiple levels of granularity moreover we identify a critical level of abstraction actions that can be used to represent visual analytic_activity with a set of general but semantically meaningful behavior types in turn the action types can be used as the semantic building blocks for insight_provenance we present a catalog of common actions identified through observations of several different visual analytic systems in addition we define a taxonomy to categorize actions into three major classes based on their semantic intent the concept of actions has been integrated into our labpsilas prototype visual analytic system harvest as the basis for its insight_provenance capabilities analytic_activity h information systems information interfaces and presentation—general information_visualization insight_provenance taxonomy visual_analytics
a central challenge in visual_analytics is the creation of accessible widely distributable analysis applications that bring the benefits of visual discovery to as broad a user base as possible moreover to support the role of visualization in the knowledge creation process it is advantageous to allow users to describe the reasoning strategies they employ while interacting with analytic environments we introduce an application suite called the scalable reasoning system srs which provides web-based and mobile interfaces for visual_analysis the service-oriented analytic framework that underlies srs provides a platform for deploying pervasive visual analytic environments across an enterprise srs represents a ldquolightweightrdquo approach to visual_analytics whereby thin client analytic applications can be rapidly deployed in a platform-agnostic fashion client applications support multiple_coordinated_views while giving analysts the ability to record evidence assumptions hypotheses and other reasoning artifacts we describe the capabilities of srs in the context of a real-world deployment at a regional law_enforcement organization c computer systems organization computer-communication networks —distributed_systems h information systems information interfaces and presentation—group and organization interfaces web_visualization analytic_reasoning law_enforcement mobile_visualization multiple_views
we seek an information-revealing representation for high-dimensional_data distributions that may contain local trends in certain subspaces examples are data that have continuous support in simple shapes with identifiable branches such data can be represented by a graph that consists of segments of locally fit principal curves or surfaces summarizing each identifiable branch we describe a new algorithm to find the optimal paths through such a principal graph the paths are optimal in the sense that they represent the longest smooth trends through the data set and jointly they cover the data set entirely with minimum overlap the algorithm is suitable for hypothesizing trends in high-dimensional_data and can assist exploratory_data_analysis and visualization g mathematics of computing mathematical software—algorithm design and analysis i computing methodologies pattern_recognition—clustering
understanding multivariate relationships is an important task in multivariate data_analysis unfortunately existing multivariate visualization_systems lose effectiveness when analyzing relationships among variables that span more than a few dimensions we present a novel multivariate visual explanation approach that helps users interactively discover multivariate relationships among a large number of dimensions by integrating automatic numerical differentiation techniques and multidimensional visualization_techniques the result is an efficient workflow for multivariate analysis model construction interactive dimension reduction and multivariate knowledge_discovery leveraging both automatic multivariate analysis and interactive multivariate data visual_exploration case studies and a formal user_study with a real dataset illustrate the effectiveness of this approach g mathematics of computing probability and statistics—multivariate statistics h information interfaces and presentation user_interfaces—user centered design dimension reduction multivariate analysis multivariate model construction multivariate visualization visual_analysis
with advances in computing techniques a large amount of high-resolution high-quality multimedia_data video and audio etc has been collected in research laboratories in various scientific disciplines particularly in social and behavioral studies how to automatically and effectively discover new knowledge from rich multimedia_data poses a compelling challenge since state-of-the-art data_mining techniques can most often only search and extract pre-defined patterns or knowledge from complex heterogeneous data in light of this our approach is to take advantages of both the power of human perception system and the power of computational algorithms more specifically we propose an approach that allows scientists to use data_mining as a first pass and then forms a closed loop of visual_analysis of current results followed by more data_mining work inspired by visualization the results of which can be in turn visualized and lead to the next round of visual_exploration and analysis in this way new insights and hypotheses gleaned from the raw data and the current level of analysis can contribute to further analysis as a first step toward this goal we implement a visualization system with three critical components 1 a smooth interface between visualization and data_mining the new analysis results can be automatically loaded into our visualization tool 2 a flexible tool to explore and query temporal_data derived from raw multimedia_data we represent temporal_data into two forms - continuous variables and event variables we have developed various ways to visualize both temporal correlations and statistics of multiple variables with the same type and conditional and high-order statistics between continuous and event variables 3 a seamless interface between raw multimedia_data and derived data our visualization tool allows users to explore compare and analyze multi-stream derived variables and simultaneously switch to access raw multimedia_data we de- - monstrate various functions in our visualization program using a set of multimedia_data including video audio and motion tracking data h information systems models and principles - user/machine_systems h database management database applications —data_mining h information storage and retrieval information search and retrieval h information interface and presentation multimedia information systems h information interface and presentation user_interfaces — graphical user_interfaces (gui) multimedia_data visual_data_mining
analysis of multidimensional data often requires careful examination of relationships across dimensions coordinated multiple view approaches have become commonplace in visual_analysis tools because they directly support expression of complex multidimensional queries using simple interactions however generating such tools remains difficult because of the need to map domain-specific data structures and semantics into the idiosyncratic combinations of interdependent data and visual_abstractions needed to reveal particular patterns and distributions in cross-dimensional relationships this paper describes 1 a method for interactively expressing sequences of multidimensional set queries by cross-filtering data values across pairs of views and 2 design strategies for constructing coordinated multiple view interfaces for cross-filtered visual_analysis of multidimensional data sets using examples of cross-filtered visualizations of data from several different domains we describe how cross-filtering can be modularized and reused across designs flexibly customized with respect to data types across multiple dimensions and incorporated into more wide-ranging multiple view designs the demonstrated analytic utility of these examples suggest that cross-filtering is a suitable design pattern for instantiation in a wide variety of visual_analysis tools d software_engineering design tools and techniques—user_interfaces h information systems database management—languages h information systems information interfaces and presentation—user_interfaces
adaptive search systems apply user_models to provide better separation of relevant and non-relevant documents in a list of results this paper presents our attempt to leverage this ability of user_models in the context of visual information analysis we developed an adaptive visualization approach for presentation and exploration of search results we simulated a visual intelligence search/analysis scenario with log data extracted from an adaptive information_foraging study and were able to verify that our method can improve the ability of traditional relevance visualization to separate relevant and irrelevant information h content analysis and indexing indexing method h information search and retrieval information filtering h online information services web-based services h user_interfaces graphical user_interfaces (gui) personalized search query relevance feedback user_model vibe visualization
many real-world visual_analytics applications involve time-oriented data i am working in a research project related to this challenge where i am responsible for the interactive_visualization part my goal are interactive_visualizations to explore such time-oriented data according to the user tasks while considering the structure of time time is composed of many granularities that are likely to have crucial influence on the formation of the data the challenge is to integrate the granularities into a detailed compound view on the data like the compound eye of insects integrates many images into one view other members of our team are experts in temporal_data_mining and user centered design the goal is to combine our research topics to an integrated system that helps domain experts to get more insight from their time-oriented data hm information systems information interfaces and presentation (eg, hci)—miscellaneous i computing methodologies computer_graphics—methodology and techniques
from instant messaging and email to wikis and blogs millions of individuals are generating content that reflects their relationships with others in the world both online and offline sincecommunication artifacts are recordings of life events we can gain insights into the social attributes and structures of the people within thiscommunication history in this paper we describe socialrank an ego- and time-centric workflow for identifying social relationships in an email corpus this workflow includes four high-level tasks discovery validation annotation and dissemination socialrank combines relationship ranking algorithms with timeline social network diagram and multidimensional scaling visualization_techniques to support these tasks h information systemscommunications applications—information browsers h information systems information interfaces and presentation—user_interfaces i computing methodologies methodology and techniques—interaction techniques
mutual funds are one of the most important investment instruments available however choosing among mutual funds is not an easy task because they vary in many different dimensions such as asset size turnover and fee structure and these characteristics may affect fund returns it is thus important to understand the relation between fund performance and these properties in this work we use a new visual analytical tool the density-based distribution map to assist in this task by visualizing various important fund characteristics from a real-world database of the us stock funds our new visual representations greatly help understand the relation between fund characteristics and returns decision_making distribution map financial_data_visualization h user/machine_systems human information processing—visualization h information systems applications types of systems—decision support mutual funds regression analysis
internet has become one of the bestcommunication and marketing tools hence designing well-structured web sites with the information or products that users look for is a crucial mission for this reason understanding web data is a decisive task to assure the success of a website in that sense data_mining techniques provide many metrics and statistics useful to automatically discover the structure contents and usage of a site this research aims at proving the usefulness of a set of information visualisation techniques in order to analyse web data using a visual web mining tool that allows the combination coordination and exploration of visualisations to get insight on web data the tool named wet provides a set of visual metaphors that represent the structure of the websites where web metrics are overlaid h information systems applicationscommunications applications—information browsers h information interfaces and presentation(i) user_interfaces—graphical user_interfaces (gui), interaction styles h information interfaces and presentation hypertext/hypermedia—navigation, user issues
while exploring data using information_visualization analysts try to make sense of the data build cases and present them to others however if the exploration is long or done in multiple sessions it can be hard for analysts to remember all interesting visualizations and the relationships among them they have seen often they will see the same or similar visualizations and are unable to recall when why and how they have seen something similar recalling and retrieving interesting visualizations are important tasks for the analysis processes such as problem_solving reasoning and conceptualization in this paper we argue that offering support for thinking based on past analysis processes is important and present a solution for this h information interfaces and presentation user_interfaces—graphical user_interfaces (gui)
we present a process for the exploration and analysis of large_databases of events a typical database is characterized by the sequential actions of a number of individual entities these entities can be compared by their similarities in sequence and changes in sequence over time the correlation of two sequences can provide important clues as to the possibility of a connection between the responsible entities but an analyst might not be able to specify the type of connection sought prior to examination our process incorporates extensive automated calculation and data_mining but permits diversity of analysis by providing visualization of results at multiple levels taking advantage of human intuition and visual processing to generate avenues of inquiry
due to the complexity of the patent domain and the huge amount of data advanced interactive visual techniques are needed to support the analysis of large patent collections and portfolios in this paper we present a new approach for visualizing the classificatory distribution of patent collections among the international patent classification ipc - todaypsilas most important internationally agreed patent classification system with about 70000 categories our approach is based on an interactive three-dimensional treemap overlaid with adjacency edge bundles d edge-bundling d-treemaps patent co-classification analysis
seismic simulations use finite_element_methods to describe ground motion the results of such numerical simulations are often difficult to interpret for decision makers we describe a terrain_rendering engine that uses photorealistic metaphors to represent typical terrain properties without representing an actual terrain in the context of ground motion a simulation of the effects of various types of earthquakes on buildings has been conducted usually such structural response simulations are carried out independently and are being visualized separate from the ground motion simulation we combine the results from both simulations in an interactive hybrid_visualization so that decision makers first responders and emergency management agencies are provided with a photo-realistic simulated view of various earthquake scenarios enabling them to study the effect of various earthquakes on buildings typical for a rural or urban area we present a method for visually analyzing large-scale simulation data from different sources ground motion simulation and structural response simulation using photorealistic metaphors we have implemented an intuitive interactive_system for visual_analysis and inspection of possible effects of various types of earthquakes on an inventory of buildings typical for a particular area the underlying rendering system can be easily adapted for other simulations such as smoke plumes or biohazards i computing methodologies computer_graphics—picture/image generation i computing methodologies computer_graphics—three-dimensional graphics and realism
visual_analytics experts realize that one effective way to push the field forward and to develop metrics for measuring the performance of various visual_analytics components is to hold an annual competition the vast 2008 challenge is the third year that such a competition was held in conjunction with the ieee visual_analytics science and technology vast symposium the authors restructured the contest format used in 2006 and 2007 to reduce the barriers to participation and offered four mini-challenges and a grand challenge mini challenge participants were to use visual analytic tools to explore one of four heterogeneous data collections to analyze specific activities of a fictitious controversial movement questions asked in the grand challenge required the participants to synthesize data from all four data sets in this paper we give a brief overview of the data sets the tasks the participation the judging and the results h information interfaces & presentations user_interfaces — evaluation/methodology contest evaluation human information interaction metrics sense making visual_analytics
the vast 2008 challenge consisted of four heterogeneous synthetic_data sets each organized into separate mini-challenges the grand challenge required integrating the raw data from these four data sets as well as integrating results and findings from team members working on specific mini-challenges modeling the problem with a semantic_network provided a means for integrating both the raw data and the subjective findings e data data structures—graphs and networks h information systems information interfaces and presentation—group and organization interfaces visual_analytics data_integration geo visualization information_visualization intelligence_analysis investigative_analysis
geotime and nspace2 are interactive visual_analytics tools that were used to examine and interpret all four of the 2008 vast challenge datasets geotime excels in visualizing event patterns in time and space or in time and any abstract landscape while nspace2 is a web-based analytical tool designed to support every step of the analytical process nspace2 is an integrating analytic environment this paper highlights the vast analytical experience with these tools that contributed to the success of these tools and this team for the third consecutive year geo-spatial information systems human information interaction temporal analysis visual_analytics
palantir is a world-class analytic platform used worldwide by governmental and financial analysts this paper provides an introduction to the platform contextualized by its application to the 2008 ieee vast contest in this challenge we explored a notional dataset about a fabricated religious movement catalanopsilas paraiso manifesto movement h information systems information interfaces and presentation—user_interfaces palantir vast  collaboration data visualization visual_analytics
we provide a description of the tools and techniques used in our analysis of the vast 2008 challenge dealing with mass movement of persons departing isla del sueno on boats for the united states during 2005-2007 we used visual_analytics to explore migration patterns characterize the choice and evolution of landing sites characterize the geographical patterns of interdictions and determine the successful landing rate our comvis tool in connection with some helper applications and google earth allowed us to explore geo-temporal characteristics of the data set and answer the challenge questions the comvis project file captures the visual_analysis context and facilitates better collaboration among team members i computer_graphics general— i computer_graphics methodology and techniques—(interaction techniques) j social and behavioral sciences sociology— visual_analytics geo-temporal_data
the geospatial visual_analytics toolkit intended for exploratory_analysis of spatial and spatio-temporal_data has been recently enriched with specific visual and computational techniques supporting analysis of data about movement we applied these and other techniques to the data and tasks of mini challenge 4 where it was necessary to analyze tracks of moving peoplecr categories and subject descriptors h12 [user/machine_systems] human information processing - visual_analytics 169 [visualization] information_visualization movement_data_aggregation geo visualization scalable_visualization spatio-temporal_data
in the mobile call mini challenge of vast 2008 contest we explored the temporalcommunication patterns of catalano/vidro social network which is reflected in the mobile call data we focus on detecting the hierarchy of the social network and try to get the important actors in it we present our tools and methods in this summary by using the visual analytic approaches we can find out not only the temporalcommunication patterns in the social network but also the hierarchy of it d software_engineering design tools and techniques—user_interfaces h database management database applications—data_mining
the adoption of visual_analytics methodologies in security applications is an approach that could lead to interesting results usually the data that has to be analyzed finds in a graphical representation its preferred nature such as spatial or temporal relationships due to the nature of these applications it is very important that key-details are made easy to identify in the context of the vast 2008 challenge we developed a visualization tool that graphically displays the movement of 82 employees of the miami department of health usa we also asked 13 users to identify potential suspects and observe what happened during an evacuation of the building caused by an explosion in this paper we explain the results of the user testing we conducted and how the users interpreted the event taken into account h models and principles user/machine_systems—human information processing h models and principles user/machine_systems—software_psychology casualties detection evacuation user testing visual_analytics
mobivis is a visual_analytics tools to aid in the process of processing and understanding complex relational_data such as social_networks at the core of these tools is the ability to filter complex networks structurally and semantically which helps us discover clusters and patterns in the organization of social_networks semantic filtering is obtained via an ontology graph based on another visual_analytics tool called ontovis in this summary we describe how these tools where used to analyze one of the mini-challenges of the 2008 vast challenge heterogeneous graph_visualization i methodology and techniques—interaction techniques visual_analytics
i describe how socialaction was used to find insights in an evolving social structure vast challenge 2008psilas mini-challenge 3 this analysis and socialaction were given the award ldquocell phone mini challenge award time visualizations of cell phone activityrdquo socialaction vast challenge
this article describes the visualization tool developed for analysing a dynamic social network of phone calls for the vast 2008 mini challenge the tool was designed to highlight temporal changes in the network by animating different network visual representations we also explain how animating these network representations helped to identify key events in the mini challenge problem scenario finally we make some suggestions for future research and development in the area dynamic social_networks graph_visualization h information interfaces and presentation user_interfaces—graphical user_interfaces (gui) i methodology and techniques interaction techniques information_visualization visual_analytics
the spadac team used various visual_analytics tools and methods to find geo-temporal patterns of migration from a caribbean island from 2005-2007 in this paper we describe the tools and methods used in the analysis these methods included generating temporal variograms dendrograms and proportionally weighted migration maps using tools such as the r statistical software package and signature analysttrade we found that there is a significant positive space-time correlation with the boat encounters especially the landings with a migratory shift further away from the point of departure over time g numerical analysis interpolation—interpolation-formulas g numerical analysis approximationnonlinear approximation h information storage and retrieval information search and retrieval—clustering i artificial intelligence applications and expert systems—cartography geo-temporal_analysis visual_analytics
staining is a technique for categorizing time-varying spatial_data that is data of things moving through space over time in staining a stain is applied in either time or space and the objects which move through the stain become marked this technique and a research prototype demonstrating the technique were developed in response to the vast 2008 contest mini-challenge evacuation traces h user_interfaces graphical user_interfaces movement_data exploratory_data_analysis interactive_displays visual_analytics visualization
the prajna project is a java toolkit designed to provide various capabilities for visualization knowledge_representation geographic displays semantic reasoning and data fusion rather than attempt to recreate the significant capabilities provided in other tools prajna instead provides software bridges to incorporate other toolkits where appropriate this challenge required the development of a custom application for visual_analysis by applying the utilities within the prajna project i developed a robust and diverse set of capabilities to solve the analytical challenge d software_engineering software architectures - domain-specific architectures information_visualization knowledge_representation software toolkit toolkit integration computer_graphics methodology and techniques - interaction techniques
one bottleneck in large-scale genome sequencing projects is reconstructing the full genome sequence from the short subsequences produced by current technologies the final stages of the genome_assembly process inevitably require manual inspection of data inconsistencies and could be greatly aided by visualization this paper presents our design decisions in translating key data features identified through discussions with analysts into a concise visual_encoding current visualization tools in this domain focus on local sequence errors making high-level inspection of the assembly difficult if not impossible we present a novel interactive graph display abyss-explorer that emphasizes the global assembly structure while also integrating salient data features such as sequence length our tool replaces manual and in some cases pen-and-paper based analysis tasks and we discuss how user feedback was incorporated into iterative design refinements finally we touch on applications of this representation not initially considered in our design phase suggesting the generality of this encoding for dna_sequence data bioinformatics_visualization dna_sequence design_study genome_assembly
a dendrogram that visualizes a clustering hierarchy is often integrated with a re-orderable matrix for pattern identification the method is widely used in many research fields including biology geography statistics and data_mining however most dendrograms do not scale up well particularly with respect to problems of graphical and cognitive information overload this research proposes a strategy that links an overview dendrogram and a detail-view dendrogram each integrated with a re-orderable matrix the overview displays only a user-controlled limited number of nodes that represent the ldquoskeletonrdquo of a hierarchy the detail view displays the sub-tree represented by a selected meta-node in the overview the research presented here focuses on constructing a concise overview dendrogram and its coordination with a detail view the proposed method has the following benefits dramatic alleviation of information overload enhanced scalability and data abstraction quality on the dendrogram and the support of data_exploration at arbitrary levels of detail the contribution of the paper includes a new metric to measure the ldquoimportancerdquo of nodes in a dendrogram the method to construct the concise overview dendrogram from the dynamically-identified important nodes and measure for evaluating the data abstraction quality for dendrograms we evaluate and compare the proposed method to some related existing methods and demonstrating how the proposed method can help users find interesting patterns through a case_study on county-level us cervical cancer mortality and demographic data dendrogram compound graphs data_abstraction_quality_metrics hierarchical_clusters reorderable_matrix
in the field of comparative genomics scientists seek to answer questions about evolution and genomic function by comparing the genomes of species to find regions of shared sequences conserve dsyntenic blocks are an important biological_data abstraction for indicating regions of shared sequences the goal of this work is to show multiple types of relationships at multiple scales in a way that is visually comprehensible in accordance with known perceptual principles we present a task analysis for this domain where the fundamental questions asked by biologists can be understood by a characterization of relationships into the four types of proximity/location size orientation and similarity/strength and the four scales of genome chromosome block and genomic feature we also propose a new taxonomy of the design space for visually encoding conservation data we present mizbee a multiscale synteny browser with the unique property of providing interactive side-by-side views of the data across the range of scales supporting exploration of all of these relationship types we conclude with case studies from two biologists who used mizbee to augment their previous automatic analysis work flow providing anecdotal evidence about the efficacy of the system for the visualization of syntenic data the analysis of conservation relationships and thecommunication of scientific insights information_visualization bioinformatics design_study synteny
a widespread use of high-throughput gene expression analysis techniques enabled the biomedical research community to share a huge body of gene expression datasets in many public databases on the web however current gene expression data repositories provide static representations of the data and support limited interactions this hinders biologists from effectively exploring shared gene expression datasets responding to the growing need for better interfaces to improve the utility of the public datasets we have designed and developed a new web-based visual interface entitled geneshelf http//bioinformaticscnmcresearchorg/geneshelf it builds upon a zoomable_grid display to represent two categorical dimensions it also incorporates an augmented_timeline with expandable time points that better shows multiple data values for the focused time point by embedding bar_charts we applied geneshelf to one of the largest microarray_datasets generated to study the progression and recovery process of injuries at the spinal cord of mice and rats we present a case_study and a preliminary qualitative user_study with biologists to show the utility and usability of geneshelf animation augmented_timeline bioinformatics_visualization gene_expression_profiling zoomable_grid
spatiotemporal analysis of sensor logs is a challenging research field due to three facts a traditional two-dimensional maps do not support multiple events to occur at the same spatial location b three-dimensional solutions introduce ambiguity and are hard to navigate and c map distortions to solve the overlap problem are unfamiliar to most users this paper introduces a novel approach to represent spatial_data changing over time by plotting a number of non-overlapping pixels close to the sensor positions in a map thereby we encode the amount of time that a subject spent at a particular sensor to the number of plotted pixels color is used in a twofold manner while distinct colors distinguish between sensor nodes in different regions the colors' intensity is used as an indicator to the temporal property of the subjects' activity the resulting visualization_technique called growth ring maps enables users to find similarities and extract patterns of interest in spatiotemporal_data by using humans' perceptual abilities we demonstrate the newly introduced technique on a dataset that shows the behavior of healthy and alzheimer transgenic male and female mice we motivate the new technique by showing that the temporal analysis based on hierarchical_clustering and the spatial analysis based on transition matrices only reveal limited results results and findings are cross-validated using multidimensional scaling while the focus of this paper is to apply our visualization for monitoring animal_behavior the technique is also applicable for analyzing data such as packet tracing geographic monitoring of sales development or mobile phone capacity planning animal_behavior dense_pixel_displays spatiotemporal_visualization visual_analytics
we present a nested model for the visualization design and validation with four layers characterize the task and data in the vocabulary of the problem domain abstract into operations and data types design visual_encoding and interaction techniques and create algorithms to execute techniques efficiently the output from a level above is input to the level below bringing attention to the design challenge that an upstream error inevitably cascades to all downstream levels this model provides prescriptive guidance for determining appropriate evaluation approaches by identifying threats to validity unique to each level we also provide three recommendations motivated by this model authors should distinguish between these levels when claiming contributions at more than one of them authors should explicitly state upstream assumptions at levels above the focus of a paper and visualization venues should accept more papers on domain characterization models design evaluation frameworks
visual_exploration of multidimensional data is a process of isolating and extracting relationships within and between dimensions coordinated multiple view approaches are particularly effective for visual_exploration because they support precise expression of heterogeneous multidimensional queries using simple interactions recent visual_analytics research has made significant progress in identifying and understanding patterns of composed views and coordinations that support fast flexible and open-ended data_exploration what is missing is formalization of the space of expressible queries in terms of visual representation and interaction this paper introduces the conjunctive visual form model in which visual_exploration consists of interactively-driven sequences of transitions between visual states that correspond to conjunctive_normal_forms in boolean logic the model predicts several new and useful ways to extend the space of rapidly expressible queries through addition of simple interactive capabilities to existing compositional patterns two recent related visual tools offer a subset of these capabilities providing a basis for conjecturing about such extensions boolean_query brushing conjunctive_normal_form exploratory_visualization multiple_views visual_abstraction
we present a novel and extensible set of interaction techniques for manipulating visualizations of networks by selecting subgraphs and then applying various commands to modify their layout or graphical properties our techniques integrate traditional rectangle and lasso selection and also support selecting a node's neighbourhood by dragging out its radius in edges using a novel kind of radial menu commands for translation rotation scaling or modifying graphical properties such as opacity and layout patterns can be performed by using a hotbox a transiently popped-up semi-transparent set of widgets that has been extended in novel ways to integrate specification of commands with 1d or 2d arguments our techniques require only one mouse button and one keyboard key and are designed for fast gestural in-place interaction we present the design and integration of these interaction techniques and illustrate their use in interactive_graph_visualization our techniques are implemented in navigator a software package for visualizing and analyzing biological_networks an initial usability study is also reported biological_networks hotbox interactive_graph_drawing marking_menus network_layout radial_menus
the identification of significant sequences in large and complex event-based temporal_data is a challenging problem with applications in many areas of today's information intensive society pure visual representations can be used for the analysis but are constrained to small data sets algorithmic search mechanisms used for larger data sets become expensive as the data size increases and typically focus on frequency of occurrence to reduce the computational complexity often overlooking important infrequent sequences andoutliers in this paper we introduce an interactive visual_data_mining approach based on an adaptation of techniques developed for web searching combined with an intuitive visual interface to facilitate user-centred exploration of the data and identification of sequences significant to that user the search algorithm used in the exploration executes in negligible time even for large_data and so no pre-processing of the selected data is required making this a completely interactive experience for the user our particular application area is social science diary data but the technique is applicable across many other disciplines event-based_data graph_similarity interactive_visual_exploration node_similarity sequence_identification
a common goal in graph_visualization research is the design of novel techniques for displaying an overview of an entire graph however there are many situations where such an overview is not relevant or practical for users as analyzing the global structure may not be related to the main task of the users that have semi-specific information needs furthermore users accessing large graph databases through an online connection or users running on less powerful mobile hardware simply do not have the resources needed to compute these overviews in this paper we advocate an interaction_model that allows users to remotely browse the immediate context graph around a specific node of interest we show how furnas' original degree_of_interest function can be adapted from trees to graphs and how we can use this metric to extract useful contextual subgraphs control the complexity of the generated visualization and direct users to interesting datapoints in the context we demonstrate the effectiveness of our approach with an exploration of a dense online_database containing over 3 million legal citations graph_visualization degree_of_interest focus+context legal_citation_networks network_visualization
the research presented in this paper compares user-generated and automatic graph_layouts following the methods suggested by van ham et al 2008 a group of users generated graph_layouts using both multi-touch_interaction on a tabletop display and mouse interaction on a desktop computer users were asked to optimize their layout for aesthetics and analytical tasks with a social network we discuss characteristics of the user-generated_layouts and interaction methods employed by users in this process we then report on a web-based study to compare these layouts with the output of popular automatic_layout_algorithms our results demonstrate that the best of the user-generated_layouts performed as well as or better than the physics-based layoutorthogonal and circular automatic layouts were found to be considerably less effective than either the physics-based layout or the best of the user-generated_layouts we highlight several attributes of the various layouts that led to highaccuracy and improved task completion time as well as aspects in which traditional automatic layout methods were unsuccessful for our tasks graph_layout automatic_layout_algorithms graph-drawing aesthetics network_layout user-generated_layout
in this paper we present a new visual way of exploring state sequences in large observational time-series a key advantage of our method is that it can directly visualize higher-order state_transitions a standard first order state transition is a sequence of two states that are linked by a transition a higher-order state transition is a sequence of three or more states where the sequence of participating states are linked together by consecutive first order state_transitions our method extends the current state-graph exploration methods by employing a two dimensional graph in which higher-order state_transitions are visualized as curved lines all transitions are bundled into thick splines so that the thickness of an edge represents the frequency of instances the bundling between two states takes into account the state_transitions before and after the transition this is done in such a way that it forms a continuous representation in which any subsequence of the timeseries is represented by a continuous smooth line the edge bundles in these graphs can be explored interactively through our incremental selection algorithm we demonstrate our method with an application in exploring labeled time-series_data from a biological survey where a clustering has assigned a single label to the data at each time-point in these sequences a large number of cyclic patterns occur which in turn are linked to specific activities we demonstrate how our method helps to find these cycles and how the interactive selection process helps to find and investigate activities biological_data graph_drawing state_transitions time_series
we explore the effects of selecting alternative layouts in hierarchical displays that show multiple aspects of large multivariate datasets including spatial and temporal characteristics hierarchical displays of this type condition a dataset by multiple discrete variable values creating nested graphical summaries of the resulting subsets in which size shape and colour can be used to show subset properties these 'small_multiples' are ordered by the conditioning variable values and are laid out hierarchically using dimensional_stacking crucially we consider the use of different layouts at different hierarchical levels so that the coordinates of the plane can be used more effectively to draw attention to trends and anomalies in the data we argue that these layouts should be informed by the type of conditioning variable and by the research question being explored we focus on space-filling rectangular layouts that provide data-dense and rich overviews of data to address research questions posed in our exploratory_analysis of spatial and temporal aspects of property sales in london we develop a notation 'hive' that describes visualisation and layout states and provides reconfiguration operators demonstrate its use for reconfiguring layouts to pursue research questions and provide guidelines for this process we demonstrate how layouts can be related through animated transitions to reduce the cognitive load associated with their reconfiguration whilst supporting the exploratory_process geo visualization exploratory guidelines hierarchical layout notation
social_photos which are taken during family events or parties represent individuals or groups of people we show in this paper how a hasse_diagram is an efficient visualization strategy for eliciting different groups and navigating through them however we do not limit this strategy to these traditional uses instead we show how it can also be used for assisting in indexing new photos indexing consists of identifying the event and people in photos it is an integral phase that takes place before searching and sharing in our method we use existing indexed photos to index new photos this is performed through a manual drag and drop procedure followed by a content fusion process that we call 'propagation' at the core of this process is the necessity to organize and visualize the photos that will be used for indexing in a manner that is easily recognizable and accessible by the user in this respect we make use of an object galois_sub-hierarchy and display it using a hasse_diagram the need for an incremental display that maintains the user's mental_map also leads us to propose a novel way of building the hasse_diagram to validate the approach we present some tests conducted with a sample of users that confirm the interest of this organization visualization and indexation approach finally we conclude by considering scalability the possibility to extract social_networks and automatically create personalised albums galois_sub-hierarchy hasse_diagram information_visualization formal_concept_analysis indexation social_photos
multivariate data sets including hundreds of variables are increasingly common in many application areas most multivariate visualization_techniques are unable to display such data effectively and a common approach is to employ dimensionality_reduction prior to visualization most existing dimensionality_reduction systems focus on preserving one or a few significant structures in data for many analysis tasks however several types of structures can be of high significance and the importance of a certain structure compared to the importance of another is often task-dependent this paper introduces a system for dimensionality_reduction by combining user-defined quality_metrics using weight functions to preserve as many important structures as possible the system aims at effective visualization and exploration of structures within large multivariate data sets and provides enhancement of diverse structures by supplying a range of automatic variable_orderings furthermore it enables a quality-guided reduction of variables through an interactive_display facilitating investigation of trade-offs between loss of structure and the number of variables to keep the generality and interactivity of the system is demonstrated through a case scenario information_visualization multidimensional scaling parallel_coordinates scatterplots
in this paper we present a novel parallel_coordinates design integrated with points scattering points in parallel_coordinates sppc by taking advantage of both parallel_coordinates and scatterplots different from most multiple_views visualization_frameworks involving parallel_coordinates where each visualization type occupies an individual window we convert two selected neighboring coordinate axes into a scatterplot directly multidimensional scaling is adopted to allow converting multiple axes into a single subplot the transition between two visual types is designed in a seamless way in our work a series of interaction tools has been developed uniform brushing functionality is implemented to allow the user to perform data selection on both points and parallel coordinate polylines without explicitly switching tools a gpu accelerated dimensional incremental multidimensional scaling dimds has been developed to significantly improve the system performance our case_study shows that our scheme is more efficient than traditional multi-view methods in performing visual_analysis tasks dimensionality_reduction interactivity quality_metrics variable_ordering
while many data sets contain multiple relationships depicting more than one data relationship within a single visualization is challenging we introduce bubble sets as a visualization_technique for data that has both a primary data relation with a semantically significant spatial organization and a significant set membership relation in which members of the same set are not necessarily adjacent in the primary layout in order to maintain the spatial rights of the primary data relation we avoid layout adjustment techniques that improve set cluster continuity and density instead we use a continuous possibly concave isocontour to delineate set membership without disrupting the primary layout optimizations minimize cluster overlap and provide for calculation of the isocontours at interactive speeds case studies show how this technique can be used to indicate multiple sets on a variety of common visualizations clustering graph_visualization spatial_layout tree_visualization
when displaying thousands of aircraft trajectories on a screen the visualization is spoiled by a tangle of trails the visual_analysis is therefore difficult especially if a specific class of trajectories in an erroneous dataset has to be studied we designed fromdady a trajectory visualization tool that tackles the difficulties of exploring the visualization of multiple trails this multidimensional data_exploration is based on scatterplots brushing pick and drop juxtaposed views and rapid visual_design users can organize the workspace composed of multiple juxtaposed views they can define the visual configuration of the views by connecting data dimensions from the dataset to bertin's visual_variables they can then brush trajectories and with a pick and drop operation they can spread the brushed information across views they can then repeat these interactions until they extract a set of relevant data thus formulating complex queries through two real-world scenarios we show how fromdady supports iterative queries and the extraction of trajectories in a dataset that contains up to 5 million data direct_manipulation iterative_exploration trajectories visualization
we present a case_study of our experience designing selltrend a visualization system for analyzing airline travel purchase requests the relevant transaction data can be characterized as multi-variate temporal and categorical event sequences and the chief problem addressed is how to help company analysts identify complex combinations of transaction attributes that contribute to failed purchase requests selltrend combines a diverse set of techniques ranging from time_series visualization to faceted_browsing and historical trend analysis in order to help analysts make sense of the data we believe that the combination of views and interaction capabilities in selltrend provides an innovative approach to this problem and to other similar types of multivariate temporally driven transaction data_analysis initial feedback from company analysts confirms the utility and benefits of the system categorical_data information_visualization investigative_analysis multiple_attributes multiple_views time_series_data transaction_analysis
spatialization displays use a geographic metaphor to arrange non-spatial_data for example spatializations are commonly applied to document collections so that document themes appear as geographic features such as hills many common spatialization interfaces use a 3-d landscape metaphor to present data however it is not clear whether 3-d spatializations afford improved speed andaccuracy for user tasks compared to similar 2-d spatializations we describe a user_study comparing users' ability to remember dot displays 2-d landscapes and 3-d landscapes for two different data densities 500 vs 1000 points participants' visual memory was statistically more accurate when viewing dot displays and 3-d landscapes compared to 2-d landscapes furthermoreaccuracy remembering a spatialization was significantly better overall for denser spatializations theseresults are of benefit to visualization designers who are contemplating the best ways to present data using spatialization techniques information interfaces and presentation evaluation / methodology landscape_visualization screen_design software_psychology user / machine systems
spatial_interactions or flows such as population migration and disease spread naturally form a weighted location-to-location network graph such geographically embedded networks graphs are usually very large for example the county-to-county migration data in the us has thousands of counties and about a million migration paths moreover many variables are associated with each flow such as the number of migrants for different age groups income levels and occupations it is a challenging task to visualize such data and discover network structures multivariate relations and their geographic patterns simultaneously this paper addresses these challenges by developing an integrated interactive_visualization_framework that consists three coupled components 1 a spatially constrained graph partitioning method that can construct a hierarchy of geographical regions communities where there are more flows or connections within regions than across regions 2 a multivariate clustering and visualization method to detect and present multivariate patterns in the aggregated region-to-region flows and 3 a highly interactive flow_mapping component to map both flow and multivariate patterns in the geographic space at different hierarchical levels the proposed approach can process relatively large_data sets and effectively discover and visualize major flow structures and multivariate relations at the same time user_interactions are supported to facilitate the understanding of both an overview_and_detailed patterns contiguity_constraints coordinated_views data_mining flow_mapping graph partitioning hierarchical_clustering multidimensional visualization spatial_interaction
when analyzing thousands of event histories analysts often want to see the events as an aggregate to detect insights and generate new hypotheses about the data an analysis tool must emphasize both the prevalence and the temporal ordering of these events additionally the analysis tool must also support flexible comparisons to allow analysts to gather visual_evidence in a previous work we introduced align rank and filter arf to accentuate temporal ordering in this paper we present temporal summaries an interactive_visualization_technique that highlights the prevalence of event occurrences temporal summaries dynamically aggregate events in multiple granularities year month week day hour etc for the purpose of spotting trends over time and comparing several groups of records they provide affordances for analysts to perform temporal range filters we demonstrate the applicability of this approach in two extensive case studies with analysts who applied temporal summaries to search filter and look for patterns in electronic_health_records and academic records human-computer_interaction information_visualization interaction_design temporal_categorical_data_visualization
hierarchical_representations are common in digital repositories yet are not always fully leveraged in their online search_interfaces this work describes resultmaps which use hierarchical treemap representations with query string-driven digital_library search_engines we describe two lab experiments which find that resultsmap users yield significantly better results over a control condition on some subjective measures and we find evidence that resultmaps have ancillary benefits via increased understanding of some aspects of repository content the resultmap system and experiments contribute an understanding of the benefits-direct and indirect-of the resultmap approach to repository search visualization treemap digital_library digital_repository evaluation infovis search_engine search visualization user studies
large multi-touch displays are expanding the possibilities of multiple-coordinated_views by allowing multiple people to interact with data in concert or independently we present lark a system that facilitates the coordination of interactions with information_visualizations on shared digital workspaces we focus on supporting this coordination according to four main criteria scoped interaction temporal flexibility spatial flexibility and changing collaboration styles these are achieved by integrating a representation of the information_visualization pipeline into the shared workspace thus explicitly indicating coordination points on data representation presentation and view levels this integrated meta-visualization supports both the awareness of how views are linked and the freedom to work in concert or independently lark incorporates these four main criteria into a coherent visualization collaboration interaction environment by providing direct visual and algorithmic support for the coordination of data_analysis actions over shared large displays co-located_work collaboration coordination information_visualization meta-visualization workspace_awareness
a great corpus of studies reports empirical evidence of how information_visualization supports comprehension and analysis of data the benefits of visualization for synchronous group knowledge work however have not been addressed extensively anecdotal evidence and use cases illustrate the benefits of synchronous collaborative information_visualization but very few empirical studies have rigorously examined the impact of visualization on group knowledge work we have consequently designed and conducted an experiment in which we have analyzed the impact of visualization on knowledge sharing in situated work groups our experimental study consists of evaluating the performance of 131 subjects all experienced managers in groups of 5 for a total of 26 groups working together on a real-life knowledge sharing task we compare 1 the control condition no visualization provided with two visualization supports 2 optimal and 3 suboptimal_visualization based on a previous survey the facilitator of each group was asked to populate the provided interactive visual template with insights from the group and to organize the contributions according to the group consensus we have evaluated the results through both objective and subjective measures our statistical_analysis clearly shows that interactive_visualization has a statistically significant objective and positive impact on the outcomes of knowledge sharing but that the subjects seem not to be aware of this in particular groups supported by visualization achieved higher productivity higher quality of outcome and greater knowledge gains no statistically significant results could be found between an optimal and a suboptimal_visualization though as classified by the pre-experiment survey subjects also did not seem to be aware of the benefits that the visualizations provided as no difference between the visualization and the control conditions was found for the self-reported measures of satisfaction a- - nd participation an implication of our study for information_visualization_applications is to extend them by using real-time group annotation functionalities that aid in the group sense making process of the represented data collaborative_and_distributed_visualization laboratory_studies visual_knowledge_representation experiment group_work knowledge sharing synchronous_situated_collaboration
we describe the design and deployment of dashiki a public website where users may collaboratively build visualization dashboards through a combination of a wiki-like syntax and interactive editors our goals are to extend existing research on social_data_analysis into presentation and organization of data from multiple sources explore new metaphors for these activities and participate more fully in the web's information ecology by providing tighter integration with real-time data to support these goals our design includes novel and low-barrier mechanisms for editing and layout of dashboard pages and visualizations connection to data sources and coordinating interaction between visualizations in addition to describing these technologies we provide a preliminary report on the public launch of a prototype based on this design including a description of the activities of our users derived from observation and interviews collaboration dashboards social_data_analysis social_software visual_analytics visualization web wikis
trees and graphs are relevant to many online tasks such as visualizing social_networks product catalogs educational portals digital libraries the semantic web concept_maps and personalized information_management spicynodes is an information-visualization technology that builds upon existing research on radial_tree_layouts and graph structures users can browse a tree clicking from node to node as well as successively viewing a node immediately related nodes and the path back to the ldquohomerdquo nodes spicynodes' layout_algorithms maintain balanced layouts using a hybrid mixture of a geometric layout a succession of spanning radial trees and force-directed_layouts to minimize overlapping nodes plus several other improvements over prior art it provides xml-based api and gui authoring tools the goal of the spicynodes project is to implement familiar principles of radial maps and focus+context with an attractive and inviting look and feel in an open system that is accessible to virtually any internet user trees_and_network_visualization focus+context hierarchy_visualization human-computer_interaction information_visualization interaction radial_tree_layout
in may of 2008 we published online a series of software_visualization videos using a method called code_swarm shortly thereafter we made the code open source and its popularity took off this paper is a study of our code swarm application comprising its design results and public response we share our design methodology including why we chose the organic information_visualization_technique how we designed for both developers and a casual audience and what lessons we learned from our experiment we validate the results produced by code_swarm through a qualitative_analysis and by gathering online user comments furthermore we successfully released the code as open source and the software community used it to visualize their own projects and shared their results as well in the end we believe code_swarm has positive implications for the future of organic information design and open source information_visualization practice software_visualization organic information_visualization software_development_history_and_evolution
modern programmable gpus represent a vast potential in terms of performance and visual flexibility for information_visualization research but surprisingly few applications even begin to utilize this potential in this paper we conjecture that this may be due to the mismatch between the high-level abstract data types commonly visualized in our field and the low-level floating-point model supported by current gpu shader languages to help remedy this situation we present a refinement of the traditional information_visualization pipeline that is amenable toimplementation using gpu shaders the refinement consists of a final image-space step in the pipeline where the multivariate data of the visualization is sampled in the resolution of the current view to concretize the theoretical aspects of this work we also present a visual programming environment for constructing visualization shaders using a simple drag-and-drop interface finally we give some examples of the use of shaders for well-known visualization_techniques gpu-acceleration high-performance_visualization interaction shader programming
during continuous user_interaction it is hard to provide rich visual feedback at interactive rates for datasets containing millions of entries the contribution of this paper is a generic architecture that ensures responsiveness of the application even when dealing with large_data and that is applicable to most types of information_visualizations our architecture builds on the separation of the main application thread and the visualization thread which can be cancelled early due to user_interaction in combination with a layer mechanism our architecture facilitates generating previews incrementally to provide rich visual feedback quickly to help avoiding common pitfalls of multi-threading we discuss synchronization andcommunication in detail we explicitly denote design choices to control trade-offs a quantitative_evaluation based on the system vi s p l ore shows fast visual feedback during continuous_interaction even for millions of entries we describe instantiations of our architecture in additional tools information_visualization architecture continuous_interaction layer multi-threading preview
despite myriad tools for visualizing data there remains a gap between the notational efficiency of high-level visualization_systems and the expressiveness and accessibility of low-level graphical systems powerful visualization_systems may be inflexible or impose abstractions foreign to visual thinking while graphical systems such as rendering apis and vector-based drawing programs are tedious for complex work we argue that an easy-to-use graphical system tailored for visualization is needed in response we contribute protovis an extensible toolkit for constructing visualizations by composing simple graphical primitives in protovis designers specify visualizations as a hierarchy of marks with visual properties defined as functions of data this representation achieves a level of expressiveness comparable to low-level graphics systems while improving efficiency - the effort required to specify a visualization - and accessibility - the effort required to learn and modify the representation we substantiate this claim through a diverse collection of examples and comparative_analysis with popular visualization tools d graphics information_visualization toolkits user_interfaces
in serial computation program profiling is often helpful for optimization of key sections of code when moving to parallel computation not only does the code execution need to be considered but alsocommunication between the different processes which can induce delays that are detrimental to performance as the number of processes increases so does the impact of thecommunication delays on performance for large-scale parallel applications it is critical to understand how thecommunication impacts performance in order to make the code more efficient there are several tools available for visualizing program execution andcommunications on parallel systems these tools generally provide either views which statistically summarize the entire program execution or process-centric views however process-centric visualizations do not scale well as the number of processes gets very large in particular the most common representation of parallel processes is a gantt chart with a row for each process as the number of processes increases these charts can become difficult to work with and can even exceed screen resolution we propose a new visualization approach that affords more scalability and then demonstrate it on systems running with up to 16384 processes information_visualization mpi profiling scalability
we discuss the design and usage of ldquowordlerdquo a web-based tool for visualizing text wordle creates tag-cloud-like displays that give careful attention to typography color and composition we describe the algorithms used to balance various aesthetic criteria and create the distinctive wordle layouts we then present the results of a study of wordle usage based both on spontaneous behaviour observed in the wild and on a large-scale survey of wordle users the results suggest that wordles have become a kind of medium of expression and that a ldquoparticipatory_culturerdquo has arisen around them visualization educational_visualization memory participatory_culture social_data_analysis tag cloud text
finding suitable less space consuming views for a document's main content is crucial to provide convenient access to large document collections on display devices of different size we present a novel compact visualization which represents the document's key semantic as a mixture of images and important key terms similar to cards in a top trumps game the key terms are extracted using an advanced text mining approach based on a fully automatic document structure extraction the images and their captions are extracted using a graphical heuristic and the captions are used for a semi-semantic image weighting furthermore we use the image color histogram for classification and show at least one representative from each non-empty image class the approach is demonstrated for the ieee infovis publications of a complete year the method can easily be applied to other publication collections and sets of documents which contain images content_extraction document collection browsing document_visualization visual_summary
visualizing the intellectual_structure of scientific domains using co-cited units such as references or authors has become a routine for domain analysis in previous studies paper-reference matrices are usually transformed into reference-reference matrices to obtain co-citation relationships which are then visualized in different representations typically as node-link networks to represent the intellectual_structures of scientific domains such network_visualizations sometimes contain tightly knit components which make visual_analysis of the intellectual_structure a challenging task in this study we propose a new approach to reveal co-citation relationships instead of using a reference-reference matrix we directly use the original paper-reference_matrix as the information source and transform the paper-reference_matrix into an fp-tree and visualize it in a java-based prototype system we demonstrate the usefulness of our approach through visual analyses of the intellectual_structure of two domains information_visualization and sloan digital sky survey sdss the results show that our visualization not only retains the major information of co-citation relationships but also reveals more detailed sub-structures of tightly knit clusters than a conventional node-link network_visualization co-citation fp-tree intellectual_structure paper-reference_matrix
with the rapid growth of the world_wide_web and electronic information services text corpus is becoming available online at an incredible rate by displaying text data in a logical layout eg color graphs text_visualization presents a direct way to observe the documents as well as understand the relationship between them in this paper we propose a novel technique exemplar-based visualization ev to visualize an extremely large text corpus capitalizing on recent advances in matrix approximation and decomposition ev presents a probabilistic multidimensional projection model in the low-rank text subspace with a sound objective function the probability of each document proportion to the topics is obtained through iterative optimization and embedded to a low dimensional space using parameter embedding by selecting the representative exemplars we obtain a compact approximation of the data this makes the visualization highly efficient and flexible in addition the selected exemplars neatly summarize the entire data set and greatly reduce the cognitive overload in the visualization leading to an easier interpretation of large text corpus empirically we demonstrate the superior performance of ev through extensive experiments performed on the publicly available text data sets exemplar large-scale document_visualization multidimensional projection
we present a new technique the phrase net for generating visual overviews of unstructured text a phrase net displays a graph whose nodes are words and whose edges indicate that two words are linked by a user-specified relation these relations may be defined either at the syntactic or lexical level different relations often produce very different perspectives on the same text taken together these perspectives often provide an illuminating visual overview of the key concepts and relations in a document or set of documents text_visualization natural_language_processing semantic_net tag cloud
a contour_tree is a powerful tool for delineating the topological evolution of isosurfaces of a single-valued function and thus has been frequently used as a means of extracting features from volumes and their time-varying behaviors several sophisticated algorithms have been proposed for constructing contour_trees while they often complicate the softwareimplementation especially for higher-dimensional cases such as time-varying_volumes this paper presents a simple yet effective approach to plotting in 3d space approximate contour_trees from a set of scattered samples embedded in the high-dimensional space our main idea is to take advantage of manifold_learning so that we can elongate the distribution of high-dimensional_data samples to embed it into a low-dimensional space while respecting its local proximity of sample points the contribution of this paper lies in the introduction of new distance metrics to manifold_learning which allows us to reformulate existing algorithms as a variant of currently available dimensionality_reduction scheme efficient reduction of data sizes together with segmentation capability is also developed to equip our approach with a coarse-to-fine analysis even for large-scale_datasets examples are provided to demonstrate that our proposed scheme can successfully traverse the features of volumes and their temporal behaviors through the constructed contour_trees contour_trees high-dimensional_data_analysis manifold_learning time-varying_volumes
this paper formalizes a novel intrinsic geometric scale_space igss of 3d surface shapes the intrinsic geometry of a surface is diffused by means of the ricci flow for the generation of a geometric scale_space we rigorously prove that this multiscale shape representation satisfies the axiomatic causality property within the theoretical framework we further present a feature-based shape representation derived from igss processing which is shown to be theoretically plausible and practically effective by integrating the concept of scale-dependent saliency into the shape description this representation is not only highly descriptive of the local structures but also exhibits several desired characteristics of global shape representations such as being compact robust to noise and computationally efficient we demonstrate the capabilities of our approach through salient geometric feature_detection and highly discriminative matching of 3d scans riemannian_manifolds scale_space feature_extraction geometric_flow
local shape descriptors compactly characterize regions of a surface and have been applied to tasks in visualization shape_matching and analysis classically curvature has be used as a shape descriptor however this differential property characterizes only an infinitesimal neighborhood in this paper we provide shape descriptors for surface meshes designed to be multi-scale that is capable of characterizing regions of varying size these descriptors capture statistically the shape of a neighborhood around a central point by fitting a quadratic surface they therefore mimic differential curvature are efficient to compute and encode anisotropy we show how simple variants of mesh operations can be used to compute the descriptors without resorting to expensive parameterizations and additionally provide a statistical approximation for reduced computational cost we show how these descriptors apply to a number of uses in visualization analysis and matching of surfaces particularly to tasks in protein surface analysis curvature descriptors npr shape_matching stylized_rendering
many techniques have been proposed to show uncertainty in data visualizations however very little is known about their effectiveness in conveying meaningful information in this paper we present a user_study that evaluates the perception of uncertainty amongst four of the most commonly used techniques for visualizing uncertainty in one-dimensional and two-dimensional data the techniques evaluated are traditional errorbars scaled size of glyphs color-mapping on glyphs and color-mapping of uncertainty on the data surface the study uses generated data that was designed to represent the systematic and random uncertainty components twenty-seven users performed two types of search tasks and two types of counting tasks on 1d and 2d datasets the search tasks involved finding data points that were least or most uncertain the counting tasks involved counting data features or uncertainty features a 4 times 4 full-factorial anova indicated a significant interaction between the techniques used and the type of tasks assigned for both datasets indicating that differences in performance between the four techniques depended on the type of task performed several one-way anovas were computed to explore the simple main effects bonferronni's correction was used to control for the family-wise error rate for alpha-inflation although we did not find a consistent order among the four techniques for all the tasks there are several findings from the study that we think are useful for uncertainty_visualization design we found a significant difference in user performance between searching for locations of high and searching for locations of low uncertainty errorbars consistently underperformed throughout the experiment scaling the size of glyphs and color-mapping of the surface performed reasonably well the efficiency of most of these techniques were highly dependent on the tasks performed we believe that these findings can be used in future uncertainty_visualization desig- - n in addition the framework developed in this user_study presents a structured approach to evaluate uncertainty_visualization_techniques as well as provides a basis for future research in uncertainty_visualization user_study uncertainty_visualization
in a user_study comparing four visualization methods for three-dimensional vector data participants used visualizations from each method to perform five simple but representative tasks 1 determining whether a given point was a critical point 2 determining the type of a critical point 3 determining whether an integral curve would advect through two points 4 determining whether swirling movement is present at a point and 5 determining whether the vector field is moving faster at one point than another the visualization methods were line and tube representations of integral curves with both monoscopic and stereoscopic viewing while participants reported a preference for stereo lines quantitative results showed performance among the tasks varied by method users performed all tasks better with methods that 1 gave a clear representation with no perceived occlusion 2 clearly visualized curve speed and direction information and 3 provided fewer rich 3d cues eg shading polygonal arrows overlap cues and surface textures these results provide quantitative support for anecdotal evidence on visualization methods the tasks and testing framework also give a basis for comparing other visualization methods for creating more effective methods and for defining additional tasks to explore further the tradeoffs among the methods d vector_fields lines stereoscopic_and_monoscopic_viewing tubes user_study visualization
visual representations of isosurfaces are ubiquitous in the scientific and engineering literature in this paper we present techniques to assess the behavior of isosurface_extraction codes where applicable these techniques allow us to distinguish whether anomalies in isosurface features can be attributed to the underlying physical process or to artifacts from the extraction process such scientific scrutiny is at the heart of verifiable_visualization - subjecting visualization algorithms to the same verification process that is used in other components of the scientific pipeline more concretely we derive formulas for the expected order ofaccuracy or convergence rate of several isosurface features and compare them to experimentally observed results in the selected codes this technique is practical in two cases it exposed actual problems inimplementations we provide the reader with the range of responses they can expect to encounter with isosurface techniques both under ldquonormal operating conditionsrdquo and also under adverse conditions armed with this information - the results of the verification process - practitioners can judiciously select the isosurface_extraction technique appropriate for their problem of interest and have confidence in its behavior isosurface_extraction marching_cubes v&v verification
we present two visualization_techniques for curve-centric volume reformation with the aim to create compelling comparative_visualizations a curve-centric volume reformation deforms a volume with regards to a curve in space to create a new space in which the curve evaluates to zero in two dimensions and spans its arc-length in the third the volume surrounding the curve is deformed such that spatial neighborhood to the curve is preserved the result of the curve-centric reformation produces images where one axis is aligned to arc-length and thus allows researchers and practitioners to apply their arc-length parameterized data visualizations in parallel for comparison furthermore we show that when visualizing dense data our technique provides an inside out projection from the curve and out into the volume which allows for inspection what is around the curve finally we demonstrate the usefulness of our techniques in the context of two application cases we show that existing data visualizations of arc-length parameterized data can be enhanced by using our techniques in addition to creating a new view and perspective on volumetric_data around curves additionally we show how volumetric_data can be brought into plotting environments that allow precise readouts in the first case we inspect streamlines in a flow_field around a car and in the second we inspect seismic volumes and well logs from drilling comparative_visualization curve-centric-reformation radial ray-casting volume_deformation
in this paper we present a method for vortex core line extraction which operates directly on the smoothed_particle_hydrodynamics sph representation and by this generates smoother and more spatially and temporally coherent results in an efficient way the underlying predictor-corrector scheme is general enough to be applied to other line-type features and it is extendable to the extraction of surfaces such as isosurfaces or lagrangian coherent_structures the proposed method exploits temporal_coherence to speed up computation for subsequent time steps we show how the predictor-corrector formulation can be specialized for several variants of vortex core line definitions including two recent unsteady extensions and we contribute a theoretical and practical comparison of these in particular we reveal a close relation between unsteady extensions of fuchs et al and weinkauf et al and we give a proof of the galilean invariance of the latter when visualizing sph data there is the possibility to use the same interpolation method for visualization as has been used for the simulation this is different from the case of finite volume simulation results where it is not possible to recover from the results the spatial interpolation that was used during the simulation such data are typically interpolated using the basic trilinear interpolant and if smoothness is required some artificial processing is added in sph data however the smoothing kernels are specified from the simulation and they provide an exact and smooth interpolation of data or gradients at arbitrary points in the domain smoothed_particle_hydrodynamics feature_extraction flow_visualization unsteady_flow vortex core lines
in this paper we investigate scalability limitations in the visualization of large-scale particle-based cosmological simulations and we present methods to reduce these limitations on current pc architectures to minimize the amount of data to be streamed from disk to the graphics subsystem we propose a visually continuous level-of-detail lod particle representation based on a hierarchical quantization scheme for particle coordinates and rules for generating coarse particle distributions given the maximal world space error per level our lod selection technique guarantees a sub-pixel screen space error during rendering a brick-based page-tree allows to further reduce the number of disk seek operations to be performed additional particle quantities like density velocity dispersion and radius are compressed at no visible loss using vector quantization of logarithmically encoded floating point values by fine-grain view-frustum culling and presence acceleration in a geometry shader the required geometry throughput on the gpu can be significantly reduced we validate the quality and scalability of our method by presenting visualizations of a particle-based cosmological dark-matter simulation exceeding 10 billion elements cosmology particle visualization scalability
in this paper we present techniques for the visualization of unsteady_flows using streak_surfaces which allow for the first time an adaptive_integration and rendering of such surfaces in real-time the techniques consist of two main components which are both realized on the gpu to exploit computational and bandwidth capacities for numerical particle integration and to minimize bandwidth requirements in the rendering of the surface in the construction stage an adaptive surface_representation is generated surface refinement and coarsening strategies are based on local surface properties like distortion and curvature we compare two different methods to generate a streak surface a by computing a patch-based surface_representation that avoids any interdependence between patches and b by computing a particle-based surface_representation including particle connectivity and by updating this connectivity during particle refinement and coarsening in the rendering stage the surface is either rendered as a set of quadrilateral surface patches using high-quality point-based approaches or a surface triangulation is built in turn from the given particle connectivity and the resulting triangle mesh is rendered we perform a comparative study of the proposed techniques with respect to surface quality visual quality and performance by visualizing streak_surfaces in real flows using different rendering options gpus unsteady_flow_visualization streak_surface_generation
time_and_streak_surfaces are ideal tools to illustrate time-varying vector_fields since they directly appeal to the intuition about coherently moving particles however efficient generation of high-quality time_and_streak_surfaces for complex large and time-varying vector_field_data has been elusive due to the computational effort involved in this work we propose a novel algorithm for computing such surfaces our approach is based on a decoupling of surface advection and surface adaptation and yields improved efficiency over other surface tracking methods and allows us to leverage inherent parallelization opportunities in the surface advection resulting in more rapid parallel computation moreover we obtain as a result of our algorithm the entire evolution of a time or streak surface in a compact representation allowing for interactive high-quality rendering visualization and exploration of the evolving surface finally we discuss a number of ways to improve surface depiction through advanced rendering and texturing while preserving interactivity and provide a number of examples for real-world datasets and analyze the behavior of our algorithm on them d vector_field_visualization flow_visualization surface_extraction time_and_streak_surfaces time-varying
we propose a new perception-guided compositing operator for color_blending the operator maintains the same rules for achromatic compositing as standard operators such as the over operator but it modifies the computation of the chromatic channels chromatic compositing aims at preserving the hue of the input colors color continuity is achieved by reducing the saturation of colors that are to change their hue value the main benefit of hue preservation is that color can be used for proper visual labeling even under the constraint of transparency rendering or image overlays therefore the visualization of nominal data is improved hue-preserving blending can be used in any existing compositing algorithm and it is particularly useful for volume_rendering the usefulness of hue-preserving blending and its visual characteristics are shown for several examples of volume_visualization color_blending illustrative_visualization image_compositing perceptual_transparency volume_rendering
the semi-transparent nature of direct volume rendered images is useful to depict layered structures in a volume however obtaining a semi-transparent result with the layers clearly revealed is difficult and may involve tedious adjustment on opacity and other rendering parameters furthermore the visual quality of layers also depends on various perceptualfactors in this paper we propose an auto-correction method for enhancing the perceived quality of the semi-transparent layers in direct volume rendered images we introduce a suite of new measures based on psychological principles to evaluate the perceptual quality of transparent structures in the rendered images by optimizing rendering parameters within an adaptive and intuitive user_interaction process the quality of the images is enhanced such that specific user requirements can be met experimental results on various datasets demonstrate the effectiveness and robustness of our method direct_volume_rendering image_enhancement layer_perception
color vision deficiency cvd affects approximately 200 million people worldwide compromising the ability of these individuals to effectively perform color and visualization-related tasks this has a significant impact on their private and professional lives we present a physiologically-based model for simulating color vision our model is based on the stage theory of human color vision and is derived from data reported in electrophysiological studies it is the first model to consistently handle normal color vision anomalous_trichromacy and dichromacy in a unified way we have validated the proposed model through an experimental evaluation involving groups of color vision deficient individuals and normal color vision ones our model can provide insights and feedback on how to improve visualization experiences for individuals with cvd it also provides a framework for testing hypotheses about some aspects of the retinal photoreceptors in color vision deficient individuals anomalous_trichromacy color_perception dichromacy models_of_color_vision simulation_of_color_vision_deficiency
we present a technique for the illustrative_rendering of 3d line_data at interactive frame rates we create depth-dependent halos around lines to emphasize tight line bundles while less structured lines are de-emphasized moreover the depth-dependent halos combined with depth cueing via line width attenuation increase depth perception extending techniques from sparse line rendering to the illustrative_visualization of dense_line_data we demonstrate how the technique can be used in particular for illustrating dti_fiber_tracts but also show examples from gas and fluid_flow_simulations and mathematics as well as describe how the technique extends to point data we report on an informal evaluation of the illustrative dti fiber tract visualizations with domain experts in neurosurgery and tractography who commented positively about the results and suggested a number of directions for future work dti gpu technique illustrative_rendering_and_visualization npr black-and-white_rendering dense_line_data
in this paper we present the first algorithm to geometrically register multiple projectors in a view-independent manner ie wallpapered on a common type of curved surface vertically extruded surface using an uncalibrated camera without attaching any obtrusive markers to the display screen further it can also tolerate large non-linear geometric distortions in the projectors as is common when mounting short throw lenses to allow a compact set-up our registration achieves sub-pixelaccuracy on a large number of different vertically extruded surfaces and the image correction to achieve this registration can be run in real time on the gpu this simple markerless registration has the potential to have a large impact on easy set-up and maintenance of large curved multi-projector displays common for visualization edutainment training and simulation applications calibration multi-projector displays registration tiled_displays
multi-projector displays show significant spatial variation in 3d color gamut due to variation in the chromaticity gamuts across the projectors vignetting effect of each projector and also overlap across adjacent projectors in this paper we present a new constrained gamut morphing algorithm that removes all these variations and results in true color seamlessness across tiled multi-projector displays our color morphing algorithm adjusts the intensities of light from each pixel of each projector precisely to achieve a smooth morphing from one projector's gamut to the other's through the overlap region this morphing is achieved by imposing precise constraints on the perceptual difference between the gamuts of two adjacent pixels in addition our gamut morphing assures a c1 continuity yielding visually pleasing appearance across the entire display we demonstrate our method successfully on a planar and a curved display using both low and high-end projectors our approach is completely scalable efficient and automatic we also demonstrate the real-time performance of our image correction algorithm on gpus for interactive applications to the best of our knowledge this is the first work that presents a scalable method with a strong foundation in perception and realizes for the first time a truly seamless display where the number of projectors cannot be deciphered color_calibration multi-projector displays tiled_displays
in this paper we describe a novel method to integrate interactive_visual_analysis and machine_learning to support the insight generation of the user the suggested approach combines the vast search and processing power of the computer with the superior reasoning and pattern_recognition capabilities of the human user an evolutionary search algorithm has been adapted to assist in the fuzzy_logic formalization of hypotheses that aim at explaining features inside multivariate volumetric_data up to now users solely rely on their knowledge and expertise when looking for explanatory theories however it often remains unclear whether the selected attribute ranges represent the real explanation for the feature of interest other selections hidden in the large number of data variables could potentially lead to similar features moreover as simulation complexity grows users are confronted with huge multidimensional data sets making it almost impossible to find meaningful hypotheses at all we propose an interactive cycle of knowledge-based analysis and automatic hypothesis generation starting from initial hypotheses created with linking and brushing the user steers a heuristic search algorithm to look for alternative or related hypotheses the results are analyzed in information_visualization views that are linked to the volume_rendering individual properties as well as global aggregates are visually presented to provide insight into the most relevant aspects of the generated hypotheses this novel approach becomes computationally feasible due to a gpuimplementation of the time-critical parts in the algorithm a thorough evaluation of search times and noise sensitivity as well as a case_study on data from the automotive domain substantiate the usefulness of the suggested approach computer-assisted multivariate data_exploration curse_of_dimensionality genetic_algorithm interactive_visual_analysis knowledge_discovery multiple competing hypotheses predictive_analysis volumetric_data
radiofrequency identification rfid is a powerful automatic remote identification technique that has wide applications to facilitate rfid deployment an rfid benchmarking instrument called agate has been invented to identify the strengths and weaknesses of different rfid technologies in various environments however the data acquired by agate are usually complextime_varying multidimensional 3d volumetric_data which are extremely challenging for engineers to analyze in this paper we introduce a set of visualization_techniques namely parallel coordinate plots orientation plots a visual_history mechanism and a 3d spatial viewer to help rfid engineers analyze benchmark data visually and intuitively with the techniques we further introduce two workflow procedures a visual optimization procedure for finding the optimum reader antenna configuration and a visual_analysis procedure for comparing the performance and identifying the flaws of rfid devices for the rfid benchmarking with focus on the performance_analysis of the agate system the usefulness and usability of the system are demonstrated in the user_evaluation rfid visual_analytics visualization
this paper describes advanced volume_visualization and quantification for applications in non-destructive_testing ndt which results in novel and highly effective interactive workflows for ndt practitioners we employ a visual approach to explore and quantify the features of interest based on transfer_functions in the parameter spaces of specific application scenarios examples are the orientations of fibres or the roundness of particles the applicability and effectiveness of our approach is illustrated using two specific scenarios of high practical relevance first we discuss the analysis of steel fibre reinforced sprayed concrete sfrspc we investigate the orientations of the enclosed steel fibres and their distribution depending on the concrete's application direction this is a crucial step in assessing the material's behavior under mechanical stress which is still in its infancy and therefore a hot topic in the building industry the second application scenario is the designation of the microstructure of ductile cast irons with respect to the contained graphite this corresponds to the requirements of the iso standard 945-1 which deals with 2d metallographic samples we illustrate how the necessary analysis steps can be carried out much more efficiently using our system for 3d volumes overall we show that a visual approach with custom transfer_functions in specific application domains offers significant benefits and has the potential of greatly improving and optimizing the workflows of domain scientists and engineers direction visualization multi-dimensional_transfer_functions non-destructive_testing volume_rendering
the widespread use of computational simulation in science and engineering provides challenging research opportunities multiple independent variables are considered and large and complex data are computed especially in the case of multi-run simulation classical visualization_techniques deal well with 2d or 3d data and also with time-dependent_data additional independent dimensions however provide interesting new challenges we present an advanced visual_analysis approach that enables a thorough investigation of families of data surfaces ie datasets with respect to pairs of independent dimensions while it is almost trivial to visualize one such data surface the visual_exploration and analysis of many such data surfaces is a grand challenge stressing the users' perception and cognition we propose an approach that integrates projections and aggregations of the data surfaces at different levels one scalar aggregate per surface a 1d profile per surface or the surface as such we demonstrate the necessity for a flexible visual_analysis system that integrates many different linked_views for making sense of this highly complex data to demonstrate its usefulness we exemplify our approach in the context of a meteorological multi-run simulation data case and in the context of the engineering domain where our collaborators are working with the simulation of elastohydrodynamic ehd lubrication bearing in the automotive industry coordinated multiple_views family_of_surfaces interactive_visual_analysis multidimensional multivariate data
we present a new algorithm to explore and visualize multivariate time-varying_data sets we identify important trend relationships among the variables based on how the values of the variables change over time and how those changes are related to each other in different spatial regions and time intervals the trend relationships can be used to describe the correlation and causal effects among the different variables to identify the temporal trends from a local region we design a new algorithm called subdtw to estimate when a trend appears and vanishes in a given time_series based on the beginning and ending times of the trends their temporal relationships can be modeled as a state machine representing the trend_sequence since a scientific data set usually contains millions of data points we propose an algorithm to extract important trend relationships in linear time complexity we design novel user_interfaces to explore the trend relationships to visualize their temporal characteristics and to display their spatial distributions we use several scientific data sets to test our algorithm and demonstrate its utilities subdtw trend_sequence trend_sequence clustering
we develop a new algorithm for isosurface_extraction and view-dependent_filtering from large time-varying fields by using a novel persistent time-octree ptot indexing structure previously the persistent octree pot was proposed to perform isosurface_extraction and view-dependent_filtering which combines the advantages of the interval tree for optimal searches of active cells and of the branch-on-need octree bono for view-dependent_filtering but it only works for steady-stateie single time step data for time-varying fields a 4d version of pot 4d-pot was proposed for 4d isocontour slicing where slicing on the time domain gives all active cells in the queried timestep and isovalue however such slicing is not output sensitive and thus the searching is sub-optimal moreover it was not known how to support view-dependent_filtering in addition to time-domain slicingin this paper we develop a novel persistent time-octree ptot indexing structure which has the advantages of pot and performs 4d isocontour slicing on the time domain with an output-sensitive and optimal searching in addition when we query the same iso value q over m consecutive time steps there is no additional searching overhead except for reporting the additional active cells compared to querying just the first time step such searching performance for finding active cells is asymptotically optimal with asymptotically optimal space and preprocessing time as well moreover our ptot supports view-dependent_filtering in addition to time-domain slicing we propose a simple and effective out-of-core scheme where we integrate our ptot with implicit occluders batched occlusion queries and batched cuda computing tasks so that we can greatly reduce the i/o cost as well as increase the amount of data being concurrently computed in gputhis results in an efficient algorithm for isosurface_extraction with view-dependent_filtering utilizing a state-of-the-art programmable gpu for ti me-varying fields larger than main memory our experiments on datasets as large as 192 gb with 4 gb per time step having no more than 870 mb of memory footprint in both preprocessing and run-time phases demonstrate the efficacy of our new technique isosurface_extraction out-of-core_methods persistent_data_structure time-varying fields view-dependent_filtering
due to its nonlinear nature the climate system shows quite high natural variability on different time scales including multiyear oscillations such as the el_nino southern oscillation phenomenon beside a shift of the mean states and of extreme values of climate variables climate change may also change the frequency or the spatial patterns of these natural climate variations wavelet analysis is a well established tool to investigate variability in the frequency domain however due to the size and complexity of the analysis results only few time_series are commonly analyzed concurrently in this paper we will explore different techniques to visually assist the user in the analysis of variability and variability changes to allow for a holistic analysis of a global climate model data set consisting of several variables and extending over 250 years our new framework and data from the ipcc ar4 simulations with the coupled climate model echam5/mpi-om are used to explore the temporal evolution of el_nino due to climate change el_nino wavelet analysis climate_variability_change_visualization multivariate data time-dependent_data
we present an interactive framework for exploring space-time and form-function relationships in experimentally collected high-resolution biomechanical data sets these data describe complex 3d motions eg chewing walking flying performed by animals and humans and captured via high-speed imaging technologies such as biplane fluoroscopy in analyzing these 3d biomechanical motions interactive 3d_visualizations are important in particular for supporting spatial analysis however as researchers in information_visualization have pointed out 2d visualizations can also be effective tools for multi-dimensional_data_analysis especially for identifying trends over time our approach therefore combines techniques from both 3d and 2d visualizations specifically it utilizes a multi-view visualization strategy including a small_multiples view of motion sequences a parallel_coordinates view and detailed 3d inspection views the resulting framework follows an overview first zoom and filter then details-on-demand style of analysis and it explicitly targets a limitation of current tools namely supporting analysis and comparison at the level of a collection of motions rather than sequential analysis of a single or small number of motions scientific motion collections appropriate for this style of analysis exist in clinical work in orthopedics and physical rehabilitation in the study of functional morphology within evolutionary biology and in other contexts an application is described based on a collaboration with evolutionary biologists studying the mechanics of chewing motions in pigs interactive_exploration of data describing a collection of more than one hundred experimentally captured pig chewing cycles is described scientific_visualization biomechanics coordinated multiple_views information_visualization
molecular_dynamics simulations of proteins play a growing role in various fields such as pharmaceutical biochemical and medical research accordingly the need for high_quality visualization of these protein systems raises highly interactive_visualization_techniques are especially needed for the analysis of time-dependent molecular simulations beside various other molecular representations the surface_representations are of high importance for these applications so far users had to accept a trade-off between rendering quality and performance - particularly when visualizing trajectories of time-dependent protein data we present a new approach for visualizing the solvent excluded surface of proteins using a gpu ray casting technique and thus achieving interactive frame rates even for long protein trajectories where conventional methods based on precomputation are not applicable furthermore we propose a semantic simplification of the raw protein data to reduce the visual complexity of the surface and thereby accelerate the rendering without impeding perception of the protein's basic shape we also demonstrate the application of our solvent excluded surface method to visualize the spatial probability density for the protein atoms over the whole period of the trajectory in one frame providing a qualitative_analysis of the protein flexibility gpu isosurfaces molecular_visualization point-based data ray casting surface_extraction time-varying_data
we demonstrate the application of advanced 3d_visualization_techniques to determine the optimal implant design and position in hip joint replacement planning our methods take as input the physiological stress distribution inside a patient's bone under load and the stress distribution inside this bone under the same load after a simulated replacement surgery the visualization aims at showing principal stress directions and magnitudes as well as differences in both distributions by visualizing changes of normal and shear stresses with respect to the principal stress directions of the physiological state a comparative_analysis of the physiological stress distribution and the stress distribution with implant is provided and the implant parameters that most closely replicate the physiological stress state in order to avoid stress shielding can be determined our method combines volume_rendering for the visualization of stress magnitudes with the tracing of short line segments for the visualization of stress directions to improve depth perception transparent shaded and antialiased lines are rendered in correct visibility order and they are attenuated by the volume_rendering we use a focus+context approach to visually guide the user to relevant regions in the data and to support a detailed stress analysis in these regions while preserving spatial_context information since all of our techniques have been realized on the gpu they can immediately react to changes in the simulated stress_tensor_field and thus provide an effective means for optimal implant selection and positioning in a computational_steering environment biomedical_visualization comparative_visualization gpu_techniques implant_planning stress_tensor_fields
rhinologists are often faced with the challenge of assessing nasal breathing from a functional point of view to derive effective therapeutic interventions while the complex nasal anatomy can be revealed by visual inspection and medical imaging only vague information is available regarding the nasal airflow itself rhinomanometry delivers rather unspecific integral information on the pressure gradient as well as on total flow and nasal flow resistance in this article we demonstrate how the understanding of physiological nasal breathing can be improved by simulating and visually analyzing nasal airflow based on an anatomically correct model of the upper human respiratory tract in particular we demonstrate how various information_visualization infovis techniques such as a highly scalableimplementation of parallel_coordinates time_series visualizations as well as unstructured grid multi-volume_rendering all integrated within a multiple linked_views framework can be utilized to gain a deeper understanding of nasal breathing evaluation is accomplished by visual_exploration of spatio-temporal airflow characteristics that include not only information on flow features but also on accompanying quantities such as temperature and humidity to our knowledge this is the first in-depth visual_exploration of the physiological function of the nose over several simulated breathing cycles under consideration of a complete model of the nasal airways realistic boundary conditions and all physically relevant time-varying quantities flow_visualization exploratory_data_analysis interactive_visual_analysis_of_scientific_data interactive visualflow_visualization time-dependent_data
particle_systems have gained importance as a methodology for sampling implicit_surfaces and segmented objects to improve mesh generation and shape analysis we propose that particle_systems have a significantly more general role in sampling structure from unsegmented_data we describe a particle system that computes samplings of crease_features ie ridges_and_valleys as lines or surfaces that effectively represent many anatomical structures in scanned medical data because structure naturally exists at a range of sizes relative to the image resolution computer vision has developed the theory of scale-space which considers an n-d image as an n + 1-d stack of images at different blurring levels our scale-space particles move through continuous four-dimensional scale-space according to spatial constraints imposed by the crease_features a particle-image energy that draws particles towards scales of maximal feature strength and an inter-particle energy that controls sampling density in space and scale to make scale-space practical for large three-dimensional data we present a spline-based interpolation across scale from a small number of pre-computed blurrings at optimally selected scales the configuration of the particle system is visualized with tensor_glyphs that display information about the local hessian of the image and the scale of the particle we use scale-space particles to sample the complex three-dimensional branching structure of airways in lung_ct and the major white_matter structures in brain dti crease_features diffusion_tensor_mri lung_ct particle_systems ridge and valley detection
medical illustration has demonstrated its effectiveness to depict salient anatomical features while hiding the irrelevant details current solutions are ineffective for visualizing fibrous structures such as muscle because typical datasets ct or mri do not contain directional details in this paper we introduce a new muscle illustration approach that leverages diffusion_tensor_imaging dti data and example-based texture_synthesis techniques beginning with a volumetric diffusion tensor image we reformulate it into a scalar field and an auxiliary guidance vector field to represent the structure and orientation of a muscle bundle a muscle mask derived from the input diffusion tensor image is used to classify the muscle structure the guidance vector field is further refined to remove noise and clarify structure to simulate the internal appearance of the muscle we propose a new two-dimensional example based solid_texture_synthesis algorithm that builds a solid texture constrained by the guidance vector field illustrating the constructed scalar field and solid texture efficiently highlights the global appearance of the muscle as well as the local shape and structure of the muscle fibers in an illustrative fashion we have applied the proposed approach to five example datasets four pig hearts and a pig leg demonstrating plausible illustration and expressiveness diffusion tensor image illustrative_visualization muscle solid_texture_synthesis
visual_exploration is essential to the visualization and analysis of densely sampled 3d dti fibers in biological speciments due to the high geometric spatial and anatomical complexity of fiber tracts previous methods for dti fiber visualization use zooming color-mapping selection and abstraction to deliver the characteristics of the fibers however these schemes mainly focus on the optimization of visualization in the 3d space where cluttering and occlusion make grasping even a few thousand fibers difficult this paper introduces a novel interaction method that augments the 3d_visualization with a 2d representation containing a low-dimensional embedding of the dti fibers this embedding preserves the relationship between the fibers and removes the visual_clutter that is inherent in 3d renderings of the fibers this new interface allows the user to manipulate the dti fibers as both 3d curves and 2d embedded points and easily compare or validate his or her results in both domains theimplementation of the framework is gpu based to achieve real-time interaction the framework was applied to several tasks and the results show that our method reduces the user's workload in recognizing 3d dti fibers and permits quick and accurate dti fiber selection diffusion_tensor_imaging fiber clustering fibers visualization_interface
fiber_tracking of diffusion_tensor_imaging dti data offers a unique insight into the three-dimensional organisation of white_matter structures in the living brain however fiber_tracking algorithms require a number of user-defined input parameters that strongly affect the output results usually the fiber_tracking parameters are set once and are then re-used for several patient datasets however the stability of the chosen parameters is not evaluated and a small change in the parameter values can give very different results the user remains completely unaware of such effects furthermore it is difficult to reproduce output results between different users we propose a visualization tool that allows the user to visually explore how small variations in parameter values affect the output of fiber_tracking with this knowledge the user cannot only assess the stability of commonly used parameter values but also evaluate in a more reliable way the output results between different patients existing tools do not provide such information a small user_evaluation of our tool has been done to show the potential of the technique diffusion_tensor_imaging fiber_tracking parameter sensitivity stopping_criteria uncertainty_visualization
we present a visual_exploration paradigm that facilitates navigation through complex fiber tracts by combining traditional 3d model viewing with lower dimensional representations to this end we create standard streamtube models along with two two-dimensional representations an embedding in the plane and a hierarchical_clustering tree for a given set of fiber tracts we then link these three representations using both interaction and color obtained by embedding fiber tracts into a perceptually uniform color space we describe an anecdotal evaluation with neuroscientists to assess the usefulness of our method in exploring anatomical and functional structures in the brain expert feedback indicates that while a standalone clinical use of the proposed method would require anatomical landmarks in the lower dimensional representations the approach would be particularly useful in accelerating tract bundle selection results also suggest that combining traditional 3d model viewing with lower dimensional representations can ease navigation through the complex fiber tract models improving exploration of the connectivity in the brain dti_fiber_tracts coloring embedding interaction
we introduce a new method for coloring 3d line fields and show results from its application in visualizing orientation in dti brain data sets the method uses boy's surface an immersion of rp2 in 3d this coloring method is smooth and one-to-one except on a set of measure zero the double curve of boy's surface dti line field colormapping orientation real_projective_plane tensor_field
one of the most common operations in exploration and analysis of various kinds of data is clustering ie discovery and interpretation of groups of objects having similar properties and/or behaviors in clustering objects are often treated as points in multi-dimensional_space of properties however structurally complex objects such as trajectories of moving entities and other kinds of spatio-temporal_data cannot be adequately represented in this manner such data require sophisticated and computationally intensive clustering algorithms which are very hard to scale effectively to large_datasets not fitting in the computer main memory we propose an approach to extracting meaningful clusters from large_databases by combining clustering and classification which are driven by a human analyst through an interactive visual interface spatio-temporal_data classification clustering geo visualization movement_data scalable_visualization trajectories
the increasing availability of motion sensors and video cameras in living spaces has made possible the analysis of motion patterns and collective behavior in a number of situations the visualization of this movement_data however remains a challenge although maintaining the actual layout of the data space is often desirable direct visualization of movement traces becomes cluttered and confusing as the spatial distribution of traces may be disparate and uneven we present proximity-based visualization as a novel approach to the visualization of movement traces in an abstract space rather than the given spatial_layout this abstract space is obtained by considering proximity data which is computed as the distance between entities and some number of important locations these important locations can range from a single fixed point to a moving point several points or even the proximities between the entities themselves this creates a continuum of proximity spaces ranging from the fixed absolute reference frame to completely relative reference frames by combining these abstracted views with the concrete spatial views we provide a way to mentally map the abstract spaces back to the real space we demonstrate the effectiveness of this approach and its applicability to visual_analytics problems such as hazard prevention migration patterns and behavioral studies spatio-temporal_visualization linked_views movement_patterns principal_component_analysis proximity temporal_trajectories
this paper demonstrates the promise of augmenting interactive multivariate representations with information from statistical processes in the domain of weather data_analysis statistical regression correlation_analysis and descriptive statistical calculations are integrated via graphical indicators into an enhanced parallel_coordinates system called the multidimensional data explorer mdx these statistical indicators which highlight significant associations in the data are complemented with interactive_visual_analysis capabilities the resulting system allows a smooth interactive and highly visual workflow the system's utility is demonstrated with an extensive hurricane climate_study that was conducted by a hurricane expert in the study the expert used a new data set of environmental weather data composed of 28 independent variables to predict annual hurricane activity mdx shows the atlantic meridional mode increases the explained variance of hurricane seasonal activity by 7-15% and removes less significant variables used in earlier studies the findings and feedback from the expert 1 validate the utility of the data set for hurricane prediction and 2 indicate that the integration of statistical processes with interactive parallel_coordinates as implemented in mdx addresses both deficiencies in traditional weather data_analysis and exhibits some of the expected benefits of visual_data_analysis climate_study correlation interaction multivariate data regression statistical_analysis visual_analytics
an increasing number of temporal categorical_databases are being collected electronic_health_records in healthcare organizations traffic incident logs in transportation systems or student records in universities finding similar records within these large_databases requires effective similarity measures that capture the searcher's intent many similarity measures exist for numerical time_series but temporal_categorical_records are different we propose a temporal categorical similarity measure the m&m match_&_mismatch_measure which is based on the concept of aligning records by sentinel events then matching events between the target and the compared records the m&m measure combines the time differences between pairs of events and the number of mismatches to accom-modate customization of parameters in the m&m measure and results interpretation we implemented similan an interactive search and visualization tool for temporal_categorical_records a usability study with 8 participants demonstrated that similan was easy to learn and enabled them to find similar records but users had difficulty understanding the m&m measure the usability study feedback led to an improved version with a continuous timeline which was tested in a pilot study with 5 participants m&m measure similan similarity_search temporal_categorical_records
radio frequency rf fingerprinting-based techniques for localization are a promising approach for ubiquitous positioning systems particularly indoors by finding unique fingerprints of rf signals received at different locations within a predefined area beforehand whenever a similar fingerprint is subsequently seen again the localization system will be able to infer a user's current location however developers of these systems face the problem of finding reliable rf fingerprints that are unique enough and adequately stable over time we present a visual_analytics system that enables developers of these localization systems to visually gain insight on whether their collected datasets and chosen fingerprint features have the necessary properties to enable a reliable rf fingerprinting-based localization system the system was evaluated by testing and debugging an existing localization system d testing and debugging debugging aids— h user_interfaces graphical user_interfaces (gui)—
cellular radio networks are continually growing in both node count and complexity it therefore becomes more difficult to manage the networks and necessary to use time and cost effective automatic algorithms to organize the networks neighbor cell relations there have been a number of attempts to develop such automatic algorithms network operators however may not trust them because they need to have an understanding of their behavior and of their reliability and performance which is not easily perceived this paper presents a novel web-enabled geo visual_analytics approach to exploration and understanding of self-organizing_network data related to cells and neighbor cell relations a demonstrator and case_study are presented in this paper developed in close collaboration with the swedish telecom company ericsson and based on large multivariate time-varying and geospatial_data provided by the company it allows the operators to follow interact with and analyze the evolution of a self-organizing_network and enhance their understanding of how an automatic algorithm configures locally-unique physical cell identities and organizes neighbor cell relations of the network the geo visual_analytics tool is tested with a self-organizing_network that is operated by the automatic neighbor relations anr algorithm the demonstrator has been tested with positive results by a group of domain experts from ericsson and will be tested in production geo visual_analytics geospatial_data sets multi-dimensional multi-layer self-organizing_network time-varying visualization
visual_analytics has become an important tool for gaining insight on large and complex collections of data numerous statistical tools and data_transformations such as projections binning and clustering have been coupled with visualization to help analysts understand data better and faster however data is inherently uncertain due to error noise or unreliable sources when making decisions based on uncertain data it is important to quantify and present to the analyst both the aggregated uncertainty of the results and the impact of the sources of that uncertainty in this paper we present a new framework to support uncertainty in the visual_analytics process through statistic methods such as uncertainty modeling propagation and aggregation we show that data_transformations such as regression principal_component_analysis and k-means clustering can be adapted to account for uncertainty this framework leads to better visualizations that improve the decision-making process and help analysts gain insight on the analytic process itself data_transformations model_fitting principal_component_analysis uncertainty
visual_exploration of multivariate data typically requires projection onto lower-dimensional representations the number of possible representations grows rapidly with the number of dimensions and manual exploration quickly becomes ineffective or even unfeasible this paper proposes automatic analysis methods to extract potentially relevant visual_structures from a set of candidate visualizations based on features the visualizations are ranked in accordance with a specified user task the user is provided with a manageable number of potentially useful candidate visualizations which can be used as a starting point for interactive data_analysis this can effectively ease the task of finding truly useful visualizations and potentially speed up the data_exploration task in this paper we present ranking measures for class-based as well as non class-based scatterplots and parallel_coordinates visualizations the proposed analysis methods are evaluated on different datasets h information storage and retrieval information search and retrieval i computer_graphics picture/image generation
in this paper we discuss dimension reduction methods for 2d visualization of high dimensional clustered_data we propose a two-stage framework for visualizing such data based on dimension reduction methods in the first stage we obtain the reduced dimensional data by applying a supervised dimension reduction method such as linear_discriminant_analysis which preserves the original cluster structure in terms of its criteria the resulting optimal reduced dimension depends on the optimization criteria and is often larger than 2 in the second stage the dimension is further reduced to 2 for visualization purposes by another dimension reduction method such as principal_component_analysis the role of the second-stage is to minimize the loss of information due to reducing the dimension all the way to 2 using this framework we propose several two-stage methods and present their theoretical characteristics as well as experimental_comparisons on both artificial and real-world text data sets d projection clustered_data dimension reduction generalized_singular_value_decomposition linear_discriminant_analysisorthogonal_centroid_method principal_component_analysis regularization
discovering and extracting linear trends and correlations in datasets is very important for analysts to understand multivariate phenomena however current widely used multivariate visualization_techniques such as parallel_coordinates and scatterplot matrices fail to reveal and illustrate such linear relationships intuitively especially when more than 3 variables are involved or multiple trends coexist in the dataset we present a novel multivariate model parameter space visualization system that helps analysts discover single and multiple linear patterns and extract subsets of data that fit a model well using this system analysts are able to explore and navigate in model parameter space interactively select and tune patterns and refine the model foraccuracy using computational techniques we build connections between model space and data space visually allowing analysts to employ their domain knowledge during exploration to better interpret the patterns they discover and their validity case studies with real datasets are used to investigate the effectiveness of the visualizations knowledge_discovery model_space_visualization multivariate linear model construction visual_analysis
latent semantic analysis lsa is a commonly-used method for automated processing modeling and analysis of unstructured text data one of the biggest challenges in using lsa is determining the appropriate model parameters to use for different data domains and types of analyses although automated methods have been developed to make rank and scaling parameter choices these approaches often make choices with respect to noise in the data without an understanding of how those choices impact analysis and problem_solving further no tools currently exist to explore the relationships between an lsa model and analysis methods our work focuses on how parameter choices impact analysis and problem_solving in this paper we present lsaview a system for interactively exploring parameter choices for lsa models we illustrate the use of lsaview's small multiple_views linked matrix-graph views and data views to analyze parameter selection and application in the context of graph_layout and clustering applications i computing methodologies natural_language_processing i computing methodologies computer_graphics text_analysis
do court cases differ from place to place what kind of picture do we get by looking at a country's collection of law cases we introduce parallel tag_clouds a new way to visualize differences amongst facets of very large metadata-rich text corpora we have pointed parallel tag_clouds at a collection of over 600000 us circuit court decisions spanning a period of 50 years and have discovered regional as well as linguistic differences between courts the visualization_technique combines graphical elements from parallel_coordinates and traditional tag_clouds to provide rich overviews of a document collection while acting as an entry point for exploration of individual texts we augment basic parallel tag_clouds with a details-in-context display and an option to visualize changes over a second facet of the data such as time we also address text mining challenges such as selecting the best words to visualize and how to do so in reasonable time periods to maintain interactivity text_visualization corpus_visualization information_retrieval tag_clouds text mining
sources of streaming information such as news syndicates publish information continuously information portals and news aggregators list the latest information from around the world enabling information consumers to easily identify events in the past 24 hours the volume and velocity of these streams causes information from prior days to quickly vanish despite its utility in providing an informative context for interpreting new information few capabilities exist to support an individual attempting to identify or understand trends and changes from streaming information over time the burden of retaining prior information and integrating with the new is left to the skills determination and discipline of each individual in this paper we present a visual_analytics system for linking essential content from information streams over time into dynamic stories that develop and change over multiple days we describe particular challenges to the analysis of streaming information and present a fundamental visual representation for showing story change and evolution over time h content analysis and indexing abstracting methods— h information search and retrieval information filtering— h digital libraries user issues— i computer_graphics picture/image generation—
a common task in literary analysis is to study characters in a novel or collection automatic entity extraction text_analysis and effective user_interfaces facilitate character analysis using our interface called posvis the scholar uses word clouds and self-organizing graphs to review vocabulary to filter by part of speech and to explore the network of characters located near characters under review further visualizations show word usages within an analysis window ie a book chapter which can be compared with a reference window ie the whole book we describe the interface and report on an early case_study with a humanities scholar design experimentation human_factors visual_analytics
the ieee visual_analytics science and technology vast symposium has held a contest each year since its inception in 2006 these events are designed to provide visual_analytics researchers and developers with analytic challenges similar to those encountered by professional information analysts the vast contest has had an extended life outside of the symposium however as materials are being used in universities and other educational settings either to help teachers of visual_analytics-related classes or for student projects we describe how we develop vast contest datasets that results in products that can be used in different settings and review some specific examples of the adoption of the vast contest materials in the classroom the examples are drawn from graduate and undergraduate courses at virginia tech and from the visual_analytics ldquosummer camprdquo run by the national visualization and analytics center in 2008 we finish with a brief discussion on evaluation metrics for education education evaluation synthetic_data
during visual_analysis users must often connect insights discovered at various points of time this process is often called ldquoconnecting the dotsrdquo when analysts interactively explore complex datasets over multiple sessions they may uncover a large number of findings as a result it is often difficult for them to recall the past insights views and concepts that are most relevant to their current line of inquiry this challenge is even more difficult during collaborative analysis tasks where they need to find connections between their own discoveries and insights found by others in this paper we describe a context-based retrieval algorithm to identify notes views and concepts from users' past analyses that are most relevant to a view or a note based on their line of inquiry we then describe a related notes recommendation feature that surfaces the most relevant items to the user as they work based on this algorithm we have implemented this recommendation feature in harvest a web based visual analytic system we evaluate the related notes recommendation feature of harvest through a case_study and discuss the implications of our approach h information search and retrieval—retrieval models
visual_analytics tools provide powerful visual representations in order to support the sense-making process in this process analysts typically iterate through sequences of steps many times varying parameters each time few visual_analytics tools support this process well nor do they provide support for visualizing and understanding the analysis process itself to help analysts understand explore reference and reuse their analysis process we present a visual_analytics system named czsaw see-saw that provides an editable and re-playable history navigation channel in addition to multiple visual representations of document collections and the entities within them in a manner inspired by jigsaw conventional history navigation tools range from basic undo and redo to branching timelines of user actions in czsaw's approach to this first user_interactions are translated into a script language that drives the underlying scripting-driven propagation system the latter allows analysts to edit analysis steps and ultimately to program them second on this base we build both a history view showing progress and alternative paths and a dependency graph showing the underlying logic of the analysis and dependency relations among the results of each step these tools result in a visual model of the sense-making process providing a way for analysts to visualize their analysis process to reinterpret the problem explore alternative paths extract analysis patterns from existing history and reuse them with other related analyses analysis process sense-making visual_analytics visual_history
despite the growing number of systems providing visual analytic support for investigative_analysis few empirical studies of the potential benefits of such systems have been conducted particularly controlled comparative evaluations determining how such systems foster insight and sensemaking is important for their continued growth and study however furthermore studies that identify how people use such systems and why they benefit or not can help inform the design of new systems in this area we conducted an evaluation of the visual_analytics system jigsaw employed in a small investigative sensemaking exercise and we compared its use to three other more traditional methods of analysis sixteen participants performed a simulated intelligence_analysis task under one of the four conditions experimental results suggest that jigsaw assisted participants to analyze the data and identify an embedded threat we describe different analysis strategies used by study participants and how computational support or the lack thereof influenced the strategies we then illustrate several characteristics of the sensemaking process identified in the study and provide design implications for investigative_analysis tools based thereon we conclude with recommendations for metrics and techniques for evaluating other visual_analytics investigative_analysis tools
this paper presents a working graph_analytics model that embraces the strengths of the traditional top-down and bottom-up approaches with a resilient crossover concept to exploit the vast middle-ground information overlooked by the two extreme analytical approaches our graph_analytics model is co-developed by users and researchers who carefully studied the functional requirements that reflect the critical thinking and interaction pattern of a real-life intelligence analyst to evaluate the model we implement a system prototype known as greenhornet which allows our analysts to test the theory in practice identify the technological and usage-related gaps in the model and then adapt the new technology in their work space the paper describes theimplementation of greenhornet and compares its strengths and weaknesses against the other prevailing models and tools graph_analytics information_visualization
in this paper we present a system for the interactive_visualization and exploration of graphs with many weakly connected components the visualization of large graphs has recently received much research attention however specific systems for visual_analysis of graph data sets consisting of many components are rare in our approach we rely on graph_clustering using an extensive set of topology descriptors specifically we use the self-organizing-map algorithm in conjunction with a user-adaptable combination of graph features for clustering of graphs it offers insight into the overall structure of the data set the clustering output is presented in a grid containing clusters of the connected components of the input graph interactive feature_selection and task-tailored data views allow the exploration of the whole graph space the system provides also tools for assessment and display of cluster quality we demonstrate the usefulness of our system by application to a shareholder network analysis problem based on a large real-world data set while so far our approach is applied to weighted directed graphs only it can be used for various graph types clustering h user_interfaces graphical user_interfaces (gui) e data structures graphs and networks picture/image generation h information search and retrieval i computer_graphics
protein complexes are formed when two or more proteins non-covalently interact to form a larger three dimensional structure with specific biological function understanding the composition of such complexes is vital to understanding cell biology at the molecular level massvis is a visual_analysis tool designed to assist the interpretation of data from a new workflow for detecting the composition of such protein complexes in biological samples the data generated by the laboratory workflow naturally lends itself to a scatter plot visualization however characteristics of this data give rise to some unique aspects not typical of a standard scatter plot we are able to take the output from tandem mass_spectrometry and render the data in such a way that it mimics more traditional two-dimensional gel techniques and at the same time reveals the correlated behavior indicative of protein complexes by computationally measuring these correlated patterns in the data membership in putative complexes can be inferred user_interactions are provided to support both an interactive discovery mode as well as an unsupervised clustering of likely complexes the specific analysis tasks led us to design a unique arrangement of item selection and coordinated detail views in order to simultaneously view different aspects of the selected item correlation_analysis information_visualization interactome mass_spectrometry proteomics visual_analysis
gene mapping is a statistical method used to localize human disease genes to particular regions of the human genome when performing such analysis a genetic likelihood space is generated and sampled which results in a multidimensional scalar field researchers are interested in exploring this likelihood space through the use of visualization previous efforts at visualizing this space though were slow and cumbersome only showing a small portion of the space at a time thus requiring the user to keep a mental picture of several views we have developed a new technique that displays much more data at once by projecting the multidimensional data into several 2d plots one plot is created for each parameter that shows the change along that parameter a radial projection is used to create another plot that provides an overview of the high dimensional surface from the perspective of a single point linking and brushing between all the plots are used to determine relationships between parameters we demonstrate our techniques on real world autism data showing how to visually examine features of the high dimensional space autism ld_analysis linkage_analysis linkage_disequilibrium multidimensional data ppl ppld posterior_probability_of_linkage visualization
we present a new application spray designed for the visual_exploration of gene expression data it is based on an extension and adaption of parallel_coordinates to support the visual_exploration of large and high-dimensional_datasets in particular we investigate the visual_analysis of gene expression data as generated by micro-array experiments we combine refined visual_exploration with statistical methods to a visual_analytics approach that proved to be particularly successful in this application domain we will demonstrate the usefulness on several multidimensional gene expression datasets from different bioinformatics applications visual_analytics bioinformatics gene expression experiments large-scale_microarray microarray_data
today online stores collect a lot of customer feedback in the form of surveys reviews and comments this feedback is categorized and in some cases responded to but in general it is underutilized - even though customer satisfaction is essential to the success of their business in this paper we introduce several new techniques to interactively analyze customer comments and ratings to determine the positive and negative opinions expressed by the customers first we introduce a new discrimination-based technique to automatically extract the terms that are the subject of the positive or negative opinion such as price or customer service and that are frequently commented on second we derive a reverse-distance-weighting method to map the attributes to the related positive and negative opinions in the text third the resulting high-dimensional feature vectors are visualized in a new summary representation that provides a quick overview we also cluster the reviews according to the similarity of the comments special thumbnails are used to provide insight into the composition of the clusters and their relationship in addition an interactive circular correlation map is provided to allow analysts to detect the relationships of the comments to other important attributes and the scores we have applied these techniques to customer comments from real-world online stores and product reviews from web sites to identify the strength and problems of different products and services and show the potential of our technique attribute extraction visual_document_analysis visual_opinion_analysis visual_sentiment_analysis
finvis is a visual_analytics tool that allows the non-expert casual user to interpret the return risk and correlation aspects of financial data and make personal finance decisions this interactive exploratory tool helps the casual decision-maker quickly choose between various financial portfolio options and view possible outcomes finvis allows for exploration of inter-temporal_data to analyze outcomes of short-term or long-term investment decisions finvis helps the user overcome cognitive limitations and understand the impact of correlation between financial instruments in order to reap the benefits of portfolio diversification because this software is accessible by non-expert users decision-makers from the general population can benefit greatly from using finvis in practical applications we quantify the value of finvis using experimental economics methods and find that subjects using the finvis software make better financial portfolio decisions as compared to subjects using a tabular version with the same information we also find that finvis engages the user which results in greater exploration of the dataset and increased learning as compared to a tabular display further participants using finvis reported increased confidence in financial decision-making and noted that they were likely to use this tool in practical application casual_information_visualization economic_decision-making personal finance visual_analytics visualization_of_risk
patents are an important economic factor in todays globalized markets therefore the analysis of patent information has become an inevitable task for a variety of interest groups the retrieval of relevant patent information is an integral part of almost every patent analysis scenario unfortunately the complexity of patent material inhibits a straightforward retrieval of all relevant patent documents and leads to iterative time-consuming approaches in practice with `patviz' a new system for interactive analysis of patent information has been developed to leverage iterative query refinement patviz supports users in building complex queries visually and in exploring patent result sets interactively thereby the visual query module introduces an abstraction layer that provides uniform access to different retrieval systems and relieves users of the burden to learn different complex query languages by establishing an integrated environment it allows for interactive reintegration of insights gained from visual result set exploration into the visual query representation we expect that the approach we have taken is also suitable to improve iterative query refinement in other visual_analytics systems patent_retrieval information_visualization multiple_coordinated_views visual_analytics
space- and time-referenced data published on the web by general people can be viewed in a dual way as independent spatio-temporal events and as trajectories of people in the geographical space these two views suppose different approaches to the analysis which can yield different kinds of valuable knowledge about places and about people we define possible types of analysis tasks related to the two views of the data and present several analysis methods appropriate for these tasks the methods are suited to large amounts of the data
although many in the community have advocated user-centered evaluations for visual analytic environments a significant barrier exists the users targeted by the visual_analytics community law_enforcement personnel professional information analysts financial analysts health care analysts etc are often inaccessible to researchers these analysts are extremely busy and their work environments and data are often classified or at least confidential furthermore their tasks often last weeks or even months it is simply not feasible to do such long-term observations to understand their jobs how then can we hope to gather enough information about the diverse user populations to understand their needs some researchers including the author have been successful in getting access to specific end-users a reasonable approach therefore would be to find a way to share user information this work outlines a proposal for developing a handbook of user profiles for use by researchers developers and evaluators user requirements user-centered evaluation visual_analytics
the current visual_analytics literature highlights design and evaluation processes that are highly variable and situation dependent which raises at least two broad challenges first lack of a standardized evaluation criterion leads to costly re-designs for each task and specific user community second this inadequacy in criterion validation raises significant uncertainty regarding visualization outputs and their related decisions which may be especially troubling in high consequence environments like those of the intelligence community as an attempt to standardize the ldquoapples and orangesrdquo of the extant situation we propose the creation of standardized evaluation tools using general principles of human cognition theoretically visual_analytics enables the user to see information in a way that should attenuate the user's memory load and increase the user's task-available cognitive resources by using general cognitive abilities like available working_memory resources as our dependent measures we propose to develop standardized evaluative capabilities that can be generalized across contexts tasks and user communities visual_analytics cognitive load evaluation
in visual_analytics menu systems are commonly adopted as supporting tools because of the complex nature of data however it is still unknown how much the interaction implicit to the interface impacts the performance of visual_analysis to show the effectiveness of two interface tools one a floating text-based menu floating menu and the other a more interactive iconic tool interactive-icon we evaluated the use and human performance of both tools within one highly interactive visual_analytics system we asked participants to answer similarly constructed straightforward questions in a genomic visualization first with one tool and then the other during task_performance we tracked completion times task errors and captured coarse-grained interactive behaviors based on the participantsaccuracy speed behaviors and post-task qualitative feedback we observed that although the interactive-icon tool supports continuous_interactions task-oriented user_evaluation did not find a significant difference between the two tools because there is a familiarity effect on the performance of solving the task questions with using floating-menu interface tool
intelligence analysts in the areas of defense and homeland security are now faced with the difficult problem of discerning the relevant details amidst massive data stores we propose a component-based visualization architecture that is built specifically to encourage the flexible exploration of geospatial event databases the proposed system is designed to deploy on a variety of display layouts from a single laptop screen to a multi-monitor tiled-display by utilizing a combination of parallel_coordinates principal components plots and other data views analysts may reduce the dimensionality of a data set to its most salient features of particular value to our target applications are understanding correlations between data layers both within a single view and across multiple_views our proposed system aims to address the limited scalability associated with coordinated multiple_views cmvs through theimplementation of an efficient core application which is extensible by the end-user coordiated multiple_views ultrascale visualization visual_analytics
interactive_visualization methods are often used to aid in the analysis of large_datasets we present a novel interactive_visualization_technique designed specifically for the analysis of location reporting patterns within large time-series_datasets we use a set of triangles with color coding to indicate the time between location reports this allows reporting patterns expected and unexpected to be easily discerned during interactive analysis we discuss the details of our method and describe evaluation both from expert opinion and from a user_study
the classic tilebars paradigm has been used to show distribution information of query terms in full-text documents however when the number of query terms becomes large it is not an easy task for users to comprehend their distribution within certain parts of a document in this paper we present a novel approach to improve the visual presentation of tilebars in which barycenter heuristic for bigraph crossing minimization is used to reorder tilebars elements the reordered tilebars can be demonstrated to provide users with better focus and navigation while exploring text documents graphical user_interfaces h information interfaces and presentation user_interfaces
complex scenario analysis requires the exploration of multiple hypotheses and supporting evidence for each argument posed knowledge-intensive organisations typically analyse large amounts of inter-related heterogeneous data to retrieve the knowledge this contains and use it to support effective decision-making we demonstrate the use of interactive graph visualisation to support hierarchical task-driven hypothesis investigation the visual investigative_analysis is guided by task and domain ontologies used to capture the structure of the investigation process and the experience gained and knowledge created in previous related investigations h information interfaces and presentation user_interfaces—graphical user_interfaces (gui) k management of computing and information systems project and people management—life cycle
many well-known time_series prediction methods have been used daily by analysts making decisions to reach a good prediction we introduce several new visual_analysis techniques of smoothing multi-scaling and weighted average with the involvement of human expert knowledge we combine them into a well-fitted method to perform prediction we have applied this approach to predict resource consumption in data center for next day planning
in modern process industry it is often difficult to analyze a manufacture process due to its numerous time-series_data analysts wish to not only interpret the evolution of data over time in a working procedure but also examine the changes in the whole production process through time to meet such analytic requirements we have developed processline an interactive_visualization tool for a large amount of time-series_data in process industry the data are displayed in a fisheye timeline processline provides good overviews for the whole production process and details for the focused working procedure a preliminary user_study using beer industry production data has shown that the tool is effective business visualization time-series_data visual_analytics visual_design
while many visualization tools exist that offer sophisticated functions for charting complex data they still expect users to possess a high degree of expertise in wielding the tools to create an effective visualization this poster presents articulate an attempt at a semi-automated visual analytic model that is guided by a conversational user interface the goal is to relieve the user of the physical burden of having to directly craft a visualization through the manipulation of a complex user-interface by instead being able to verbally articulate what the user wants to see and then using natural_language_processing and heuristics to semi-automatically create a suitable visualization graphical user_interfaces h information interfaces and presentation user_interfaces
in this poster paper we present beads a high dimensional data cluster visualization by having a 2-d representation of shape and spread of the cluster the cluster division component the bead shape identification and cluster shape composition form the core of the system beads visualization consists of a 2-d plot standard 2-d shapes which are used as metaphors to represent corresponding high-dimensional shapes of beads the final resulting images convey the relative placement of beads with respect to the cluster center the shape of the beads we give a textual summary of the beads and their 2-d placement on the beads plot in tabular format along with the image
the poster introduces interactive multiobjective optimization imo as a field offering new application possibilities and challenges for visual_analytics va and aims at inspiring collaboration between the two fields our aim is to collect new ideas in order to be able to utilize va techniques more effectively in our user interface development simulation-based imo methods are developed for complex problem_solving where the expert decision maker analyst should be supported during the iterative process of eliciting preference information and examining the resulting output data imo is a subfield of multiple criteria decision_making mcdm in simulation-based imo the optimization task is formulated in a mathematical model containing several conflicting objectives and constraints depending on decision variables while using imo methods the analyst progressively provides preference information in order to find the most satisfactory compromise between the conflicting objectives in the poster theimplementations of two new imo methods are used as examples to demonstrate concrete challenges of interaction_design one of them is described in this summary interactive multiobjective optimization information_visualization interaction_design visual_analytics
icexplorer is a tool for analyzing variations in ice cover on lake erie it enhances the data and pre-packaged analysis currently available in the great lakes ice atlas and serves as an example of a small focused application where simple but carefully-chosen visualizations interaction techniques and automated data_analysis are combined to create an effective tool for advancing scientific research earth and atmospheric sciences h information systems applicationscommunications applications graphical user_interfaces, interaction styles information browsers h information interfaces and presentation user_interfaces j physical sciences and engineering
our visualization tool for the vast 2009 traffic mini challenge timeliner visualizes badge and network traffic data together in a single timeline the two views of per-employee and per-day with various filtering interactions enable users to analyze easily employees activities at a particular moment of interest as well as their general daily patterns using timeliner we present several hypotheses for the task at hand and their validation processes which reveals various aspects of the data h information systems user/machine_systems—human information processing h database management database applications—data_mining
we present our visualization_systems and findings for the badge and network traffic as well as the social network and geospatial challenges of the 2009 vast contest the summary starts by presenting an overview of our time_series encoding of badge information and network traffic our findings suggest that employee 30 may be of interest in the second part of the paper we describe our system for finding subgraphs in the social network subject to degree constraints subsequently we present our most likely candidate network which is similar to scenario b f theory of computation nonnumerical algorithms and problems—pattern matching h information systems information interfaces and presentation—general
palantir is an analytic platform currently used worldwide by both governmental and financial analysts this paper provides a brief overview of the platform examines our 2009 ieee vast challenge submission and highlights several key analytic and visualization features we used in our analysis palantir vast  collaboration data_integration visual_analytics
processing is a very powerful visualization language which combines software concepts with principles of visual form and interaction artists designers and architects use it but it is also a very effective programming language in the area of visual_analytics in the following contribution processing is utilized in order to visually analyze data provided by ieee vast 2009 mini challenge badge and network traffic the applied process is iterative and each stage of the analytical reasoning_process is accompanied by customized software development the visual model the process and the technical solution will be briefly introduced d coding tools and techniques object-oriented_programming h user/machine_systems visual_analytics
in the vast challenge 2009 suspicious behavior had to be detected applying visual_analytics to heterogeneous data such as network traffic social network enriched with geo-spatial attributes and finally video surveillance data this paper describes some of the awarded parts from our solution entry h information interfaces & presentations user_interfaces - graphical user_interfaces (gui) i methodology and techniques interaction techniques
the hrl anomaly analysis tool was developed as part of the ieee vast challenge 2009 one of the tasks involved processing badge and network traffic in order to detect and identify a fictitious embassy employee suspected of leaking information the tool is designed to assist an analyst in detecting analyzing and visualizing anomalies and their relationships two key visualizations in our submission present how we identified the suspicious traffic using network_visualization and how subsequently we connected that activity to an employee using an alibi table anomaly analysis vast  information_visualization intelligence_analysis social_network_analysis
we hypothesized that potential spies would try to use other employees' terminals in order to not draw attention to themselves we define one type of suspicious activity as ip use on a terminal when the owner is inside the classified area we created a timeline_visualization of ip usage overlaid with classified area entrances and exits the vertical axis divides the timelines into 31 rows one for each day of the month the horizontal axis represents the time of day from early morning to late evening a single employee's entire month is viewed all at once using this visualization the employee being viewed can be changed using the arrow keys every ip event is represented by a vertical bar positioned at the exact time of its appearance we color the ip events by port number which is either intranet http tomcat or email and size the bar based on the outgoing data size whenever an employee enters the classified area a semi-transparent yellow region is drawn until that user exits the classified area in rare cases when the user double enters the region is twice as opaque and in the other rare case where a user leaves the exits without entering a red region is drawn until the next time the employee enters the legend key and office diagram showing the current selected employee highlighted in red can be seen in the top left-hand corner
today's challenging task in intelligent data processing is not to store large_volumes of interlinked data but to visualize explore and understand its explicit or implicit relationships our solution to this is the viscover system viscover combines semantic technologies with interactive_exploration and visualization_techniques able to analyze large_volumes of structured data we briefly describe our viscover system and show its potential using the example of the vast 2009 social network and geospatial_data set h information systems information storage and retrieval—information search and retrieval h information interfaces and presentation user_interfaces— i computing methodologies artificial intelligence—knowledge_representation formalisms and methods
a team of five worked on this challenge to identify a possible criminal structure within the flitter social network initially we worked on the problem individually deliberately not sharing any data results or conclusions this maximised the chances of spotting any blunders unjustified assumptions or inferences and allowed us to triangulate any common conclusions after an agreed period we shared our results demonstrating the visualization_applications we had built and the reasoning behind our conclusions this sharing of assumptions encouraged us to incorporate uncertainty in our visualization approaches as it became clear that there was a number of possible interpretations of the rules and assumptions governing the challenge this summary of the work emphasises one of those applications detailing the geographic analysis and uncertainty handling of the network data
cytoscape is a popular open source tool for biologists to visualize interaction networks we find that it offers most of the desired functionality for visual_analytics on graph data to guide us in the identification of the underlying social structure we demonstrate its utility in the identification of the social structure in the vast 2009 flitter mini challenge cytoscape social_networks
in this article i describe the tools and techniques used to generate competing hypotheses for the vast 2009 flitter mini challenge i will describe how i approached solving the social_networks and the importance of the geospatial relationships to determine that ldquosocial structure form ardquo was the best matching social network visual_analytics geo visualization information_visualization investigative_analysis
we present a visually supported search and browsing system for network-type data especially a novel module for subgraph_search with a gui to define subgraphs for queries we describe how this prototype was applied for the vast challenge 2009 flitter mini challenge heterogeneous graph_visualization visual_analytics
the vast 2009 challenge consisted of three heterogeneous synthetic_data sets organized into separate mini-challenges with minimal correspondence information the challenge task was the identification of a suspected data theft from cyber and real-world traces the grand challenge required integrating the findings from the mini challenges into a plausible consistent scenario a mixture of linked customized tools based on queryable models and rapid prototyping as well as generic analysis tools developed in-house helped us correctly solve all of the mini challenges a collaborative analytic process was employed to reconstruct the scenario and to propose the correct steps for the reliable identification of the criminal organization based on activity traces of its members h information interfaces and presentation user_interfaces—gui i artificial intelligence vision and scene understanding—video analysis
as a solution to the vast 2009 traffic mini visualization challenge we built the badge and network traffic bnt tool to create animations of the events taking place in the embassy using the embassy layout the prox-card and web-access entries and their time-stamps we animated color-based flagging of events the bnt tool highlights logical anomalies occuring in the badge and network traffic data with color-coded alerts prior to the animated visualization the tool analyzes data with respect to various aspects using i the amount of data transfers ii destination ips access patterns iii employee's browsing patterns and iv employee's entry log into the restricted area any abnormality noticed is immediately reported to the user in the form of plots in this presentation we list out the various analyses performed and how they were utilized in the visualization a few screenshots of the tool are provided to illustrate our analytic information presentation
as a solution to the vast 2009 traffic mini visualization challenge we built the badge and network traffic bnt tool to create animations of the events taking place in the embassy using the embassy layout the prox-card and web-access entries and their time-stamps we animated color-based flagging of events the bnt tool highlights logical anomalies occuring in the badge and network traffic data with color-coded alerts prior to the animated visualization the tool analyzes data with respect to various aspects using i the amount of data transfers ii destination ips access patterns iii employee's browsing patterns and iv employee's entry log into the restricted area any abnormality noticed is immediately reported to the user in the form of plots in this presentation we list out the various analyses performed and how they were utilized in the visualization a few screenshots of the tool are provided to illustrate our analytic information presentation
the internet traffic challenge required the development of a custom application to analyze internet traffic patterns coupled with building access records to solve this challenge the author applied the prajna project an open-source java toolkit designed to provide various capabilities for visualization knowledge_representation semantic reasoning and data fusion by applying some of the capabilities of prajna to this challenge the author could quickly develop a custom application for visual_analysis the author determined that he could solve some of the analytical components of this challenge using automated reasoning techniques prajna includes interfaces to incorporate automated reasoners into visual applications by blending the automated reasoning_processes with visual_analysis the author could design a flexible useful application to solve this challenge information_visualization knowledge_representation semantic reasoning software toolkit
shading is an important feature for the comprehension of volume datasets but is difficult to implement accurately current techniques based on pre-integrated direct_volume_rendering approximate the volume_rendering integral by ignoring non-linear gradient variations between front and back samples which might result in cumulated shading errors when gradient variations are important and / or when the illumination function features high frequencies in this paper we explore a simple approach for pre-integrated volume_rendering with non-linear gradient interpolation between front and back samples we consider that the gradient smoothly varies along a quadratic curve instead of a segment in-between consecutive samples this not only allows us to compute more accurate shaded pre-integrated look-up tables but also allows us to more efficiently process shading amplifying effects based on gradient filtering an interesting property is that the pre-integration tables we use remain two-dimensional as for usual pre-integrated classification we conduct experiments using a full hardware approach with the blinn-phong illumination model as well as with a non-photorealistic illumination model direct_volume_rendering gradient interpolation pre-integration
we investigate the use of a fourier-domain derivative error kernel to quantify the error incurred while estimating the gradient of a function from scalar point samples on a regular lattice we use the error kernel to show that gradient reconstruction quality is significantly enhanced merely by shifting the reconstruction kernel to the centers of the principal lattice directions additionally we exploit the algebraic similarities between the scalar and derivative error kernels to design asymptotically optimal gradient estimation filters that can be factored into an infinite impulse response interpolation prefilter and a finite impulse response directional derivative filter this leads to a significant performance gain both in terms ofaccuracy and computational efficiency the interpolation prefilter provides an accurate scalar approximation and can be re-used to cheaply compute directional derivatives on-the-fly without the need to store gradients we demonstrate the impact of our filters in the context of volume_rendering of scalar_data sampled on the cartesian and body-centered_cubic_lattices our results rival those obtained from other competitive gradient estimation methods while incurring no additional computational or storage overhead approximation body centered cubic lattice derivative frequency_error_kernel gradient interpolation lattice reconstruction sampling
we extend direct_volume_rendering with a unified model for generalized isosurfaces also called interval_volumes allowing a wider spectrum of visual classification we generalize the concept of scale-invariant_opacity-typical for isosurface rendering-to semi-transparent interval_volumes scale-invariant rendering is independent of physical space dimensions and therefore directly facilitates the analysis of data characteristics our model represents sharp isosurfaces as limits of interval_volumes and combines them with features of direct_volume_rendering our objective is accurate rendering guaranteeing that all isosurfaces and interval_volumes are visualized in a crack-free way with correct spatial ordering we achieve simultaneous direct and interval_volume_rendering by extending preintegration and explicit peak finding with data-driven splitting of ray integration and hybrid computation in physical and data domains our algorithm is suitable for efficient parallel_processing for interactive applications as demonstrated by our cudaimplementation direct_volume_rendering interval_volume isosurface preintegration ray casting scale-invariant_opacity
practical volume_visualization pipelines are never without compromises and errors a delicate and often-studied component is the interpolation of off-grid samples where aliasing can lead to misleading artifacts and blurring potentially hiding fine details of critical importance the verifiable_visualization_framework we describe aims to account for these errors directly in the volume generation stage and we specifically target volumetric_data obtained via computed_tomography ct reconstruction in this case the raw data are the x-ray projections obtained from the scanner and the volume data generation process is the ct algorithm our framework informs the ct reconstruction process of the specific filter intended for interpolation in the subsequent visualization process and this in turn ensures an accurate interpolation there at a set tolerance here we focus on fast trilinear_interpolation in conjunction with an octree-type mixed resolution volume representation without t-junctions efficient rendering is achieved by a space-efficient and locality-optimized representation which can straightforwardly exploit fast fixed-function pipelines on gpus direct_volume_rendering computed_tomography filtered_back-projection verifiable_visualization
volume ray-casting with a higher order reconstruction_filter and/or a higher sampling rate has been adopted in direct_volume_rendering frameworks to provide a smooth reconstruction of the volume scalar and/or to reduce artifacts when the combined frequency of the volume and transfer_function is high while it enables high-quality volume_rendering it cannot support interactive rendering due to its high computational cost in this paper we propose a fast high-quality volume ray-casting algorithm which effectively increases the sampling rate while a ray traverses the volume intensity values are uniformly reconstructed using a high-order convolution filter additional samplings referred to as virtual samplings are carried out within a ray segment from a cubic spline curve interpolating those uniformly reconstructed intensities these virtual samplings are performed by evaluating the polynomial function of the cubic spline curve via simple arithmetic operations the min max blocks are refined accordingly for accurate empty_space_skipping in the proposed method experimental results demonstrate that the proposed algorithm also exploiting fast cubic texture filtering supported by programmable gpus offers renderings as good as a conventional ray-casting algorithm using high-order reconstruction_filtering at the same sampling rate while delivering 25x to 33x rendering speed-up gpu curve_interpolation direct_volume_rendering high_quality
high_quality volume_rendering of sph data requires a complex order-dependent resampling of particle quantities along the view rays in this paper we present an efficient approach to perform this task using a novel view-space discretization of the simulation domain our method draws upon recent work on gpu-based particle voxelization for the efficient resampling of particles into uniform grids we propose a new technique that leverages a perspective grid to adaptively discretize the view-volume giving rise to a continuous level-of-detail sampling structure and reducing memory requirements compared to a uniform grid in combination with a level-of-detail representation of the particle set the perspective grid allows effectively reducing the amount of primitives to be processed at run-time we demonstrate the quality and performance of our method for the rendering of fluid and gas dynamics sph simulations consisting of many millions of particles gpu_resampling particle visualization ray-casting volume_rendering
applying certain visualization_techniques to datasets described on unstructured_grids requires the interpolation of variables of interest at arbitrary locations within the dataset's domain of definition typical solutions to the problem of finding the grid element enclosing a given interpolation point make use of a variety of spatial subdivision schemes however existing solutions are memory- intensive do not scale well to large grids or do not work reliably on grids describing complex geometries in this paper we propose a data structure and associated construction algorithm for fast cell_location in unstructured_grids and apply it to the interpolation problem based on the concept of bounding interval hierarchies the proposed approach is memory-efficient fast and numerically robust we examine the performance characteristics of the proposed approach and compare it to existing approaches using a number of benchmark problems related to vector_field_visualization furthermore we demonstrate that our approach can successfully accommodate large_datasets and discuss application to visualization on both cpus and gpus cell_location interpolation unstructured_grids vector_field_visualization
interactivity is key to exploration of volume data interactivity may be hindered due to manyfactors eg large_data sizehigh resolution or complexity of a data set or an expensive rendering algorithm we present a novel framework for visualizing volumedata that enables interactive_exploration using proxy images without accessing the original 3d data data_exploration using directvolume_rendering requires multiple often redundant accesses to possibly large amounts of data the notion of visualization by proxyrelies on the ability to defer operations traditionally used for exploring 3d data to a more suitable intermediate representation forinteraction - proxy images such operations include view changes transfer_function exploration and relighting while previous workhas addressed specific interaction needs we provide a complete solution that enables real-time interaction with large_data sets andhas low hardware and storage requirements deferred_interaction image-based_rendering volume_distortion_camera volume_visualization
we introduce a flexible technique for interactive_exploration of vector_field_data through classification derived from user-specified feature templates our method is founded on the observation that while similar features within the vector field may be spatially disparate they share similar neighborhood characteristics users generate feature-based_visualizations by interactively highlighting well-accepted and domain specific representative feature points feature exploration begins with the computation of attributes that describe the neighborhood of each sample within the input vector field compilation of these attributes forms a representation of the vector field samples in the attribute space we project the attribute points onto the canonical 2d plane to enable interactive_exploration of the vector field using a painting interface the projection encodes the similarities between vector field points within the distances computed between their associated attribute points the proposed method is performed at interactive rates for enhanced user_experience and is completely flexible as showcased by the simultaneous identification of diverse feature types data_clustering feature_classification high-dimensional_data user_interaction vector field
streak_surfaces are among the most important features to support 3d unsteady_flow exploration but they are also among the computationally most demanding furthermore to enable a feature driven analysis of the flow one is mainly interested in streak_surfaces that show separation profiles and thus detect unstable manifolds in the flow the computation of such separation surfaces requires to place seeding structures at the separation locations and to let the structures move correspondingly to these locations in the unsteady_flow since only little knowledge exists about the time evolution of separating streak_surfaces at this time an automated exploration of 3d unsteady_flows using such surfaces is not feasible therefore in this paper we present an interactive approach for the visual_analysis of separating streak_surfaces our method draws upon recent work on the extraction of lagrangian coherent_structures lcs and the real-time_visualization of streak_surfaces on the gpu we propose an interactive technique for computing ridges in the finite time lyapunov exponent ftle field at each time step and we use these ridges as seeding structures to track streak_surfaces in the time-varying flow by showing separation surfaces in combination with particle trajectories and by letting the user interactively change seeding parameters such as particle density and position visually guided exploration of separation profiles in 3d is provided to the best of our knowledge this is the first time that the reconstruction and display of semantic separable surfaces in 3d unsteady_flows can be performed interactively giving rise to new possibilities for gaining insight into complex flow phenomena gpus unsteady_flow_visualization feature_extraction streak_surface_generation
this paper introduces a new streamline_placement and selection algorithm for 3d vector_fields instead of considering the problem as a simple feature search in data space we base our work on the observation that most streamline fields generate a lot of self-occlusion which prevents proper visualization in order to avoid this issue we approach the problem in a view-dependent fashion and dynamically determine a set of streamlines which contributes to data understanding without cluttering the view since our technique couples flow characteristic criteria and view-dependent streamline selection we are able achieve the best of both worlds relevant flow description and intelligible uncluttered pictures we detail an efficient gpuimplementation of our algorithm show comprehensive visual results on multiple datasets and compare our method with existing flow depiction techniques our results show that our technique greatly improves the readability of streamline_visualizations on different datasets without requiring user intervention streamlines vector_fields view-dependent
in flow simulations the behavior and properties of particle trajectories often depend on the physical geometry contained in the simulated environment understanding the flow in and around the geometry itself is an important part of analyzing the data previous work has often utilized focus+context rendering techniques with an emphasis on showing trajectories while simplifying or illustratively rendering the physical areas our research instead emphasizes the local relationship between particle paths and geometry by using a projected multi-field_visualization_technique the correlation between a particle path and its surrounding area is calculated on-the-fly and displayed in a non-intrusive manner in addition we support visual_exploration and comparative_analysis through the use of linked information_visualization such as manipulatable curve plots and one-on-one similarity plots our technique is demonstrated on particle trajectories from a groundwater simulation and a computer room airflow simulation where the flow of particles is highly influenced by the dense geometry coordinated_linked_views flow_visualization focus+context_visualization multi-field_visualization
symmetric second-order tensor_fields play a central role in scientific and biomedical studies as well as in image_analysis and feature-extraction methods the utility of displaying tensor_field samples has driven the development of visualization_techniques that encode the tensor shape and orientation into the geometry of a tensor glyph with some exceptions these methods work only for positive-definite tensors ie having positive eigenvalues such as diffusion_tensors we expand the scope of tensor_glyphs to all symmetric second-order tensors in two and three dimensions gracefully and unambiguously depicting any combination of positive and negative eigenvalues we generalize a previous method of superquadric glyphs for positive-definite tensors by drawing upon a larger portion of the superquadric shape_space supplemented with a coloring that indicates the tensor's quadratic form we show that encoding arbitrary eigenvalue sign combinations requires design choices that differ fundamentally from those in previous work on traceless tensors arising in the study of liquid_crystals our method starts with a design of 2-d tensor_glyphs guided by principles of symmetry and continuity and creates 3-d glyphs that include the 2-d glyphs in their axis-aligned cross-sections a key ingredient of our method is a novel way of mapping from the shape_space of three-dimensional symmetric second-order tensors to the unit square we apply our new glyphs to stress_tensors from mechanics geometry_tensors and hessians from image_analysis and rate-of-deformation tensors in computational fluid dynamics geometry_tensors glyph_design rate-of-deformation tensors stress_tensors tensor_glyphs
we present tangeoms a tangible geospatial modeling visualization system that couples a laser scanner projector and a flexible physical three-dimensional model with a standard geospatial information system gis to create a tangible_user_interface for terrain data tangeoms projects an image of real-world data onto a physical terrain model users can alter the topography of the model by modifying the clay surface or placing additional objects on the surface the modified model is captured by an overhead laser scanner then imported into a gis for analysis and simulation of real-world processes the results are projected back onto the surface of the model providing feedback on the impact of the modifications on terrain parameters and simulated processes interaction with a physical model is highly intuitive allowing users to base initial design decisions on geospatial_data test the impact of these decisions in gis simulations and use the feedback to improve their design we demonstrate the system on three applications investigating runoff management within a watershed assessing the impact of storm surge on barrier islands and exploring landscape rehabilitation in military training areas collaborative_visualization geographic/geospatial_visualization human-computer_interaction tangible_user_interface terrain_visualization visualization system
we present the design and evaluation of fi3d a direct-touch data_exploration technique for 3d_visualization spaces the exploration of three-dimensional data is core to many tasks and domains involving scientific_visualizations thus effective data navigation techniques are essential to enable comprehension understanding and analysis of theinformation_space while evidence exists that touch can provide higher-bandwidth input somesthetic information that is valuable when interacting with virtual_worlds and awareness when working in collaboration scientific data_exploration in 3d poses unique challenges to the development of effective data manipulations we present a technique that provides touch interaction with 3d scientific data spaces in 7 dof this interaction does not require the presence of dedicated objects to constrain the mapping a design decision important for many scientific datasets such as particle_simulations in astronomy or physics we report on an evaluation that compares the technique to conventional mouse-based interaction our results show that touch interaction is competitive in interaction speed for translation and integrated interaction is easy to learn and use and is preferred for exploration and wayfinding tasks to further explore the applicability of our basic technique for other types of scientific_visualizations we present a second case_study adjusting the interaction to the illustrative_visualization of fiber tracts of the brain and the manipulation of cutting_planes in this context d navigation and exploration direct-touch_interaction evaluation illustrative_visualization wall_displays
we present the first distributed paradigm for multiple users to interact simultaneously with large tiled rear projection display walls unlike earlier works our paradigm allows easy scalability across different applications interaction modalities displays and users the novelty of the design lies in its distributed nature allowing well-compartmented application independent and application specific modules this enables adapting to different 2d applications and interaction modalities easily by changing a few application specific modules we demonstrate four challenging 2d applications on a nine projector display to demonstrate the application scalability of our method map visualization virtual graffiti virtual bulletin board and an emergency management system we demonstrate the scalability of our method to multiple interaction modalities by showing both gesture-based and laser-based user_interfaces finally we improve earlier distributed methods to register multiple projectors previous works need multiple patterns to identify the neighbors the configuration of the display and the registration across multiple projectors in logarithmic time with respect to the number of projectors in the display we propose a new approach that achieves this using a single pattern based on specially augmented qr codes in constant time further previous distributed registration algorithms are prone to large misregistrations we propose a novel radially cascading geometric registration technique that yields significantly betteraccuracy thus our improvements allow a significantly more efficient and accurate technique for distributed self-registration of multi-projector display walls distributed_algorithms gesture-based_interaction human-computer_interaction multi-user_interaction tiled_displays
many visualization_applications benefit from displaying content on real-world objects rather than on a traditional display eg a monitor this type of visualization display is achieved by projecting precisely controlled illumination from multiple projectors onto the real-world colored objects for such a task the placement of the projectors is critical in assuring that the desired visualization is possible using ad hoc projector placement may cause some appearances to suffer from color shifting due to insufficient projector light radiance being exposed onto the physical surface this leads to an incorrect appearance and ultimately to a false and potentially misleading visualization in this paper we present a framework to discover the optimal position and orientation of the projectors for such projection-based visualization displays an optimal projector placement should be able to achieve the desired visualization with minimal projector light radiance when determining optimal projector placement object visibility surface reflectance properties and projector-surface distance and orientation need to be considered we first formalize a theory for appearance editing image formation and construct a constrained linear system of equations that express when a desired novel appearance or visualization is possible given a geometric and surface reflectance model of the physical surface then we show how to apply this constrained system in an adaptive search to efficiently discover the optimal projector placement which achieves the desired appearance constraints can be imposed on the maximum radiance allowed by the projectors and the projectors' placement to support specific goals of various visualization_applications we perform several real-world and simulated appearance edits and visualizations to demonstrate the improvement obtained by our discovered projector placement over ad hoc projector placement interaction_design mobile_and_ubiquitous_visualization large_and_high-resolution_displays
every year since the article “how will big pictures emerge from a sea of biological_data” appeared in science the question becomes more compelling we are now accumulating information about biological sequences structures and interactions faster than we have the power to make sense of them for hundreds of years prior to this practical considerations coerced biological research into reductionism there are simply too many components in a biological system for a biologist to examine the whole picture with the tools formerly available over the past decade this has rapidly changed as biological information has become cheap and plentiful due to the advent of high-throughput tools making it possible for the frst time to ask questions on time and length scales that were previously intractable the relaxation of the practical limitations on systems-level analysis has also brought a change in the philosophy of how we regard biology moving towards a holistic method of research and interpretation this places systems biology in stark contrast to traditional biological research and for good reason in the words of denis noble “systems biology is about putting together rather than taking apart integration rather than reduction it starts with what we have learned from the reductionist approach and then it goes further” this shift from reductionism is essential for as we know from studying complex systems the whole is greater than the sum of the parts with this new approach we are able to explore scientifc territory that has previously been untouched due to physical impossibility and philosophical differences the complexity of the tangled web of nonlinear interactions between genes proteins and the environment necessitates the development of simplifed models to illuminate biological functions merely generating networks of interactions is not enough providing us with far too much information in a single view without emphasizin- the important features of the map when we use google maps and look at a picture of the united states it doesn't show us every city we would never see evanston illinois being shown at that level of detail only large and recognizable cities are shown to help us orient the map once we zoom in other smaller cities and features become visible giving us more relevant information in a manner that is usable simply generating networks without any type of analysis or visualization is akin to showing a map of the united states with every state city and town marked on it in my talk i will describe the advances we have made in developing new visualization methods and the challenges still remaining
dimstiller is a system for dimensionality_reduction and analysis it frames the task of understanding and transforming input dimensions as a series of analysis steps where users transform data tables by chaining together different techniques called operators into pipelines of expressions the individual operators have controls and views that are linked together based on the structure of the expression users interact with the operator controls to tune parameter choices with immediate visual feedback guiding the exploration of local neighborhoods of the space of possible data tables dimstiller also provides global guidance for navigating data-table space through expression templates called workflows which permit re-use of common patterns of analysis
in risk_assessment applications well informed decisions are made based on huge amounts of multi-dimensional_data in many domains not only the risk of a wrong decision but in particular the trade-off between the costs of possible decisions are of utmost importance in this paper we describe a framework tightly integrating interactive_visual_exploration with machine_learning to support the decision_making process the proposed approach uses a series of interactive 2d visualizations of numeric and ordinal data combined with visualization of classification models these series of visual elements are further linked to the classifier's performance visualized using an interactive performance curve an interactive decision point on the performance curve allows the decision maker to steer the classification model and instantly identify the critical cost changing data elements in the various linked visualizations the critical data elements are represented as images in order to trigger associations related to the knowledge of the expert in this context the data visualization and classification results are not only linked together but are also linked back to the classification model such a visual_analytics framework allows the user to interactively explore the costs of his decisions for different settings of the model and accordingly use the most suitable classification model and make more informed and reliable decisions a case_study on data from the forensic psychiatry domain reveals the usefulness of the suggested approach classification decision_boundary_visualization interactive_visual_exploration multi-dimensional_space visual_analytics
modern visualization methods are needed to cope with very high-dimensional_data efficient visual analytical techniques are required to extract the information content in these data the large number of possible projections for each method which usually grow quadrat-ically or even exponentially with the number of dimensions urges the necessity to employ automatic reduction techniques automatic sorting or selecting the projections based on their information-bearing content different quality measures have been successfully applied for several specified user tasks and established visualization_techniques like scatterplots scatterplot matrices or parallel_coordinates many other popular visualization_techniques exist but due to the structural differences the measures are not directly applicable to them and new approaches are needed in this paper we propose new quality measures for three popular visualization methods radviz pixel-oriented displays and table lenses our experiments show that these measures efficiently guide the visual_analysis task h information storage and retrieval information search and retrieval i computer_graphics picture/image generation
we present an interactive visual_analytics system for classification ivisclassifier based on a supervised dimension reduction method linear_discriminant_analysis lda given high-dimensional_data and associated cluster labels lda gives their reduced dimensional representation which provides a good overview about the cluster structure instead of a single two- or three-dimensional scatter plot ivisclassifier fully interacts with all the reduced dimensions obtained by lda through parallel_coordinates and a scatter plot furthermore it significantly improves the interactivity and interpretability of lda lda enables users to understand each of the reduced dimensions and how they influence the data by reconstructing the basis vector into the original data domain by using heat maps ivisclassifier gives an overview about the cluster relationship in terms of pairwise distances between cluster centroids both in the original space and in the reduced dimensional space equipped with these functionalities ivisclassifier supports users' classification tasks in an efficient way using several facial image data we show how the above analysis is performed h information interfaces and presentation user_interfaces-theory_and_methods
data sets in astronomy are growing to enormous sizes modern astronomical surveys provide not only image data but also catalogues of millions of objects stars galaxies each object with hundreds of associated parameters exploration of this very high-dimensional_data space poses a huge challenge subspace clustering is one among several approaches which have been proposed for this purpose in recent years however many clustering algorithms require the user to set a large number of parameters without any guidelines some methods also do not provide a concise summary of the datasets or if they do they lack additional important information such as the number of clusters present or the significance of the clusters in this paper we propose a method for ranking subspaces for clustering which overcomes many of the above limitations first we carry out a transformation from parametric space to discrete image space where the data are represented by a grid-based density field then we apply so-called connected_morphological_operators on this density field of astronomical objects that provides visual support for the analysis of the important subspaces clusters in subspaces correspond to high-intensity regions in the density image the importance of a cluster is measured by a new quality criterion based on the dynamics of local maxima of the density connected operators are able to extract such regions with an indication of the number of clusters present the subspaces are visualized during computation of the quality measure so that the user can interact with the system to improve the results in the result stage we use three visualization toolkits linked within a graphical user interface so that the user can perform an in-depth exploration of the ranked subspaces evaluation based on synthetic as well as real astronomical datasets demonstrates the power of the new method we recover various known astronomical relations directly from the data with little or no a pri- - ori assumptions hence our method holds good prospects for discovering new relations as well subspace_finding astronomical data_clustering_high-dimensional_data connected_morphological_operators visual_exploration
visualization of multi-dimensional_data is challenging due to the number of complex correlations that may be present in the data but that are difficult to be visually identified one of the main causes for this problem is the inherent loss of information that occurs when high-dimensional_data is projected into 2d or 3d although 2d scatterplots are ubiquitous due to their simplicity and familiarity there are not a lot of variations on their basic metaphor in this paper we present a new way of visualizing multidimensional data using scatterplots we extend 2d scatterplots using sensitivity coefficients to highlight local variation of one variable with respect to another when applied to a scatterplot these sensitivities can be understood as velocities and the resulting visualization resembles a flow_field we also present a number of operations based on flow-field_analysis that help users navigate select and cluster points in an efficient manner we show the flexibility and generality of this approach using a number of multidimensional data sets across different domains data_transformations model_fitting principal_component_analysis uncertainty
modern machine_learning techniques provide robust approaches for data-driven modeling and critical information extraction while human experts hold the advantage of possessing high-level intelligence and domain-specific expertise we combine the power of the two for anomaly_detection in gps data by integrating them through a visualization and human-computer_interaction interface in this paper we introduce gpsvas gps visual_analytics system a system that detects anomalies in gps data using the approach of visual_analytics a conditional random field crf model is used as the machine_learning component for anomaly_detection in streaming gps traces a visualization component and an interactive user interface are built to visualize the data_stream display significant analysis results ie anomalies or uncertain predications and hidden information extracted by the anomaly_detection model which enable human experts to observe the real-time data behavior and gain insights into the data flow human experts further provide guidance to the machine_learning model through the interaction tools the learning model is then incrementally improved through an active_learning procedure feature evaluation and selection h models and principles user/machine_systems-human information processing h information interfaces and presentation user_interfaces-graphics user_interfaces i pattern_recognition design methodology-pattern analysis
events that happened in the past are important for understanding the ongoing processes predicting future developments and making informed decisions significant and/or interesting events tend to attract many people some people leave traces of their attendance in the form of computer-processable data such as records in the databases of mobile phone operators or photos on photo sharing web sites we developed a suite of visual_analytics methods for reconstructing past events from these activity traces our tools combine geocomputations interactive geo visualizations and statistical methods to enable integrated analysis of the spatial temporal and thematic components of the data including numeric attributes and texts we demonstrate the utility of our approach on two large real data sets mobile phone calls in milano during 9 days and flickr photos made on british isles during 5 years event_detection geo visualization scalable_visualization spatio-temporal_data time_series analysis
the process of learning models from raw data typically requires a substantial amount of user input during the model initialization phase we present an assistive visualization system which greatly reduces the load on the users and makes the process of model initialization and refinement more efficient problem-driven and engaging utilizing a sequence segmentation task with a hidden markov model as an example we assign each token in the sequence a feature vector based on its various properties within the sequence these vectors are then clustered according to similarity generating a layout of the individual tokens in form of a node link diagram where the length of the links is determined by the feature vector similarity users may then tune the weights of the feature vector components to improve the segmentation which is visualized as a better separation of the clusters also as individual clusters represent different classes the user can now work at the cluster level to define token classes instead of labelling one entry at time inconsistent entries visually identify themselves by locating at the periphery of clusters and the user then helps refine the model by resolving these inconsistencies our system therefore makes efficient use of the knowledge of its users only requesting user assistance for non-trivial data items it so allows users to visually analyse data at a higher more abstract level improving scalability data_clustering human-computer_interaction visual_knowledge_discovery visual_knowledge_representation
visual_exploration and analysis is a process of discovering and dissecting the abundant and complex attribute relationships that pervade multidimensional data recent research has identified and characterized patterns of multiple_coordinated_views such as cross-filtered views in which rapid sequences of simple interactions can be used to express queries on subsets of attribute values in visualizations designed around these patterns for the most part distinct views serve to visually isolate each attribute from the others although the brush-and-click simplicity of visual isolation facilitates discovery of many-to-many relationships between attributes dissecting these relationships into more fine-grained one-to-many relationships is interactively tedious and worse visually fragmented over prolonged sequences of queries this paper describes 1 a method for interactively dissecting multidimensional data by iteratively slicing and manipulating a multigraph representation of data values and value co-occurrences and 2 design strategies for extending the construction of coordinated multiple view interfaces for dissection as well as discovery of attribute relationships in multidimensional data sets using examples from different domains we describe how attribute_relationship_graphs can be combined with cross-filtered views modularized for reuse across designs and integrated into broader visual_analysis tools the exploratory and analytic utility of these examples suggests that an attribute relationship graph would be a useful addition to a wide variety of visual_analysis tools d software_engineering design tools and techniques-user_interfaces h information systems database management-languages h information systems information interfaces and presentation-user_interfaces
the massive amount of financial time_series_data that originates from the stock market generates large amounts of complex data of high interest however adequate solutions that can effectively handle the information in order to gain insight and to understand the market mechanisms are rare in this paper we present two techniques and applications that enable the user to interactively analyze large amounts of time_series_data in real-time in order to get insight into the development of assets market sectors countries and the financial market as a whole the first technique allows users to quickly analyze combinations of single assets market sectors as well as countries compare them to each other and to visually discover the periods of time where market sectors and countries get into turbulence the second application clusters a selection of large amounts of financial time_series_data according to their similarity and analyzes the distribution of the assets among market sectors this allows users to identify the characteristic graphs which are representative for the development of a particular market sector and also to identify the assets which behave considerably differently compared to other assets in the same sector both applications allow the user to perform investigative exploration techniques and interactive_visual_analysis in real-time explorative_analysis financial information_visualization time_series_clustering time_series_data visual_analytics
during the last decades electronic textual information has become the world's largest and most important information source daily newspapers books scientific and governmental publications blogs and private messages have grown into a wellspring of endless information and knowledge since neither existing nor new information can be read in its entirety we rely increasingly on computers to extract and visualize meaningful or interesting topics and documents from this huge information reservoir in this paper we extend improve and combine existing individual approaches into an overall framework that supports topologi-cal analysis of high dimensional document point clouds given by the well-known tf-idf document-term weighting method we show that traditional distance-based approaches fail in very high dimensional spaces and we describe an improved two-stage method for topology-based projections from the original high dimensionalinformation_space to both two dimensional 2-d and three dimensional 3-d visualizations to demonstrate theaccuracy and usability of this framework we compare it to methods introduced recently and apply it to complex document and patent collections h information interfaces and presentation user_interfaces-theory_and_methods i pattern_recognition clustering-algorithms
text_visualization becomes an increasingly more important research topic as the need to understand massive-scale textual information is proven to be imperative for many people and businesses however it is still very challenging to design effective visual metaphors to represent large corpora of text due to the unstructured and high-dimensional nature of text in this paper we propose a data model that can be used to represent most of the text corpora such a data model contains four basic types of facets time category content unstructured and structured facet to understand the corpus with such a data model we develop a hybrid_visualization by combining the trend graph with tag-clouds we encode the four types of data facets with four separate visual dimensions to help people discover evolutionary and correlation patterns we also develop several visual_interaction methods that allow people to interactively analyze text by one or more facets finally we present two case studies to demonstrate the effectiveness of our solution in support of multi-faceted visual_analysis of text corpora multi-facet data visualization text_visualization
in this paper we present a new web-based visual_analytics system vizcept which is designed to support fluid collaborative analysis of large textual intelligence datasets the main approach of the design is to combine individual workspace and shared visualization in an integrated environment collaborating analysts will be able to identify concepts and relationships from the dataset based on keyword searches in their own workspace and collaborate visually with other analysts using visualization tools such as a concept map view and a timeline view the system allows analysts to parallelize the work by dividing initial sets of concepts investigating them on their own workspace and then integrating individual findings automatically on shared visualizations with support for interaction and personal graph_layout in real time in order to develop a unified plot we highlight several design considerations that promotecommunication and analytic performance in small team synchronous collaboration we report the result of a pair of case_study applications including collaboration andcommunication methods analysis strategies and user behaviors under a competition setting in the same location at the same time the results of these demonstrate the tool's effectiveness for synchronous collaborative construction and use of visualizations in intelligence data_analysis collaborative_visualization intelligence_analysis text and document data
journalists increasingly turn to social media sources such as facebook or twitter to support their coverage of various news events for large-scale events such as televised debates and speeches the amount of content on social media can easily become overwhelming yet still contain information that may aid and augment reporting via individual content items as well as via aggregate information from the crowd's response in this work we present a visual analytic tool vox civitas designed to help journalists and media professionals extract news value from large-scale aggregations of social media content around broadcast events we discuss the design of the tool present the text_analysis techniques used to enable the presentation and provide details on the visual and interaction_design we provide an exploratory evaluation based on a user_study in which journalists interacted with the system to explore and report on a dataset of over one hundred thousand twitter messages collected during the us state of the union presidential address in 2010 computational_journalism computer_assisted_reporting sensemaking social media
we present a tool that is specifically designed to support a writer in revising a draft-version of a document in addition to showing which paragraphs and sentences are difficult to read and understand we assist the reader in understanding why this is the case this requires features that are expressive predictors of readability and are also semantically understandable in the first part of the paper we therefore discuss a semi-automatic feature_selection approach that is used to choose appropriate measures from a collection of 141 candidate readability features in the second part we present the visual_analysis tool visra which allows the user to analyze the feature values across the text and within single sentences the user can choose different visual representations accounting for differences in the size of the documents and the availability of information about the physical and logical layout of the documents we put special emphasis on providing as much transparency as possible to ensure that the user can purposefully improve the readability of a sentence several case-studies are presented that show the wide range of applicability of our tool i pattern_recognition design methodology-feature evaluation and selection i document and text_processing document capture-document_analysis
diagnosing faults in an operational computer network is a frustrating time-consuming exercise despite advances automatic diagnostic tools are far from perfect they occasionally miss the true culprit and are mostly only good at narrowing down the search to a few potential culprits this uncertainty and the inability to extract useful sense from tool output renders most tools not usable to administrators to bridge this gap we present netclinic a visual_analytics system that couples interactive_visualization with an automated diagnostic tool for enterprise networks it enables administrators to verify the output of the automatic analysis at different levels of detail and to move seamlessly across levels while retaining appropriate context a qualitative user_study shows that netclinic users can accurately identify the culprit even when it is not present in the suggestions made by the automated component we also find that supporting a variety of sensemaking strategies is a key to the success of systems that enhance automated diagnosis information_visualization network diagnosis semantic_graph_layout sensemaking visual_analytics
information_foraging and sensemaking with heterogeneous information are context-dependent activities thus visual_analytics tools to support these activities must incorporate context but context is a difficult concept to define model and represent creating and representing context in support of visually-enabled reasoning about complex problems with complex information is a complementary but different challenge than that addressed in context-aware computing in the latter the goal is automated adaptation of the system to meet user needs for applications such as mobile location-based services where information about the location the user and the user goals filters what gets presented on a small mobile device in contrast for visual_analytics-enabled information_foraging and sensemaking the user is likely to take an active role in foraging for the contextual information needed to support sensemaking in relation to some multifaceted problem in this paper we address the challenges of constructing and representing context within visual interfaces that support analytical reasoning in crisis_management and humanitarian relief the challenges stem from the diverse forms of information that can provide context and difficulty in defining and operationalizing context itself here we pay particular attention to document foraging to support construction of the geographic and historical context within which monitoring and sensemaking can be carried out specifically we present the concept of geo-historical context ghc and outline an empirical assessment of both the concept and itsimplementation in the context discovery application a web-based tool that supports document foraging and sensemaking context foraging geographic_information_retrieval mapping sensemaking text_analysis
wikipedia has been built to gather encyclopedic knowledge using a collaborative social process that has proved its effectiveness however the workload required for raising the quality and increasing the coverage of wikipedia is exhausting the community based on several participatory design sessions with active wikipedia contributors aka wikipedians we have collected a set of measures related to wikipedia activity that if available and visualized effectively could spare a lot of monitoring time to these wikipedians allowing them to focus on quality and coverage of wikipedia instead of spending their time navigating heavily to track vandals and copyright infringements however most of these measures cannot be computed on the fly using the available wikipedia api therefore we have designed an open architecture called wikireactive to compute incrementally and maintain several aggregated measures on the french wikipedia this aggregated_data is available as a web service and can be used to overlay information on wikipedia articles through wikipedia skins or for new services for wikipedians or people studying wikipedia this article describes the architecture its performance and some of its uses database management h logical design-schema and subschema database management h system-query_processing information storage and retrieval h online information services-web-based services
insight externalization ie refers to the process of capturing and recording the semantics of insights in decision_making and problem_solving to reduce human effort automated insight externalization aie is desired most existing ie approaches achieve automation by capturing events eg clicks and key presses or actions eg panning and zooming in this paper we propose a novel aie approach named click2annotate it allows semi-automatic insight annotation that captures low-level analytics task results eg clusters andoutliers which have higher semantic richness and abstraction levels than actions and events click2annotate has two significant benefits first it reduces human effort required in ie and generates annotations easy to understand second the rich semantic information encoded in the annotations enables various insight_management activities such as insight browsing and insight retrieval we present a formal user_study that proved this first benefit we also illustrate the second benefit by presenting the novel insight_management activities we developed based on click2annotate namely scented insight browsing and faceted insight search annotation decision_making insight_management multidimensional visualization visual_analytics
finding patterns in temporal_data is an important data_analysis task in many domains static visualizations can help users easily see certain instances of patterns but are not specially designed to support systematic analysis tasks such as finding all instances of a pattern automatically vizpattern is an interactive visual query environment that uses a comic strip metaphor to enable users to easily and quickly define and locate complex temporal patterns evaluations provide evidence that vizpattern is applicable in many domains and that it enables a wide variety of users to answer questions about temporal_data faster and with fewer errors than existing state-of-the-art visual_analysis systems h information systems information search and retrieval-query formulation h information interfaces and presentation user interface-user-centered_design
this paper highlights the important role that record-keeping ie taking notes and saving charts plays in collaborative data_analysis within the business domain the discussion of record-keeping is based on observations from a user_study in which co-located teams worked on collaborative visual_analytics tasks using large interactive wall and tabletop displays part of our findings is a collaborative data_analysis framework that encompasses note_taking as one of the main activities we observed that record-keeping was a critical activity within the analysis process based on our observations we characterize notes according to their content scope and usage and describe how they fit into a process of collaborative data_analysis we then discuss suggestions for the design of collaborative visual_analytics tools collaboration history note_taking provenance recording tabletop wall display
co-located collaboration can be extremely valuable during complex visual_analytics tasks this paper presents an exploratory study of a system designed to support collaborative visual_analysis tasks on a digital tabletop display fifteen participant pairs employed cam-biera a visual_analytics system to solve a problem involving 240 digital documents our analysis supported by observations system logs questionnaires and interview data explores how pairs approached the problem around the table we contribute a unique rich understanding of how users worked together around the table and identify eight types of collaboration styles that can be used to identify how closely people work together while problem_solving we show how the closeness of teams' collaboration influenced how well they performed on the task overall we further discuss the role of the tabletop for visual_analytics tasks and derive novel design implications for future co-located collaborative tabletop problem_solving systems k information interfaces and presentation group and organization interfaces
the final product of an analyst's investigation using a visualization is often a report of the discovered knowledge as well as the methods employed and reasoning behind the discovery we believe that analysts may have difficulty keeping track of their knowledge_discovery process and will require tools to assist in accurately recovering their reasoning we first report on a study examining analysts' recall of their strategies and methods demonstrating their lack of memory of the path of knowledge_discovery we then explore whether a tool visualizing the steps of the visual_analysis can aid users in recalling their reasoning_process the results of our second study indicate that visualizations of interaction logs can serve as an effective memory aid allowing analysts to recall additional details of their strategies and decisions visual_analytics reasoning_process_visualization
interaction and manual manipulation have been shown in the cognitive science literature to play a critical role in problem_solving given different types of interactions or constraints on interactions a problem can appear to have different degrees of difficulty while this relationship between interaction and problem_solving has been well studied in the cognitive science literatures the visual_analytics community has yet to exploit this understanding for analytical problem_solving in this paper we hypothesize that constraints on interactions and constraints encoded in visual representations can lead to strategies of varying effectiveness during problem_solving to test our hypothesis we conducted a user_study in which participants were given different levels of interaction constraints when solving a simple math game called number scrabble number scrabble is known to have an optimal visual problem isomorph and the goal of this study is to learn if and how the participants could derive the isomorph and to analyze the strategies that the participants utilize in solving the problem our results indicate that constraints on interactions do affect problem_solving and that while the optimal visual_isomorph is difficult to derive certain interaction constraints can lead to a higher chance of deriving the isomorph interaction problem_solving visual_isomorph
these current studies explored the impact of individual_differences in personalityfactors on interface interaction and learning performance behaviors in both an interactive_visualization and a menu-driven web table in two studies participants were administered 3 psychometric measures designed to assess locus of control extraversion and neuroticism participants were then asked to complete multiple procedural learning tasks in each interface results demonstrated that all three measures predicted completion times additionally results analyses demonstrated personalityfactors also predicted the number of insights participants reported while completing the tasks in each interface we discuss how these findings advance our ongoing research in the personal equation of interaction cognition and perception_theory embodied_cognition visual_analytics visualization_taxonomies_and_models
we propose a set of techniques that support visual interpretation of trajectory clusters by transforming absolute time references into relative positions within temporal cycles or with respect to the starting and/or ending times of the trajectories we demonstrate the work of the approach on a real data set about individual movement over one year
optimization problems are typically addressed by purely automatic approaches for multi-objective problems however a single best solution often does not exist in this case it is necessary to analyze trade-offs between many conflicting goals within a given application context this poster describes an approach that tightly integrates automatic algorithms for multi-objective optimization and interactive multivariate visualizations ad-hoc selections support a flexible definition of input data for subsequent algorithms these algorithms in turn represent their result as derived data attributes that can be assigned to visualizations or be used as a basis for further selections eg to constrain the result set this enables a guided search that still involves the knowledge of domain experts we describe our approach in the context of multi-run simulation data from the application domain of car engine design h models and principles user/machine_systems-human_factors h information interfaces and presentation user_interfaces-interaction styles
the self-organizing map som algorithm is a popular and widely used cluster algorithm its constraint to organize clusters on a grid structure makes it very amenable to visualization on the other hand the grid constraint may lead to reduced clusteraccuracy and reliability compared to other clustering methods not implementing this restriction we propose a visual cluster analysis system that allows to validate the output of the som algorithm by comparison with alternative_clustering methods specifically visual_mappings overlaying alternative_clustering results onto the som are proposed we apply our system on an example data set and outline main analytical use cases h information systems information systems applications i computing methodologies methodology and techniques
in command and control c2 environments decision makers must rapidly understand and address key temporal relationships that exist between critical tasks as conditions fluctuate however traditional temporal displays such as mission timelines fail to support user understanding of and reasoning about critical relationships we have developed visualization methods to compactly and effectively convey key temporal constraints in this paper we present examples of our visualization approach and describe how we are exploring interaction methods within an integrated visualization workspace to support user awareness of temporal constraints temporal relationships temporal_visualization
advanced battlespace network_visualization_techniques are required within the modern air operations center aoc to improve cross-domain situation_awareness and to support planning and decision-making we present a visualization toolkit to address this need that supports the integration of network health and status information and meta-information with other traditional aoc information resources and activities across air space and cyber domains applications include the development of battlespace visualization technologies that will improve warfighters' decision-making response time and provide enhanced flexibility for mission planning by efficiently revealing affordances for leveraging disrupting or enhancing network connectivity
in this paper we introduce alida an active_learning intent discerning agent for visual_analytics interfaces as users interact with and explore data in a visual_analytics environment they are each developing their own unique analytic process the goal of alida is to observe and record the human-computer_interactions and utilize these observations as a means of supporting user exploration alida does this by using interaction to make decision about user interest as such alida is designed to track the decision history interactions of a user this history is then utilized to enhance the user's decision-making process by allowing the user to return to previously visited search states as well as providing suggestions of other search states that may be of interest based on past exploration modalities the agent passes these suggestions or decisions back to an interactive_visualization prototype and these suggestions are used to guide the user either by suggesting searches or changes to the visualization view current work has tested alida under the exploration of homonyms for users wishing to explore word linkages within a dictionary ongoing work includes using alida to guide users in transfer_function_design for volume_rendering within scientific gateways artificial intelligence cognition intent discernment volume_rendering
we created a visual chat application for use during hazardous weather events the application nwschat2 allows national weather service forecasters media members and storm trackers to communicate with each other basing their conversation on a common shared radar map of the storm users can additionally annotate the map with `pins' or draw notes with a stylus these annotations are automatically shared with all other users the collaborative nature of nwschat2 makes it well-suited for disseminating information to all users during weather emergencies collaboration coordinated multiple_views emergency_response hazardous weather instant messaging
the detection of previously unknown frequently occurring patterns in time_series often called motifs has been recognized as an important task to find these motifs we use an advanced temporal_data_mining algorithm since our algorithm usually finds hundreds of motifs we need to analyze and access the discovered motifs for this purpose we introduce three novel visual_analytics methods 1 motif layout using colored rectangles for visualizing the occurrences and hierarchical relationships of motifs in a multivariate time_series 2 motif distortion for enlarging or shrinking motifs as appropriate for easy analysis and 3 motif merging to combine a number of identical adjacent motif instances without cluttering the display we have applied and evaluated our methods using two real-world data sets data center cooling and oil well production
data sets that contain geospatial and temporal elements can be challenging to analyze in particular it can be difficult to determine how the data have changed over spatial and temporal ranges in this poster we present a visual approach for representing the pair-wise differences between geographically and temporally binned data in addition to providing a novel method for visualizing such geo-temporal differences gtdiff provides a high degree of interactivity that supports the exploration and analysis of the data h information systems information interfaces and presentation-user_interfaces i computing methodologies computer_graphics-methodology and techniques
since its inception the field of visual_analytics has undergone tremendous growth in understanding how to create interactive visual tools to solve analytical problems however with few exceptions most of these tools have been designed for single users in desktop environments while often effective on their own most single-user systems do not reflect the collaborative nature of solving real-world analytical tasks many intelligence analysts for example have been observed to switch repeatedly between working alone and collaborating with members of a small team in this paper we propose that a complete visual analytical system designed for solving real-world tasks ought to have two integrated components a single-user desktop system and a mirroring system suitable for a collaborative environment
although the discovery and analysis ofcommunication patterns in large and complex email datasets are difficult tasks they can be a valuable source of information this paper presents emailtime's capabilities through several examples emailtime is a visual_analysis of email correspondence patterns over the course of time that interactively portrays personal and interpersonal networks using the correspondence in the email dataset we suggest that integrating both statistics and visualizations in order to display information about the email datasets may simplify its evaluation email email correspondents emailtime enron visual_analysis
this paper presents a case_study with enron email dataset to explore the behaviors of email users within different organizational positions we defined email behavior as the email activity level of people regarding a series of measured metrics eg sent and received emails numbers of email addresses etc these metrics were calculated through emailtime a visual_analysis tool of email correspondence over the course of time results showed specific patterns in the email datasets of different organizational positions case_study email emailtime enron visual_analysis
we present a unified framework for data processing mining and interactive_visualization of large-scale neuroanatomical databases the input data is assumed to lie in a specific atlas space or simply exist as a separate collection users can specify their own atlas for comparative analyses the original data exist as mri images in standard formats it is uploaded to a remote server and processed offline by a parallelized pipeline workflow this workflow transforms the data to represent it as both volumetric and triangular_mesh cortical surfaces we use multiresolution representations to scale complexity to data storage availability as well as graphical processing performance our workflow implements predefined metrics for clustering and classification and data projection schemes to aid in visualization additionally the system provides a visual query interface for performing selection requests based on user-defined search criteria i computer_graphics three-dimensional graphics and realism i viewing algorithms i applications
we present an iterative strategy for finding a relevant subset of attributes for the purpose of classification in high-dimensional heterogeneous data sets the attribute subset is used for the construction of a classifier function in order to cope with the challenge of scalability the analysis is split into an overview of all attributes and a detailed analysis of small groups of attributes the overview provides generic information on statistical dependencies between attributes with this information the user can select groups of attributes and an analytical method for their detailed analysis the detailed analysis involves the identification of redundant attributes via classification or regression and the creation of summarizing attributes via clustering or dimension reduction our strategy does not prescribe specific analytical methods instead we recursively combine the results of different methods to find or generate a subset of attributes to use for classification data_clustering data_filtering dimensionality_reduction high-dimensional_data
this paper presents an interactive interface synchronized with a simulation framework for exploring complex scenarios this interface exploits visual_analysis for facilitating the understanding of complex situation by human users d/ animation information_visualization interaction line & surface graph animation synchronization
geometric wavelets is a new multi-scale data representation technique which is useful for a variety of applications such as data_compression interpretation and anomaly_detection we have developed an interactive_visualization with multiple linked_views to help users quickly explore data sets and understand this novel construction currently the interface is being used by applied mathematicians to view results and gain new insights speeding methods development h information interfaces and presentation user_interfaces-graphical user_interfaces (gui) i pattern_recognition models-geometric
complex combinations of coordinated multiple_views are increasingly used to design tools for highly interactive_visual_exploration_and_analysis of multidimensional data while complex coordination patterns provide substantial utility through expressive querying they also exhibit usability problems for users when learning required interaction sequences recalling past queries and interpreting visual states as visual_analysis tools grow more sophisticated there is a growing need to make them more understandable as well our long-term goal is to exploit natural language familiarity and literacy to directly facilitate individual and collaborative use of visual_analysis tools in this poster we present work in progress on an automatically generated query-to-question user interface to translate interactive states during visual_analysis into an accompanying visual log of formatted text our effort currently focuses on a symmetric and thus relatively simple coordination pattern cross-filtered views we describe our current thinking about query-to-question translation in a typical cross-filtered visualization of movies people and genres in the internet movie database coordinated multiple_views cross-filtered queries interaction states natural language generation visual provenance
we present a custom visual_analytics system developed in conjunction with the test and evaluation community of the us army we designed and implemented a visual programming environment for configuring a variety of interactive_visual_analysis capabilities our abstraction of the visualization process is based on insights gained from interviews conducted with expert users we show that this model allowed analysts to implement multiple visual_analysis capabilities for network performance anomalous sensor activity and engagement results long-term interaction with expert users led to development of several custom visual_analysis techniques we have conducted training sessions with expert users and are working to evaluate the success of our work based on performance metrics captured in a semi-automated fashion during these training sessions we have also integrated collaborative analysis features such as annotations and shared content visualization_system_and_toolkit_design
predicting protein structures has long been a grand-challenge problem fine-grained computational simulation of folding events from a protein's synthesis to its final stable structure remains computationally intractable therefore methods which derive constraints from other sources are attractive to date constraints derived from known structures have proven to be highly successful however these cannot be applied to molecules with no identifiable neighbors having already-determined structures for such molecules structural constraints must be derived in other ways one popular approach has been the statistical_analysis of large families of proteins with the hope that residues that “change together” co-evolve imply that those residues are in contact unfortunately despite repeated attempts to use this data to deduce structural constraints this approach has met with minimal success the consensus of current literature concludes that there is simply too little information contained within the correlated mutations of many protein families to reliably and generally predict structural constraints recent work in my laboratory challenges this conclusion for some time we have been developing methods mavl/stickwrld to visualize the pattern of co-evolved mutations within sequence families while our analysis of individual correlations agrees with the literature consensus we have recently discovered that the visualized pattern of correlations is highly suggestive of structural relationships in our preliminary test cases human researchers can unambiguously determine many positive structural constraints by visual_analysis of statistical sequence information alone often with no training on interpretation of the visualization results herein we report the visualization design that supports this visual_analytics approach to identifying high-confidence hypotheses about protein folding from protein sequence and illustrate preliminary results from th- - is research our approach entails a higher-dimensional extension of parallel_coordinates which illuminates distant shared sub-tuples of the vectors representing each protein sequence when these sub-tuples occur with an over abundance compared to expectations it simultaneously eliminates all representations of tuples which occur with frequency near the expected norm the result is a minimally-occluded representation of outlier and only outlier co-occurrences within the sequence families bioinformatics clustering classification and association rules interactive data_exploration
the visual_analysis of video content is an important research topic due to the huge amount of video data that is generated every day annotating this data will become a major problem since the amount of videos further increases with this work we introduce a system that combines a visualization tool with automatic video segmentation techniques and a characteristic key-frame extraction a summary of the content of a whole video in one view is realized furthermore the user can interactively browse through the video via our visualization_interface to get more detailed information the system is adapted to two application scenarios and a third application is discussed for future work
this poster describes our progress in developing an interactive linear modeling system that supports the modeling approach described by daniel and wood our visual interface permits analysts to build sets of possible models and then creates appropriate visualizations to permit human-in-the-loop model comparison and selection h information interfaces and presentation user_interfaces-graphical user_interfaces
the field of visualization has addressed navigation of very large_datasets usually meshes and volumes significantly less attention has been devoted to the issues surrounding navigation of very large images in the last few years the explosive growth in the resolution of camera sensors and robotic image acquisition techniques has widened the gap between the display and image resolutions to three orders of magnitude or more this paper presents the first steps towards navigation of very large images particularly landscape images from an interactive_visualization perspective the grand challenge in navigation of very large images is identifying regions of potential interest in this paper we outline a three-step approach in the first step we use multi-scale saliency to narrow down the potential areas of interest in the second step we outline a method based on statistical signatures to further cull out regions of high conformity in the final step we allow a user to interactively identify the exceptional regions of high interest that merit further attention we show that our approach of progressive elicitation is fast and allows rapid identification of regions of interest unlike previous work in this area our approach is scalable and computationally reasonable on very large images we validate the results of our approach by comparing them to user-tagged regions of interest on several very large landscape images from the internet anomaly_detection guided_interaction image_saliency interactive_visualization scene_perception very large scale images
video storyboard which is a form of video_visualization summarizes the major events in a video using illustrative_visualization there are three main technical challenges in creating a video storyboard a event classification b event selection and c event illustration among these challenges a is highly application-dependent and requires a significant amount of application specific semantics to be encoded in a system or manually specified by users this paper focuses on challenges b and c in particular we present a framework for hierarchical event representation and an importance-based selection algorithm for supporting the creation of a video storyboard from a video we consider the storyboard to be an event summarization for the whole video whilst each individual illustration on the board is also an event summarization but for a smaller time window we utilized a 3d_visualization template for depicting and annotating events in illustrations to demonstrate the concepts and algorithms developed we use snooker video_visualization as a case_study because it has a concrete and agreeable set of semantic definitions for events and can make use of existing techniques of event_detection and 3d_reconstruction in a reliable manner nevertheless most of our concepts and algorithms developed for challenges b and c can be applied to other application areas illustrative_visualization multimedia_visualization time_series_data
as microscopes have a very shallow depth of field z-stacks ie sets of images shot at different focal planes are often acquired to fully capture a thick sample such stacks are viewed by users by navigating them through the mouse wheel we propose a new technique of visualizing 3d point line or area markers in such focus_stacks by displaying them with a depth-dependent defocus simulating the microscope's optics this leverages on the microscopists' ability to continuously twiddle focus while implicitly performing a shape-from-focus reconstruction of the 3d structure of the sample user studies confirm that the approach is effective and can complement more traditional techniques such as color-based cues we provide twoimplementations one of which computes defocus in real time on the gpu and examples of their application depth of field focus_stacks microscopy
area-preserving maps are found across a wide range of scientific and engineering problems their study is made challenging by the significant computational effort typically required for their inspection but more fundamentally by the fractal complexity of salient structures the visual inspection of these maps reveals a remarkable topological picture consisting of fixed or periodic points embedded in so-called island chains invariant_manifolds and regions of ergodic behavior this paper is concerned with the effective visualization and precise topological_analysis of area-preserving maps with two degrees of freedom from numerical or analytical data specifically a method is presented for the automatic extraction and characterization of fixed points and the computation of their invariant_manifolds also known as separatrices to yield a complete picture of the structures present within the scale and complexity bounds selected by the user this general approach offers a significant improvement over the visual representations that are so far available for area-preserving maps the technique is demonstrated on a numerical simulation of magnetic confinement in a fusion reactor poincaré map area-preserving maps chaos dynamical_systems invariant_manifolds topology
medical imaging plays a central role in a vast range of healthcare practices the usefulness of 3d_visualizations has been demonstrated for many types of treatment_planning nevertheless full access to 3d renderings outside of the radiology department is still scarce even for many image-centric specialties our work stems from the hypothesis that this under-utilization is partly due to existing visualization_systems not taking the prerequisites of this application domain fully into account we have developed a medical visualization table intended to better fit the clinical reality the overall design goals were two-fold similarity to a real physical situation and a very low learning threshold this paper describes the development of the visualization table with focus on key design decisions the developed features include two novel interaction components for touch tables a user_study including five orthopedic surgeons demonstrates that the system is appropriate and useful for this application domain medical visualization multitouch tabletop display treatment_planning
because of the ever increasing size of output data from scientific simulations supercomputers are increasingly relied upon to generate visualizations one use of supercomputers is to generate field_lines from large scale flow_fields when generating field_lines in parallel the vector field is generally decomposed into blocks which are then assigned to processors since various regions of the vector field can have different flow complexity processors will require varying amounts of computation time to trace their particles causing load imbalance and thus limiting the performance speedup to achieve load-balanced streamline_generation we propose a workload-aware partitioning algorithm to decompose the vector field into partitions with near equal workloads since actual workloads are unknown beforehand we propose a workload estimation algorithm to predict the workload in the local vector field a graph-based representation of the vector field is employed to generate these estimates once the workloads have been estimated our partitioning algorithm is hierarchically applied to distribute the workload to all partitions we examine the performance of our workload estimation and workload-aware partitioning algorithm in several timings studies which demonstrates that by employing these methods better scalability can be achieved with little overhead d vector_field_visualization flow_visualization parallel_processing streamlines
direct_volume_rendering has become a popular method for visualizing volumetric_datasets even though computers are continually getting faster it remains a challenge to incorporate sophisticated illumination models into direct_volume_rendering while maintaining interactive frame rates in this paper we present a novel approach for advanced_illumination in direct_volume_rendering based on gpu ray-casting our approach features directional soft shadows taking scattering into account ambient_occlusion and color bleeding effects while achieving very competitive frame rates in particular multiple dynamic lights and interactive transfer_function changes are fully supported commonly direct_volume_rendering is based on a very simplified discrete version of the original volume_rendering integral including the development of the original exponential_extinction into a-blending in contrast to a-blending forming a product when sampling along a ray the original exponential_extinction coefficient is an integral and its discretization a riemann sum the fact that it is a sum can cleverly be exploited to implement volume lighting effects ie soft directional shadows ambient_occlusion and color bleeding we will show how this can be achieved and how it can be implemented on the gpu ambient_occlusion exponential_extinction gpu ray-casting shadows volume_rendering
we present a gpu-based ray-tracing system for the accurate and interactive_visualization of cut-surfaces through 3d simulations of physical processes created from spectral/hp high-order finite_element_methods when used by the numerical analyst to debug the solver the ability for the imagery to precisely reflect the data is critical in practice the investigator interactively selects from a palette of visualization tools to construct a scene that can answer a query of the data this is effective as long as the implicit contract of image quality between the individual and the visualization system is upheld opengl rendering of scientific_visualizations has worked remarkably well for exploratory_visualization for most solver results this is due to the consistency between the use of first-order representations in the simulation and the linear assumptions inherent in opengl planar fragments and color-space interpolation unfortunately the contract is broken when the solver discretization is of higher-order there have been attempts to mitigate this through the use of spatial adaptation and/or texture_mapping these methods do a better job of approximating what the imagery should be but are not exact and tend to be view-dependent this paper introduces new rendering mechanisms that specifically deal with the kinds of native data generated by high-order finite_element solvers the exploratory_visualization tools are reassessed and cast in this system with the focus on imageaccuracy this is accomplished in a gpu setting to ensure interactivity gpu ray-tracing gpu-based_root-finding high-order finite_elements cut-plane_extraction cutsurface_extraction spectral/hp_elements
percutaneous radiofrequency ablation rfa is becoming a standard minimally invasive clinical procedure for the treatment of liver tumors however planning the applicator placement such that the malignant tissue is completely destroyed is a demanding task that requires considerable experience in this work we present a fast gpu-based real-time approximation of the ablation zone incorporating the cooling effect of liver vessels weighted distance_fields of varying rf applicator types are derived from complex numerical simulations to allow a fast estimation of the ablation zone furthermore the heat-sink effect of the cooling blood flow close to the applicator's electrode is estimated by means of a preprocessed thermal equilibrium representation of the liver parenchyma and blood_vessels utilizing the graphics card the weighted distance_field incorporating the cooling blood flow is calculated using a modular shader framework which facilitates the real-time_visualization of the ablation zone in projected slice views and in volume_rendering the proposed methods are integrated in our software assistant prototype for planning rfa therapy the software allows the physician to interactively place virtual rf applicator models the real-time_visualization of the corresponding approximated ablation zone facilitates interactive evaluation of the tumor coverage in order to optimize the applicator's placement such that all cancer cells are destroyed by the ablation gpu radiofrequency ablation ablation_zone_visualization distance_field interaction volume_rendering
we present a new framework for feature-based statistical_analysis of large-scale scientific data and demonstrate its effectiveness by analyzing features from direct numerical simulations dns of turbulent combustion turbulent flows are ubiquitous and account for transport and mixing processes in combustion astrophysics fusion and climate modeling among other disciplines they are also characterized by coherent structure or organized motion ie nonlocal entities whose geometrical features can directly impact molecular mixing and reactive processes while traditional multi-point statistics provide correlative information they lack nonlocal structural information and hence fail to provide mechanistic causality information between organized fluid motion and mixing and reactive processes hence it is of great interest to capture and track flow features and their statistics together with their correlation with relevant scalar quantities eg temperature or species concentrations in our approach we encode the set of all possible flow features by pre-computing merge trees augmented with attributes such as statistical moments of various scalar_fields eg temperature as well as length-scales computed via spectral analysis the computation is performed in an efficient streaming manner in a pre-processing step and results in a collection of meta-data that is orders of magnitude smaller than the original simulation data this meta-data is sufficient to support a fully flexible and interactive analysis of the features allowing for arbitrary thresholds providing per-feature statistics and creating various global diagnostics such as cumulative density functions cdfs histograms or time-series we combine the analysis with a rendering of the features in a linked-view browser that enables scientists to interactively explore visualize and analyze the equivalent of one terabyte of simulation data we highlight the utility of this new framework for combustion s- ience however it is applicable to many other science domains data_analysis data_exploration multi-variate_data statistics topology_visualization_in_physical_sciences_and_engineering
we present a quasi_interpolation framework that attains the optimal approximation-order of voronoi_splines for reconstruction of volumetric_data sampled on general lattices the quasi_interpolation framework of voronoi_splines provides an unbiased reconstruction method across various lattices therefore this framework allows us to analyze and contrast the sampling-theoretic performance of general lattices using signal reconstruction in an unbiased manner our quasi_interpolation methodology is implemented as an efficient fir filter that can be applied online or as a preprocessing step we present visual and numerical experiments that demonstrate the improvedaccuracy of reconstruction across lattices using the quasi_interpolation framework box spline quasi_interpolation volume_visualization voronoi_spline
we present topological_spines-a new visual representation that preserves the topological and geometric structure of a scalar field this representation encodes the spatial relationships of the extrema of a scalar field together with the local volume and nesting structure of the surrounding contours unlike other topological representations such as contour_trees our approach preserves the local geometric structure of the scalar field including structural cycles that are useful for exposing symmetries in the data to obtain this representation we describe a novel mechanism based on the extraction of extremum_graphs-sparse subsets of the morse-smale_complex that retain the important structural information without the clutter and occlusion problems that arise from visualizing the entire complex directly extremum_graphs form a natural multiresolution structure that allows the user to suppress noise and enhance topological features via the specification of a persistence range applications of our approach include the visualization of 3d_scalar_fields without occlusion artifacts and the exploratory_analysis of high-dimensional functions morse-smale_complex scalar_field_topology extremum_graph topological_spine
sparse irregular sampling is becoming a necessity for reconstructing large and high-dimensional signals however the analysis of this type of data remains a challenge one issue is the robust selection of neighborhoods - a crucial part of analytic tools such as topological decomposition clustering and gradient estimation when extracting the topology of sparsely sampled data common neighborhood strategies such as k-nearest neighbors may lead to inaccurate results either due to missing neighborhood connections which introduce false extrema or due to spurious connections which conceal true extrema other neighborhoods such as the delaunay_triangulation are costly to compute and store even in relatively low dimensions in this paper we address these issues we present two new types of neighborhood_graphs a variation on and a generalization of empty region graphs which considerably improve the robustness of neighborhood-based analysis tools such as topological decomposition our findings suggest that these neighborhood_graphs lead to more accurate topological representations of low- and high- dimensional data sets at relatively low cost both in terms of storage and computation time we describe the implications of our work in the analysis and visualization of scalar functions and provide general strategies for computing and applying our neighborhood_graphs towards robust data_analysis neighborhood_graphs sparsely sampled data topology
flood disasters are the most common natural risk and tremendous efforts are spent to improve their simulation and management however simulation-based investigation of actions that can be taken in case of flood emergencies is rarely done this is in part due to the lack of a comprehensive framework which integrates and facilitates these efforts in this paper we tackle several problems which are related to steering a flood simulation one issue is related to uncertainty we need to account for uncertain knowledge about the environment such as levee-breach locations furthermore the steering process has to reveal how these uncertainties in the boundary conditions affect the confidence in the simulation outcome another important problem is that the simulation setup is often hidden in a black-box we expose system internals and show that simulation_steering can be comprehensible at the same time this is important because the domain expert needs to be able to modify the simulation setup in order to include local knowledge and experience in the proposed solution users steer parameter studies through the world lines interface to account for input uncertainties the transport of steering information to the underlying data-flow components is handled by a novel meta-flow the meta-flow is an extension to a standard data-flow network comprising additional nodes and ropes to abstract parameter control the meta-flow has a visual representation to inform the user about which control operations happen finally we present the idea to use the data-flow diagram itself for visualizing steering information and simulation results we discuss a case-study in collaboration with a domain expert who proposes different actions to protect a virtual city from imminent flooding the key to choosing the best response strategy is the ability to compare different regions of the parameter space while retaining an understanding of what is happening inside the data-flow system data-flow emergency/disaster_management meta-flow parameter_study uncertainty visual_knowledge_discovery visualization_system_and_toolkit_design visualization_of_control
in toponomics the function protein pattern in cells or tissue the toponome is imaged and analyzed for applications in toxicology new drug development and patient-drug-interaction the most advanced imaging technique is robot-driven multi-parameter fluorescence_microscopy this technique is capable of co-mapping hundreds of proteins and their distribution and assembly in protein clusters across a cell or tissue sample by running cycles of fluorescence tagging with monoclonal antibodies or other affinity reagents imaging and bleaching in situ the imaging results in complex multi-parameter data composed of one slice or a 3d volume per affinity reagent biologists are particularly interested in the localization of co-occurring proteins the frequency of co-occurrence and the distribution of co-occurring proteins across the cell we present an interactive_visual_analysis approach for the evaluation of multi-parameter_fluorescence_microscopy_data_in_toponomics multiple linked_views facilitate the definition of features by brushing multiple dimensions the feature specification result is linked to all views establishing a focus+context_visualization in 3d in a new attribute view we integrate techniques from graph_visualization each node in the graph represents an affinity reagent while each edge represents two co-occurring affinity reagent bindings the graph_visualization is enhanced by glyphs which encode specific properties of the binding the graph view is equipped with brushing facilities by brushing in the spatial and attribute domain the biologist achieves a better understanding of the function protein patterns of a cell furthermore an interactive table view is integrated which summarizes unique fluorescence patterns we discuss our approach with respect to a cell probe containing lymphocytes and a prostate tissue section fluorescence_microscopy graph_visualizationprotein_interaction toponomics visual_analytics
in this paper we address the difficult problem of parameter-finding in image_segmentation we replace a tedious manual process that is often based on guess-work and luck by a principled approach that systematically explores the parameter space our core idea is the following two-stage technique we start with a sparse_sampling of the parameter space and apply a statistical model to estimate the response of the segmentation algorithm the statistical model incorporates a model of uncertainty of the estimation which we use in conjunction with the actual estimate in visually guiding the user towards areas that need refinement by placing additional sample points in the second stage the user navigates through the parameter space in order to determine areas where the response value goodness of segmentation is high in our exploration we rely on existing ground-truth images in order to evaluate the "goodness" of an image_segmentation technique we evaluate its usefulness by demonstrating this technique on two image_segmentation algorithms a three parameter model to detect microtubules in electron tomograms and an eight parameter model to identify functional regions in dynamic positron emission tomography scans gaussian_process_model image_segmentation parameter exploration
large observations and simulations in scientific research give rise to high-dimensional_data sets that present many challenges and opportunities in data_analysis and visualization researchers in application domains such as engineering computational biology climate_study imaging and motion capture are faced with the problem of how to discover compact representations of highdimensional data while preserving their intrinsic structure in many applications the original data is projected onto low-dimensional space via dimensionality_reduction techniques prior to modeling one problem with this approach is that the projection step in the process can fail to preserve structure in the data that is only apparent in high dimensions conversely such techniques may create structural illusions in the projection implying structure not present in the original high-dimensional_data our solution is to utilize topological techniques to recover important structures in high-dimensional_data that contains non-trivial topology specifically we are interested in high-dimensional branching structures we construct local circle-valued coordinate functions to represent such features subsequently we perform dimensionality_reduction on the data while ensuring such structures are visually preserved additionally we study the effects of global circular structures on visualizations our results reveal never-before-seen structures on real-world data sets from a variety of applications dimensionality_reduction circular_coordinates topological_analysis visualization
continuous parallel_coordinates cpc are a contemporary visualization_technique in order to combine several scalar_fields given over a common domain they facilitate a continuous view for parallel_coordinates by considering a smooth scalar field instead of a finite number of straight lines we show that there are feature curves in cpc which appear to be the dominant structures of a cpc we present methods to extract and classify them and demonstrate their usefulness to enhance the visualization of cpcs in particular we show that these feature curves are related to discontinuities in continuous scatterplots csp we show this by exploiting a curve-curve duality between parallel and cartesian coordinates which is a generalization of the well-known point-line duality furthermore we illustrate the theoretical considerations concluding we discuss relations and aspects of the cpc's/csp's features concerning the data_analysis features parallel_coordinates topology_visualization
in this paper we present a user_study in which we have investigated the influence of seven state-of-the-art volumetric illumination models on the spatial_perception of volume rendered images within the study we have compared gradient-based shading with half angle slicing directional occlusion shading multidirectional occlusion shading shadow volume propagation spherical harmonic lighting as well as dynamic ambient_occlusion to evaluate these models users had to solve three tasks relying on correct depth as well as size perception our motivation for these three tasks was to find relations between the used illumination model useraccuracy and the elapsed time in an additional task users had to subjectively judge the output of the tested models after first reviewing the models and their features we will introduce the individual tasks and discuss their results we discovered statistically significant differences in the testing performance of the techniques based on these findings we have analyzed the models and extracted those features which are possibly relevant for the improved spatial_comprehension in a relational task we believe that a combination of these distinctive features could pave the way for a novel illumination model which would be optimized based on our findings volumetric illumination spatial_comprehension volume_rendering
in this paper we present a framework to define transfer_functions from a target distribution provided by the user a target distribution can reflect the data importance or highly relevant data value interval or spatial segmentation our approach is based on acommunication channel between a set of viewpoints and a set of bins of a volume data set and it supports 1d as well as 2d transfer_functions including the gradient information the transfer_functions are obtained by minimizing the informational divergence or kullback-leibler_distance between the visibility distribution captured by the viewpoints and a target distribution selected by the user the use of the derivative of the informational divergence allows for a fast optimization process different target distributions for 1d and 2d transfer_functions are analyzed together with importance-driven and view-based techniques information_theory informational divergence kullback-leibler_distance transfer_function
overlaid reference elements need to be sufficiently visible to effectively relate to the underlying information but not so obtrusive that they clutter the presentation we seek to create guidelines for presenting such structures through experimental studies to define boundary conditions for visual intrusiveness we base our work on the practice of designers who use transparency to integrate overlaid grids with their underlying imagery previous work discovered a useful range of alpha values for black or white grids overlayed on scatterplot images rendered in shades of gray over gray backgrounds of different lightness values this work compares black grids to blue and red ones on different image types of scatterplots and maps we expected that the coloured grids over grayscale images would be more visually salient than black ones resulting in lower alpha values instead we found that there was no significant difference between the boundaries set for red and black grids but that the boundaries for blue grids were set consistently higher more opaque as in our previous study alpha values are affected by image density rather than image type and are consistently lower than many default settings these results have implications for the design of subtle reference structures information_visualization applied_perception automated presentation computational_aesthetics visual_design
a new type of glyph is introduced to visualize unsteady_flow with static images allowing easier analysis of time-dependent phenomena compared to animated visualization adopting the visual metaphor of radar displays this glyph represents flow directions by angles and time by radius in spherical coordinates dense seeding of flow radar glyphs on the flow domain naturally lends itself to multi-scale_visualization zoomed-out views show aggregated overviews zooming-in enables detailed analysis of spatial and temporal characteristics uncertainty_visualization is supported by extending the glyph to display possible ranges of flow directions the paper focuses on 2d flow but includes a discussion of 3d flow as well examples from cfd and the field of stochastic hydrogeology show that it is easy to discriminate regions of different spatiotemporal flow behavior and regions of different uncertainty variations in space and time the examples also demonstrate that parameter studies can be analyzed because the glyph_design facilitates comparative_visualization finally different variants of interactive gpu-acceleratedimplementations are discussed visualization glyph uncertainty unsteady_flow
the unguided visual_exploration of volumetric_data can be both a challenging and a time-consuming undertaking identifying a set of favorable vantage points at which to start exploratory expeditions can greatly reduce this effort and can also ensure that no important structures are being missed recent research efforts have focused on entropy-based viewpoint_selection criteria that depend on scalar values describing the structures of interest in contrast we propose a viewpoint suggestion pipeline that is based on feature-clustering in high-dimensional space we use gradient/normal variation as a metric to identify interesting local events and then cluster these via k-means to detect important salient composite features next we compute the maximum possible exposure of these composite feature for different viewpoints and calculate a 2d entropy map parameterized in longitude and latitude to point out promising view orientations superimposed onto an interactive track-ball interface users can then directly use this entropy map to quickly navigate to potentially interesting viewpoints where visibility-based transfer_functions can be employed to generate volume_renderings that minimize occlusions to give full exploration freedom to the user the entropy map is updated on the fly whenever a view has been selected pointing to new and promising but so far unseen view directions alternatively our system can also use a set-cover optimization algorithm to provide a minimal set of views needed to observe all features the views so generated could then be saved into a list for further inspection or into a gallery for a summary presentation direct_volume_rendering ant colony optimization entropy k-means set-cover_problem view_suggestion
the combination of volume data acquired by multiple modalities has been recognized as an important but challenging task modalities often differ in the structures they can delineate and their joint information can be used to extend the classification space however they frequently exhibit differing types of artifacts which makes the process of exploiting the additional information non-trivial in this paper we present a framework based on an information-theoretic measure of isosurface_similarity between different modalities to overcome these problems the resulting similarity space provides a concise overview of the differences between the two modalities and also serves as the basis for an improved selection of features multimodal classification is expressed in terms of similarities and dissimilarities between the isosurfaces of individual modalities instead of data value combinations we demonstrate that our approach can be used to robustly extract features in applications such as dual energy computed_tomography of parts in industrial manufacturing multimodal data surface_similarity volume_visualization
asymmetric_tensor_field_visualization can provide important insight into fluid flows and solid deformations existing techniques for asymmetric_tensor_fields focus on the analysis and simply use evenly-spaced hyperstreamlines on surfaces following eigenvectors and dual-eigenvectors in the tensor_field in this paper we describe a hybrid_visualization_technique in which hyperstreamlines and elliptical glyphs are used in real and complex domains respectively this enables a more faithful representation of flow behaviors inside complex domains in addition we encode tensor magnitude an important quantity in tensor_field_analysis using the density of hyperstreamlines and sizes of glyphs this allows colors to be used to encode other important tensor quantities to facilitate quick visual_exploration of the data from different viewpoints and at different resolutions we employ an efficient image-space approach in which hyperstreamlines and glyphs are generated quickly in the image plane the combination of these techniques leads to an efficient tensor_field_visualization system for domain scientists we demonstrate the effectiveness of our visualization_technique through applications to complex simulated engine fluid flow and earthquake deformation data feedback from domain expert scientists who are also co-authors is provided tensor_field_visualization asymmetric_tensor_fields earthquakeengineering fluid dynamics glyph_packing hyperstreamline_placement solid deformation vector_field_visualization view-dependent_visualization
analyzing either high-frequency shape detail or any other 2d fields scalar or vector embedded over a 3d geometry is a complex task since detaching the detail from the overall shape can be tricky an alternative approach is to move to the 2d space resolving shape reasoning to easier image_processing techniques in this paper we propose a novel framework for the analysis of 2d information distributed over 3d geometry based on a locally smooth parametrization technique that allows us to treat local 3d data in terms of image content the proposed approach has been implemented as a sketch-based system that allows to design with a few gestures a set of possibly overlapping parameterizations of rectangular portions of the surface we demonstrate that due to the locality of the parametrization the distortion is under an acceptable threshold while discontinuities can be avoided since the parametrized geometry is always homeomorphic to a disk we show the effectiveness of the proposed technique to solve specific cultural_heritage ch tasks the analysis of chisel marks over the surface of a unfinished sculpture and the local comparison of multiple photographs mapped over the surface of an artwork for this very difficult task we believe that our framework and the corresponding tool are the first steps toward a computer-based shape reasoning system able to support ch scholars with a medium they are more used to cultural_heritage surface_characterization image_processing interactive inspection mesh_parameterization
when visualizing tubular 3d structures external representations are often used for guidance and display and such views in 2d can often contain occlusions virtual dissection methods have been proposed where the entire 3d structure can be mapped to the 2d plane though these will lose context by straightening curved_sections we present a new method of creating maps of 3d tubular structures that yield a succinct view while preserving the overall geometric structure given a dominant view plane for the structure its curve skeleton is first projected to a 2d skeleton this 2d skeleton is adjusted to account for distortions in length modified to remove intersections and optimized to preserve the shape of the original 3d skeleton based on this shaped 2d skeleton a boundary for the map of the object is obtained based on a slicing path through the structure and the radius around the skeleton the sliced structure is conformally mapped to a rectangle and then deformed via harmonic mapping to match the boundary placement this flattened map preserves the general geometric context of a 3d object in a 2d display and rendering of this flattened map can be accomplished using volumetric ray casting we have evaluated our method on real datasets of human colon models geometry-based technique biomedical_visualization conformal_mapping medical visualization volume_rendering
parameterization of complex surfaces constitutes a major means of visualizing highly convoluted geometric structures as well as other properties associated with the surface it also enables users with the ability to navigate orient and focus on regions of interest within a global view and overcome the occlusions to inner concavities in this paper we propose a novel area-preserving_surface_parameterization method which is rigorous in theory moderate in computation yet easily extendable to surfaces of non-disc and closed-boundary topologies starting from the distortion induced by an initial parameterization an area restoring diffeomorphic flow is constructed as a lie_advection of differential 2-forms along the manifold which yields equality of the area elements between the domain and the original surface at its final state existence and uniqueness of result are assured through an analytical derivation based upon a triangulated surface_representation we also present an efficient algorithm in line with discrete differential modeling as an exemplar application the utilization of this method for the effective visualization of brain cortical imaging modalities is presented compared with conformal methods our method can reveal more subtle surface patterns in a quantitative manner it therefore provides a competitive alternative to the existing parameterization techniques for better surface-based analysis in various scenarios area-preserving_surface_parameterization lie_advection differential_forms surface visualization
a fundamental challenge for time-varying volume data_analysis and visualization is the lack of capability to observe and track data change or evolution in an occlusion-free controllable and adaptive fashion in this paper we propose to organize a timevarying data set into a hierarchy of states by deriving transition probabilities among states we construct a global map that captures the essential transition_relationships in the time-varying_data we introduce the transgraph a graph-based representation to visualize hierarchical state transition_relationships the transgraph not only provides a visual_mapping that abstracts data evolution over time in different levels of detail but also serves as a navigation tool that guides data_exploration and tracking the user interacts with the transgraph and makes connection to the volumetric_data through brushing_and_linking a set of intuitive queries is provided to enable knowledge extraction from time-varying_data we test our approach with time-varying_data sets of different characteristics and the results show that the transgraph can effectively augment our ability in understanding time-varying_data time-varying_data_visualization hierarchical_representation states transition_relationship user interface
visual_analysis is widely used to study the behavior of molecules of particular interest are the analysis of molecular interactions and the investigation of binding sites for large molecules however it is difficult to detect possible binding sites and paths leading to these sites by pure visual inspection in this paper we present new methods for the computation and visualization of potential molecular paths using a novel filtering method we extract the significant paths from the voronoi diagram of spheres for the interactive_visualization of molecules and their paths we present several methods using deferred shading and other state-of-theart techniques to allow for a fast overview of reachable regions of the molecule we illuminate the molecular surface using a large number of light sources placed on the extracted paths we also provide a method to compute the extension surface of selected paths and visualize it using the skin surface furthermore we use the extension surface to clip the molecule to allow easy visual tracking of even deeply buried paths the methods are applied to several proteins to demonstrate their usefulness molecular_visualization data_filtering geometry-based_techniques view-dependent_visualization
study of symmetric or repeating patterns in scalar_fields is important in scientific data_analysis because it gives deep insights into the properties of the underlying phenomenon though geometric symmetry has been well studied within areas like shape processing identifying symmetry in scalar_fields has remained largely unexplored due to the high computational cost of the associated algorithms we propose a computationally efficient algorithm for detecting symmetric patterns in a scalar field distribution by analysing the topology of level_sets of the scalar field our algorithm computes the contour_tree of a given scalar field and identifies subtrees that are similar we define a robust similarity measure for comparing subtrees of the contour_tree and use it to group similar subtrees together regions of the domain corresponding to subtrees that belong to a common group are extracted and reported to be symmetric identifying symmetry in scalar_fields finds applications in visualization data_exploration and feature_detection we describe two applications in detail symmetry-aware transfer_function_design and symmetry-aware isosurface_extraction scalar_field_symmetry contour_tree isosurface_extraction persistence similarity measure transfer_function_design
this paper introduces a novel importance measure for critical_points in 2d scalar_fields this measure is based on a combination of the deep structure of the scale_space with the well-known concept of homological persistence we enhance the noise robust persistence measure by implicitly taking the hill- ridge- and outlier-like spatial extent of maxima and minima into account this allows for the distinction between different types of extrema based on their persistence at multiple scales our importance measure can be computed efficiently in an out-of-core setting to demonstrate the practical relevance of our method we apply it to a synthetic and a real-world data set and evaluate its performance and scalability scale_space discrete_morse_theory persistence
multi-valued data sets are increasingly common with the number of dimensions growing a number of multi-variate_visualization_techniques have been presented to display such data however evaluating the utility of such techniques for general data sets remains difficult thus most techniques are studied on only one data set another criticism that could be levied against previous evaluations of multi-variate_visualizations is that the task doesn't require the presence of multiple variables at the same time the taxonomy of tasks that users may perform visually is extensive we designed a task trend localization that required comparison of multiple data values in a multi-variate_visualization we then conducted a user_study with this task evaluating five multivariate visualization_techniques from the literature brush strokes data-driven spots oriented slivers color_blending dimensional_stacking and juxtaposed grayscale maps we report the results and discuss the implications for both the techniques and the task user_study multi-variate_visualization visual_analytics visual_task_design
flows through tubular structures are common in many fields including blood flow in medicine and tubular fluid flows in engineering the analysis of such flows is often done with a strong reference to the main flow direction along the tubular boundary in this paper we present an approach for straightening the visualization of tubular flow by aligning the main reference direction of the flow ie the center line of the bounding tubular structure with one axis of the screen we are able to natively juxtapose 1 different visualizations of the same flow either utilizing different flow_visualization_techniques or by varying parameters of a chosen approach such as the choice of seeding locations for integration-based flow_visualization 2 the different time steps of a time-dependent flow 3 different projections around the center line  and 4 quantitative flow_visualizations in immediate spatial relation to the more qualitative classical flow_visualization we describe how to utilize this approach for an informative interactive_visual_analysis we demonstrate the potential of our approach by visualizing two datasets from two different fields an arterial blood flow measurement and a tubular gas flow simulation from the automotive industry comparative_visualization data reformation flow_visualization
we present the visual_analysis of a biologically inspired cfd simulation of the deformable flapping wings of a dragonfly as it takes off and begins to maneuver using vortex_detection and integration-based flow lines the additional seed placement and perceptual challenges introduced by having multiple dynamically deforming objects in the highly unsteady 3d flow domain are addressed a brief overview of the high speed photogrammetry setup used to capture the dragonfly takeoff parametric surfaces used for wing reconstruction cfd solver and underlying flapping flight theory is presented to clarify the importance of several unsteady flight mechanisms such as the leading edge vortex that are captured visually a novel interactive seed placement method is used to simplify the generation of seed curves that stay in the vicinity of relevant flow phenomena as they move with the flapping wings this method allows a user to define and evaluate the quality of a seed's trajectory over time while working with a single time step the seed curves are then used to place particles streamlines and generalized_streak_lines the novel concept of flowing seeds is also introduced in order to add visual context about the instantaneous vector_fields surrounding smoothly animate streak lines tests show this method to be particularly effective at visually capturing vortices that move quickly or that exist for a very brief period of time in addition an automatic camera animation method is used to address occlusion issues caused when animating the immersed wing boundaries alongside many geometric_flow lines each visualization method is presented at multiple time steps during the up-stroke and down-stroke to highlight the formation attachment and shedding of the leading edge vortices in pairs of wings also the visualizations show evidence of wake capture at stroke reversal which suggests the existence of previously unknown unsteady lift generation mechanisms that are unique to qua- wing insects flow_visualization flowing_seed_points insect_flight streak lines streamlines unsteady_flow vortex_visualization
acceleration is a fundamental quantity of flow_fields that captures galilean invariant properties of particle_motion considering the magnitude of this field minima represent characteristic structures of the flow that can be classified as saddle- or vortex-like we made the interesting observation that vortex-like minima are enclosed by particularly pronounced ridges this makes it possible to define boundaries of vortex_regions in a parameter-free way utilizing scalar_field_topology a robust algorithm can be designed to extract such boundaries they can be arbitrarily shaped an efficient tracking algorithm allows us to display the temporal evolution of vortices various vortex models are used to evaluate the method we apply our method to two-dimensional model systems from computational fluid dynamics and compare the results to those arising from existing definitions vortex_regions feature_extraction time-dependent flow_fields
we consider the problem of extracting discrete two-dimensional vortices from a turbulent flow in our approach we use a reference model describing the expected physics and geometry of an idealized vortex the model allows us to derive a novel correlation between the size of the vortex and its strength measured as the square of its strain minus the square of its vorticity for vortex_detection in real models we use the strength parameter to locate potential vortex cores then measure the similarity of our ideal analytical vortex and the real vortex core for different strength thresholds this approach provides a metric for how well a vortex core is modeled by an ideal vortex moreover this provides insight into the problem of choosing the thresholds that identify a vortex by selecting a target coefficient of determination ie statistical confidence we determine on a per-vortex basis what threshold of the strength parameter would be required to extract that vortex at the chosen confidence we validate our approach on real data from a global ocean simulation and derive from it a map of expected vortex strengths over the global ocean vortex_extraction feature_extraction statistical data_analysis
research in the field of complex fluids such as polymer solutions particulate suspensions and foams studies how the flow of fluids with different material parameters changes as a result of various constraints surface evolver the standard solver software used to generate foam simulations provides large complex time-dependent_data sets with hundreds or thousands of individual bubbles and thousands of time steps however this software has limited visualization capabilities and no foam specific visualization software exists we describe the foam research application area where we believe visualization has an important role to play we present a novel application that provides various techniques for visualization exploration and analysis of time-dependent 2d foam simulation data we show new features in foam simulation data and new insights into foam behavior discovered using our application surface evolver bubble-scale_simulation time-dependent_visualizations
in this paper we propose a volume_visualization system that accepts direct_manipulation through a sketch-based what you see is what you get wysiwyg approach similar to the operations in painting applications for 2d images in our system a full set of tools have been developed to enable direct_volume_rendering manipulation of color transparency contrast brightness and other optical properties by brushing a few strokes on top of the rendered volume image to be able to smartly identify the targeted features of the volume our system matches the sparse sketching_input with the clustered features both in image space and volume space to achieve interactivity both special algorithms to accelerate the input identification and feature matching have been developed and implemented in our system without resorting to tuning transfer_function parameters our proposed system accepts sparse stroke inputs and provides users with intuitive flexible and effective interaction during volume data_exploration and visualization feature space human-computer_interaction sketching_input transfer_functions volume_rendering
this paper presents a novel framework for visualizing volumetric_data specified on complex polyhedral grids without the need to perform any kind of a priori tetrahedralization these grids are composed of polyhedra that often are non-convex and have an arbitrary number of faces where the faces can be non-planar with an arbitrary number of vertices the importance of such grids in state-of-the-art simulation packages is increasing rapidly we propose a very compact face-based data structure for representing such meshes for visualization called two-sided face sequence lists tsfsl as well as an algorithm for direct gpu-based ray-casting using this representation the tsfsl data structure is able to represent the entire mesh topology in a 1d tsfsl data array of face records which facilitates the use of efficient 1d texture accesses for visualization in order to scale to large_data sizes we employ a mesh decomposition into bricks that can be handled independently where each brick is then composed of its own tsfsl array this bricking enables memory savings and performance improvements for large meshes we illustrate the feasibility of our approach with real-world application results by visualizing highly complex polyhedral data from commercial state-of-the-art simulation packages gpu-based_visualization volume_rendering polyhedral grids unstructured_grids
in recent years many volumetric illumination models have been proposed which have the potential to simulate advanced lighting effects and thus support improved image comprehension although volume ray-casting is widely accepted as the volume_rendering technique which achieves the highest image quality so far no volumetric illumination algorithm has been designed to be directly incorporated into the ray-casting process in this paper we propose image plane sweep volume_illumination ipsvi which allows the integration of advanced_illumination effects into a gpu-based volume ray-caster by exploiting the plane sweep paradigm thus we are able to reduce the problem complexity and achieve interactive frame rates while supporting scattering as well as shadowing since all illumination computations are performed directly within a single rendering pass ipsvi does not require any preprocessing nor does it need to store intermediate results within an illumination volume it therefore has a significantly lower memory footprint than other techniques this makes ipsvi directly applicable to large_data sets furthermore the integration into a gpu-based ray-caster allows for high image quality as well as improved rendering performance by exploiting early ray termination this paper discusses the theory behind ipsvi describes itsimplementation demonstrates its visual results and provides performance measurements advanced_illumination gpu-based ray-casting interactive_volume_rendering
large scale and structurally complex volume datasets from high-resolution 3d imaging devices or computational simulations pose a number of technical challenges for interactive_visual_analysis in this paper we present the first integration of a multiscale volume representation based on tensor approximation within a gpu-accelerated out-of-core multiresolution_rendering framework specific contributions include a a hierarchical brick-tensor_decomposition approach for pre-processing large volume data b a gpu accelerated tensor_reconstructionimplementation exploiting cuda capabilities and c an effective tensor-specific quantization strategy for reducing data transfer bandwidth and out-of-core memory footprint our multiscale representation allows for the extraction analysis and display of structural features at variable spatial scales while adaptive level-of-detail rendering methods make it possible to interactively explore large_datasets within a constrained memory footprint the quality and performance of our prototype system is evaluated on large structurally complex datasets including gigabyte-sized micro-tomographic volumes gpu/cuda interactive_volume_visualization multiresolution_rendering multiscale tensor_reconstruction
color vision deficiency cvd affects a high percentage of the population worldwide when seeing a volume_visualization result persons with cvd may be incapable of discriminating the classification information expressed in the image if the color_transfer_function or the color_blending used in the direct_volume_rendering is not appropriate conventional methods used to address this problem adopt advanced image_recoloring techniques to enhance the rendering results frame-by-frame unfortunately problematic perceptual results may still be generated this paper proposes an alternative solution that complements the image_recoloring scheme by reconfiguring the components of the direct_volume_rendering dvr pipeline our approach optimizes the mapped colors of a transfer_function to simulate cvd-friendly effect that is generated by applying the image_recoloring to the results with the initial transfer_function the optimization process has a low computational complexity and only needs to be performed once for a given transfer_function to achieve detail-preserving and perceptually natural semi-transparent effects we introduce a new color composition mode that works in the color space of dichromats experimental results and a pilot study demonstrates that our approach can yield dichromats-friendly and consistent volume_visualization in real-time dichromacy direct_volume_rendering image_recoloring volume_classification
better understanding of hemodynamics conceivably leads to improved diagnosis and prognosis of cardiovascular diseases therefore an elaborate analysis of the blood-flow in heart and thoracic arteries is essential contemporary mri techniques enable acquisition of quantitative time-resolved flow information resulting in 4d velocity fields that capture the blood-flow behavior visual_exploration of these fields provides comprehensive insight into the unsteady blood-flow behavior and precedes a quantitative analysis of additional blood-flow parameters the complete inspection requires accurate segmentation of anatomical structures encompassing a time-consuming and hard-to-automate process especially for malformed morphologies we present a way to avoid the laborious segmentation process in case of qualitative inspection by introducing an interactive virtual probe this probe is positioned semi-automatically within the blood-flow_field and serves as a navigational object for visual_exploration the difficult task of determining position and orientation along the view-direction is automated by a fitting approach aligning the probe with the orientations of the velocity field the aligned probe provides an interactive seeding basis for various flow_visualization approaches we demonstrate illustration-inspired particles integral lines and integral_surfaces conveying distinct characteristics of the unsteady blood-flow lastly we present the results of an evaluation with domain experts valuing the practical use of our probe and flow_visualization_techniques flow_visualization illustrative_visualization multivalued_images phase-contrast_cine_mri probing
in modern clinical practice planning access paths to volumetric target structures remains one of the most important and most complex tasks and a physician's insufficient experience in this can lead to severe complications or even the death of the patient in this paper we present a method for safety evaluation and the visualization of access paths to assist physicians during preoperative planning as a metaphor for our method we employ a well-known and thus intuitively perceivable natural phenomenon that is usually called crepuscular rays using this metaphor we propose several ways to compute the safety of paths from the region of interest to all tumor voxels and show how this information can be visualized in real-time using a multi-volume_rendering system furthermore we show how to estimate the extent of connected safe areas to improve common medical 2d multi-planar reconstruction mpr views we evaluate our method by means of expert interviews an online survey and a retrospective evaluation of 19 real abdominal radio-frequency ablation rfa interventions with expert decisions serving as a gold standard the evaluation results show clear evidence that our method can be successfully applied in clinical practice without introducing substantial overhead work for the acting personnel finally we show that our method is not limited to medical applications and that it can also be useful in other fields accessibility medical visualization ray casting
an instant and quantitative assessment of spatial distances between two objects plays an important role in interactive applications such as virtual model assembly medical operation_planning or computational_steering while some research has been done on the development of distance-based measures between two objects only very few attempts have been reported to visualize such measures in interactive scenarios in this paper we present two different approaches for this purpose and we investigate the effectiveness of these approaches for intuitive 3d implant positioning in a medical operation_planning system the first approach uses cylindrical glyphs to depict distances which smoothly adapt their shape and color to changing distances when the objects are moved this approach computes distances directly on the polygonal object representations by means of ray/triangle mesh intersection the second approach introduces a set of slices as additional geometric structures and uses color coding on surfaces to indicate distances this approach obtains distances from a precomputed distance_field of each object the major findings of the performed user_study indicate that a visualization that can facilitate an instant and quantitative analysis of distances between two objects in interactive 3d scenarios is demanding yet can be achieved by including additional monocular cues into the visualization distance_visualization biomedical_visualization distance_fields glyphs implant_planning
blood flow and derived data are essential to investigate the initiation and progression of cerebral_aneurysms as well as their risk of rupture an effective visual_exploration of several hemodynamic attributes like the wall shear stress wss and the inflow_jet is necessary to understand the hemodynamics moreover the correlation between focus-and-context attributes is of particular interest an expressive visualization of these attributes and anatomic information requires appropriate visualization_techniques to minimize visual_clutter and occlusions we present the flowlens as a focus-and-context approach that addresses these requirements we group relevant hemodynamic attributes to pairs of focus-and-context attributes and assign them to different anatomic scopes for each scope we propose several flowlens visualization templates to provide a flexible visual filtering of the involved hemodynamic pairs a template consists of the visualization of the focus attribute and the additional depiction of the context attribute inside the lens furthermore the flowlens supports local probing and the exploration of attribute changes over time the flowlens minimizes visual_cluttering occlusions and provides a flexible exploration of a region of interest we have applied our approach to seven representative datasets including steady and unsteady_flow data from cfd simulations and 4d_pc-mri measurements informal user interviews with three domain experts confirm the usefulness of our approach aneurysm flow_visualization focus-and-context illustrative_rendering
multi-material_components which contain metal parts surrounded by plastic materials are highly interesting for inspection using industrial 3d_x-ray_computed_tomography 3dxct examples of this application scenario are connectors or housings with metal inlays in the electronic or automotive industry a major problem of this type of components is the presence of metal which causes streaking artifacts and distorts the surrounding media in the reconstructed volume streaking artifacts and dark-band artifacts around metal components significantly influence the material characterization especially for the plastic components in specific cases these artifacts even prevent a further analysis due to the nature and the different characteristics of artifacts the development of an efficient artifact-reduction technique in reconstruction-space is rather complicated in this paper we present a projection-space pipeline for metal-artifacts reduction the proposed technique first segments the metal in the spatial domain of the reconstructed volume in order to separate it from the other materials then metal parts are forward-projected on the set of projections in a way that metal-projection regions are treated as voids subsequently the voids which are left by the removed metal are interpolated in the 2d_projections finally the metal is inserted back into the reconstructed 3d volume during the fusion stage we present a visual_analysis tool allowing for interactive parameter estimation of the metal segmentation the results of the proposed artifact-reduction technique are demonstrated on a test part as well as on real world components for these specimens we achieve a significant reduction of metal artifacts allowing an enhanced material characterization d x-ray computed_tomography metal-artifact_reduction multi-material_components visual_analysis
in this paper we present a systematization of techniques that use quality_metrics to help in the visual_exploration of meaningful patterns in high-dimensional_data in a number of recent papers different quality_metrics are proposed to automate the demanding search through large spaces of alternative visualizations eg alternative projections or ordering allowing the user to concentrate on the most promising visualizations suggested by the quality_metrics over the last decade this approach has witnessed a remarkable development but few reflections exist on how these methods are related to each other and how the approach can be developed further for this purpose we provide an overview of approaches that use quality_metrics in high-dimensional_data_visualization and propose a systematization based on a thorough literature review we carefully analyze the papers and derive a set offactors for discriminating the quality_metrics visualization_techniques and the process itself the process is described through a reworked version of the well-known information_visualization pipeline we demonstrate the usefulness of our model by applying it to several existing approaches that use quality_metrics and we provide reflections on implications of our model for future research high-dimensional_data_visualization quality_metrics
many well-cited theories for visualization design state that a visual representation should be optimized for quick and immediate interpretation by a user distracting elements like decorative "chartjunk" or extraneous information are avoided so as not to slow comprehension yet several recent studies in visualization research provide evidence that non-efficient visual elements may benefit comprehension and recall on the part of users similarly findings from studies related to learning from visual displays in various subfields of psychology suggest that introducing cognitive difficulties to visualization interaction can improve a user's understanding of important information in this paper we synthesize empirical results from cross-disciplinary research on visual information representations providing a counterpoint to efficiency-based design_theory with guidelines that describe how visual difficulties can be introduced to benefit comprehension and recall we identify conditions under which the application of visual difficulties is appropriate based on underlyingfactors in visualization interaction like active_processing and engagement we characterize effective graph design as a trade-off between efficiency and learning difficulties in order to provide information_visualization infovis researchers and practitioners with a framework for organizing explorations of graphs for which comprehension and recall are crucial we identify implications of this view for the design and evaluation of information_visualizations desirable difficulites active_processing cognitive_efficiency engagement individual_differences
we propose a new framework for visualising tables of counts proportions and probabilities we call our framework product plots alluding to the computation of area as a product of height and width and the statistical concept of generating a joint_distribution from the product of conditional and marginal distributions the framework with extensions is sufficient to encompass over 20 visualisations previously described in fields of statistical_graphics and infovis including bar_charts mosaic_plots treemaps equal area plots and fluctuation diagrams statistics bar_chart conditional_distribution joint_distribution mosaic plot treemap
narrative_visualizations combine conventions of communicative and exploratory information_visualization to convey an intended story we demonstrate visualization rhetoric as an analytical framework for understanding how design techniques that prioritize particular interpretations in visualizations that "tell a story" can significantly affect end-user interpretation we draw a parallel between narrative_visualization interpretation and evidence from framing studies in political messaging decision-making and literary studies devices for understanding the rhetorical nature of narrative information_visualizations are presented informed by the rigorous application of concepts from critical theory semiotics journalism and political theory we draw attention to how design tactics represent additions or omissions of information at various levels-the data visual representation textual annotations and interactivity-and how visualizations denote and connote phenomena with reference to unstated viewing conventions and codes classes of rhetorical techniques identified via a systematic analysis of recent narrative_visualizations are presented and characterized according to their rhetorical contribution to the visualization we describe how designers and researchers can benefit from the potentially positive aspects of visualization rhetoric in designing engaging layered narrative_visualizations and how our framework can shed light on how a visualization design prioritizes specific interpretations we identify areas where future inquiry into visualization rhetoric can improve understanding of visualization interpretation rhetoric connotation denotation framing_effects narrative_visualization semiotics
current information_visualization_techniques assume unrestricted access to data however privacy protection is a key issue for a lot of real-world data analyses corporate data medical records etc are rich in analytical value but cannot be shared without first going through a transformation step where explicit identifiers are removed and the data is sanitized researchers in the field of data_mining have proposed different techniques over the years for privacy-preserving data publishing and subsequent mining techniques on such sanitized data a well-known drawback in these methods is that for even a small guarantee of privacy the utility of the datasets is greatly reduced in this paper we propose an adaptive technique for privacy preser vation in parallel_coordinates based on knowledge about the sensitivity of the data we compute a clustered representation on the fly which allows the user to explore the data without breaching privacy through the use of screen-space privacy metrics the technique adapts to the user's screen parameters and interaction we demonstrate our method in a case_study and discuss potential attack scenarios parallel_coordinates clustering privacy
evaluating comparing and interpreting related pieces of information are tasks that are commonly performed during visual_data_analysis and in many kinds of information-intensive work synchronized visual highlighting of related elements is a well-known technique used to assist this task an alternative approach which is more invasive but also more expressive is visual linking in which line connections are rendered between related elements in this work we present context-preserving visual_links as a new method for generating visual_links the method specifically aims to fulfill the following two goals first visual_links should minimize the occlusion of important information second links should visually stand out from surrounding information by minimizing visual interference we employ an image-based analysis of visual saliency to determine the important regions in the original representation a consequence of the image-based approach is that our technique is application-independent and can be employed in a large number of visual_data_analysis scenarios in which the underlying content cannot or should not be altered we conducted a controlled experiment that indicates that users can find linked elements in complex visualizations more quickly and with greater subjective satisfaction than in complex visualizations in which plain highlighting is used context-preserving visual_links were perceived as visually more attractive than traditional visual_links that do not account for the context information visual_links connectedness highlighting image-based routing saliency
computing and visualizing sets of elements and their relationships is one of the most common tasks one performs when analyzing and organizing large amounts of data common representations of sets such as convex or concave geometries can become cluttered and difficult to parse when these sets overlap in multiple or complex ways eg when multiple elements belong to multiple sets in this paper we present a design_study of a novel set visual representation linesets consisting of a curve connecting all of the set's elements our approach to design the visualization differs from traditional methodology used by the infovis community we first explored the potential of the visualization concept by running a controlled experiment comparing our design sketches to results from the state-of-the-art technique our results demonstrated that linesets are advantageous for certain tasks when compared to concave shapes we discuss animplementation of linesets based on simple heuristics and present a study demonstrating that our generated curves do as well as human-drawn ones finally we present two applications of our technique in the context of search tasks on a map and community analysis tasks in social_networks set_visualization clustering faceted_data_visualization graph_visualization
traditional layered graph depictions such as flow charts are in wide use yet as graphs grow more complex these depictions can become difficult to understand quilts are matrix-based depictions for layered_graphs designed to address this problem in this research we first improve quilts by developing three design alternatives and then compare the best of these alternatives to better-known node-link and matrix depictions a primary weakness in quilts is their depiction of skip links links that do not simply connect to a succeeding layer therefore in our first study we compare quilts using color-only text-only and mixed color and text skip link depictions finding that path finding with the color-only depiction is significantly slower and less accurate and that in certain cases the mixed depiction offers an advantage over the text-only depiction in our second study we compare quilts using the mixed depiction to node-link_diagrams and centered matrices overall results show that users can find paths through graphs significantly faster with quilts 466 secs than with node-link 583 secs or matrix 712 secs diagrams this speed advantage is still greater in large graphs eg in 200 node graphs 554 secs vs 711 secs for node-link and 842 secs for matrix depictions graph_drawing layered_graphs matrix based depiction node-link diagram
the aspect_ratio of a plot has a dramatic impact on our ability to perceive trends and patterns in the data previous approaches for automatically selecting the aspect_ratio have been based on adjusting the orientations or angles of the line segments in the plot in contrast we recommend a simple effective method for selecting the aspect_ratio minimize the arc length of the data curve while keeping the area of the plot constant the approach is parameterization invariant robust to a wide range of inputs preserves visual symmetries in the data and is a compromise between previously proposed techniques further we demonstrate that it can be effectively used to select the aspect_ratio of contour plots we believe arc length should become the default aspect_ratio_selection method aspect_ratio_selection banking to  degrees orientation resolution
in modeling and analysis of longitudinal social_networks visual_exploration is used in particular to complement and inform other methods the most common graphical representations for this purpose appear to be animations and small_multiples of intermediate states depending on the type of media available we present an alternative approach based on matrix representation of gestaltlines a combination of tufte's sparklines with glyphs based on gestalt theory as a result we obtain static compact yet data-rich diagrams that support specifically the exploration of evolving dyadic relations and persistent group structure although at the expense of cross-sectional network views and indirect linkages glyphbased techniques network_visualization social_networks time_series_data visual_knowledge_discovery_and_representation
large_volumes of real-world data often exhibit inhomogeneities vertically in the form of correlated or independent dimensions and horizontally in the form of clustered or scattered data items in essence these inhomogeneities form the patterns in the data that researchers are trying to find and understand sophisticated statistical methods are available to reveal these patterns however the visualization of their outcomes is mostly still performed in a one-view-fits-all manner in contrast our novel visualization approach visbricks acknowledges the inhomogeneity of the data and the need for different visualizations that suit the individual characteristics of the different data subsets the overall visualization of the entire data set is patched together from smaller visualizations there is one visbrick for each cluster in each group of interdependent dimensions whereas the total impression of all visbricks together gives a comprehensive high-level overview of the different groups of data each visbrick independently shows the details of the group of data it represents state-of-the-art brushing and visual linking between all visbricks furthermore allows the comparison of the groupings and the distribution of data items among them in this paper we introduce the visbricks visualization concept discuss its design rationale andimplementation and demonstrate its usefulness by applying it to a use case from the field of biomedicineinhomogeneous_data_multiform_visualization multiple_coordinated_views
data-driven documents d3 is a novel representation-transparent approach to visualization for the web rather than hide the underlying scenegraph within a toolkit-specific abstraction d3 enables direct inspection and manipulation of a native representation the standard document object model dom with d3 designers selectively bind input data to arbitrary document elements applying dynamic transforms to both generate and modify content we show how representational transparency improves expressiveness and better integrates with developer tools than prior approaches while offering comparable notational efficiency and retaining powerful declarative components immediate evaluation of operators further simplifies debugging and allows iterative development additionally we demonstrate how d3 transforms naturally enable animation and interaction with dramatic performance improvements over intermediate representations d graphics information_visualization toolkits user_interfaces
multivariate data visualization is a classic topic for which many solutions have been proposed each with its own strengths and weaknesses in standard solutions the structure of the visualization is fixed we explore how to give the user more freedom to define visualizations our new approach is based on the usage of flexible linked axes the user is enabled to define a visualization by drawing and linking axes on a canvas each axis has an associated attribute and range which can be adapted links between pairs of axes are used to show data in either scatter plot- or parallel_coordinates plot-style flexible linked axes enable users to define a wide variety of different visualizations these include standard methods such as scatter plot matrices radar charts and pcps [11] less well known approaches such as hyperboxes [1] timewheels [17] and many-to-many relational parallel coordinate displays [14] and also custom visualizations consisting of combinations of scatter plots and pcps furthermore our method allows users to define composite visualizations that automatically support brushing_and_linking we have discussed our approach with ten prospective users who found the concept easy to understand and highly promising multivariate data parallel_coordinates plot scatter plot visualization
generation of synthetic_datasets is a common practice in many research areas such data is often generated to meet specific needs or certain conditions that may not be easily found in the original real data the nature of the data varies according to the application area and includes text graphs social or weather data among many others the common process to create such synthetic_datasets is to implement small scripts or programs restricted to small problems or to a specific application in this paper we propose a framework designed to generate high dimensional datasets users can interactively create and navigate through multi dimensional datasets using a suitable graphical user-interface the data creation is driven by statistical distributions based on a few user-defined parameters first a grounding dataset is created according to given inputs and then structures and trends are included in selected dimensions andorthogonal projection planes furthermore our framework supports the creation of complex non-orthogonal trends and classified datasets it can successfully be used to create synthetic_datasets simulating important trends as multidimensional clusters correlations andoutliers synthetic_data_generation high-dimensional_data interaction multivariate data
in this paper we present a new technique and prototype graph_visualization system stereoscopic highlighting to help answer accessibility and adjacency queries when interacting with a node-link diagram our technique utilizes stereoscopic depth to highlight regions of interest in a 2d graph by projecting these parts onto a plane closer to the viewpoint of the user this technique aims to isolate and magnify specific portions of the graph that need to be explored in detail without resorting to other highlighting techniques like color or motion which can then be reserved to encode other data attributes this mechanism of stereoscopic highlighting also enables focus+context views by juxtaposing a detailed image of a region of interest with the overall graph which is visualized at a further depth with correspondingly less detail in order to validate our technique we ran a controlled experiment with 16 subjects comparing static visual highlighting to stereoscopic highlighting on 2d and 3d graph_layouts for a range of tasks our results show that while for most tasks the difference in performance between stereoscopic highlighting alone and static visual highlighting is not statistically significant users performed better when both highlighting methods were used concurrently in more complicated tasks 3d layout with static visual highlighting outperformed 2d layouts with a single highlighting method however it did not outperform the 2d layout utilizing both highlighting techniques simultaneously based on these results we conclude that stereoscopic highlighting is a promising technique that can significantly enhance graph_visualizations for certain use cases graph_visualization stereo_displays virtual_reality
the analysis of large dynamic_networks poses a challenge in many fields ranging from large bot-nets to social_networks as dynamic_networks exhibit different characteristics eg being of sparse or dense structure or having a continuous or discrete time line a variety of visualization_techniques have been specifically designed to handle these different aspects of network structure and time this wide range of existing techniques is well justified as rarely a single visualization is suitable to cover the entire visual_analysis instead visual representations are often switched in the course of the exploration of dynamic graphs as the focus of analysis shifts between the temporal and the structural aspects of the data to support such a switching in a seamless and intuitive manner we introduce the concept of in situ visualization- a novel strategy that tightly integrates existing visualization_techniques for dynamic_networks it does so by allowing the user to interactively select in a base visualization a region for which a different visualization_technique is then applied and embedded in the selection made this permits to change the way a locally selected group of data items such as nodes or time points are shown - right in the place where they are positioned thus supporting the user's overall mental_map using this approach a user can switch seamlessly between different visual representations to adapt a region of a base visualization to the specifics of the data within it or to the current analysis focus this paper presents and discusses the in situ visualization strategy and its implications for dynamic graph_visualization furthermore it illustrates its usefulness by employing it for the visual_exploration of dynamic_networks from two different fields model versioning and wireless mesh networks dynamic_graph_data multi-focus+context multiform_visualization
we present a novel dynamic graph_visualization_technique based on node-link_diagrams the graphs are drawn side-byside from left to right as a sequence of narrow stripes that are placed perpendicular to the horizontal time line the hierarchically organized vertices of the graphs are arranged on vertical parallel lines that bound the stripes directed edges connect these vertices from left to right to address massive overplotting of edges in huge graphs we employ a splatting approach that transforms the edges to a pixel-based scalar field this field represents the edge densities in a scalable way and is depicted by non-linear color mapping the visualization method is complemented by interaction techniques that support data_exploration by aggregation filtering brushing and selective data zooming furthermore we formalize graph patterns so that they can be interactively highlighted on demand a case_study on software releases explores the evolution of call graphs extracted from the junit open source software project in a second application we demonstrate the scalability of our approach by applying it to a bibliography dataset containing more than 15 million paper titles from 60 years of research history producing a vast amount of relations between title words dynamic graph_visualization graph_splatting software_evolution software_visualization
the node-link diagram is an intuitive and venerable way to depict a graph to reduce clutter and improve the readability of node-link views holten & van wijk's force-directed edge_bundling employs a physical_simulation to spatially group graph edges while both useful and aesthetic this technique has shortcomings it bundles spatially proximal edges regardless of direction weight or graph connectivity as a result high-level directional edge patterns are obscured we present divided edge_bundling to tackle these shortcomings by modifying the forces in the physical_simulation directional lanes appear as an emergent property of edge direction by considering graph topology we only bundle edges related by graph structure finally we aggregate edge weights in bundles to enable more accurate visualization of total bundle weights we compare visualizations created using our technique to standard force-directed edge_bundling matrix diagrams and clustered graphs we find that divided edge_bundling leads to visualizations that are easier to interpret and reveal both familiar and previously obscured patterns graph_visualization aggregation edge_bundling node-link_diagrams physical_simulation
in this paper we present a novel approach for constructing bundled layouts of general graphs as layout cues for bundles we use medial axes or skeletons of edges which are similar in terms of position information we combine edge_clustering distance_fields and 2d skeletonization to construct progressively bundled layouts for general graphs by iteratively attracting edges towards the centerlines of level_sets of their distance_fields apart from clustering our entire pipeline is image-based with an efficientimplementation in graphics_hardware besides speed andimplementation simplicity our method allows explicit control of the emphasis on structure of the bundled layout ie the creation of strongly branching organic-like or smooth bundles we demonstrate our method on several large real-world graphs graph_layouts edge bundles image-based_information_visualization
birds are unrivaled windows into biotic processes at all levels and are proven indicators of ecological well-being understanding the determinants of species distributions and their dynamics is an important aspect of ecology and is critical for conservation and management through crowdsourcing since 2002 the ebird project has been collecting bird observation records these observations together with local-scale environmental covariates such as climate habitat and vegetation phenology have been a valuable resource for a global community of educators land managers ornithologists and conservation biologists by associating environmental inputs with observed patterns of bird occurrence predictive models have been developed that provide a statistical framework to harness available data for predicting species distributions and making inferences about species-habitat associations understanding these models however is challenging because they require scientists to quantify and compare multiscale spatialtemporal patterns a large series of coordinated or sequential plots must be generated individually programmed and manually composed for analysis this hampers the exploration and is a barrier to making the cross-species comparisons that are essential for coordinating conservation and extracting important ecological information to address these limitations as part of a collaboration among computer scientists statisticians biologists and ornithologists we have developed birdvis an interactive_visualization system that supports the analysis of spatio-temporal bird distribution models birdvis leverages visualization_techniques and uses them in a novel way to better assist users in the exploration of interdependencies among model parameters furthermore the system allows for comparative_visualization through coordinated_views providing an intuitive interface to identify relevant correlations and patterns we justify our design decisions and present case s- udies that show how birdvis has helped scientists obtain new evidence for existing hypotheses as well as formulate new hypotheses in their domain ornithology multiscale analysis spatial_data species_distribution_models temporal_data
the relationship between candidates' position on a ballot paper and vote rank is explored in the case of 5000 candidates for the uk 2010 local government elections in the greater london area this design_study uses hierarchical spatially arranged graphics to represent two locations that affect candidates at very different scales the geographical areas for which they seek election and the spatial location of their names on the ballot paper this approach allows the effect of position bias to be assessed that is the degree to which the position of a candidate's name on the ballot paper influences the number of votes received by the candidate and whether this varies geographically results show that position bias was significant enough to influence rank order of candidates and in the case of many marginal electoral wards to influence who was elected to government position bias was observed most strongly for liberal democrat candidates but present for all major political parties visual_analysis of classification of candidate names by ethnicity suggests that this too had an effect on votes received by candidates in some cases overcoming alphabetic name bias the results found contradict some earlier research suggesting that alphabetic name bias was not sufficiently significant to affect electoral outcome and add new evidence for the geographic and ethnicity influences on voting behaviour the visual approach proposed here can be applied to a wider range of electoral data and the patterns identified and hypotheses derived from them could have significant implications for the design of ballot papers and the conduct of fair elections voting bias democracy election geo visualization governance governance hierarchy treemaps
in this paper we introduce overview visualization tools for large-scale multiple genome alignment data genome alignment visualization and more generally sequence alignment visualization are an important tool for understanding genomic sequence data as sequencing techniques improve and more data become available greater demand is being placed on visualization tools to scale to the size of these new datasets when viewing such large_data we necessarily cannot convey details rather we specifically design overview tools to help elucidate large-scale patterns perceptual science signal_processing theory and generality provide a framework for the design of such visualizations that can scale well beyond current approaches we present sequence surveyor a prototype that embodies these ideas for scalable multiple whole-genome alignment overview visualization sequence surveyor visualizes sequences in parallel displaying data using variable color position and aggregation encodings we demonstrate how perceptual science can inform the design of visualization_techniques that remain visually manageable at scale and how signal_processing concepts can inform aggregation schemes that highlight global trendsoutliers and overall data distributions as the problem scales these techniques allow us to visualize alignments with over 100 whole bacterial-sized genomes bioinformatics_visualization perception_theory scalability_issues visual_design
image_analysis algorithms are often highly parameterized and much human input is needed to optimize parameter settings this incurs a time cost of up to several days we analyze and characterize the conventional parameter optimization process for image_analysis and formulate user requirements with this as input we propose a change in paradigm by optimizing parameters based on parameter sampling and interactive_visual_exploration to save time and reduce memory load users are only involved in the first step - initialization of sampling - and the last step - visual_analysis of output this helps users to more thoroughly explore the parameter space and produce higher quality results we describe a custom sampling plug-in we developed for cellprofiler - a popular biomedical image_analysis framework our main focus is the development of an interactive_visualization_technique that enables users to analyze the relationships between sampled input parameters and corresponding output we implemented this in a prototype called paramorama it provides users with a visual overview of parameters and their sampled values user-defined areas of interest are presented in a structured way that includes image-based output and a novel layout_algorithm to find optimal parameter settings users can tag high- and low-quality results to refine their search we include two case studies to illustrate the utility of this approach information_visualization image_analysis parameter space sampling visual_analytics
understanding how topics evolve in text data is an important and challenging task although much work has been devoted to topic analysis the study of topic_evolution has largely been limited to individual topics in this paper we introduce textflow a seamless integration of visualization and topic mining techniques for analyzing various evolution patterns that emerge from multiple topics we first extend an existing analysis technique to extract three-level features the topic_evolution trend the critical_event and the keyword correlation then a coherent visualization that consists of three new visual components is designed to convey complex relationships between them through interaction the topic mining model and visualization can communicate with each other to help users refine the analysis result and gain insights into the data progressively finally two case studies are conducted to demonstrate the effectiveness and usefulness of textflow in helping users understand the major topic_evolution patterns in time-varying text data critical_event hierarchical dirichlet process text_visualization topic_evolution
visual representations of time-series are useful for tasks such as identifying trends patterns and anomalies in the data many techniques have been devised to make these visual representations more scalable enabling the simultaneous display of multiple variables as well as the multi-scale display of time-series of very high resolution or that span long time periods there has been comparatively little research on how to support the more elaborate tasks associated with the exploratory visual_analysis of timeseries eg visualizing derived values identifying correlations or discovering anomalies beyond obviousoutliers such tasks typically require deriving new time-series from the original data trying different functions and parameters in an iterative manner we introduce a novel visualization_technique called chronolenses aimed at supporting users in such exploratory tasks chronolenses perform on-the-fly transformation of the data points in their focus area tightly integrating visual_analysis with user actions and enabling the progressive construction of advanced visual_analysis pipelines exploratory_visualization focus+context interaction techniques lens time-series_data
we propose incremental logarithmic time-series technique as a way to deal with time-based representations of large and dynamic event data sets in limited space modern data visualization problems in the domains of news analysis network_security and financial applications require visual_analysis of incremental data which poses specific challenges that are normally not solved by static visualizations the incremental nature of the data implies that visualizations have to necessarily change their content and still provide comprehensible representations in particular in this paper we deal with the need to keep an eye on recent events together with providing a context on the past and to make relevant patterns accessible at any scale our technique adapts to the incoming data by taking care of the rate at which data items occur and by using a decay function to let the items fade away according to their relevance since access to details is also important we also provide a novel distortion magnifying lens technique which takes into account the distortions introduced by the logarithmic time scale to augment readability in selected areas of interest we demonstrate the validity of our techniques by applying them on incremental data coming from online news streams in different time frames incremental_visualization event based data lens_distortion
node-link_diagrams are an effective and popular visualization approach for depicting hierarchical structures and for showing parent-child relationships in this paper we present the results of an eye_tracking experiment investigating traditionalorthogonal and radial node-link tree layouts as a piece of empirical basis for choosing between those layouts eye_tracking was used to identify visual_exploration behaviors of participants that were asked to solve a typical hierarchy exploration task by inspecting a static tree diagram finding the least common ancestor of a given set of marked leaf nodes to uncover exploration strategies we examined fixation points duration and saccades of participants' gaze trajectories for the non-radial diagrams we additionally investigated the effect of diagram orientation by switching the position of the root node to each of the four main orientations we also recorded and analyzed correctness of answers as well as completion times in addition to the eye movement_data we found out that traditional andorthogonal tree layouts significantly outperform radial_tree_layouts for the given task furthermore by applying trajectory_analysis techniques we uncovered that participants cross-checked their task solution more often in the radial than in the non-radial layouts hierarchy_visualization eye_tracking node-link_layout user_study
network data often contain important attributes from various dimensions such as social affiliations and areas of expertise in a social network if such attributes exhibit a tree structure visualizing a compound graph consisting of tree and network structures becomes complicated how to visually reveal patterns of a network over a tree has not been fully studied in this paper we propose a compound graph model treenet to support visualization and analysis of a network at multiple levels of aggregation over a tree we also present a visualization design treenetviz to offer the multiscale and cross-scale exploration and interaction of a treenet graph treenetviz uses a radial space-filling rsf visualization to represent the tree structure a circle layout with novel optimization to show aggregated networks derived from treenet and an edge_bundling technique to reduce visual complexity our circular layout_algorithm reduces both total edge-crossings and edge length and also considers hierarchical structure constraints and edge weight in a treenet graph these experiments illustrate that the algorithm can reduce visual_cluttering in treenet graphs our case_study also shows that treenetviz has the potential to support the analysis of a compound graph by revealing multiscale and cross-scale network patterns compound graph treenetviz multiscale and cross-scale network_and_tree_visualization
an alternative form to multidimensional projections for the visual_analysis of data represented in multidimensional spaces is the deployment of similarity_trees such as neighbor joining trees they organize data objects on the visual plane emphasizing their levels of similarity with high capability of detecting and separating groups and subgroups of objects besides this similarity-based hierarchical_data organization some of their advantages include the ability to decrease point clutter high precision and a consistent view of the data set during focusing offering a very intuitive way to view the general structure of the data set as well as to drill down to groups and subgroups of interest disadvantages of similarity_trees based on neighbor joining strategies include their computational cost and the presence of virtual nodes that utilize too much of the visual space this paper presents a highly improved version of the similarity tree technique the improvements in the technique are given by two procedures the first is a strategy that replaces virtual nodes by promoting real leaf nodes to their place saving large portions of space in the display and maintaining the expressiveness and precision of the technique the second improvement is animplementation that significantly accelerates the algorithm impacting its use for larger data sets we also illustrate the applicability of the technique in visual_data_mining showing its advantages to support visual classification of data sets with special attention to the case of image classification we demonstrate the capabilities of the tree for analysis and iterative manipulation and employ those capabilities to support evolving to a satisfactory data organization and classification image classification multidimensional projections similarity_trees
we present the results of a user_study that compares different ways of representing dual-scale data charts dual-scale_charts incorporate two different data resolutions into one chart in order to emphasize data in regions of interest or to enable the comparison of data from distant regions while some design_guidelines exist for these types of charts there is currently little empirical evidence on which to base their design we fill this gap by discussing the design space of dual-scale cartesian-coordinate charts and by experimentally comparing the performance of different chart types with respect to elementary graphical_perception tasks such as comparing lengths and distances our study suggests that cut-out charts which include collocated full context and focus are the best alternative and that superimposed charts in which focus_and_context overlap on top of each other should be avoided dual-scale_charts focus+context quantitative experiment
heart disease is the number one killer in the united states and finding indicators of the disease at an early stage is critical for treatment and prevention in this paper we evaluate visualization_techniques that enable the diagnosis of coronary artery disease a key physical quantity of medical interest is endothelial shear stress ess low ess has been associated with sites of lesion formation and rapid progression of disease in the coronary arteries having effective visualizations of a patient's ess data is vital for the quick and thorough non-invasive evaluation by a cardiologist we present a task_taxonomy for hemodynamics based on a formative user_study with domain experts based on the results of this study we developed hemovis an interactive_visualization_application for heart disease diagnosis that uses a novel 2d tree diagram representation of coronary artery trees we present the results of a formal quantitative user_study with domain experts that evaluates the effect of 2d versus 3d artery representations and of color maps on identifying regions of low ess we show statistically significant results demonstrating that our 2d visualizations are more accurate and efficient than 3d representations and that a perceptually appropriate color map leads to fewer diagnostic mistakes than a rainbow color map quantitative_evaluation biomedical and medical visualization qualitative_evaluation
providing effective feedback on resource consumption in the home is a key challenge of environmental conservation efforts one promising approach for providing feedback about residential energy_consumption is the use of ambient and artistic visualizations pervasive computing technologies enable the integration of such feedback into the home in the form of distributed point-of-consumption feedback devices to support decision-making in everyday activities however introducing these devices into the home requires sensitivity to the domestic context in this paper we describe three abstract visualizations and suggest four design requirements that this type of device must meet to be effective pragmatic aesthetic ambient and ecological we report on the findings from a mixed methods user_study that explores the viability of using ambient and artistic feedback in the home based on these requirements our findings suggest that this approach is a viable way to provide resource use feedback and that both the aesthetics of the representation and the context_of_use are important elements that must be considered in this design space ambient visualization casual infovis distributed_visualization informative_art sustainability
working with three domain specialists we investigate human-centered approaches to geo visualization following an iso13407 taxonomy covering context_of_use requirements and early stages of design our case_study undertaken over three years draws attention to repeating trends that generic approaches fail to elicit adequate requirements for geovis application design that the use of real data is key to understanding needs and possibilities that trust and knowledge must be built and developed with collaborators these processes take time but modified human-centred approaches can be effective a scenario developed through contextual inquiry but supplemented with domain data and graphics is useful to geovis designers wireframe paper and digital prototypes enable successfulcommunication between specialist and geovis domains when incorporating real and interesting data prompting exploratory behaviour and eliciting previously unconsidered requirements paper prototypes are particularly successful at eliciting suggestions especially for novel visualization enabling specialists to explore their data freely with a digital prototype is as effective as using a structured task protocol and is easier to administer autoethnography has potential for framing the design process we conclude that a common understanding of context_of_use domain data and visualization possibilities are essential to successful geovis design and develop as this progresses hc approaches can make a significant contribution here however modified approaches applied with flexibility are most promising we advise early collaborative engagement with data - through simple transient visual artefacts supported by data sketches and existing designs - before moving to successively more sophisticated data wireframes and data prototypes evaluation context_of_use design field study geo visualization prototypes requirements sketching
while it is still most common for information_visualization researchers to develop new visualizations from a data-or taskdriven perspective there is growing interest in understanding the types of visualizations people create by themselves for personal use as part of this recent direction we have studied a large collection of whiteboards in a research institution where people make active use of combinations of words diagrams and various types of visuals to help them further their thought processes our goal is to arrive at a better understanding of the nature of visuals that are created spontaneously during brainstorming thinking communicating and general problem_solving on whiteboards we use the qualitative approaches of open coding interviewing and affinity diagramming to explore the use of recognizable and novel visuals and the interplay between visualization and diagrammatic elements with words numbers and labels we discuss the potential implications of our findings on information_visualization design visualization diagrams observational_study whiteboards
we consider moving objects as multivariate time-series by visually analyzing the attributes patterns may appear that explain why certain movements have occurred density maps as proposed by scheepens et al [25] are a way to reveal these patterns by means of aggregations of filtered subsets of trajectories since filtering is often not sufficient for analysts to express their domain knowledge we propose to use expressions instead we present a flexible architecture for density maps to enable custom versatile exploration using multiple density fields the flexibility comes from a script depicted in this paper as a block diagram which defines an advanced computation of a density field we define six different types of blocks to create compose and enhance trajectories or density fields blocks are customized by means of expressions that allow the analyst to model domain knowledge the versatility of our architecture is demonstrated with several maritime use cases developed with domain experts our approach is expected to be useful for the analysis of objects in other domains geographical_information_systems kernel density estimation multivariate data trajectories and raster_maps
we introduce a focus+context method to visualize a complicated metro map of a modern city on a small displaying area the context of our work is with regard the popularity of mobile devices the best route to the destination which can be obtained from the arrival time of trains is highlighted the stations on the route enjoy larger spaces whereas the other stations are rendered smaller and closer to fit the whole map into a screen to simplify the navigation and route planning for visitors we formulate various map characteristics such as octilinear transportation lines and regular station distances into energy terms we then solve for the optimal layout in a least squares sense in addition we label the names of stations that are on the route of a passenger according to human preferences occlusions and consistencies of label positions using the graph cuts method our system achieves real-time performance by being able to report instant information because of the carefully designed energy terms we apply our method to layout a number of metro maps and show the results and timing statistics to demonstrate the feasibility of our technique focus+context_visualization graph_labeling metro map octilinear_layout optimization
flow_maps are thematic maps that visualize the movement of objects such as people or goods between geographic regions one or more sources are connected to several targets by lines whose thickness corresponds to the amount of flow between a source and a target good flow_maps reduce visual_clutter by merging bundling lines smoothly and by avoiding self-intersections most flow_maps are still drawn by hand and only few automated methods exist some of the known algorithms do not support edge-bundling and those that do cannot guarantee crossing-free flows we present a new algorithmic method that uses edge-bundling and computes crossing-free flows of high visual quality our method is based on so-called spiral trees a novel type of steiner tree which uses logarithmic spirals spiral trees naturally induce a clustering on the targets and smoothly bundle lines our flows can also avoid obstacles such as map features region outlines or even the targets we demonstrate our approach with extensive experiments automated_cartography flow_maps spiral trees
geodemographic classifiers characterise populations by categorising geographical areas according to the demographic and lifestyle characteristics of those who live within them the dimension-reducing quality of such classifiers provides a simple and effective means of characterising population through a manageable set of categories but inevitably hides heterogeneity which varies within and between the demographic categories and geographical areas sometimes systematically this may have implications for their use which is widespread in government and commerce for planning marketing and related activities we use novel interactive_graphics to delve into oac - a free and open geodemographic classifier that classifies the uk population in over 200000 small geographical areas into 7 super-groups 21 groups and 52 sub-groups our graphics provide access to the original 41 demographic variables used in the classification and the uncertainty associated with the classification of each geographical area on-demand it also supports comparison geographically and by category this serves the dual purpose of helping understand the classifier itself leading to its more informed use and providing a more comprehensive view of population in a comprehensible manner we assess the impact of these interactive_graphics on experienced oac users who explored the details of the classification its uncertainty and the nature of between - and within - class variation and then reflect on their experiences visualization of the complexities and subtleties of the classification proved to be a thought-provoking exercise both confirming and challenging users' understanding of population the oac classifier and the way it is used in their organisations users identified three contexts for which the techniques were deemed useful in the context of local government confirming the validity of the proposed methods geodemographics oac cartography classification uncertainty
mobile users of maps typically need detailed information about their surroundings plus some context information about remote places in order to avoid that the map partly gets too dense cartographers have designed mapping functions that enlarge a user-defined focus region - such functions are sometimes called fish-eye projections the extra map space occupied by the enlarged focus region is compensated by distorting other parts of the map we argue that in a map showing a network of roads relevant to the user distortion should preferably take place in those areas where the network is sparse therefore we do not apply a predefined mapping function instead we consider the road network as a graph whose edges are the road segments we compute a new spatial mapping with a graph-based optimization approach minimizing the square sum of distortions at edges our optimization method is based on a convex quadratic program cqp cqps can be solved in polynomial time important requirements on the output map are expressed as linear inequalities in particular we show how to forbid edge crossings we have implemented our method in a prototype tool for instances of different sizes our method generated output maps that were far less distorted than those generated with a predefined fish-eye projection future work is needed to automate the selection of roads relevant to the user furthermore we aim at fast heuristics for application in real-time systems cartography fish-eye view graph_drawing optimization quadratic_programming schematic_maps
multidimensional projection techniques have experienced many improvements lately mainly regarding computational times andaccuracy however existing methods do not yet provide flexible enough mechanisms for visualization-oriented fully interactive applications this work presents a new multidimensional projection technique designed to be more flexible and versatile than other methods this novel approach called local affine multidimensional projection lamp relies onorthogonal mapping theory to build accurate local transformations that can be dynamically modified according to user knowledge theaccuracy flexibility and computational efficiency of lamp is confirmed by a comprehensive set of comparisons lamp's versatility is exploited in an application which seeks to correlate data that in principle has no connection as well as in visual_exploration of textual documents high dimensional data multidimensional projection visual_data_mining
parallel_coordinates is a popular and well-known multivariate data visualization_technique however one of their inherent limitations has to do with the rendering of very large_data sets this often causes an overplotting problem and the goal of the visual_information_seeking mantra is hampered because of a cluttered overview and non-interactive update rates in this paper we propose two novel solutions namely angular_histograms and attribute curves these techniques are frequency-based approaches to large high-dimensional_data_visualization they are able to convey both the density of underlying polylines and their slopes angular_histogram and attribute curves offer an intuitive way for the user to explore the clustering linear correlations andoutliers in large_data sets without the over-plotting and clutter problems associated with traditional parallel_coordinates we demonstrate the results on a wide variety of data sets including real-world high-dimensional biological_data finally we compare our methods with the other popular frequency-based algorithms angular_histogram attribute curves parallel_coordinates
clustering as a fundamental data_analysis technique has been widely used in many analytic applications however it is often difficult for users to understand and evaluate multidimensional clustering results especially the quality of clusters and their semantics for large and complex data high-level statistical information about the clusters is often needed for users to evaluate cluster quality while a detailed display of multidimensional attributes of the data is necessary to understand the meaning of clusters in this paper we introduce dicon an icon-based cluster visualization that embeds statistical information into a multi-attribute display to facilitate cluster interpretation evaluation and comparison we design a treemap-like icon to represent a multidimensional cluster and the quality of the cluster can be conveniently evaluated with the embedded statistical information we further develop a novel layout_algorithm which can generate similar icons for similar clusters making comparisons of clusters easier user_interaction and clutter_reduction are integrated into the system to help users more effectively analyze and refine clustering results for large_datasets we demonstrate the power of dicon through a user_study and a case_study in the healthcare domain our evaluation shows the benefits of the technique especially in support of complex multidimensional cluster analysis clustering information_visualization visual_analysis
summary form only given this talk will discuss the role of visual thinking in scientific discovery and technological invention visual thinking uses picture-like representations as internal mental_models or as external depictions such as diagrams the first part of the talk will analyze the role of visual thinking in 100 great discoveries and 100 great inventions the second part will discuss the contribution of visual thinking to developing new theories in the social sciences based on advances in cognitive science cognitive-affective mapping is a new technique for visualizing the role of emotion in social cognition empathica is a new graphical system for resolving conflicts by increasing empathy using cognitive-affective maps
we have observed increasing interest in visual_analytics tools and their applications in investigative_analysis despite the growing interest and substantial studies regarding the topic understanding the major roadblocks of using such tools from novice users' perspectives is still limited therefore we attempted to identify such “visual analytic roadblocks” for novice users in an investigative_analysis scenario to achieve this goal we reviewed the existing models theories and frameworks that could explain the cognitive processes of human-visualization interaction in investigative_analysis then we conducted a qualitative_experiment with six novice participants using a slightly modified version of pair analytics and analyzed the results through the open-coding method as a result we came up with four visual analytic roadblocks and explained these roadblocks using existing cognitive models and theories we also provided design suggestions to overcome these roadblocks visual_analytics cognitive model framework investigative_analysis qualitative_experiment roadblock
in recent years diverse quality measures to support the exploration of high-dimensional_data sets have been proposed such measures can be very useful to rank and select information-bearing projections of very high dimensional data when the visual_exploration of all possible projections becomes unfeasible but even though a ranking of the low dimensional projections may support the user in the visual_exploration task different measures deliver different distances between the views that do not necessarily match the expectations of human perception as an alternative solution we propose a perception-based approach that similar to the existing measures can be used to select information bearing projections of the data specifically we construct a perceptual embedding for the different projections based on the data from a psychophysics study and multi-dimensional_scaling this embedding together with a ranking function is then used to estimate the value of the projections for a specific user task in a perceptual sense
while intelligence_analysis has been a primary target domain for visual_analytics system development relatively little user and task analysis has been conducted within this area our research community's understanding of the work processes and practices of intelligence analysts is not deep enough to adequately address their needs without a better understanding of the analysts and their problems we cannot build visual_analytics systems that integrate well with their work processes and truly provide benefit to them in order to close this knowledge gap we conducted a longitudinal observational field study of intelligence analysts in training within the intelligence program at mercyhurst college we observed three teams of analysts each working on an intelligence problem for a ten-week period based upon study findings we describe and characterize processes and methods of intelligence_analysis that we observed make clarifications regarding the processes and practices and suggest design implications for visual_analytics systems for intelligence_analysis intelligence_analysis qualitatvie user_study
traditionally the visual_analysis of hierarchies respectively trees is conducted by focusing on one given hierarchy however in many research areas multiple differing hierarchies need to be analyzed simultaneously in a comparative way - in particular to highlight differences between them which sometimes can be subtle a prominent example is the analysis of so-called phylogenetic_trees in biology reflecting hierarchical evolutionary relationships among a set of organisms typically the analysis considers multiple phylogenetic_trees either to account for statistical significance or for differences in derivation of such evolutionary hierarchies for example based on different input data such as the 16s ribosomal rna and protein sequences of highly conserved enzymes the simultaneous analysis of a collection of such trees leads to more insight into the evolutionary process we introduce a novel visual_analytics approach for the comparison of multiple hierarchies focusing on both global and local structures a new tree_comparison score has been elaborated for the identification of interesting patterns we developed a set of linked hierarchy views showing the results of automatic tree_comparison on various levels of details this combined approach offers detailed assessment of local and global tree similarities the approach was developed in close cooperation with experts from the evolutionary biology domain we apply it to a phylogenetic data set on bacterial ancestry demonstrating its application benefit
tabular_data are pervasive although tables often describe multivariate data without explicit network semantics it may be advantageous to explore the data modeled as a graph or network for analysis even when a given table design conveys some static network semantics analysts may want to look at multiple networks from different perspectives at different levels of abstraction and with different edge semantics we present a system called ploceus that offers a general approach for performing multi-dimensional and multi-level network-based visual_analysis on multivariate tabular_data powered by an underlying relational algebraic framework ploceus supports flexible construction and transformation of networks through a direct_manipulation interface and integrates dynamic network manipulation with visual_exploration for a seamless analytic experience
the study of complex activities such as scientific production and software development often require modeling connections among heterogeneous entities including people institutions and artifacts despite numerous advances in algorithms and visualization_techniques for understanding such social_networks the process of constructing network models and performing exploratory_analysis remains difficult and time-consuming in this paper we present orion a system for interactive modeling transformation and visualization of network data orion's interface enables the rapid manipulation of large graphs-including the specification of complex linking relationships-using simple drag-and-drop operations with desired node types orion maps these user_interactions to statements in a declarative workflow language that incorporates both relational operators eg selection aggregation and joins and network analytics eg centrality measures we demonstrate how these features enable analysts to flexibly construct and compare networks in domains such as online health communities academic collaboration and distributed software development data_management data_transformation end-user_programming graphs social_network_analysis visualization
there are a growing number of machine_learning algorithms which operate on graphs example applications for these algorithms include predicting which customers will recommend products to their friends in a viral marketing campaign using a customer network predicting the topics of publications in a citation network or predicting the political affiliations of people in a social network it is important for an analyst to have tools to help compare the output of these machine_learning algorithms in this work we present g-pare a visual analytic tool for comparing two uncertain_graphs where each uncertain graph is produced by a machine_learning algorithm which outputs probabilities over node labels g-pare provides several different views which allow users to obtain a global overview of the algorithms output as well as focused views that show subsets of nodes of interest by providing an adaptive exploration environment g-pare guides the users to places in the graph where two algorithms predictions agree and places where they disagree this enables the user to follow cascades of misclassifications by comparing the algorithms outcome with the ground truth after describing the features of g-pare we illustrate its utility through several use cases based on networks from different domains comparative_analysis model comparison uncertain_graphs visualizing uncertainty
as people continue to author and share increasing amounts of information in social media the opportunity to leverage such information for relationship discovery tasks increases in this paper we describe a set of systems that mine aggregate and infer a social graph from social media inside an enterprise resulting in over 73 million relationships between 450000 people we then describe sandvis a novel visual_analytics tool that supports people-centric tasks like expertise location team building and team coordination in the enterprise we also provide details of a 12-month-long large-scale deployment to almost 1800 users from which we extract dominant use cases from log and interview data by integrating social position evidence and facets into sandvis we demonstrate how users can use a visual_analytics tool to reflect on existing relationships as well as build new relationships in an enterprise setting information discovery social_data_mining social_networks social_visualization
existing research suggests that individual personality differences are correlated with a user's speed andaccuracy in solving problems with different types of complex visualization_systems in this paper we extend this research by isolatingfactors in personality traits as well as in the visualizations that could have contributed to the observed correlation we focus on a personality trait known as “locus of control” which represents a person's tendency to see themselves as controlled by or in control of external events to isolate variables of the visualization design we control extraneousfactors such as color interaction and labeling and specifically focus on the overall layout style of the visualizations we conduct a user_study with four visualizations that gradually shift from an indentation metaphor to a containment metaphor and compare the participants' speedaccuracy and preference with their locus of control our findings demonstrate that there is indeed a correlation between the two participants with an internal locus of control perform more poorly with visualizations that employ a containment metaphor while those with an external locus of control perform well with such visualizations we discuss a possible explanation for this relationship based in cognitive psychology and propose that these results can be used to better understand how people use visualizations and how to adapt visual_analytics design to an individual user's needs
this article describes “obvious” a meta-toolkit that abstracts and encapsulates information_visualization toolkits implemented in the java language it intends to unify their use and postpone the choice of which concrete toolkits to use later-on in the development of visual_analytics applications we also report on the lessons we have learned when wrapping popular toolkits with obvious namely prefuse the infovis toolkit partly improvise jung and other data_management libraries we show several examples on the uses of obvious how the different toolkits can be combined for instance sharing their data models we also show how weka and rapid-miner two popular machine-learning toolkits have been wrapped with obvious and can be used directly with all the other wrapped toolkits we expect obvious to start a co-evolution process obvious is meant to evolve when more components of information_visualization_systems will become consensual it is also designed to help information_visualization_systems adhere to the best_practices to provide a higher level of interoperability and leverage the domain of visual_analytics
asynchronous collaborative visual_analytics acva leverages group sensemaking by releasing the constraints on when where and who works collaboratively a significant task to be addressed before acva can reach its full potential is effective common ground construction namely the process in which users evaluate insights from individual work to develop a shared understanding of insights and collectively pool them this is challenging due to the lack of instantcommunication and scale of collaboration in acva we propose a novel visual_analytics approach that automatically gathers organizes and summarizes insights to form common ground with reduced human effort the rich set of visualization and interaction techniques provided in our approach allows users to effectively and flexibly control the common ground construction and review explore and compare insights in detail a working prototype of the approach has been implemented we have conducted a case_study and a user_study to demonstrate its effectiveness visual_analytics asynchronous_collaboration insight multidimensional visualization
we propose a method for the semi-automated refinement of the results of feature subset selection algorithms feature subset selection is a preliminary step in data_analysis which identifies the most useful subset of features columns in a data table so-called filter techniques use statistical ranking measures for the correlation of features usually a measure is applied to all entities rows of a data table however the differing contributions of subsets of data entities are masked by statistical aggregation feature and entity subset selection are thus highly interdependent due to the difficulty in visualizing a high-dimensional_data table most feature subset selection algorithms are applied as a black box at the outset of an analysis our visualization_technique smartstripes allows users to step into the feature subset selection process it enables the investigation of dependencies and interdependencies between different feature and entity subsets a user may even choose to control the iterations manually taking into account the ranking measures the contributions of different entity subsets as well as the semantics of the features
in visual_analytics sensemaking is facilitated through interactive_visual_exploration of data throughout this dynamic process users combine their domain knowledge with the dataset to create insight therefore visual analytic tools exist that aid sensemaking by providing various interaction techniques that focus on allowing users to change the visual representation through adjusting parameters of the underlying statistical model however we postulate that the process of sensemaking is not focused on a series of parameter adjustments but instead a series of perceived connections and patterns within the data thus how can models for visual analytic tools be designed so that users can express their reasoning on observations the data instead of directly on the model or tunable parameters observation level and thus “observation” in this paper refers to the data points within a visualization in this paper we explore two possible observation-level interactions namely exploratory and expressive within the context of three statistical methods probabilistic principal_component_analysis ppca multidimensional scaling mds and generative topographic mapping gtm we discuss the importance of these two types of observation level interactions in terms of how they occur within the sensemaking process further we present use cases for gtm mds and ppca illustrating how observation level interaction can be incorporated into visual analytic tools observation-level interaction statistical_models visual_analytics
sensitivity_analysis is a powerful method for discovering the significantfactors that contribute to targets and understanding the interaction between variables in multivariate datasets a number of sensitivity_analysis methods fall into the class of local analysis in which the sensitivity is defined as the partial derivatives of a target variable with respect to a group of independent variables incorporating sensitivity_analysis in visual analytic tools is essential for multivariate phenomena analysis however most current multivariate visualization_techniques do not allow users to explore local patterns individually for understanding the sensitivity from a pointwise view in this paper we present a novel pointwise local pattern exploration system for visual sensitivity_analysis using this system analysts are able to explore local patterns and the sensitivity at individual data points which reveals the relationships between a focal point and its neighbors during exploration users are able to interactively change the derivative coefficients to perform sensitivity_analysis based on different requirements as well as their domain knowledge each local pattern is assigned an outlier factor so that users can quickly identify anomalous local patterns that do not conform with the global pattern users can also compare the local pattern with the global pattern both visually and statistically finally the local pattern is integrated into the original attribute space using color mapping and jittering which reveals the distribution of the partial derivatives case studies with real datasets are used to investigate the effectiveness of the visualizations and interactions knowledge_discovery local pattern visualization sensitivity_analysis
to make informed decisions an expert has to reason with multi-dimensional heterogeneous data and analysis results of these items in such datasets are typically represented by features however as argued in cognitive science features do not yield an optimal space for human reasoning in fact humans tend to organize complex information in terms of prototypes or known cases rather than in absolute terms when confronted with unknown data items humans assess them in terms of similarity to these prototypical elements interestingly an analogues similarity-to-prototype approach where prototypes are taken from the data has been successfully applied in machine_learning combining such a machine_learning approach with human prototypical reasoning in a visual_analytics context requires to integrate similarity-based classification with interactive_visualizations to that end the data prototypes should be visually represented to trigger direct associations to cases familiar to the domain experts in this paper we propose a set of highly interactive_visualizations to explore data and classification results in terms of dissimilarities to visually represented prototypes we argue that this approach not only supports human reasoning_processes but is also suitable to enhance understanding of heterogeneous data the proposed framework is applied to a risk_assessment case_study in forensic psychiatry dissimilarity based classification dissimilarity based visualization interactive_visualization prototypes visual_analytics
we present a system for the interactive construction and analysis of decision_trees that enables domain experts to bring in domain specific knowledge we identify different user tasks and corresponding requirements and develop a system incorporating a tight integration of visualization interaction and algorithmic support domain experts are supported in growing pruning optimizing and analysing decision_trees furthermore we present a scalable decision tree_visualization optimized for exploration we show the effectiveness of our approach by applying the methods to two use cases the first case illustrates the advantages of interactive construction the second case demonstrates the effectiveness of analysis of decision_trees and exploration of the structure of the data
we propose a visual_analytics procedure for analyzing movement_data ie recorded tracks of moving objects it is oriented to a class of problems where it is required to determine significant places on the basis of certain types of events occurring repeatedly in movement_data the procedure consists of four major steps 1 event extraction from trajectories 2 event clustering and extraction of relevant places 3 spatio-temporal aggregation of events or trajectories 4 analysis of the aggregated_data all steps are scalable with respect to the amount of the data under analysis we demonstrate the use of the procedure by example of two real-world problems requiring analysis at different spatial scales movement spatial_clustering spatial events spatio-temporal_clustering spatio-temporal_data trajectories
route suggestion is an important feature of gps navigation systems recently microsoft t-drive has been enabled to suggest routes chosen by experienced taxi drivers for given source/destination pairs in given time periods which often take less time than the routes calculated according to distance however in real environments taxi drivers may use different routes to reach the same destination which we call route diversity in this paper we first propose a trajectory visualization method that examines the regions where the diversity exists and then develop several novel visualization_techniques to display the high dimensional attributes and statistics associated with different routes to help users analyze diversity patterns our techniques have been applied to the real trajectory data of thousands of taxis and some interesting findings about route diversity have been obtained we further demonstrate that our system can be used not only to suggest better routes for drivers but also to analyze traffic bottlenecks for transportation management
geographically-grounded situational awareness sa is critical to crisis_management and is essential in many other decision_making domains that range from infectious disease monitoring through regional planning to political campaigning social media are becoming an important information input to support situational assessment to produce awareness in all domains here we present a geo visual_analytics approach to supporting sa for crisis events using one source of social media twitter specifically we focus on leveraging explicit and implicit geographic_information for tweets on developing place-time-theme indexing schemes that support overview+detail methods and that scale analytical capabilities to relatively large tweet volumes and on providing visual interface methods to enable understanding of place time and theme components of evolving situations our approach is user-centered using scenario-based_design_methods that include formal scenarios to guide design and validateimplementation as well as a systematic claims analysis to justify design choices and provide a framework for future testing the work is informed by a structured survey of practitioners and the end product of phase-i development is demonstrated / validated throughimplementation in senseplace2 a map-based web application initially focused on tweets but extensible to other media crisis_management geo visualization scenario-based_design situational awareness social_media_analytics spatio-temporal analysis text_analytics
in modeling infectious diseases scientists are studying the mechanisms by which diseases spread predicting the future course of the outbreak and evaluating strategies applied to control an epidemic while recent work has focused on accurately modeling disease spread less work has been performed in developing interactive decision support tools for analyzing the future course of the outbreak and evaluating potential disease mitigation strategies the absence of such tools makes it difficult for researchers analysts and public health officials to evaluate response measures within outbreak scenarios as such our research focuses on the development of an interactive decision support environment in which users can explore epidemic models and their impact this environment provides a spatiotemporal view where users can interactively utilize mitigative response measures and observe the impact of their decision over time our system also provides users with a linked decision history visualization and navigation tool that support the simultaneous comparison of mortality and infection rates corresponding to different response measures at different points in time
diagnosing a large-scale sensor network is a crucial but challenging task particular challenges include the resource and bandwidth constraints on sensor nodes the spatiotemporally dynamic network behaviors and the lack of accurate models to understand such behaviors in a hostile environment in this paper we present the sensor anomaly visualization engine save a system that fully leverages the power of both visualization and anomaly_detection analytics to guide the user to quickly and accurately diagnose sensor network failures and faults save combines customized visualizations over separate sensor data facets as multiple_coordinated_views temporal expansion model correlation graph and dynamic projection views are proposed to effectively interpret the topological correlational and dimensional sensor data dynamics and their anomalies through a case_study with real-world sensor network system and administrators we demonstrate that save is able to help better locate the system problem and further identify the root cause of major sensor network failure scenarios
cellular biology deals with studying the behavior of cells current time-lapse imaging microscopes help us capture the progress of experiments at intervals that allow for understanding of the dynamic and kinematic behavior of the cells on the other hand these devices generate such massive amounts of data 250gb of data per experiment that manual sieving of data to identify interesting patterns becomes virtually impossible in this paper we propose an end-to-end system to analyze time-lapse images of the cultures of human neural stem cells hnsc that includes an image_processing system to analyze the images to extract all the relevant geometric and statistical features within and between images a database management system to manage and handle queries on the data a visual analytic system to navigate through the data and a visual query system to explore different relationships and correlations between the parameters in each stage of the pipeline we make novel algorithmic and conceptual contributions and the entire system design is motivated by many different yet unanswered exploratory questions pursued by our neurobiologist collaborators with a few examples we show how such abstract biological queries can be analyzed and answered by our system neuroscience cell_imaging data_management exploration navigation query_processing stem_cell_segmentation tracking visual_analytics
in this paper we present our collaborative work with the us coast guard's ninth district and atlantic area commands where we developed a visual_analytics system to analyze historic response operations and assess the potential risks in the maritime environment associated with the hypothetical allocation of coast guard resources the system includes linked_views and interactive_displays that enable the analysis of trends patterns and anomalies among the us coast guard search and rescue sar operations and their associated sorties our system allows users to determine the potential change in risks associated with closing certain stations in terms of response time potential lives and property lost and provides optimal direction as to the nearest available station we provide maritime risk_assessment tools that allow analysts to explore coast guard coverage for sar operations and identify regions of high risk the system also enables a thorough assessment of all sar operations conducted by each coast guard station in the great lakes region our system demonstrates the effectiveness of visual_analytics in analyzing risk within the maritime domain and is currently being used by analysts at the coast guard atlantic area coast guard visual_analytics risk_assessment
scalable and effective analysis of large text corpora remains a challenging problem as our ability to collect textual data continues to increase at an exponential rate to help users make sense of large text corpora we present a novel visual_analytics system parallel-topics which integrates a state-of-the-art probabilistic topic model latent_dirichlet_allocation lda with interactive_visualization to describe a corpus of documents paralleltopics first extracts a set of semantically meaningful topics using lda unlike most traditional clustering techniques in which a document is assigned to a specific cluster the lda model accounts for different topical aspects of each individual document this permits effective full text_analysis of larger documents that may contain multiple topics to highlight this property of the model paralleltopics utilizes the parallel coordinate metaphor to present the probabilistic distribution of a document across topics such representation allows the users to discover single-topic vs multi-topic documents and the relative importance of each topic to a document of interest in addition since most text corpora are inherently temporal paralleltopics also depicts the topic_evolution over time we have applied paralleltopics to exploring and analyzing several text corpora including the scientific proposals awarded by the national science foundation and the publications in the vast community over the years to demonstrate the efficacy of paralleltopics we conducted several expert evaluations the results of which are reported in this paper
to make decisions about the long-term preservation and access of large digital_collections archivists gather information such as the collections' contents their organizational structure and their file format composition to date the process of analyzing a collection - from data gathering to exploratory_analysis and final conclusions - has largely been conducted using pen and paper methods to help archivists analyze large-scale digital_collections for archival purposes we developed an interactive visual_analytics application the application narrows down different kinds of information about the collection and presents them as meaningful data views multiple_views and analysis features can be linked or unlinked on demand to enable researchers to compare and contrast different analyses and to identify trends we describe and present two user scenarios to show how the application allowed archivists to learn about a collection withaccuracy facilitated decision-making and helped them arrive at conclusions digital_collections archival_analysis data_curation visual_analytics
a perennially interesting research topic in the field of visual_analytics is how to effectively develop systems that support organizational users' decision-making and reasoning_processes the problem is however most domain analytical practices generally vary from organization to organization this leads to diverse designs of visual_analytics systems in incorporating domain analytical processes making it difficult to generalize the success from one domain to another exacerbating this problem is the dearth of general models of analytical workflows available to enable such timely and effective designs to alleviate these problems we present a two-stage framework for informing the design of a visual_analytics system this design framework builds upon and extends current practices pertaining to analytical workflow and focuses in particular on incorporating both general domain analysis processes as well as individual's analytical activities we illustrate both stages and their design components through examples and hope this framework will be useful for designing future visual_analytics systems we validate the soundness of our framework with two visual_analytics systems namely entity workspace [8] and patviz [37] design_theory hci visual_analytics
projection pursuit has been an effective method for finding interesting low-dimensional usually 2d_projections in multidimensional spaces unfortunately projection pursuit is not scalable to high-dimensional spaces we introduce a novel method for approximating the results of projection pursuit to find class-separating views by using random projections we build an analytic visualization platform based on this algorithm that is scalable to extremely large problems then we discuss its extension to the recognition of other noteworthy configurations in high-dimensional spaces
visual_analytics “the science of analytical reasoning facilitated by visual interactive interfaces” [5] puts high demands on the applications visualization as well as interaction capabilities due to their size large high-resolution screens have become popular display devices especially when used in collaborative data_analysis scenarios however traditional interaction methods based on combinations of computer mice and keyboards often do not scale to the number of users or the size of the display modern smart phones featuring multi-modal input/output and considerable memory offer a way to address these issues in the last couple of years they have become common everyday life gadgets in this paper we conduct an extensive user_study comparing the experience of test candidates when using traditional input devices and metaphors with the one when using new smart phone based techniques like multi-modal drag and tilt candidates were asked to complete various interaction tasks relevant for most applications on a large monitor-based high-resolution tiled wall system our study evaluates both user performance and satisfaction identifying strengths and weaknesses of the researched interaction methods in specific tasks results reveal good performance of users in certain tasks when using the new interaction techniques even first-time users were able to complete a task faster with the smart phone than with traditional devices
it is a laborious process to quantify relationship patterns within a feature-rich archive for example understanding the degree of neuroanatomical similarity between the scanned subjects of a magnetic_resonance_imaging mri repository is a nontrivial task in this work we present a coordinated multiple view cmv system for visually analyzing collections of feature-rich datasets a query-based user interface operates on a feature-respective data scheme and is geared towards domain experts that are non-specialists in informatics and analytics we employ multi-dimensional_scaling mds to project feature surface_representations into three-dimensions where proximity in location is proportional to the feature similarity through query feedback and environment navigation the user groups clusters that exhibit probable trends across feature and attribute the system provides supervised classification methods for determining attribute classes within the user selected groups finally using visual or analytical feature-wise exploration the user determines intra-group feature commonality coordinated multiple_views data_mining query-based visualization
chi showed how to treat visualization programing models abstractly this provided a firm theoretical basis for the data-state model of visualization however chi's models did not look deeper into fine-grained program properties such as execution semantics we present conditionally deterministic and resource bounded semantics for the data flow model of visualization based on e-frp these semantics are used in the stencil system to move between data state and data flow execution build task-based parallelism and build complex analysis chains for dynamic_data this initial work also shows promise for other complex operators compilation techniques to enable efficient use of time and space and mixing task and data parallelism dynamic_data semantics
agent-based simulation has become a key technique for modeling and simulating dynamic complicated behaviors in social and behavioral sciences lacking the appropriate tools and support it is difficult for social scientists to thoroughly analyze the results of these simulations in this work we capture the complex relationships between discrete simulation states by visualizing the data as a temporal graph in collaboration with expert analysts we identify two graph structures which capture important relationships between pivotal states in the simulation and their inevitable outcomes finally we demonstrate the utility of these structures in the interactive analysis of a large-scale social science simulation of political power in present-day thailand
concerns about verification mean the humanitarian community are reluctant to use information collected during crisis events even though such information could potentially enhance the response effort consequently a program of research is presented that aims to evaluate the degree to which uncertainty and bias are found in public collections of incident reports gathered during crisis events these datasets exemplify a class whose members have spatial and temporal attributes are gathered from heterogeneous sources and do not have readily available attribution information an interactive software prototype and existing software are applied to a dataset related to the current armed conflict in libya to identify `intrinsic' characteristics against which uncertainty and bias can be evaluated requirements on the prototype are identified which in time will be expanded into full research objectives
it is common to classify data in hierarchies they provide a comprehensible way of understanding big amounts of data from budgets to organizational_charts or even the stock market trees are everywhere and people find them easy to use however when analysts need to compare two versions of the same tree structure or two related taxonomies the task is not so easy much work has been done on this topic but almost all of it has been restricted to either compare the trees by topology or by the node attribute values with this project we are proposing treeversity a framework for comparing tree structures both by structural changes and by differences in the node attributes this paper is based on our previous work on comparing traffic agencies using lifeflow [1 2] and on a first prototype of treeversity
twitter currently receives about 190 million tweets small text-based web posts a day in which people share their comments regarding a wide range of topics a large number of tweets include opinions about products and services however with twitter being a relatively new phenomenon these tweets are underutilized as a source for evaluating customer sentiment to explore high-volume twitter data we introduce three novel time-based visual_sentiment_analysis techniques 1 topic-based sentiment analysis that extracts maps and measures customer opinions 2 stream analysis that identifies interesting tweets based on their density negativity and influence characteristics and 3 pixel cell-based sentiment calendars and high density geo maps that visualize large_volumes of data in a single view we applied these techniques to a variety of twitter data eg movies amusement parks and hotels to show their distribution and patterns and to identify influential opinions sentiment analysis topic extraction twitter analysis visual_opinion_analysis
recent work in visual_analytics has explored the extent to which information regarding analyst action and reasoning can be inferred from interaction however these methods typically rely on humans instead of automatic extraction techniques furthermore there is little discussion regarding the role of user frustration when interacting with a visual interface we demonstrate that automatic extraction of user frustration is possible given action-level visualization interaction logs an experiment is described which collects data that accurately reflects user emotion transitions and corresponding interaction sequences this data is then used in building hiddenmarkov models hmms which statistically connect interaction events with frustration the capabilities of hmms in predicting user frustration are tested using standard machine_learning evaluation methods the resulting classifier serves as a suitable predictor of user frustration that performs similarly across different users and datasets
this paper presents a novel system for analyzing temporal changes in bloggers' activities and interests on a topic through a 3d_visualization of dependency structures related to the topic having a dependency database built from a blog archive our 3d_visualization_framework helps users to interactively exploring temporal changes in bloggers' activities and interests related to the topic
understanding users' interactions is considered as one of the important research topics in visual_analytics although numerous empirical user studies have been performed to understand a user's interaction a limited study has been successful in connecting the user's interaction to his/her reasoning in this paper we present an approach of understanding experts' interactive analysis by connecting their interactions to conclusions ie findings through a state transition approach
the researchers explore the intersections between information assurance and risk using visual_analysis of text mining operations the methodological approach involves searching for and extracting for analysis those abstracts and keywords groupings that relate to risk within a defined subset of scientific research journals this analysis is conducted through a triangulated study incorporating visualizations produced using both starlight and in-spire visual_analysis software the results are definitional showing current attitudes within the information assurance research community towards risk management strategies while simultaneously demonstrating the value of visual_analysis processes when engaging in sense making of a large body of knowledge information assurance information security risk risk management visual_analysis
faced with a large high-dimensional_dataset many turn to data_analysis approaches that they understand less well than the domain of their data an expert's knowledge can be leveraged into many types of analysis via a domain-specific distance function but creating such a function is not intuitive to do by hand we have created a system that shows an initial visualization adapts to user feedback and produces a distance function as a result specifically we present a multidimensional scaling mds visualization and an iterative feedback mechanism for a user to affect the distance function that informs the visualization without having to adjust the parameters of the visualization directly an encouraging experimental result suggests that using this tool data attributes with useless data are given low importance in the distance function
kd-photomap is a web-based visual_analytics system for browsing collections of geotagged flickr photographs in search of interesting pictures places and events spatial filtering of the data is performed through zooming moving or searching along the map temporal filtering is possible through defining time windows using interactive histograms and calendar controls information about the number and spatiotemporal distribution of photos captured in an explored area is continuously provided using various visual_cues
graph rewriting systems are easily described and explained they can be seen as a game where one iterates transformation rules on an initial graph until some condition is met a rule describes a local pattern ie a subgraph that must be identified in a graph and specifies how to transform this subgraph the graph rewriting formalism is at the same time extremely rich and complex making the study of a model expressed in terms of graph rewriting quite challenging for instance predicting whether rules can be applied in any order is often difficult when modelling complex systems graphical formalisms have clear advantages they are more intuitive and make it easier to visualize a system and convey intuitions about it this work focuses on the design of an interactive visual graph rewriting system which supports graphical manipulations and computation to reason and simulate on a system porgy has been designed based on regular exchanges with graph rewriting systems experts and users over the past three years the design choices relied on a careful methodology inspired from munzner's nested process model for visualization design and validation [4]
this poster describes an approach to facilitate comparisons in multi-dimensional categorical_data the key idea is to represent over- or under-proportional relationships explicitly on an overview level the visualization of various measures conveys pair-wise relationships between categorical dimensions for more details interaction supports to relate a single category to all categories of multiple dimensions we discuss methods for representing relationships and visualization-driven strategies for ordering dimensions and categories and we illustrate the approach by means of data from a social survey
we present a visual_analysis tool for mining correlations in county-level multidimensional gun/homicide data the tool uses 2d pexels heatmaps linked-views dynamic queries and details-on-demand to analyze annual county-level data on firearm homicide rates and gun availability as well as various socio-demographic measures a statistical significance filter was implemented as a visual means to validate exploratory hypotheses results from expert evaluations indicate that our methods outperform typical graphical techniques used by statisticians such as bar graphs scatterplots and residual plots to show spatial and temporal relationships our visualization has the potential to convey the impact of gun availability on firearm homicides to the public health arena and the general public
we present city sentinel an in-house built visual analytic software capable of handling a large collection of textual documents by combining diverse text mining and visualization tools we applied this tool for the vast challenge 2011 mini challenge 1 over millions of tweet messages we demonstrate how city sentinel aided the analyst in retrieving the hidden information from the tweet messages to analyze and locate a hypothetical epidemic outbreak
the microblog challenge presented an opportunity to use commercial software for visual_analysis an epidemic outbreak occurred in the city of vastopolis requiring visualizations of symptoms and their spread over time using these tools analysts could successfully identify the outbreak's origin and pattern of dispersion the maps used to analyze the data and present the results provided clear easily understood representations and presented a logical explanation of a complex progression of events arcgis information_visualization spatial analysis
we presented scatterblogs a system for microblog_analysis that seamlessly integrates search backend and visual frontend it provides powerful automatic algorithms for detecting spatio-temporal `anomalies' within blog entries as well as corresponding visual representations and interaction facilities for inspecting anomalies or exploiting them in further analytic steps apart from that we consider the system's combinatoric facilities for building complex hypotheses from temporal spatial and content-related aspects an important feature this was the key for creating a cross-checked analysis for mc1
we present epspread an analysis and storyboarding tool for geolocated microblogging data individual time points and ranges are analysed through queries heatmaps word clouds and streamgraphs the underlying narrative is shown on a storyboard-style timeline for discussion refinement and presentation the tool was used to analyse data from the vast challenge 2011 mini-challenge 1 tracking the spread of an epidemic using microblogging data in this article we describe how the tool was used to identify the origin and track the spread of the epidemic coordinated multiple_views vast  visual_analytics epidemic visualization information_visualization
mobileanalymator mobile analysis animator is a visual analytic system designed to analyze geospatial-temporal_data on mobile devices the system is an internet based application that allows analysts to work in flexile enviornments at anytime its client side is developed by adobe flash to animate and interact with data the server side uses java and mysql to query compute and serve data the analyst can run the analytical task from a tablet or computer with internet connection mobileanalymator adopted spatial and temporal autocorrelations in the interface design and integrated tangible interaction in the navigation to support analysis process
for the vast 2011 network_security mini-challenge we adopted geo visual analytic methods and applied them in the field of network_security we used the geoviz toolkit [1] to represent cyber security events by fabricating a simple “geography” of several sets of blocks one for the workstations one for the servers and one for the internet using arcgis 10 by esri - environmental system research institute security data was tabulated using perl scripts to parse the logs in order to create representations of event frequency and where they occurred on the network the tabulated security data was then added as attributes of the geography exploration of the data and subsequent analysis of the meaning and impact of the cyber security events was made possible using the geoviz toolkit coordinated and multiple_views geoviz toolkit cyber security geo visual_analytics situation_awareness
we present a multiple_views visualization for the security data in the vast 2010 mini challenge 2 the visualization is used to monitor log event activity on the network log data included in the challenge interactions are provided that allow analysts to investigate suspicious activity and escalate events as needed additionally a database application is used to allow sql queries for more detailed investigation
to visualize security trends for the data set provided by the vast 2011 mini challenge #2 a custom tool has been developed open source tools [12] web programming languages [47] and an open source database [3] has been used to work with the data and create a visualization for security log files containing network_security trends in this paper the tools and methods used for the analysis are described the methods include the log synchronization with different timezone and the development of heat maps and parallel_coordinates charts to develop the visualization processing and canvas [47] was used heat map logs security trends vast challenge visual_analysis
analyst's workspace is a sensemaking environment designed specifically for use of large high-resolution displays it employs a spatial workspace to integrate foraging and synthesis activities into a unified process in this paper we describe how analyst's workspace solved the vast 2011 mini-challenge #3 and discuss some of the unique features of the environment visual_analytics high-resolution displays intelligence_analysis large space
this article describes our analytic process and experience of using the jigsaw system in working on the vast 2011 mini challenge 3 we describe how we extracted and worked with entities from the documents and how jigsaw's computational analysis capabilities and visualizations scaffolded the investigation based on our experiences we discuss desirable features that would enhance the analytic power of jigsaw visual_analytics data ingestion evidence marshalling information_visualization investigative_analysis
nspace2 is an innovative visual_analytics tool that was the primary platform used to search evaluate and organize the data in the vast 2011 mini challenge #3 dataset nspace2 is a web-based tool that is designed to facilitate the back-and-forth flow of the multiple steps of an analysis process including search data triage organization sense-making and reporting this paper describes how nspace2 was used to assist every step of the analysis process for this vast challenge analysis workflow human information interaction sense-making visual_analytics
the task of the vast 2011 grand challenge was to investigate potential terrorist activities and their relation to the spread of an epidemic three different data sets were provided as part of three mini challenges mcs mc 1 was about analyzing geo-tagged microblogging twitter messages to characterize the spread of an epidemic mc 2 required analyzing threats to a computer network using a situational awareness approach in mc 3 possible criminal and terrorist activities were to be analyzed based on a collection of news articles to solve the grand challenge insight from each of the individual mcs had to be integrated appropriately
a professional biography of mary czerwinski of microsoft research is presented her speech which is not included was entitled "trends and topics from the last 17 years at microsoft research"
what you are doing as visualization researchers and developers is critical and in fact your role is more important than ever in this age of massive data i and many others desperately want to use your work but sometimes i just cannot seem to wrap my head around what you are showing-even if it really looks cool cool doesn't cut it for me this talk will give examples from my own successes and failures in photography and graphics and suggest with a little imagination and open minds there might be some lessons learned from my own commitment to delving into and communicating information
topological techniques have proven highly successful in analyzing and visualizing scientific data as a result significant efforts have been made to compute structures like the morse-smale_complex as robustly and efficiently as possible however the resulting algorithms while topologically consistent often produce incorrect connectivity as well as poor geometry these problems may compromise or even invalidate any subsequent analysis moreover such techniques may fail to improve even when the resolution of the domain mesh is increased thus producing potentially incorrect results even for highly resolved functions to address these problems we introduce two new algorithms i a randomized algorithm to compute the discrete gradient of a scalar field that converges under refinement and ii a deterministic variant which directly computes accurate geometry and thus correct connectivity of the ms complex the first algorithm converges in the sense that on average it produces the correct result and its standard deviation approaches zero with increasing mesh resolution the second algorithm uses two ordered traversals of the function to integrate the probabilities of the first to extract correct near optimal geometry and connectivity we present an extensive empirical_study using both synthetic and real-world data and demonstrates the advantages of our algorithms in comparison with several popular approaches morse-smale_complex topology topological methods
this paper presents a visualization approach for detecting and exploring similarity in the temporal variation of field data we provide an interactive technique for extracting correlations from similarity matrices which capture temporal similarity of univariate functions we make use of the concept to extract periodic and quasiperiodic behavior at single spatial points as well as similarity between different locations within a field and also between different data sets the obtained correlations are utilized for visual_exploration of both temporal and spatial relationships in terms of temporal similarity our entire pipeline offers visual_interaction and inspection allowing for the flexibility that in particular time-dependent_data_analysis techniques require we demonstrate the utility and versatility of our approach by applying ourimplementation to data from both simulation and measurement time-dependent fields comparative_visualization interactive_recurrence_analysis similarity_analysis
in nuclear science density functional theory dft is a powerful tool to model the complex interactions within the atomic nucleus and is the primary theoretical approach used by physicists seeking a better understanding of fission however dft simulations result in complex multivariate datasets in which it is difficult to locate the crucial `scission' point at which one nucleus fragments into two and to identify the precursors to scission the joint contour net jcn has recently been proposed as a new data structure for the topological_analysis of multivariate scalar_fields analogous to the contour_tree for univariate fields this paper reports the analysis of dft simulations using the jcn the first application of the jcn technique to real data it makes three contributions to visualization i a set of practical methods for visualizing the jcn ii new insight into the detection of nuclear scission and iii an analysis of aesthetic criteria to drive further work on representing the jcn topology multifields scalar_fields
we present knotpad an interactive paper-like system for visualizing and exploring mathematical knots we exploit topological drawing and math-aware deformation methods in particular to enable and enrich our interactions with knot diagrams whereas most previous efforts typically employ physically based modeling to simulate the 3d dynamics of knots and ropes our tool offers a reidemeister move based interactive environment that is much closer to the topological problems being solved in knot_theory yet without interfering with the traditional advantages of paper-based analysis and manipulation of knot diagrams drawing knot diagrams with many crossings and producing their equivalent is quite challenging and error-prone knotpad can restrict user manipulations to the three types of reidemeister moves resulting in a more fluid yet mathematically correct user_experience with knots for our principal test case of mathematical knots knotpad permits us to draw and edit their diagrams empowered by a family of interactive techniques furthermore we exploit supplementary interface elements to enrich the user_experiences for example knotpad allows one to pull and drag on knot diagrams to produce mathematically valid moves navigation enhancements in knotpad provide still further improvement by remembering and displaying the sequence of valid moves applied during the entire interaction knotpad allows a much cleaner exploratory interface for the user to analyze and study knot equivalence all these methods combine to reveal the complex spatial relationships of knot diagrams with a mathematically true and rich user_experience knot_theory math visualization
metal oxides are important for many technical applications for example alumina aluminum oxide is the most commonly-used ceramic in microelectronic devices thanks to its excellent properties experimental studies of these materials are increasingly supplemented with computer simulations molecular_dynamics md simulations can reproduce the material behavior very well and are now reaching time scales relevant for interesting processes like crack_propagation in this work we focus on the visualization of induced electric dipole moments on oxygen atoms in crack_propagation simulations the straightforward visualization using glyphs for the individual atoms simple shapes like spheres or arrows is insufficient for providing information about the data set as a whole as our contribution we show for the first time that fractional anisotropy values computed from the local neighborhood of individual atoms of md simulation data depict important information about relevant properties of the field of induced electric dipole moments iso surfaces in the field of fractional anisotropy as well as adjustments of the glyph representation allow the user to identify regions of correlated orientation we present novel and relevant findings for the application domain resulting from these visualizations like the influence of mechanical forces on the electrostatic properties visualization_in_physical_sciences_and_engineering glyph-based_techniques point-based data time-varying_data
we introduce a simple yet powerful method called the cumulative heat_diffusion for shape-based_volume_analysis while drastically reducing the computational cost compared to conventional heat_diffusion unlike the conventional heat_diffusion process where the diffusion is carried out by considering each node separately as the source we simultaneously consider all the voxels as sources and carry out the diffusion hence the term cumulative heat_diffusion in addition we introduce a new operator that is used in the evaluation of cumulative heat_diffusion called the volume gradient operator vgo vgo is a combination of the lbo and a data-driven operator which is a function of the half gradient the half gradient is the absolute value of the difference between the voxel intensities the vgo by its definition captures the local shape information and is used to assign the initial heat values furthermore vgo is also used as the weighting parameter for the heat_diffusion process we demonstrate that our approach can robustly extract shape-based features and thus forms the basis for an improved classification and exploration of features based on shape heat_diffusion classification shape-based_volume_analysis transfer_function volume gradient operator
in the last decades cosmological n-body dark_matter simulations have enabled ab initio studies of the formation of structure in the universe gravity amplified small density fluctuations generated shortly after the big bang leading to the formation of galaxies in the cosmic web these calculations have led to a growing demand for methods to analyze time-dependent particle based simulations rendering methods for such n-body simulation data usually employ some kind of splatting approach via point based rendering primitives and approximate the spatial distributions of physical quantities using kernel interpolation techniques common in sph smoothed_particle_hydrodynamics-codes this paper proposes three gpu-assisted rendering approaches based on a new more accurate method to compute the physical densities of dark_matter simulation data it uses full phase-space information to generate a tetrahedral tessellation of the computational domain with mesh vertices defined by the simulation's dark_matter particle positions over time the mesh is deformed by gravitational forces causing the tetrahedral cells to warp and overlap the new methods are well suited to visualize the cosmic web in particular they preserve caustics regions of high density that emerge when several streams of dark_matter particles share the same location in space indicating the formation of structures like sheets filaments and halos we demonstrate the superior image quality of the new approaches in a comparison with three standard rendering techniques for n-body simulation data astrophysics dark_matter n-body_simulations tetrahedral_grids
the us department of energy's doe office of environmental_management doe/em currently supports an effort to understand and predict the fate of nuclear contaminants and their transport in natural and engineered systems geologists hydrologists physicists and computer scientists are working together to create models of existing nuclear waste sites to simulate their behavior and to extrapolate it into the future we use visualization as an integral part in each step of this process in the first step visualization is used to verify model setup and to estimate critical parameters high-performance_computing simulations of contaminant transport produces massive amounts of data which is then analyzed using visualization software specifically designed for parallel_processing of large amounts of structured and unstructured_data finally simulation results are validated by comparing simulation results to measured current and historical field data we describe in this article how visual_analysis is used as an integral part of the decision-making process in the planning of ongoing and future treatment options for the contaminated nuclear waste sites lessons learned from visually analyzing our large-scale simulation runs will also have an impact on deciding on treatment measures for other contaminated sites visual_analytics data_management environmental_management high-performance_computing parallel_rendering
we evaluate and compare video_visualization_techniques based on fast-forward a controlled_laboratory_user_study n = 24 was conducted to determine the trade-off between support of object identification and motion perception two properties that have to be considered when choosing a particular fast-forward visualization we compare four different visualizations two representing the state-of-the-art and two new variants of visualization introduced in this paper the two state-of-the-art methods we consider are frame-skipping and temporal blending of successive frames our object trail visualization leverages a combination of frame-skipping and temporal blending whereas predictive trajectory visualization supports motion perception by augmenting the video frames with an arrow that indicates the future object trajectory our hypothesis was that each of the state-of-the-art methods satisfies just one of the goals support of object identification or motion perception thus they represent both ends of the visualization design the key findings of the evaluation are that object trail visualization supports object identification whereas predictive trajectory visualization is most useful for motion perception however frame-skipping surprisingly exhibits reasonable performance for both tasks furthermore we evaluate the subjective performance of three different playback speed visualizations for adaptive fast-forward a subdomain of video fast-forward video_visualization adaptive fast-forward controlled_laboratory_user_study
due to the inherent characteristics of the visualization process most of the problems in this field have strong ties with human cognition and perception this makes the human brain and sensory system the only truly appropriate evaluation platform for evaluating and fine-tuning a new visualization method or paradigm however getting humans to volunteer for these purposes has always been a significant obstacle and thus this phase of the development process has traditionally formed a bottleneck slowing down progress in visualization research we propose to take advantage of the newly emerging field of human_computation hc to overcome these challenges hc promotes the idea that rather than considering humans as users of the computational system they can be made part of a hybrid computational loop consisting of traditional computation resources and the human brain and sensory system this approach is particularly successful in cases where part of the computational problem is considered intractable using known computer algorithms but is trivial to common sense human knowledge in this paper we focus on hc from the perspective of solving visualization problems and also outline a framework by which humans can be easily seduced to volunteer their hc resources we introduce a purpose-driven game titled “disguise” which serves as a prototypical example for how the evaluation of visualization algorithms can be mapped into a fun and addicting activity allowing this task to be accomplished in an extensive yet cost effective way finally we sketch out a framework that transcends from the pure evaluation of existing visualization methods to the design of a new one human_computation color_blending evaluation perception
multivariate visualization_techniques have attracted great interest as the dimensionality of data sets grows one premise of such techniques is that simultaneous visual representation of multiple variables will enable the data analyst to detect patterns amongst multiple variables such insights could lead to development of new techniques for rigorous numerical analysis of complex relationships hidden within the data two natural questions arise from this premise which multivariate visualization_techniques are the most effective for high-dimensional_data sets how does the analysis task change this utility ranking we present a user_study with a new task to answer the first question we provide some insights to the second question based on the results of our study and results available in the literature our task led to significant differences in error response time and subjective workload ratings amongst four visualization_techniques we implemented three integrated techniques data-driven spots oriented slivers and attribute blocks as well as a baseline case of separate grayscale images the baseline case fared poorly on all three measures whereas datadriven spots yielded the bestaccuracy and was among the best in response time these results differ from comparisons of similar techniques with other tasks and we review all the techniques tasks and results from our work and previous work to understand the reasons for this discrepancy quantitative_evaluation multivariate visualization texture perception visual_task_design
color mapping and semitransparent layering play an important role in many visualization scenarios such as information_visualization and volume_rendering the combination of color and transparency is still dominated by standard alpha-compositing using the porter-duff over operator which can result in false colors with deceiving impact on the visualization other more advanced methods have also been proposed but the problem is still far from being solved here we present an alternative to these existing methods specifically devised to avoid false colors and preserve visual depth ordering our approach is data driven and follows the recently formulated knowledge-assisted_visualization kav paradigm preference data that have been gathered in web-based user surveys are used to train a support-vector machine model for automatically predicting an optimized hue-preserving blending we have applied the resulting model to both volume_rendering and a specific information_visualization_technique illustrative parallel coordinate plots comparative renderings show a significant improvement over previous approaches in the sense that false colors are completely removed and important properties such as depth ordering and blending vividness are better preserved due to the generality of the defined data-driven blending operator it can be easily integrated also into other visualization_frameworks color_blending hue preservation knowledge-assisted_visualization parallel_coordinates volume_rendering
we report the impact of display_characteristics stereo and size on task_performance in diffusion magnetic_resonance_imaging dmri in a user_study with 12 participants the hypotheses were that 1 adding stereo and increasing display size would improve taskaccuracy and reduce completion time and 2 the greater the complexity of a spatial task the greater the benefits of an improved display thus we expected to see greater performance gains when detailed visual reasoning was required participants used dense streamtube visualizations to perform five representative tasks 1 determine the higher average fractional anisotropy fa values between two regions 2 find the endpoints of fiber tracts 3 name a bundle 4 mark a brain lesion and 5 judge if tracts belong to the same bundle contrary to our hypotheses we found the task completion time was not improved by the use of the larger display and that performanceaccuracy was hurt rather than helped by the introduction of stereo in our study with dense dmri data bigger was not always better thus cautious should be taken when selecting displays for scientific_visualization_applications we explored the results further using the body-scale unit and subjective size and stereo experiences display_characteristics diffusion_tensor_mri virtual environment
existing methods for analyzing separation of streamlines are often restricted to a finite time or a local area in our paper we introduce a new method that complements them by allowing an infinite-time-evaluation of steady planar vector_fields our algorithm unifies combinatorial and probabilistic methods and introduces the concept of separation in time-discrete markov-chains we compute particle distributions instead of the streamlines of single particles we encode the flow into a map and then into a transition matrix for each time direction finally we compare the results of our grid-independent algorithm to the popular finite-time-lyapunov-exponents and discuss the discrepancies vector_field_topology feature_extraction flow_visualization uncertainty
integral flow surfaces constitute a widely used flow_visualization tool due to their capability to convey important flow information such as fluid transport mixing and domain segmentation current flow surface rendering techniques limit their expressiveness however by focusing virtually exclusively on displacement visualization visually neglecting the more complex notion of deformation such as shearing and stretching that is central to the field of continuum_mechanics to incorporate this information into the flow surface visualization and analysis process we derive a metric_tensor_field that encodes local surface deformations as induced by the velocity_gradient of the underlying flow_field we demonstrate how properties of the resulting metric_tensor_field are capable of enhancing present surface visualization and generation methods and develop novel surface querying sampling and visualization_techniques the provided results show how this step towards unifying classic flow_visualization and more advanced concepts from continuum_mechanics enables more detailed and improved flow analysis vector field continuum_mechanics deformation integral_surfaces metric_tensor velocity_gradient
room air flow and air exchange are important aspects for the design of energy-efficient buildings as a result simulations are increasingly used prior to construction to achieve an energy-efficient design we present a visual_analysis of air flow generated at building entrances which uses a combination of revolving doors and air curtains the resulting flow pattern is challenging because of two interacting flow patterns on the one hand the revolving door acts as a pump on the other hand the air curtain creates a layer of uniformly moving warm air between the interior of the building and the revolving door lagrangian coherent_structures lcs which by definition are flow barriers are the method of choice for visualizing the separation and recirculation behavior of warm and cold air flow the extraction of lcs is based on the finite-time_lyapunov_exponent ftle and makes use of a ridge definition which is consistent with the concept of weak lcs both ftle computation and ridge_extraction are done in a robust and efficient way by making use of the fast fourier_transform for computing scale-space derivatives visualization_in_physical_sciences_and_engineering topology-based_techniques vector_field_data
despite the ongoing efforts in turbulence research the universal properties of the turbulence small-scale structure and the relationships between small- and large-scale turbulent motions are not yet fully understood the visually guided exploration of turbulence features including the interactive selection and simultaneous visualization of multiple features can further progress our understanding of turbulence accomplishing this task for flow_fields in which the full turbulence spectrum is well resolved is challenging on desktop computers this is due to the extreme resolution of such fields requiring memory and bandwidth capacities going beyond what is currently available to overcome these limitations we present a gpu system for feature-based turbulence visualization that works on a compressed flow_field representation we use a wavelet-based compression scheme including run-length and entropy encoding which can be decoded on the gpu and embedded into brick-based volume ray-casting this enables a drastic reduction of the data to be streamed from disk to gpu memory our system derives turbulence properties directly from the velocity_gradient tensor and it either renders these properties in turn or generates and renders scalar feature volumes the quality and efficiency of the system is demonstrated in the visualization of two unsteady turbulence simulations each comprising a spatio-temporal resolution of 10244 on a desktop computer the system can visualize each time step in 5 seconds and it achieves about three times this rate for the visualization of a scalar feature volume_visualization_system_and_toolkit_design data_compression data_streaming vector_fields volume_rendering
cerebral_aneurysms are a pathological vessel dilatation that bear a high risk of rupture for the understanding and evaluation of the risk of rupture the analysis of hemodynamic information plays an important role besides quantitative hemodynamic information also qualitative flow characteristics eg the inflow_jet and impingement_zone are correlated with the risk of rupture however the assessment of these two characteristics is currently based on an interactive visual investigation of the flow_field obtained by computational_fluid_dynamics_(cfd) or blood flow measurements we present an automatic and robust detection as well as an expressive visualization of these characteristics the detection can be used to support a comparison eg of simulation results reflecting different treatment options our approach utilizes local streamline properties to formalize the inflow_jet and impingement_zone we extract a characteristic seeding curve on the ostium on which an inflow_jet boundary contour is constructed based on this boundary contour we identify the impingement_zone furthermore we present several visualization_techniques to depict both characteristics expressively thereby we consideraccuracy and robustness of the extracted characteristics minimal visual_clutter and occlusions an evaluation with six domain experts confirms that our approach detects both hemodynamic characteristics reasonably cfd cerebral_aneurysm hemodynamic_visualization
the 3d_visualization of astronomical nebulae is a challenging problem since only a single 2d_projection is observable from our fixed vantage point on earth we attempt to generate plausible and realistic looking volumetric visualizations via a tomographic approach that exploits the spherical or axial symmetry prevalent in some relevant types of nebulae different types of symmetry can be implemented by using different randomized distributions of virtual cameras our approach is based on an iterative compressed sensing reconstruction algorithm that we extend with support for position-dependent volumetric regularization and linear equality constraints we present a distributed multi-gpuimplementation that is capable of reconstructing high-resolution datasets from arbitrary projections its robustness and scalability are demonstrated for astronomical imagery from the hubble space telescope the resulting volumetric_data is visualized using direct_volume_rendering compared to previous approaches our method preserves a much higher amount of detail and visual variety in the 3d_visualization especially for objects with only approximate symmetry astronomical_visualization direct_volume_rendering distributed_volume_reconstruction
a fundamental characteristic of fluid flow is that it causes mixing introduce a dye into a flow and it will disperse mixing can be used as a method to visualize and characterize flow because mixing is a process that occurs over time it is a 4d problem that presents a challenge for computation visualization and analysis motivated by a mixing problem in geophysics we introduce a combination of methods to analyze transform and finally visualize mixing in simulations of convection in a self-gravitating 3d spherical shell representing convection in the earth's mantle geophysicists use tools such as the finite_element model citcoms to simulate convection and introduce massless passive tracers to model mixing the output of geophysical flow simulation is hard to analyze for domain experts because of overall data size and complexity in addition information overload and occlusion are problems when visualizing a whole-earth model to address the large size of the data we rearrange the simulation data using intelligent indexing for fast file access and efficient caching to address information overload and interpret mixing we compute tracer_concentration statistics which are used to characterize mixing in mantle convection models our visualization uses a specially tailored version of direct_volume_rendering the most important adjustment is the use of constant opacity because of this special area of application i e the rendering of a spherical shell many computations for volume_rendering can be optimized these optimizations are essential to a smooth animation of the time-dependent simulation data our results show how our system can be used to quickly assess the simulation output and test hypotheses regarding earth's mantle convection the integrated processing pipeline helps geoscientists to focus on their main task of analyzing mantle homogenization earth mantle geophysics convection flow_visualization large_data_system tracer_concentration
planetary topography is the result of complex interactions between geological processes of which faulting is a prominent component surface-rupturing earthquakes cut and move landforms which develop across active faults producing characteristic surface displacements across the fault geometric models of faults and their associated surface displacements are commonly applied to reconstruct these offsets to enable interpretation of the observed topography however current 2d techniques are limited in their capability to convey both the three-dimensional kinematics of faulting and the incremental sequence of events required by a given reconstruction here we present a real-time system for interactive retro-deformation of faulted topography to enable reconstruction of fault displacement within a high-resolution sub 1m/pixel 3d terrain_visualization we employ geometry shaders on the gpu to intersect the surface mesh with fault-segments interactively specified by the user and transform the resulting surface blocks in realtime according to a kinematic model of fault motion our method facilitates a human-in-the-loop approach to reconstruction of fault displacements by providing instant visual feedback while exploring the parameter space thus scientists can evaluate the validity of traditional point-to-point reconstructions by visually examining a smooth interpolation of the displacement in 3d we show the efficacy of our approach by using it to reconstruct segments of the san andreas fault california as well as a graben structure in the noctis labyrinthus region on mars terrain_rendering fault simulation interactive mesh_deformation
geoscientific modeling and simulation helps to improve our understanding of the complex earth system during the modeling process validation of the geoscientific model is an essential step in validation it is determined whether the model output shows sufficient agreement with observation data measures for this agreement are called goodness of fit in the geosciences analyzing the goodness of fit is challenging due to its manifold dependencies 1 the goodness of fit depends on the model parameterization whose precise values are not known 2 the goodness of fit varies in space and time due to the spatio-temporal dimension of geoscientific models 3 the significance of the goodness of fit is affected by resolution and preciseness of available observational data 4 the correlation between goodness of fit and underlying modeled and observed values is ambiguous in this paper we introduce a visual_analysis concept that targets these challenges in the validation of geoscientific models - specifically focusing on applications where observation data is sparse unevenly distributed in space and time and imprecise which hinders a rigorous analytical approach our concept developed in close cooperation with earth system modelers addresses the four challenges by four tailored visualization components the tight linking of these components supports a twofold interactive drill-down in model parameter space and in the set of data samples which facilitates the exploration of the numerous dependencies of the goodness of fit we exemplify our visualization concept for geoscientific modeling of glacial isostatic adjustments in the last 100000 years validated against sea levels indicators - a prominent example for sparse and imprecise observation data an initial use case and feedback from earth system modelers indicate that our visualization concept is a valuable complement to the range of validation methods earth_science_visualization coordinated multiple_views model validation sea_level_indicators spatio-temporal_visualization
the most important resources to fulfill today's energy demands are fossil fuels such as oil and natural gas when exploiting hydrocarbon reservoirs a detailed and credible model of the subsurface_structures is crucial in order to minimize economic and ecological risks creating such a model is an inverse problem reconstructing structures from measured reflection seismics the major challenge here is twofold first the structures in highly ambiguous seismic_data are interpreted in the time domain second a velocity model has to be built from this interpretation to match the model to depth measurements from wells if it is not possible to obtain a match at all positions the interpretation has to be updated going back to the first step this results in a lengthy back and forth between the different steps or in an unphysical velocity model in many cases this paper presents a novel integrated approach to interactively creating subsurface models from reflection seismics it integrates the interpretation of the seismic_data using an interactive horizon extraction technique based on piecewise global optimization with velocity modeling computing and visualizing the effects of changes to the interpretation and velocity model on the depth-converted model on the fly enables an integrated feedback loop that enables a completely new connection of the seismic_data in time domain and well data in depth domain using a novel joint time/depth visualization depicting side-by-side views of the original and the resulting depth-converted data domain experts can directly fit their interpretation in time domain to spatial ground truth data we have conducted a domain expert evaluation which illustrates that the presented workflow enables the creation of exact subsurface models much more rapidly than previous approaches seismic visualization exploded_views seismic_interpretation volume_deformation
scientists engineers and physicians are used to analyze 3d data with slice-based visualizations radiologists for example are trained to read slices of medical imaging data despite the numerous examples of sophisticated 3d rendering techniques domain experts who still prefer slice-based visualization do not consider these to be very useful since 3d renderings have the advantage of providing an overview at a glance while 2d depictions better serve detailed analyses it is of general interest to better combine these methods recently there have been attempts to bridge this gap between 2d and 3d renderings these attempts include specialized techniques for volume picking in medical imaging data that result in repositioning slices in this paper we present a new volume picking technique called wysiwyp “what you see is what you pick” that in contrast to previous work does not require pre-segmented_data or metadata and thus is more generally applicable the positions picked by our method are solely based on the data itself the transfer_function and the way the volumetric rendering is perceived by the user to demonstrate the utility of the proposed method we apply it to automated positioning of slices in volumetric scalar_fields from various application areas finally we present results of a user_study in which 3d locations selected by users are compared to those resulting from wysiwyp the user_study confirms our claim that the resulting positions correlate well with those perceived by the user picking wysiwyg volume_rendering
data selection is a fundamental task in visualization because it serves as a pre-requisite to many follow-up interactions efficient spatial selection in 3d point cloud datasets consisting of thousands or millions of particles can be particularly challenging we present two new techniques teddyselection and cloudlasso that support the selection of subsets in large particle 3d datasets in an interactive and visually intuitive manner specifically we describe how to spatially select a subset of a 3d particle cloud by simply encircling the target particles on screen using either the mouse or direct-touch input based on the drawn lasso our techniques automatically determine a bounding selection surface around the encircled particles based on their density this kind of selection technique can be applied to particle datasets in several application domains teddyselection and cloudlasso reduce and in some cases even eliminate the need for complex multi-step selection processes involving boolean operations this was confirmed in a formal controlled user_study in which we compared the more flexible cloudlasso technique to the standard cylinder-based selection technique this study showed that the former is consistently more efficient than the latter - in several cases the cloudlasso selection time was half that of the corresponding cylinder-based selection d interaction direct-touch_interaction spatial selection
in a variety of application areas the use of simulation_steering in decision_making is limited at best research focusing on this problem suggests that most user_interfaces are too complex for the end user our goal is to let users create and investigate multiple alternative scenarios without the need for special simulation expertise to simplify the specification of parameters we move from a traditional manipulation of numbers to a sketch-based input approach users steer both numeric parameters and parameters with a spatial correspondence by sketching a change onto the rendering special visualizations provide immediate visual feedback on how the sketches are transformed into boundary conditions of the simulation models since uncertainty with respect to many intertwined parameters plays an important role in planning we also allow the user to intuitively setup complete value ranges which are then automatically transformed into ensemble simulations the interface and the underlying system were developed in collaboration with experts in the field of flood_management the real-world data they have provided has allowed us to construct scenarios used to evaluate the system these were presented to a variety of flood response personnel and their feedback is discussed in detail in the paper the interface was found to be intuitive and relevant although a certain amount of training might be necessary emergency/disaster_management ensemblesimulation_steering flood_management integrated_visualization_system interaction_design sketch-based_steering uncertainty_visualization
the process of surface perception is complex and based on several influencingfactors eg shading silhouettes occluding contours and top down cognition theaccuracy of surface perception can be measured and the influencingfactors can be modified in order to decrease the error in perception this paper presents a novel concept of how a perceptual evaluation of a visualization_technique can contribute to its redesign with the aim of improving the match between the distal and the proximal stimulus during analysis of data from previous perceptual studies we observed that the slant of 3d surfaces visualized on 2d screens is systematically underestimated the visible trends in the error allowed us to create a statistical model of the perceived surface slant based on this statistical model we obtained from user experiments we derived a new shading model that uses adjusted surface normals and aims to reduce the error in slant perception the result is a shape-enhancement of visualization which is driven by an experimentally-founded statistical model to assess the efficiency of the statistical shading model we repeated the evaluation experiment and confirmed that the error in perception was decreased results of both user experiments are publicly-available datasets shading evaluation perception statistical_analysis surface slant
the study of aerosol composition for air quality research involves the analysis of high-dimensional single particle mass_spectrometry_data we describe apply and evaluate a novel interactive visual framework for dimensionality_reduction of such data our framework is based on non-negative matrix factorization with specifically defined regularization terms that aid in resolving mass spectrum ambiguity thereby visualization assumes a key role in providing insight into and allowing to actively control a heretofore elusive data processing step and thus enabling rapid analysis meaningful to domain scientists in extending existing black box schemes we explore design choices for visualizing interacting with and steering the factorization process to produce physically meaningful results a domain-expert evaluation of our system performed by the air quality research experts involved in this effort has shown that our method and prototype admits the finding of unambiguous and physically correct lower-dimensional basis transformations of mass_spectrometry_data at significantly increased speed and a higher degree of ease dimension reduction mass_spectrometry_data matrix factorization multidimensional data visualization visual_encodings_of_numerical_error_metrics
this paper presents the first volume_visualization system that scales to petascale volumes imaged as a continuous stream of high-resolution electron microscopy images our architecture scales to dense anisotropic petascale volumes because it 1 decouples construction of the 3d multi-resolution representation required for visualization from data acquisition and 2 decouples sample access time during ray-casting from the size of the multi-resolution hierarchy our system is designed around a scalable multi-resolution virtual memory architecture that handles missing data naturally does not pre-compute any 3d multi-resolution representation such as an octree and can accept a constant stream of 2d image tiles from the microscopes a novelty of our system design is that it is visualization-driven we restrict most computations to the visible volume data leveraging the virtual memory architecture missing data are detected during volume ray-casting as cache misses which are propagated backwards for on-demand out-of-core processing 3d blocks of volume data are only constructed from 2d microscope image tiles when they have actually been accessed during ray-casting we extensively evaluate our system design choices with respect to scalability and performance compare to previous best-of-breed systems and illustrate the effectiveness of our system for real microscopy data from neuroscience petascale_volume_exploration high-resolution microscopy high-throughput_imaging neuroscience
in this work we address the problem of lossless_compression of scientific and medical floating-point volume data we propose two prediction-based compression methods that share a common framework which consists of a switched prediction scheme wherein the best predictor out of a preset group of linear predictors is selected such a scheme is able to adapt to different datasets as well as to varying statistics within the data the first method called ape adaptive polynomial encoder uses a family of structured interpolating polynomials for prediction while the second method which we refer to as ace adaptive combined encoder combines predictors from previous work with the polynomial predictors to yield a more flexible powerful encoder that is able to effectively decorrelate a wide range of data in addition in order to facilitate efficient visualization of compressed data our scheme provides an option to partition floating-point values in such a way as to provide a progressive representation we compare our two compressors to existing state-of-the-art lossless floating-point compressors for scientific data with our data suite including both computer simulations and observational measurements the results demonstrate that our polynomial predictor ape is comparable to previous approaches in terms of speed but achieves better compression rates on average ace our combined predictor while somewhat slower is able to achieve the best compression rate on all datasets with significantly better rates on most of the datasets volume_compression floating-point_compression lossless_compression
in many fields of science or engineering we are confronted with uncertain data for that reason the visualization of uncertainty received a lot of attention especially in recent years in the majority of cases gaussian distributions are used to describe uncertain behavior because they are able to model many phenomena encountered in science therefore in most applications uncertain data is or is assumed to be gaussian distributed if such uncertain data is given on fixed positions the question of interpolation arises for many visualization approaches in this paper we analyze the effects of the usual linear_interpolation schemes for visualization of gaussian distributed data in addition we demonstrate that methods known in geostatistics and machine_learning have favorable properties for visualization purposes in this case gaussian_process interpolation uncertainty
finite_element fe models are frequently used in engineering and life sciences within time-consuming simulations in contrast with the regular grid structure facilitated by volumetric_data sets as used in medicine or geosciences fe models are defined over a non-uniform grid elements can have curved faces and their interior can be defined through high-order basis_functions which pose additional challenges when visualizing these models during ray-casting the uniformly distributed sample points along each viewing ray must be transformed into the material space defined within each element the computational complexity of this transformation makes a straightforward approach inadequate for interactive data_exploration in this paper we introduce a novel coherency-based method which supports the interactive_exploration of fe models by decoupling the expensive world-to-material space transformation from the rendering stage thereby allowing it to be performed within a precomputation stage therefore our approach computes view-independent proxy rays in material space which are clustered to facilitate data reduction during rendering these proxy rays are accessed and it becomes possible to visually analyze high-order fe models at interactive frame rates even when they are time-varying or consist of multiple modalities within this paper we provide the necessary background about the fe data describe our decoupling method and introduce our interactive rendering algorithm furthermore we provide visual results and analyze the error introduced by the presented approach finite_element visualization gpu-based ray-casting
this paper presents the element visualizer elvis a new open-source scientific_visualization system for use with high-order finite_element solutions to pdes in three dimensions this system is designed to minimize visualization errors of these types of fields by querying the underlying finite_element basis_functions eg high-order polynomials directly leading to pixel-exact representations of solutions and geometry the system interacts with simulation data through runtime plugins which only require users to implement a handful of operations fundamental to finite_element solvers the data in turn can be visualized through the use of cut surfaces contours isosurfaces and volume_rendering these visualization algorithms are implemented using nvidia's optix gpu-based ray-tracing engine which provides accelerated ray traversal of the high-order geometry and cuda which allows for effective parallel evaluation of the visualization algorithms the direct interface between elvis and the underlying data differentiates it from existing visualization tools current tools assume the underlying data is composed of linear primitives high-order data must be interpolated with linear functions as a result in this work examples drawn from aerodynamic simulations-high-order discontinuous galerkin finite_element solutions of aerodynamic flows in particular-will demonstrate the superiority of elvis' pixel-exact approach when compared with traditional linear-interpolation methods such methods can introduce a number of inaccuracies in the resulting visualization making it unclear if visual artifacts are genuine to the solution data or if these artifacts are the result of interpolation errors linear methods additionally cannot properly visualize curved geometries elements or boundaries which can greatly inhibit developers' debugging efforts as we will show pixel-exact visualization exhibits none of these issues removing the visualization scheme as a source of - ncertainty for engineers using elvis high-order finite_elements contours cut surface_extraction discontinuous galerkin fluid_flow_simulation isosurfaces spectral/hp_elements
in order to assess the reliability of volume_rendering it is necessary to consider the uncertainty associated with the volume data and how it is propagated through the volume_rendering algorithm as well as the contribution to uncertainty from the rendering algorithm itself in this work we show how to apply concepts from the field of reliable computing in order to build a framework for management of uncertainty in volume_rendering with the result being a self-validating computational model to compute a posteriori uncertainty bounds we begin by adopting a coherent unifying possibility-based representation of uncertainty that is able to capture the various forms of uncertainty that appear in visualization including variability imprecision and fuzziness next we extend the concept of the fuzzy transform in order to derive rules for accumulation and propagation of uncertainty this representation and propagation of uncertainty together constitute an automated framework for management of uncertainty in visualization which we then apply to volume_rendering the result which we call fuzzy volume_rendering is an uncertainty-aware rendering algorithm able to produce more complete depictions of the volume data thereby allowing more reliable conclusions and informed decisions finally we compare approaches for self-validated computation in volume_rendering demonstrating that our chosen method has the ability to handle complex uncertainty while maintaining efficiency uncertainty_visualization verifiable_visualization volume_rendering
computed_tomography angiography cta is commonly used in clinical routine for diagnosing vascular diseases the procedure involves the injection of a contrast agent into the blood stream to increase the contrast between the blood_vessels and the surrounding tissue in the image data cta is often visualized with direct_volume_rendering dvr where the enhanced image contrast is important for the construction of transfer_functions tfs for increased efficiency clinical routine heavily relies on preset tfs to simplify the creation of such visualizations for a physician in practice however tf presets often do not yield optimal images due to variations in mixture concentration of contrast agent in the blood stream in this paper we propose an automatic optimization-based method that shifts tf presets to account for general deviations and local variations of the intensity of contrast enhanced blood_vessels some of the advantages of this method are the following it computationally automates large parts of a process that is currently performed manually it performs the tf shift locally and can thus optimize larger portions of the image than is possible with manual interaction the method is based on a well known vesselness descriptor in the definition of the optimization criterion the performance of the method is illustrated by clinically relevant ct angiography datasets displaying both improved structural overviews of vessel trees and improved adaption to local variations of contrast concentration direct_volume_rendering transfer_functions vessel_visualization
visual_exploration of volumetric_datasets to discover the embedded features and spatial structures is a challenging and tedious task in this paper we present a semi-automatic approach to this problem that works by visually segmenting the intensity-gradient 2d histogram of a volumetric_dataset into an exploration hierarchy our approach mimics user exploration behavior by analyzing the histogram with the normalized-cut multilevel segmentation technique unlike previous work in this area our technique segments the histogram into a reasonable set of intuitive components that are mutually exclusive and collectively exhaustive we use information-theoretic measures of the volumetric_data segments to guide the exploration this provides a data-driven coarse-to-fine hierarchy for a user to interactively navigate the volume in a meaningful manner information-guided_exploration volume_exploration normalized cut volume_classification
in this paper we enable interactive volumetric global_illumination by extending photon_mapping techniques to handle interactive transfer_function tf and material editing in the context of volume_rendering we propose novel algorithms and data structures for finding and evaluating parts of a scene affected by these parameter changes and thus support efficient updates of the photon map in direct_volume_rendering dvr the ability to explore volume data using parameter changes such as editable tfs is of key importance advanced global_illumination techniques are in most cases computationally too expensive as they prevent the desired interactivity our technique decreases the amount of computation caused by parameter changes by introducing historygrams which allow us to efficiently reuse previously computed photon media interactions along the viewing rays we utilize properties of the light_transport equations to subdivide a view-ray into segments and independently update them when invalid unlike segments of a view-ray photon scattering events within the volumetric medium needs to be sequentially updated using our historygram approach we can identify the first invalid photon interaction caused by a property change and thus reuse all valid photon interactions combining these two novel concepts supports interactive editing of parameters when using volumetric photon_mapping in the context of dvr as a consequence we can handle arbitrarily shaped and positioned light sources arbitrary phase functions bidirectional reflectance distribution functions and multiple scattering which has previously not been possible in interactive dvr volume_rendering global_illumination participating_media photon_mapping
lighting_design is a complex but fundamental problem in many fields in volume_visualization direct_volume_rendering generates an informative image without external lighting as each voxel itself emits radiance however external lighting further improves the shape and detail perception of features and it also determines the effectiveness of thecommunication of feature information the human_visual_system is highly effective in extracting structural information from images and to assist it further this paper presents an approach to structure-aware automatic_lighting_design by measuring the structural changes between the images with and without external lighting given a transfer_function and a viewpoint the optimal lighting parameters are those that provide the greatest enhancement to structural information - the shape and detail information of features are conveyed most clearly by the optimal lighting parameters besides lighting goodness the proposed metric can also be used to evaluate lighting_similarity and stability between two sets of lighting parameters lighting_similarity can be used to optimize the selection of multiple light sources so that different light sources can reveal distinct structural information our experiments with several volume data sets demonstrate the effectiveness of the structure-aware lighting_design approach it is well suited to use by novices as it requires little technical understanding of the rendering parameters associated with direct_volume_rendering automatic_lighting_design lighting_similarity lighting_stability structural dissimilarity volume_rendering
the extraction of significant structures in arbitrary high-dimensional_data sets is a challenging task moreover classifying data points as noise in order to reduce a data set bears special relevance for many application domains standard methods such as clustering serve to reduce problem complexity by providing the user with classes of similar entities however they usually do not highlight relations between different entities and require a stopping criterion eg the number of clusters to be detected in this paper we present a visualization pipeline based on recent advancements in algebraic topology more precisely we employ methods from persistent_homology that enable topological_data_analysis on high-dimensional_data sets our pipeline inherently copes with noisy data and data sets of arbitrary dimensions it extracts central structures of a data set in a hierarchical manner by using a persistence-based filtering algorithm that is theoretically well-founded we furthermore introduce persistence rings a novel visualization_technique for a class of topological features-the persistence intervals-of large_data sets persistence rings provide a unique topological signature of a data set which helps in recognizing similarities in addition we provide interactive_visualization_techniques that assist the user in evaluating the parameter space of our method in order to extract relevant structures we describe and evaluate our analysis pipeline by means of two very distinct classes of data sets first a class of synthetic_data sets containing topological objects is employed to highlight the interaction capabilities of our method second in order to affirm the utility of our technique we analyse a class of high-dimensional real-world data sets arising from current research in cultural_heritage topological_persistence clustering multivariate data
this paper introduces a new feature analysis and visualization method for multifield datasets our approach applies a surface-centric model to characterize salient features and form an effective schematic representation of the data we propose a simple geometrically motivated multifield feature definition this definition relies on an iterative algorithm that applies existing theory of skeleton derivation to fuse the structures from the constitutive fields into a coherent data description while addressing noise and spurious details this paper also presents a new method for non-rigid surface registration between the surfaces of consecutive time steps this matching is used in conjunction with clustering to discover the interaction patterns between the different fields and their evolution over time we document the unified visual_analysis achieved by our method in the context of several multifield problems from large-scale time-varying simulations multifield surface_structures time-varying
in this paper we explore how the capacity limits of attention influence the effectiveness of information_visualizations we conducted a series of experiments to test how visual feature type color vs motion layout and variety of visual elements impacted user performance the experiments tested users' abilities to 1 determine if a specified target is on the screen 2 detect an odd-ball deviant target different from the other visible objects and 3 gain a qualitative overview by judging the number of unique categories on the screen our results show that the severe capacity limits of attention strongly modulate the effectiveness of information_visualizations particularly the ability to detect unexpected information keeping in mind these capacity limits we conclude with a set of design_guidelines which depend on a visualization's intended use perception attention color goal-oriented_design layout motion nominal_axis user_study
we present an ethnographic study of design differences in visual presentations between academic disciplines characterizing design conventions between users and data domains is an important step in developing hypotheses tools and design_guidelines for information_visualization in this paper disciplines are compared at a coarse scale between four groups of fields social natural and formal sciences and the humanities two commonplace presentation types were analyzed electronic slideshows and whiteboard “chalk talks” we found design differences in slideshows using two methods - coding and comparing manually-selected features like charts and diagrams and an image-based analysis using pca called eigenslides in whiteboard talks with controlled topics we observed design behaviors including using representations and formalisms from a participant's own discipline that suggest authors might benefit from novel assistive tools for designing presentations based on these findings we discuss opportunities for visualization ethnography and human-centered authoring tools for visual information presentations design information_visualization visual_analysis
for information_visualization researchers eye_tracking has been a useful tool to investigate research participants' underlying cognitive processes by tracking their eye movements while they interact with visual techniques we used an eye tracker to better understand why participants with a variant of a tabular visualization called `simulsort' outperformed ones with a conventional table and typical one-column sorting feature ie typical sorting the collected eye-tracking data certainly shed light on the detailed cognitive processes of the participants simulsort helped with decision-making tasks by promoting efficient browsing behavior and compensatory decision-making strategies however more interestingly we also found unexpected eye-tracking patterns with simul- sort we investigated the cause of the unexpected patterns through a crowdsourcing-based study ie experiment 2 which elicited an important limitation of the eye_tracking method incapability of capturing peripheral_vision this particular result would be a caveat for other visualization researchers who plan to use an eye tracker in their studies in addition the method to use a testing stimulus ie influential column in experiment 2 to verify the existence of such limitations would be useful for researchers who would like to verify their eye_tracking results visualized_decision_making crowdsourcing eye_tracking limitations peripheral_vision quantitative_empirical_study
design studies are an increasingly popular form of problem-driven visualization research yet there is little guidance available about how to do them effectively in this paper we reflect on our combined experience of conducting twenty-one design studies as well as reading and reviewing many more and on an extensive literature review of other field work methods and methodologies based on this foundation we provide definitions propose a methodological framework and provide practical guidance for conducting design studies we define a design_study as a project in which visualization researchers analyze a specific real-world problem faced by domain experts design a visualization system that supports solving this problem validate the design and reflect about lessons learned in order to refine visualization design_guidelines we characterize two axes - a task clarity axis from fuzzy to crisp and an information location axis from the domain expert's head to the computer - and use these axes to reason about design_study contributions their suitability and uniqueness from other approaches the proposed methodological framework consists of 9 stages learn winnow cast discover design implement deploy reflect and write for each stage we provide practical guidance and outline potential pitfalls we also conducted an extensive literature survey of related methodological approaches that involve a significant amount of qualitative field work and compare design_study methodology to that of ethnography grounded theory and action research design_study framework methodology visualization
lineups [4 28] have been established as tools for visual_testing similar to standard statistical inference tests allowing us to evaluate the validity of graphical findings in an objective manner in simulation studies [12] lineups have been shown as being efficient the power of visual tests is comparable to classical tests while being much less stringent in terms of distributional assumptions made this makes lineups versatile yet powerful tools in situations where conditions for regular statistical tests are not or cannot be met in this paper we introduce lineups as a tool for evaluating the power of competing graphical designs we highlight some of the theoretical properties and then show results from two studies evaluating competing designs both studies are designed to go to the limits of our perceptual abilities to highlight differences between designs we use bothaccuracy and speed of evaluation as measures of a successful design the first study compares the choice of coordinate system polar versus cartesian coordinates the results show strong support in favor of cartesian coordinates in finding fast and accurate answers to spotting patterns the second study is aimed at finding shift differences between distributions both studies are motivated by data problems that we have recently encountered and explore using simulated data to evaluate the plot designs under controlled conditions amazon mechanical_turk mturk is used to conduct the studies the lineups provide an effective mechanism for objectively evaluating plot designs efficiency_of_displays lineups power_comparison visual inference
recently there has been increasing research interest in displaying graphs with curved_edges to produce more readable visualizations while there are several automatic techniques little has been done to evaluate their effectiveness empirically in this paper we present two experiments studying the impact of edge curvature on graph readability the goal is to understand the advantages and disadvantages of using curved_edges for common graph tasks compared to straight line segments which are the conventional choice for showing edges in node-link_diagrams we included several edge variations straight edges edges with different curvature levels and mixed straight and curved_edges during the experiments participants were asked to complete network tasks including determination of connectivity shortest path node degree and common neighbors we also asked the participants to provide subjective ratings of the aesthetics of different edge types the results show significant performance differences between the straight and curved_edges and clear distinctions between variations of curved_edges graph curved_edges evaluation visualization
we present a novel technique-compressed adjacency matrices-for visualizing gene regulatory networks these directed networks have strong structural characteristics out-degrees with a scale-free distribution in-degrees bound by a low maximum and few and small cycles standard visualization_techniques such as node-link_diagrams and adjacency matrices are impeded by these network characteristics the scale-free distribution of out-degrees causes a high number of intersecting edges in node-link_diagrams adjacency matrices become space-inefficient due to the low in-degrees and the resulting sparse network compressed adjacency matrices however exploit these structural characteristics by cutting open and rearranging an adjacency_matrix we achieve a compact and neatly-arranged visualization compressed adjacency matrices allow for easy detection of subnetworks with a specific structure so-called motifs which provide important knowledge about gene regulatory networks to domain experts we summarize motifs commonly referred to in the literature and relate them to network analysis tasks common to the visualization domain we show that a user can easily find the important motifs in compressed adjacency matrices and that this is hard in standard adjacency_matrix and node-link_diagrams we also demonstrate that interaction techniques for standard adjacency matrices can be used for our compressed variant these techniques include rearrangement clustering highlighting and filtering network adjacency_matrix gene_regulation scale-free
the performance of massively parallel applications is often heavily impacted by the cost ofcommunication among compute nodes however determining how to best use the network is a formidable task made challenging by the ever increasing size and complexity of modern supercomputers this paper applies visualization_techniques to aid parallel application developers in understanding the network activity by enabling a detailed exploration of the flow of packets through the hardware interconnect in order to visualize this large and complex data we employ two linked_views of the hardware network the first is a 2d view that represents the network structure as one of several simplified planar projections this view is designed to allow a user to easily identify trends and patterns in the network traffic the second is a 3d view that augments the 2d view by preserving the physical network topology and providing a context that is familiar to the application developers using the massively parallel multi-physics code pf3d as a case_study we demonstrate that our tool provides valuable insight that we use to explain and optimize pf3d's performance on an ibm blue gene/p system performance_analysis network traffic visualization projected_graph_layouts
we investigate the cognitive impact of various layout features-symmetry alignment collinearity axis alignment andorthogonality - on the recall of network_diagrams graphs this provides insight into how people internalize these diagrams and what features should or shouldn't be utilised when designing static and interactive network-based visualisations participants were asked to study remember and draw a series of small network_diagrams each drawn to emphasise a particular visual feature the visual_features were based on existing theories of perception and the task enabled visual processing at the visceral level only our results strongly support the importance of visual_features such as symmetry collinearity andorthogonality while not showing any significant impact for node-alignment or parallel edges network_diagrams diagram recall experiment graph_layout perceptual_theories visual_features
we propose a technique that allows straight-line graph_drawings to be rendered interactively with adjustable level of detail the approach consists of a novel combination of edge cumulation with density-based node aggregation and is designed to exploit common graphics_hardware for speed it operates directly on graph data and does not require precomputed hierarchies or meshes as proof of concept we present animplementation that scales to graphs with millions of nodes and edges and discuss several example applications graph_visualization opengl edge_aggregation
this paper presents two linked empirical studies focused on uncertainty_visualization the experiments are framed from two conceptual perspectives first a typology of uncertainty is used to delineate kinds of uncertainty matched with space time and attribute components of data second concepts from visual semiotics are applied to characterize the kind of visual signification that is appropriate for representing those different categories of uncertainty this framework guided the two experiments reported here the first addresses representation intuitiveness considering both visual_variables and iconicity of representation the second addresses relative performance of the most intuitive abstract and iconic representations of uncertainty on a map reading task combined results suggest initial guidelines for representing uncertainty and discussion focuses on practical applicability of results uncertainty_visualization semiotics uncertainty categories visual_variables
classifying a set of objects into clusters can be done in numerous ways producing different results they can be visually compared using contingency tables [27] mosaicplots [13] fluctuation diagrams [15] tableplots [20]  modified parallel_coordinates plots [28] parallel_sets plots [18] or circos diagrams [19] unfortunately the interpretability of all these graphical displays decreases rapidly with the numbers of categories and clusterings in his famous book a semiology of graphics [5] bertin writes “the discovery of an ordered concept appears as the ultimate point in logical simplification since it permits reducing to a single instant the assimilation of series which previously required many instants of study” or in more everyday language if you use good orderings you can see results immediately that with other orderings might take a lot of effort this is also related to the idea of effect ordering [12] that data should be organised to reflect the effect you want to observe this paper presents an efficient algorithm based on bertin's idea and concepts related to kendall's t [17] which finds informative joint orders for two or more nominal classification variables we also show how these orderings improve the various displays and how groups of corresponding categories can be detected using a top-down partitioning algorithm different clusterings based on data on the environmental performance of cars sold in germany are used for illustration all presented methods are available in the r package extracat which is used to compute the optimized orderings for the example dataset order_optimization classification fluctuation diagrams seriation
we present the results of two user studies on the perception of visual_variables on tiled high-resolution wall-sized displays we contribute an understanding of and indicators predicting how large variations in viewing distances and viewing angles affect the accurate perception of angles areas and lengths our work thus helps visualization researchers with design considerations on how to create effective visualizations for these spaces the first study showed that perceptionaccuracy was impacted most when viewers were close to the wall but differently for each variable angle area length our second study examined the effect of perception when participants could move freely compared to when they had a static viewpoint we found that a far but static viewpoint was as accurate but less time consuming than one that included free motion based on our findings we recommend encouraging viewers to stand further back from the display when conducting perception estimation tasks if tasks need to be conducted close to the wall display important information should be placed directly in front of the viewer or above and viewers should be provided with an estimation of the distortion effects predicted by our work-or encouraged to physically navigate the wall in specific ways to reduce judgement error information_visualization perception wall-displays
uncertainty can arise in any stage of a visual_analytics process especially in data-intensive applications with a sequence of data_transformations additionally throughout the process of multidimensional multivariate data_analysis uncertainty due to data_transformation and integration may split merge increase or decrease this dynamic characteristic along with other features of uncertainty pose a great challenge to effective uncertainty-aware visualization this paper presents a new framework for modeling uncertainty and characterizing the evolution of the uncertainty information through analytical processes based on the framework we have designed a visual metaphor called uncertainty flow to visually and intuitively summarize how uncertainty information propagates over the whole analysis pipeline our system allows analysts to interact with and analyze the uncertainty information at different levels of detail three experiments were conducted to demonstrate the effectiveness and intuitiveness of our design uncertainty_visualization error_ellipsoids uncertainty_fusion uncertainty propagation uncertainty_quantification
people have difficulty understanding statistical information and are unaware of their wrong judgments particularly in bayesian_reasoning psychology studies suggest that the way bayesian problems are represented can impact comprehension but few visual_designs have been evaluated and only populations with a specific background have been involved in this study a textual and six visual representations for three classic problems were compared using a diverse subject pool through crowdsourcing visualizations included area-proportional euler_diagrams glyph representations and hybrid diagrams combining both our study failed to replicate previous findings in that subjects'accuracy was remarkably lower and visualizations exhibited no measurable benefit a second experiment confirmed that simply adding a visualization to a textual bayesian problem is of little help even when the text refers to the visualization but suggests that visualizations are more effective when the text is given without numerical values we discuss our findings and the need for more such experiments to be carried out on heterogeneous populations of non-experts bayesian_reasoning euler_diagrams base rate fallacy crowdsourcing glyphs probabilistic_judgment
we propose a method to highlight query hits in hierarchically clustered collections of interrelated items such as digital libraries or knowledge bases the method is based on the idea that organizing search results similarly to their arrangement on a fixed reference map facilitates orientation and assessment by preserving a user's mental_map here the reference map is built from an mds layout of the items in a voronoi treemap representing their hierarchical_clustering and we use techniques from dynamic_graph_layout to align query results with the map the approach is illustrated on an archive of newspaper articles search results dynamic_graph_layout edge_bundling mental_map multidimensional scaling voronoi_treemaps
we present a method for automatically building typographic maps that merge text and spatial_data into a visual representation where text alone forms the graphical features we further show how to use this approach to visualize spatial_data such as traffic density crime rate or demographic data the technique accepts a vector representation of a geographic map and spatializes the textual labels in the space onto polylines and polygons based on user-defined visual attributes and constraints our sampleimplementation runs as a web service spatializing shape files from the openstreetmap project into typographic maps for any region geo visualization label_placement spatial_data text_visualization
visualizing trajectory attribute data is challenging because it involves showing the trajectories in their spatio-temporal context as well as the attribute values associated with the individual points of trajectories previous work on trajectory visualization addresses selected aspects of this problem but not all of them we present a novel approach to visualizing trajectory attribute data our solution covers space time and attribute values based on an analysis of relevant visualization tasks we designed the visualization solution around the principle of stacking trajectory bands the core of our approach is a hybrid 2d/3d display a 2d map serves as a reference for the spatial_context and the trajectories are visualized as stacked 3d trajectory bands along which attribute values are encoded by color time is integrated through appropriate ordering of bands and through a dynamic_query mechanism that feeds temporally aggregated information to a circular time display an additional 2d time graph shows temporal information in full detail by stacking 2d trajectory bands our solution is equipped with analytical and interactive mechanisms for selecting and ordering of trajectories and adjusting the color mapping as well as coordinated highlighting and dedicated 3d navigation we demonstrate the usefulness of our novel visualization by three examples related to radiation surveillance traffic analysis and maritime navigation user feedback obtained in a small experiment indicates that our hybrid 2d/3d solution can be operated quite well visualization exploratory_analysis interaction spatio-temporal_data trajectory attribute data
all major web mapping services use the web_mercator projection this is a poor choice for maps of the entire globe or areas of the size of continents or larger countries because the mercator projection shows medium and higher latitudes with extreme areal distortion and provides an erroneous impression of distances and relative areas the web_mercator projection is also not able to show the entire globe as polar latitudes cannot be mapped when selecting an alternative projection for information_visualization rivalingfactors have to be taken into account such as map scale the geographic area shown the map's height-to-width ratio and the type of cartographic visualization it is impossible for a single map projection to meet the requirements for all thesefactors the proposed composite map projection combines several projections that are recommended in cartographic literature and seamlessly morphs map space as the user changes map scale or the geographic region displayed the composite projection adapts the map's geometry to scale to the map's height-to-width ratio and to the central latitude of the displayed area by replacing projections and adjusting their parameters the composite projection shows the entire globe including poles it portrays continents or larger countries with less distortion optionally without areal distortion and it can morph to the web_mercator projection for maps showing small regions html canvas multi-scale_map web_mercator web_cartography web_map_projection web mapping
in this paper we investigate the problem of labeling point sites in focus regions of maps or diagrams this problem occurs for example when the user of a mapping service wants to see the names of restaurants or other pois in a crowded downtown area but keep the overview over a larger area our approach is to place the labels at the boundary of the focus region and connect each site with its label by a linear connection which is called a leader in this way we move labels from the focus region to the less valuable context region surrounding it in order to make the leader layout well readable we present algorithms that rule out crossings between leaders and optimize other characteristics such as total leader length and distance between labels this yields a new variant of the boundary labeling problem which has been studied in the literature other than in traditional boundary labeling where leaders are usually schematized polylines we focus on leaders that are either straight-line segments or bezier curves further we present algorithms that given the sites find a position of the focus region that optimizes the above characteristics we also consider a variant of the problem where we have more sites than space for labels in this situation we assume that the sites are prioritized by the user alternatively we take a new facility-location perspective which yields a clustering of the sites we label one representative of each cluster if the user wishes we apply our approach to the sites within a cluster giving details on demand focus+context techniques data_clustering geographic/geospatial_visualization mobile_and_ubiquitous_visualization
we characterize the design space of the algorithms that sequentially tile a rectangular_area with smaller fixed-surface rectangles this space consist of five independent dimensions order size score recurse and phrase each of these dimensions describe a particular aspect of such layout tasks this class of layouts is interesting because beyond encompassing simple grids tables and trees it also includes all kinds of treemaps involving the placement of rectangles for instance slice and dice squarified strip and pivot layouts are various points in this five dimensional space many classic statistics visualizations such as 100% stacked_bar_charts mosaic_plots and dimensional_stacking are also instances of this class a few new and potentially interesting points in this space are introduced such as spiral treemaps and variations on the strip layout the core algorithm is implemented as a javascript prototype that can be used as a layout component in a variety of infoviz toolkits layout dimensional_stacking grids mosaic_plots squarified and pivot variations) strip tables &amp  tree layouts treemaps (slice and dice visualization_models
glyph-based_visualization can offer elegant and concise presentation of multivariate information while enhancing speed and ease in visual_search experienced by users as with icon designs glyphs are usually created based on the designers' experience and intuition often in a spontaneous manner such a process does not scale well with the requirements of applications where a large number of concepts are to be encoded using glyphs to alleviate such limitations we propose a new systematic process for glyph_design by exploring the parallel between the hierarchy of concept categorization and the ordering of discriminative capacity of visual channels we examine the feasibility of this approach in an application where there is a pressing need for an efficient and effective means to visualize workflows of biological experiments by processing thousands of workflow records in a public archive of biological experiments we demonstrate that a cost-effective glyph_design can be obtained by following a process of formulating a taxonomy with the aid of computation identifying visual channels hierarchically and defining application-specific abstraction and metaphors glyph-based_techniques bioinformatics_visualization design_methodologies taxonomies
comparing slopes is a fundamental graph reading task and the aspect_ratio chosen for a plot influences how easy these comparisons are to make according to banking to 45° a classic design guideline first proposed and studied by cleveland et al aspect_ratios that center slopes around 45° minimize errors in visual judgments of slope ratios this paper revisits this earlier work through exploratory pilot studies that expand cleveland et al's experimental design we develop an empirical model of slope ratio estimation that fits more extreme slope ratio judgments and two common slope ratio estimation strategies we then run two experiments to validate our model in the first we show that our model fits more generally than the one proposed by cleveland et al and we find that in general slope ratio errors are not minimized around 45° in the second experiment we explore a novel hypothesis raised by our model that visible baselines can substantially mitigate errors made in slope judgments we conclude with an application of our model to aspect_ratio_selection banking to  degrees aspect_ratio_selection orientation resolution slope_perception
datasets with a large number of dimensions per data item hundreds or more are challenging both for computational and visual_analysis moreover these dimensions have different characteristics and relations that result in sub-groups and/or hierarchies over the set of dimensions such structures lead to heterogeneity within the dimensions although the consideration of these structures is crucial for the analysis most of the available analysis methods discard the heterogeneous relations among the dimensions in this paper we introduce the construction and utilization of representativefactors for the interactive_visual_analysis of structures in high-dimensional_datasets first we present a selection of methods to investigate the sub-groups in the dimension set and associate representativefactors with those groups of dimensions second we introduce how thesefactors are included in the interactive_visual_analysis cycle together with the original dimensions we then provide the steps of an analytical procedure that iteratively analyzes the datasets through the use of representativefactors we discuss how our methods improve the reliability and interpretability of the analysis process by enabling more informed selections of computational tools finally we demonstrate our techniques on the analysis of brain imaging study results that are performed over a large group of subjects interactive_visual_analysis high-dimensional_data_analysis
reading a visualization can involve a number of tasks such as extracting comparing or aggregating numerical values yet most of the charts that are published in newspapers reports books and on the web only support a subset of these tasks in this paper we introduce graphical overlays-visual elements that are layered onto charts to facilitate a larger set of chart reading tasks these overlays directly support the lower-level perceptual and cognitive processes that viewers must perform to read a chart we identify five main types of overlays that support these processes the overlays can provide 1 reference structures such as gridlines 2 highlights such as outlines around important marks 3 redundant encodings such as numerical data labels 4 summary statistics such as the mean or max and 5 annotations such as descriptive text for context we then present an automated system that applies user-chosen graphical overlays to existing chart bitmaps our approach is based on the insight that generating most of these graphical overlays only requires knowing the properties of the visual marks and axes that encode the data but does not require access to the underlying data values thus our system analyzes the chart bitmap to extract only the properties necessary to generate the desired overlay we also discuss techniques for generating interactive overlays that provide additional controls to viewers we demonstrate several examples of each overlay type for bar pie and line_charts visualization graph_comprehension graphical_perception overlays
a discourse parser is a natural_language_processing system which can represent the organization of a document based on a rhetorical structure tree-one of the key data structures enabling applications such as text summarization question answering and dialogue generation computational linguistics researchers currently rely on manually exploring and comparing the discourse_structures to get intuitions for improving parsing algorithms in this paper we present daviewer an interactive_visualization system for assisting computational linguistics researchers to explore compare evaluate and annotate the results of discourse parsers an iterative user-centered_design process with domain experts was conducted in the development of daviewer we report the results of an informal formative study of the system to better understand how the proposed visualization and interaction techniques are used in the real research environment discourse_structure computational_linguisitics interaction techniques tree_comparison visual_analytics
when and where is an idea dispersed social media like twitter has been increasingly used for exchanging information opinions and emotions about events that are happening across the world here we propose a novel visualization design “whisper” for tracing the process of information_diffusion in social media in real time our design highlights three major characteristics of diffusion processes in social media the temporal trend social-spatial extent and community response of a topic of interest such social spatiotemporal processes are conveyed based on a sunflower metaphor whose seeds are often dispersed far away in whisper we summarize the collective responses of communities on a given topic based on how tweets were retweeted by groups of users through representing the sentiments extracted from the tweets and tracing the pathways of retweets on a spatial hierarchical layout we use an efficient flux line-drawing algorithm to trace multiple pathways so the temporal and spatial patterns can be identified even for a bursty event a focused diffusion series highlights key roles such as opinion leaders in the diffusion process we demonstrate how our design facilitates the understanding of when and where a piece of information is dispersed and what are the social responses of the crowd for large-scale events including political campaigns and natural disasters initial feedback from domain experts suggests promising use for today's information consumption and dispersion in the wild information_visualization social media contagion information_diffusion microblogging spatiotemporal patterns
event sequence data is common in many domains ranging from electronic medical records emrs to sports events moreover such sequences often result in measurable outcomes eg life or death win or loss collections of event sequences can be aggregated together to form event progression pathways these pathways can then be connected with outcomes to model how alternative chains of events may lead to different results this paper describes the outflow_visualization_technique designed to 1 aggregate multiple event sequences 2 display the aggregate pathways through different event states with timing and cardinality 3 summarize the pathways' corresponding outcomes and 4 allow users to explore externalfactors that correlate with specific pathway state_transitions results from a user_study with twelve participants show that users were able to learn how to use outflow easily with limited training and perform a range of tasks both accurately and rapidly outflow information_visualization state_diagram state transition temporal event sequences
for many applications involving time_series_data people are often interested in the changes of item values over time as well as their ranking changes for example people search many words via search_engines like google and bing every day analysts are interested in both the absolute searching number for each word as well as their relative rankings both sets of statistics may change over time for very large time_series_data with thousands of items how to visually present ranking changes is an interesting challenge in this paper we propose rankexplorer a novel visualization method based on themeriver to reveal the ranking changes our method consists of four major components 1 a segmentation method which partitions a large set of time_series curves into a manageable number of ranking categories 2 an extended themeriver view with embedded color bars and changing glyphs to show the evolution of aggregation values related to each ranking category over time as well as the content changes in each ranking category 3 a trend curve to show the degree of ranking changes over time 4 rich user_interactions to support interactive_exploration of ranking changes we have applied our method to some real time_series_data and the case studies demonstrate that our method can reveal the underlying patterns related to ranking changes which might otherwise be obscured in traditional visualizations themeriver time-series_data interaction techniques ranking change
storyline_visualization is a technique used to depict the temporal dynamics of social interactions this visualization_technique was first introduced as a hand-drawn illustration in xkcd's “movie narrative charts” [21] if properly constructed the visualization can convey both global trends and local interactions in the data however previous methods for automating storyline_visualizations are overly simple failing to achieve some of the essential principles practiced by professional illustrators this paper presents a set of design considerations for generating aesthetically pleasing and legible storyline_visualizations our layout_algorithm is based on evolutionary computation allowing us to effectively incorporate multiple objective functions we show that the resulting visualizations have significantly improved aesthetics and legibility compared to existing techniques layout_algorithm design_study storyline_visualization timeline_visualization
the importance of interaction to information_visualization infovis and in particular of the interplay between interactivity and cognition is widely recognized [12 15 32 55 70] this interplay combined with the demands from increasingly large and complex datasets is driving the increased significance of interaction in infovis in parallel there have been rapid advances in many facets of interaction technologies however infovis interactions have yet to take full advantage of these new possibilities in interaction technologies as they largely still employ the traditional desktop mouse and keyboard setup of wimp windows icons menus and a pointer interfaces in this paper we reflect more broadly about the role of more “natural” interactions for infovis and provide opportunities for future research we discuss and relate general hci interaction_models to existing infovis interaction classifications by looking at interactions from a novel angle taking into account the entire spectrum of interactions our discussion of infovis-specific interaction_design considerations helps us identify a series of underexplored attributes of interaction that can lead to new more “natural” interaction techniques for infovis design considerations nui (natural_user_interface) interaction post-wimp
in this paper we propose a new strategy for graph_drawing utilizing layouts of many sub-graphs supplied by a large group of people in a crowd sourcing manner we developed an algorithm based on laplacian constrained distance embedding to merge subgraphs submitted by different users while attempting to maintain the topological information of the individual input layouts to facilitate collection of layouts from many people a light-weight interactive_system has been designed to enable convenient dynamic viewing modification and traversing between layouts compared with other existing graph_layout_algorithms our approach can achieve more aesthetic and meaningful layouts with high user preference graph_layout laplacian_matrix crowd sourcing editing force directed layout merging stress model
we present pivotpaths an interactive_visualization for exploring faceted information resources during both work and leisure we increasingly interact withinformation_spaces that contain multiple facets and relations such as authors keywords and citations of academic publications or actors and genres of movies to navigate these interlinked resources today one typically selects items from facet lists resulting in abrupt changes from one subset of data to another while filtering is useful to retrieve results matching specific criteria it can be difficult to see how facets and items relate and to comprehend the effect of filter operations in contrast the pivotpaths interface exposes faceted relations as visual paths in arrangements that invite the viewer to `take a stroll' through aninformation_space pivotpaths supports pivot operations as lightweight interaction techniques that trigger gradual transitions between views we designed the interface to allow for casual traversal of large collections in an aesthetically pleasing manner that encourages exploration and serendipitous discoveries this paper shares the findings from our iterative design-and-evaluation process that included semi-structured interviews and a two-week deployment of pivotpaths applied to a large_database of academic publications information_visualization animation exploratory_search information_seeking interactivity node-link_diagrams
visual_comparison is an intrinsic part of interactive data_exploration and analysis the literature provides a large body of existing solutions that help users accomplish comparison tasks these solutions are mostly of visual nature and custom-made for specific data we ask the question if a more general support is possible by focusing on the interaction aspect of comparison tasks as an answer to this question we propose a novel interaction concept that is inspired by real-world behavior of people comparing information printed on paper in line with real-world interaction our approach supports users 1 in interactively specifying pieces of graphical information to be compared 2 in flexibly arranging these pieces on the screen and 3 in performing the actual comparison of side-by-side and overlapping arrangements of the graphical information complementary visual_cues and add-ons further assist users in carrying out comparison tasks our concept and the integrated interaction techniques are generally applicable and can be coupled with different visualization_techniques we implemented an interactive prototype and conducted a qualitative user_study to assess the concept's usefulness in the context of three different visualization_techniques the obtained feedback indicates that our interaction techniques mimic the natural behavior quite well can be learned quickly and are easy to apply to visual_comparison tasks interaction techniques human-computer_interaction natural interaction visual_comparison visualization
we present a network_visualization design_study focused on supporting automotive engineers who need to specify and optimize traffic patterns for in-carcommunication networks the task and data abstractions that we derived support actively making changes to an overlay network where logicalcommunication specifications must be mapped to an underlying physical network these abstractions are very different from the dominant use case in visual network analysis namely identifying clusters and central nodes that stems from the domain of social_network_analysis our visualization tool relex was created and iteratively refined through a full user-centered_design process that included a full problem characterization phase before tool design began paper prototyping iterative refinement in close collaboration with expert users for formative evaluation deployment in the field with real analysts using their own data usability testing with non-expert users and summative evaluation at the end of the deployment in the summative post-deployment study which entailed domain experts using the tool over several weeks in their daily practice we documented many examples where the use of relex simplified or sped up their work compared to previous practices network_visualization automotive change management design_study traffic_optimization traffic_routing
this paper reports on a between-subject comparative online_study of three information_visualization demonstrators that each displayed the same dataset by way of an identical scatterplot technique yet were different in style in terms of visual and interactive embellishment we validated stylistic adherence and integrity through a separate experiment in which a small cohort of participants assigned our three demonstrators to predefined groups of stylistic examples after which they described the styles with their own words from the online_study we discovered significant differences in how participants execute specific interaction operations and the types of insights that followed from them however in spite of significant differences in apparent usability enjoyability and usefulness between the style demonstrators no variation was found on the self-reported depth expert-rated depth confidence or difficulty of the resulting insights three different methods of insight analysis have been applied revealing how style impacts the creation of insights ranging from higher-level pattern seeking to a more reflective and interpretative engagement with content which is what underlies the patterns as this study only forms the first step in determining how the impact of style in information_visualization could be best evaluated we propose several guidelines and tips on how to gather compare and categorize insights through an online evaluation study particularly in terms of analyzing the concise yet wide variety of insights and observations in a trustworthy and reproducable manner visualization aesthetics design evaluation online_study style user_experience
we present and evaluate a framework for constructing sketchy style information_visualizations that mimic data graphics drawn by hand we provide an alternative renderer for the processing graphics environment that redefines core drawing primitives including line polygon and ellipse rendering these primitives allow higher-level graphical features such as bar_charts line_charts treemaps and node-link_diagrams to be drawn in a sketchy style with a specified degree of sketchiness the framework is designed to be easily integrated into existing visualizationimplementations with minimal programming modification or design effort we show examples of use for statistical_graphics conveying spatial imprecision and for enhancing aesthetic and narrative qualities of visualization we evaluate user perception of sketchiness of areal features through a series of stimulus-response tests in order to assess users' ability to place sketchiness on a ratio scale and to estimate area results suggest relative area judgment is compromised by sketchy rendering and that its influence is dependent on the shape being rendered they show that degree of sketchiness may be judged on an ordinal scale but that its judgement varies strongly between individuals we evaluate higher-level impacts of sketchiness through user testing of scenarios that encourage user engagement with data visualization and willingness to critique visualization design results suggest that where a visualization is clearly sketchy engagement may be increased and that attitudes to participating in visualization annotation are more positive the results of our work have implications for effective information_visualization design that go beyond the traditional role of sketching as a tool for prototyping or its use for an indication of general uncertainty npr hand-drawn non-photorealistic_rendering sketch uncertainty_visualization
in written and spokencommunications figures of speech eg metaphors and synecdoche are often used as an aid to help convey abstract or less tangible concepts however the benefits of using rhetorical illustrations or embellishments in visualization have so far been inconclusive in this work we report an empirical_study to evaluate hypotheses that visual_embellishments may aid memorization visual_search and concept comprehension one major departure from related experiments in the literature is that we make use of a dual-task methodology in our experiment this design offers an abstraction of typical situations where viewers do not have their full attention focused on visualization eg in meetings and lectures the secondary task introduces “divided attention” and makes the effects of visual_embellishments more observable in addition it also serves as additional masking in memory-based trials the results of this study show that visual_embellishments can help participants better remember the information depicted in visualization on the other hand visual_embellishments can have a negative impact on the speed of visual_search the results show a complex pattern as to the benefits of visual_embellishments in helping participants grasp key concepts from visualization visual_embellishments cognition evaluation icons long-term_memory metaphors visual_search working_memory
we report on results of a series of user studies on the perception of four visual_variables that are commonly used in the literature to depict uncertainty to the best of our knowledge we provide the first formal evaluation of the use of these variables to facilitate an easier reading of uncertainty in visualizations that rely on line graphical primitives in addition to blur dashing and grayscale we investigate the use of `sketchiness' as a visual variable because it conveys visual impreciseness that may be associated with data quality inspired by work in non-photorealistic_rendering and by the features of hand-drawn lines we generate line trajectories that resemble hand-drawn strokes of various levels of proficiency-ranging from child to adult strokes-where the amount of perturbations in the line corresponds to the level of uncertainty in the data our results show that sketchiness is a viable alternative for the visualization of uncertainty in lines and is as intuitive as blur although people subjectively prefer dashing style over blur grayscale and sketchiness we discuss advantages and limitations of each technique and conclude with design considerations on how to deploy these visual_variables to effectively depict various levels of uncertainty for line marks uncertainty_visualization perception qualitative_evaluation quantitative_evaluation
current interfaces for common information_visualizations such as bar graphs line graphs and scatterplots usually make use of the wimp windows icons menus and a pointer interface paradigm with its frequently discussed problems of multiple levels of indirection via cascading menus dialog boxes and control panels recent advances in interface capabilities such as the availability of pen_and_touch interaction challenge us to re-think this and investigate more direct access to both the visualizations and the data they portray we conducted a wizard of oz study to explore applying pen_and_touch interaction to the creation of information_visualization_interfaces on interactive whiteboards without implementing a plethora of recognizers our wizard acted as a robust and flexible pen_and_touch recognizer giving participants maximum freedom in how they interacted with the system based on our qualitative_analysis of the interactions our participants used we discuss our insights about pen_and_touch interactions in the context of learnability and the interplay between pen_and_touch gestures we conclude with suggestions for designing pen_and_touch enabled interactive_visualization_interfaces pen_and_touch wizard of oz data_exploration interaction whiteboard
in this paper we present the deeptree exhibit a multi-user multi-touch interactive_visualization of the tree of life we developed deeptree to facilitate collaborative_learning of evolutionary concepts we will describe an iterative process in which a team of computer scientists learning scientists biologists and museum curators worked together throughout design development and evaluation we present the importance of designing the interactions and the visualization hand-in-hand in order to facilitate active_learning the outcome of this process is a fractal-based tree layout that reduces visual complexity while being able to capture all life on earth a custom rendering and navigation engine that prioritizes visual appeal and smooth fly-through and a multi-user interface that encourages collaborative exploration while offering guided discovery we present an evaluation showing that the large_dataset encouraged free exploration triggers emotional responses and facilitates visitor engagement and informal learning informal_science_education collaborative_learning large tree_visualizations multi-touch_interaction
interactive_visualizations can allow science museum visitors to explore new worlds by seeing and interacting with scientific data however designing interactive_visualizations for informal_learning_environments such as museums presents several challenges first visualizations must engage visitors on a personal level second visitors often lack the background to interpret visualizations of scientific data third visitors have very limited time at individual exhibits in museums this paper examines these design considerations through the iterative development and evaluation of an interactive exhibit as a visualization tool that gives museumgoers access to scientific data generated and used by researchers the exhibit prototype living liquid encourages visitors to ask and answer their own questions while exploring the time-varying global distribution of simulated marine microbes using a touchscreen interface iterative development proceeded through three rounds of formative evaluations using think-aloud protocols and interviews each round informing a key visualization design decision 1 what to visualize to initiate inquiry 2 how to link data at the microscopic scale to global patterns and 3 how to include additional data that allows visitors to pursue their own questions data from visitor evaluations suggests that when designing visualizations for public audiences one should 1 avoid distracting visitors from data that they should explore 2 incorporate background information into the visualization 3 favor understandability over scientificaccuracy and 4 layer data accessibility to structure inquiry lessons learned from this case_study add to our growing understanding of how to use visualizations to actively engage learners with scientific data information_visualization_evaluation informal_learning_environments science_museums user_interaction user studies
while intuitive time-series visualizations exist for common datasets student course history data is difficult to represent using traditional visualization_techniques due its concurrent nature a visual composition process is developed and applied to reveal trends across various groupings by working closely with educators analytic strategies and techniques are developed to leverage the visualization_composition to reveal unknown trends in the data furthermore clustering algorithms are developed to group common course-grade histories for further analysis lastly variations of the composition process are implemented to reveal subtle differences in the underlying data these analytic tools and techniques enabled educators to confirm expected trends and to discover new ones clustering aggregate_visualization student_performance_analysis visualization_composition
sports analysts live in a world of dynamic games flattened into tables of numbers divorced from the rinks pitches and courts where they were generated currently these professional analysts use r stata sas and other statistical software packages for uncovering insights from game data quantitative sports consultants seek a competitive advantage both for their clients and for themselves as analytics becomes increasingly valued by teams clubs and squads in order for the information_visualization community to support the members of this blossoming industry it must recognize where and how visualization can enhance the existing analytical workflow in this paper we identify three primary stages of today's sports analyst's routine where visualization can be beneficially integrated 1 exploring a dataspace 2 sharing hypotheses with internal colleagues and 3 communicating findings to stakeholdersworking closely with professional ice hockey analysts we designed and built snapshot a system to integrate visualization into the hockey intelligence gathering process snapshot employs a variety of information_visualization_techniques to display shot data yet given the importance of a specific hockey statistic shot length we introduce a technique the radial heat map through a user_study we received encouraging feedback from several professional analysts both independent consultants and professional team personnel visual_knowledge_discovery human computer interaction hypothesis_testing visual_evidence visual_knowledge_representation
significant effort has been devoted to designing clustering algorithms that are responsive to user feedback or that incorporate prior domain knowledge in the form of constraints however users desire more expressive forms of interaction to influence clustering outcomes in our experiences working with diverse application scientists we have identified an interaction style scatter/gather clustering that helps users iteratively restructure clustering results to meet their expectations as the names indicate scatter and gather are dual primitives that describe whether clusters in a current segmentation should be broken up further or alternatively brought back together by combining scatter and gather operations in a single step we support very expressive dynamic restructurings of data scatter/gather clustering is implemented using a nonlinear_optimization framework that achieves both locality of clusters and satisfaction of user-supplied constraints we illustrate the use of our scatter/gather clustering approach in a visual analytic application to study baffle shapes in the bat biosonar ears and nose system we demonstrate how domain experts are adept at supplying scatter/gather constraints and how our framework incorporates these constraints effectively without requiring numerous instance-level constraints scatter/gather clustering alternative_clustering constrained_clustering
performing exhaustive searches over a large number of text documents can be tedious since it is very hard to formulate search queries or define filter criteria that capture an analyst's information need adequately classification through machine_learning has the potential to improve search and filter tasks encompassing either complex or very specific information needs individually unfortunately analysts who are knowledgeable in their field are typically not machine_learning specialists most classification methods however require a certain expertise regarding their parametrization to achieve good results supervised machine_learning algorithms in contrast rely on labeled data which can be provided by analysts however the effort for labeling can be very high which shifts the problem from composing complex queries or defining accurate filters to another laborious task in addition to the need for judging the trained classifier's quality we therefore compare three approaches for interactive classifier training in a user_study all of the approaches are potential candidates for the integration into a larger retrieval system they incorporate active_learning to various degrees in order to reduce the labeling effort as well as to increase effectiveness two of them encompass interactive_visualization for letting users explore the status of the classifier in context of the labeled documents as well as for judging the quality of the classifier in iterative feedback loops we see our work as a step towards introducing user controlled classification methods in addition to text search and filtering for increasing recall in analytics scenarios involving large corpora visual_analytics active_learning classification human computer interaction information_retrieval user_evaluation
contingency tables summarize the relations between categorical variables and arise in both scientific and business domains asymmetrically large two-way contingency tables pose a problem for common visualization methods the contingency wheel has been recently proposed as an interactive visual method to explore and analyze such tables however the scalability and readability of this method are limited when dealing with large and dense tables in this paper we present contingency wheel++ new visual_analytics methods that overcome these major shortcomings 1 regarding automated methods a measure of association based on pearson's residuals alleviates the bias of the raw residuals originally used 2 regarding visualization methods a frequency-based abstraction of the visual elements eliminates overlapping and makes analyzing both positive and negative associations possible and 3 regarding the interactive_exploration environment a multi-level overview+detail interface enables exploring individual data items that are aggregated in the visualization or in the table using coordinated_views we illustrate the applicability of these new methods with a use case and show how they enable discovering and analyzing nontrivial patterns and associations in large_categorical_data large_categorical_data contingency_table_analysis information interfaces and representation visual_analytics
visual_analytics is “the science of analytical reasoning facilitated by visual interactive interfaces” [70] the goal of this field is to develop tools and methodologies for approaching problems whose size and complexity render them intractable without the close coupling of both human and machine analysis researchers have explored this coupling in many venues vast vis infovis chi kdd iui and more while there have been myriad promising examples of human-computer collaboration there exists no common language for comparing systems or describing the benefits afforded by designing for such collaboration we argue that this area would benefit significantly from consensus about the design attributes that define and distinguish existing techniques in this work we have reviewed 1271 papers from many of the top-ranking conferences in visual_analytics human-computer_interaction and visualization from these we have identified 49 papers that are representative of the study of human-computer collaborative problem-solving and provide a thorough overview of the current state-of-the-art our analysis has uncovered key patterns of design hinging on humanand machine-intelligence affordances and also indicates unexplored avenues in the study of this area the results of this analysis provide a common framework for understanding these seemingly disparate branches of inquiry which we hope will motivate future work in the field human_computation framework human_complexity theory
while the formal evaluation of systems in visual_analytics is still relatively uncommon particularly rare are case studies of prolonged system use by domain analysts working with their own data conducting case studies can be challenging but it can be a particularly effective way to examine whether visual_analytics systems are truly helping expert users to accomplish their goals we studied the use of a visual_analytics system for sensemaking tasks on documents by six analysts from a variety of domains we describe their application of the system along with the benefits issues and problems that we uncovered findings from the studies identify features that visual_analytics systems should emphasize as well as missing capabilities that should be addressed these findings inform design implications for future systems visual_analytics case_study qualitative_evaluation
visual analytic tools aim to support the cognitively demanding task of sensemaking their success often depends on the ability to leverage capabilities of mathematical models visualization and human intuition through flexible usable and expressive interactions spatially clustering data is one effective metaphor for users to explore similarity and relationships between information adjusting the weighting of dimensions or characteristics of the dataset to observe the change in the spatial_layout semantic interaction is an approach to user_interaction in such spatializations that couples these parametric modifications of the clustering model with users' analytic operations on the data eg direct document movement in the spatialization highlighting text search etc in this paper we present results of a user_study exploring the ability of semantic interaction in a visual analytic prototype forcespire to support sensemaking we found that semantic interaction captures the analytical reasoning of the user through keyword weighting and aids the user in co-creating a spatialization based on the user's reasoning and intuition user_interaction analytic_reasoning sensemaking visual_analytics visualization
eye movement analysis is gaining popularity as a tool for evaluation of visual displays and interfaces however the existing methods and tools for analyzing eye movements and scanpaths are limited in terms of the tasks they can support and effectiveness for large_data and data with high variation we have performed an extensive empirical_evaluation of a broad range of visual_analytics methods used in analysis of geographic movement_data the methods have been tested for the applicability to eye_tracking data and the capability to extract useful knowledge about users' viewing behaviors this allowed us to select the suitable methods and match them to possible analysis tasks they can support the paper describes how the methods work in application to eye_tracking data and provides guidelines for method selection depending on the analysis tasks visual_analytics eye_tracking movement_data trajectory_analysis
we present a visual_analytics approach that addresses the detection of interesting patterns in numerical time_series specifically from environmental sciences crucial for the detection of interesting temporal patterns are the time scale and the starting points one is looking at our approach makes no assumption about time scale and starting position of temporal patterns and consists of three main steps an algorithm to compute statistical values for all possible time scales and starting positions of intervals visual identification of potentially interesting patterns in a matrix_visualization and interactive_exploration of detected patterns we demonstrate the utility of this approach in two scientific scenarios and explain how it allowed scientists to gain new insight into the dynamics of environmental systems time_series analysis multiscale visualization visual_analytics
visual_analytics emphasizes the interplay between visualization analytical procedures performed by computers and human perceptual and cognitive activities human reasoning is an important element in this context there are several theories in psychology and hci explaining open-ended and exploratory reasoning five of these theories sensemaking theories gestalt theories distributed_cognition graph_comprehension theories and skill-rule-knowledge models are described in this paper we discuss their relevance for visual_analytics in order to do this more systematically we developed a schema of categories relevant for visual_analytics research and evaluation all these theories have strengths but also weaknesses in explaining interaction with visual_analytics systems a possibility to overcome the weaknesses would be to combine two or more of these theories cognitive_theory interaction_design problem_solving reasoning visual_knowledge_discovery
organizations rely on data analysts to model customer engagement streamline operations improve production inform business decisions and combat fraud though numerous analysis and visualization tools have been built to improve the scale and efficiency at which analysts can work there has been little research on how analysis takes place within the social and organizational context of companies to better understand the enterprise analysts' ecosystem we conducted semi-structured interviews with 35 data analysts from 25 organizations across a variety of sectors including healthcare retail marketing and finance based on our interview data we characterize the process of industrial data_analysis and document how organizational features of an enterprise impact it we describe recurring pain points outstanding challenges and barriers to adoption for visual analytic tools finally we discuss design implications and opportunities for visual_analysis research data_analysis enterprise visualization
web clickstream data are routinely collected to study how users browse the web or use a service it is clear that the ability to recognize and summarize user behavior patterns from such data is valuable to e-commerce companies in this paper we introduce a visual_analytics system to explore the various user behavior patterns reflected by distinct clickstream clusters in a practical analysis scenario the system first presents an overview of clickstream clusters using a self-organizing map with markov chain models then the analyst can interactively explore the clusters through an intuitive user interface he can either obtain summarization of a selected group of data or further refine the clustering result we evaluated our system using two different datasets from ebay analysts who were working on the same data have confirmed the system's effectiveness in extracting user behavior patterns from complex datasets and enhancing their ability to reason
for a user to perceive continuous interactive response time in a visualization tool the rule of thumb is that it must process deliver and display rendered results for any given interaction in under 100 milliseconds in many visualization_systems successive interactions trigger independent queries and caching of results consequently computationally expensive queries like multidimensional clustering cannot keep up with rapid sequences of interactions precluding visual benefits such as motion parallax in this paper we describe a heuristic prefetching technique to improve the interactive response time of kmeans clustering in dynamic_query visualizations of multidimensional data we address the tradeoff between high interaction and intense query computation by observing how related interactions on overlapping data subsets produce similar clustering results and characterizing these similarities within a parameter space of interaction we focus on the two-dimensional parameter space defined by the minimum and maximum values of a time range manipulated by dragging and stretching a one-dimensional filtering lens over a plot of time_series_data using calculation of nearest neighbors of interaction points in parameter space we reuse partial query results from prior interaction sequences to calculate both an immediate best-effort clustering result and to schedule calculation of an exact result the method adapts to user_interaction patterns in the parameter space by reprioritizing the interaction neighbors of visited points in the parameter space a performance study on mesonet meteorological data demonstrates that the method is a significant improvement over the baseline scheme in which interaction triggers on-demand exact-range clustering with lru caching we also present initial evidence that approximate temporary clustering results are sufficiently accurate compared to exact results to convey useful cluster structure during rapid and protracted interaction d software_engineering design tools and techniques — user_interfaces h information systems database management — languages h information systems information interfaces and presentation — user_interfaces
learning of classifiers to be used as filters within the analytical reasoning_process leads to new and aggravates existing challenges such classifiers are typically trained ad-hoc with tight time constraints that affect the amount and the quality of annotation data and thus also the users' trust in the classifier trained we approach the challenges of ad-hoc training by inter-active_learning which extends active_learning by integrating human experts' background knowledge to greater extent in contrast to active_learning not only does inter-active_learning include the users' expertise by posing queries of data instances for labeling but it also supports the users in comprehending the classifier model by visualization besides the annotation of manually or automatically selected data instances users are empowered to directly adjust complex classifier models therefore our model visualization facilitates the detection and correction of inconsistencies between the classifier model trained by examples and the user's mental_model of the class definition visual feedback of the training process helps the users assess the performance of the classifier and thus build up trust in the filter created we demonstrate the capabilities of inter-active_learning in the domain of video_visual_analytics and compare its performance with the results of random_sampling and uncertainty sampling of training sets h information systems information storage and retrieval — information search and retrieval i computing methodologies artificial intelligence — learning
finding patterns and trends in spatial and temporal_datasets has been a long studied problem in statistics and different domains of science this paper presents a visual_analytics approach for the interactive_exploration and analysis of spatiotemporal correlations among multivariate datasets our approach enables users to discover correlations and explore potentially causal or predictive links at different spatiotemporal aggregation levels among the datasets and allows them to understand the underlying statistical foundations that precede the analysis our technique utilizes the pearson's product-moment correlation coefficient andfactors in the lead or lag between different datasets to detect trends and periodic patterns amongst them visual_analytics correlative analysis
an essential element of exploratory_data_analysis is the use of revealing low-dimensional projections of high-dimensional_data projection pursuit has been an effective method for finding interesting low-dimensional projections of multidimensional spaces by optimizing a score function called a projection pursuit index however the technique is not scalable to high-dimensional spaces here we introduce a novel method for discovering noteworthy views of high-dimensional_data spaces by using binning and random projections we define score functions akin to projection pursuit indices that characterize visual patterns of the low-dimensional projections that constitute feature subspaces we also describe an analytic multivariate visualization platform based on this algorithm that is scalable to extremely large problems high-dimensional_data random projections
ever improving computing power and technological advances are greatly augmenting data collection and scientific observation this has directly contributed to increased data complexity and dimensionality motivating research of exploration techniques for multidimensional data consequently a recent influx of work dedicated to techniques and tools that aid in understanding multidimensional datasets can be observed in many research fields including biology engineering physics and scientific computing while the effectiveness of existing techniques to analyze the structure and relationships of multidimensional data varies greatly few techniques provide flexible mechanisms to simultaneously visualize and actively explore high-dimensional spaces in this paper we present an inverse linear affine multidimensional projection coined ilamp that enables a novel interactive_exploration technique for multidimensional data ilamp operates in reverse to traditional projection_methods by mapping low-dimensional information into a high-dimensional space this allows users to extrapolate instances of a multidimensional dataset while exploring a projection of the data to the planar domain we present experimental results that validate ilamp measuring the quality and coherence of the extrapolated data as well as demonstrate the utility of ilamp to hypothesize the unexplored regions of a high-dimensional space
in explorative data_analysis the data under consideration often resides in a high-dimensional hd data space currently many methods are available to analyze this type of data so far proposed automatic approaches include dimensionality_reduction and cluster analysis whereby visual-interactive methods aim to provide effective visual_mappings to show relate and navigate hd data furthermore almost all of these methods conduct the analysis from a singular perspective meaning that they consider the data in either the original hd data space or a reduced version thereof additionally hd data spaces often consist of combined features that measure different properties in which case the particular relationships between the various properties may not be clear to the analysts a priori since it can only be revealed if appropriate feature combinations subspaces of the data are taken into consideration considering just a single subspace is however often not sufficient since different subspaces may show complementary conjointly or contradicting relations between data items useful information may consequently remain embedded in sets of subspaces of a given hd input data space relying on the notion of subspaces we propose a novel method for the visual_analysis of hd data in which we employ an interestingness-guided subspace search algorithm to detect a candidate set of subspaces based on appropriately defined subspace similarity functions we visualize the subspaces and provide navigation facilities to interactively explore large sets of subspaces our approach allows users to effectively compare and relate subspaces with respect to involved dimensions and clusters of objects we apply our approach to synthetic and real data sets we thereby demonstrate its support for understanding hd data from different perspectives effectively yielding a more complete view on hd data display_algorithms h database applications data_mining h information search and retrieval selection process i picture/image generation
we introduce the concept of just-in-time_descriptive_analytics as a novel application of computational and statistical techniques performed at interaction-time to help users easily understand the structure of data as seen in visualizations fundamental to just-intime descriptive analytics is a identifying visual_features such as clustersoutliers and trends user might observe in visualizations automatically b determining the semantics of such features by performing statistical_analysis as the user is interacting and c enriching visualizations with annotations that not only describe semantics of visual_features but also facilitate interaction to support high-level understanding of data in this paper we demonstrate just-in-time_descriptive_analytics applied to a point-based multi-dimensional_visualization_technique to identify and describe clustersoutliers and trends we argue that it provides a novel user_experience of computational techniques working alongside of users allowing them to build faster qualitative mental_models of data by demonstrating its application on a few use-cases techniques used to facilitate just-in-time_descriptive_analytics are described in detail along with their runtime performance characteristics we believe this is just a starting point and much remains to be researched as we discuss open issues and opportunities in improving accessibility and collaboration just-in-time_descriptive_analytics feature identification and characterization point-based_visualizations
the world's corpora of data grow in size and complexity every day making it increasingly difficult for experts to make sense out of their data although machine_learning offers algorithms for finding patterns in data automatically they often require algorithm-specific parameters such as an appropriate distance function which are outside the purview of a domain expert we present a system that allows an expert to interact directly with a visual representation of the data to define an appropriate distance function thus avoiding direct_manipulation of obtuse model parameters adopting an iterative approach our system first assumes a uniformly weighted euclidean distance function and projects the data into a two-dimensional scatterplot view the user can then move incorrectly-positioned data points to locations that reflect his or her understanding of the similarity of those data points relative to the other data points based on this input the system performs an optimization to learn a new distance function and then re-projects the data to redraw the scatter-plot we illustrate empirically that with only a few iterations of interaction and optimization a user can achieve a scatterplot view and its corresponding distance function that reflect the user's knowledge of the data in addition we evaluate our system to assess scalability in data size and data dimension and show that our system is computationally efficient and can provide an interactive or near-interactive user_experience
text data such as online news and microblogs bear valuable insights regarding important events and responses to such events events are inherently temporal evolving over time existing visual_text_analysis systems have provided temporal views of changes based on topical themes extracted from text data but few have associated topical themes with events that cause the changes in this paper we propose an interactive visual_analytics system leadline to automatically identify meaningful events in news and social media data and support exploration of the events to characterize events leadline integrates topic_modeling event_detection and named entity recognition techniques to automatically extract information regarding the investigative 4 ws who what when and where for each event to further support analysis of the text corpora through events leadline allows users to interactively examine meaningful events using the 4 ws to develop an understanding of how and why through representing large-scale text corpora in the form of meaningful events leadline provides a concise summary of the corpora leadline also supports the construction of simple narratives through the exploration of events to demonstrate the efficacy of leadline in identifying events and supporting exploration two case studies were conducted using news and social media data
the common n-gram cng classifier is a text_classification algorithm based on the comparison of frequencies of character n-grams strings of characters of length n that are the most common in the considered documents and classes of documents we present a text analytic visualization system that employs the cng approach for text_classification and uses the differences in frequency values of common n-grams in order to visually compare documents at the sub-word level the visualization method provides both an insight into n-gram characteristics of documents or classes of documents and a visual interpretation of the workings of the cng classifier visual_analytics text_classification visual_text_analysis
reconstruction of shredded documents remains a significant challenge creating a better document reconstruction system enables not just recovery of information accidentally lost but also understanding our limitations against adversaries' attempts to gain access to information existing approaches to reconstructing shredded documents adopt either a predominantly manual eg crowd-sourcing or a near automatic approach we describe deshredder a visual analytic approach that scales well and effectively incorporates user input to direct the reconstruction process deshredder represents shredded pieces as time_series and uses nearest neighbor matching techniques that enable matching both the contours of shredded pieces as well as the content of shreds themselves more importantly deshred-der's interface support visual_analytics through user_interaction with similarity matrices as well as higher level assembly through more complex stitching functions we identify a functional task_taxonomy leading to design considerations for constructing deshredding solutions and describe how deshredder applies to problems from the darpa shredder challenge through expert evaluations h information interfaces and presentation user_interfaces — graphical i image_processing and computer vistion feature representation — i document and text_processing document capture — graphics recognition and interpretation
distributed_cognition and embodiment provide compelling models for how humans think and interact with the environment our examination of the use of large high-resolution displays from an embodied perspective has lead directly to the development of a new sensemaking environment called analyst's workspace aw aw leverages the embodied resources made more accessible through the physical nature of the display to create a spatial workspace by combining spatial_layout of documents and other artifacts with an entity-centric explorative investigative approach aw aims to allow the analyst to externalize elements of the sensemaking process as a part of the investigation integrated into the visual representations of the data itself in this paper we describe the various capabilities of aw and discuss the key principles and concepts underlying its design emphasizing unique design principles for designing visual analytic tools for large high-resolution displays embodiment distributed_cognition highresolution display large sensemaking space
increasingly social network datasets contain social attribute information about actors and their relationship analyzing such network with social attributes requires making sense of not only its structural features but also the relationship between social features in attributes and network structures existing social_network_analysis tools are usually weak in supporting complex analytical tasks involving both structural and social features and often overlook users' needs for sensemaking tools that help to gather synthesize and organize information of these features to address these challenges we propose a sensemaking framework of social-network visual_analytics in this paper this framework considers both bottom-up processes which are about constructing new understandings based on collected information and top-down processes which concern using prior knowledge to guide information collection in analyzing social_networks from both social and structural perspectives the framework also emphasizes the externalization of sensemaking processes through interactive_visualization guided by the framework we develop a system socialnetsense to support the sensemaking in visual_analytics of social_networks with social attributes the example of using our system to analyze a scholar collaboration network shows that our approach can help users gain insight into social_networks both structurally and socially and enhance their process awareness in visual_analytics social network socialnetsense sensemaking visual_analytics visualization
recent advances in technology have enabled social media services to support space-time indexed data and internet users from all over the world have created a large volume of time-stamped geo-located data such spatiotemporal_data has immense value for increasing situational awareness of local events providing insights for investigations and understanding the extent of incidents their severity and consequences as well as their time-evolving nature in analyzing social media data researchers have mainly focused on finding temporal trends according to volume-based importance hence a relatively small volume of relevant messages may easily be obscured by a huge data set indicating normal situations in this paper we present a visual_analytics approach that provides users with scalable and interactive social media data_analysis and visualization including the exploration and examination of abnormal topics and events within various social media data sources such as twitter flickr and youtube in order to find and understand abnormal events the analyst can first extract major topics from a set of selected messages and rank them probabilistically using latent_dirichlet_allocation he can then apply seasonal trend decomposition together with traditional control chart methods to find unusual peaks andoutliers within topic time_series our case studies show that situational awareness can be improved by incorporating the anomaly and trend examination techniques into a highly interactive_visual_analysis process h information storage and retrieval h information interfaces and presentation user_interfaces — gui information search and retrieval — information filtering relevance feedback
in the surveillance of road tunnels video data plays an important role for a detailed inspection and as an input to systems for an automated detection of incidents in disaster scenarios like major accidents however the increased amount of detected incidents may lead to situations where human operators lose a sense of the overall meaning of that data a problem commonly known as a lack of situation_awareness the primary contribution of this paper is a design_study of alvis a system designed to increase situation_awareness in the surveillance of road tunnels the design of alvis is based on a simplified tunnel model which enables an overview of the spatiotemporal development of scenarios in real-time the visualization explicitly represents the present state the history and predictions of potential future developments concepts for situation-sensitive prioritization of information ensure scalability from normal operation to major disaster scenarios the visualization enables an intuitive access to live and historic video for any point in time and space we illustrate alvis by means of a scenario and report qualitative feedback by tunnel experts and operators this feedback suggests that alvis is suitable to save time in recognizing dangerous situations and helps to maintain an overview in complex disaster scenarios k management of computing and information systems project and people management—life cycle
due to the ever growing volume of acquired data and information users have to be constantly aware of the methods for their exploration and for interaction of these not each might be applicable to the data at hand or might reveal the desired result owing to this innovations may be used inappropriately and users may become skeptical in this paper we propose a knowledge-assisted interface for medical visualization which reduces the necessary effort to use new visualization methods by providing only the most relevant ones in a smart way consequently we are able to expand such a system with innovations without the users to worry about when where and especially how they may or should use them we present an application of our system in the medical domain and give qualitative feedback from domain experts fuzzy_logic interaction visualization
visual_analytics va system development started in academic research institutions where novel visualization_techniques and open source toolkits were developed simultaneously small software companies sometimes spin-offs from academic research institutions built solutions for specific application domains in recent years we observed the following trend some small va companies grew exponentially at the same time some big software vendors such as ibm and sap started to acquire successful va companies and integrated the acquired va components into their existing frameworks generally the application domains of va systems have broadened substantially this phenomenon is driven by the generation of more and more data of high volume and complexity which leads to an increasing demand for va solutions from many application domains in this paper we survey a selection of state-of-the-art commercial va frameworks complementary to an existing survey on open source va tools from the survey results we identify several improvement opportunities as future research directions h information systems information systems applications k computing milieux the computer industry — markets
we focus on visual_analysis of space- and time-referenced categorical_data which describe possible states of spatial geographical objects or locations and their changes over time the analysis of these data is difficult as there are only limited possibilities to analyze the three aspects location time and category simultaneously we present a new approach which interactively combines a visualization of categorical changes over time b various spatial_data displays c computational techniques for task-oriented selection of time steps they provide an expressive visualization with regard to either the overall evolution over time or unusual changes we apply our approach on two use cases demonstrating its usefulness for a wide variety of tasks we analyze data from movement tracking and meteorologic areas using our approach expected events could be detected and new insights were gained
visualizations embody design choices about data access data_transformation visual representation and interaction to interpret a static visualization a person must identify the correspondences between the visual representation and the underlying data these correspondences become moving targets when a visualization is dynamic dynamics may be introduced in a visualization at any point in the analysis and visualization process for example the data itself may be streaming shifting subsets may be selected visual representations may be animated and interaction may modify presentation in this paper we focus on the impact of dynamic_data we present a taxonomy and conceptual framework for understanding how data changes influence the interpretability of visual representations visualization_techniques are organized into categories at various levels of abstraction the salient characteristics of each category and task suitability are discussed through examples from the scientific literature and popular practices examining the implications of dynamically updating visualizations warrants attention because it directly impacts the interpretability and thus utility of visualizations the taxonomy presented provides a reference point for further exploration of dynamic_data visualization_techniques dynamic_data interpretation
this poster provides an analytical model for examining performances of ir systems based on the discounted cumulative gain family of metrics and visualization for interacting and exploring the performances of the system under examination moreover we propose machine_learning approach to learn the ranking model of the examined system in order to be able to conduct a “what-if” analysis and visually explore what can happen if you adopt a given solution before having to actually implement it
in recent years the quantity of time_series_data generated in a wide variety of domains grown consistently thus it is difficult for analysts to process and understand this overwhelming amount of data in the specific case of time_series_data another problem arises time_series can be highly interrelated this problem becomes even more challenging when a set of parameters influences the progression of a time_series however while most visual_analysis techniques support the analysis of short time periods eg one day or one week they fail to visualize large-scale time_series ranging over one year or more in our approach we present a time_series matrix_visualization that tackles this problem its primary advantages are that it scales to a large number of time_series with different start and end points and allows for the visual_comparison / correlation_analysis of a set of influencingfactors to evaluate our approach we applied our technique to a real-world data set showing the impact of local weather conditions on the efficiency of photovoltaic power plants h information search and retrieval design tools and techniques information filtering —
we present an analytics-based framework for simultaneous visualization of large surface data collections arising in clinical neuroimaging studies termed informatics visualization for neuroimaging invizian this framework allows the visualization of both cortical surfaces characteristics and feature relatedness in unison it also uses dimension reduction methods to derive new coordinate systems using a jensen-shannon divergence metric for positioning cortical surfaces in a metric space such that the proximity in location is proportional to neuroanatomical similarity feature data such as thickness and volume are colored on the cortical surfaces and used to display both subject-specific feature values and global trends within the population additionally a query-based framework allows the neuroscience researcher to investigate probable correlations between neuroanatomical and subject patient attribute values such as age and diagnosis i computer_graphics i viewing algorithms i applications three-dimensional graphics and realism
within the visual_analytics research agenda there is an interest on studying the applicability of multimodal information representation and interaction techniques for the analytical reasoning_process the present study summarizes a pilot experiment conducted to understand the effects of augmenting visualizations of affectively-charged information using auditory graphs we designed an audio visual representation of social comments made to different news posted on a popular website and their affective dimension using a sentiment analysis tool for short texts participants of the study were asked to create an assessment of the affective valence trend positive or negative of the news articles using for it the visualizations and sonifications the conditions were tested looking for speed/accuracy trade off comparing the visual representation with an audio visual one we discuss our preliminary findings regarding the design of augmented information-representation h information interfaces and presentation user_interfaces(d, h, i) — k computers and society public policy issues — human safety
temporal awareness is pivotal to successful real-time dynamic decision_making in a wide range of command and control situations particularly in safety-critical environments however little explicit support for operators' temporal awareness is provided by decision support systems dss for time-critical decisions in the context of functional simulations of naval anti-air warfare and emergency_response management the present study compares operator support provided by two display formats in both environments we contrast a baseline condition to a condition in which a temporal display was integrated to the original interface to support operators' temporal awareness we also wish to establish whether theimplementation of time-based dsss may also come with drawbacks on cognitive functioning and performance temporal awareness decision support human_factors time management
there is limited understanding of the relationship between neighbourhoods demographic characteristics and domestic energy_consumption habits we report upon research that combines datasets relating to household energy use with geodemographics to enable better understanding of uk energy user types a novel interactive interface is planned to evaluate the performance of specifically created energy-based data classifications the research aims to help local governments and the energy industry in targeting households and populations for new energy saving schemes and in improving efforts to promote sustainable energy_consumption the new classifications may also stimulate consumption awareness amongst domestic users this poster reports on initial visual findings and describes the research methodology data sources and future visualisation requirements h information systems database management — database applications h information systems information interfaces and presentation — user_interfaces i computing methodologies computer_graphics — applications
despite the extensive work done in the scientific_visualization community on the creation and optimization of spatial_data structures there has been little adaptation of these structures in visual_analytics and information_visualization in this work we present how we modify a space-partioning time spt tree - a structure normally used in direct-volume_rendering - for geospatial-temporal_visualizations we also present optimization techniques to improve the traversal speed of our structure through locational codes and bitwise comparisons finally we present the results of an experiment that quantitatively evaluates our modified spt tree with and without our optimizations our results indicate that retrieval was nearly three times faster when using our optimizations and are consistent across multiple trials our finding could have implications for performance in using our modified spt tree in large-scale geospatial temporal visual_analytics software
we introduce translational science a research discipline from medicine and show how adapting it for visual_analytics can improve the design and evaluation of visual_analytics interfaces translational science “translates” knowledge from the lab to the real-world to “ground truth” by incorporating a 3 phase program of research phase 1 & 2 include protocols for research in the lab and field and phase 3 focuses on dissemination and documentation we discuss these phases and how they may be applied to visual_analytics research cognition evaluation experimental studies
in this paper we present a case_study where we incorporate goms goals operators methods and selectors [2] task analysis into the design process of a visual_analysis tool we performed goms analysis on an electroencephalography eeg analyst's current data_analysis strategy to identify important user tasks and unnecessary user actions in his current workflow we then designed an eeg data visual_analysis tool based on the goms analysis result evaluation results show that the tool we have developed eegvis allows the user to analyze eeg data with reduced subjective cognitive load faster speed and increased confidence in the analysis quality the positive evaluation results suggest that our design process demonstrates an effective application of goms analysis to discover opportunities for designing better tools to support the user's visual_analysis process human_factors user-centered_design
cyber physical systems cps such as smart buildings and data centers are richly instrumented systems composed of tightly coupled computational and physical elements that generate large amounts of data to explore cps data and obtain actionable insights we construct a radial pixel_visualization rpv system which uses multiple concentric rings to show the data in a compact circular layout of small polygons pixel cells each of which represents an individual data value rpv provides an effective visual representation of locality and periodicity of the high volume multivariate data_streams and seamlessly combines them with the results of an automated analysis in the outermost ring the results of correlation_analysis and peak point detection are highlighted our explorations demonstrates how rpv can help administrators to identify periodic thermal hot spots understand data center energy_consumption and optimize it workload radial pixel_visualization correlations cyber physical system peaks time-series_data
existing research suggests that individual personality differences can influence performance with visualizations in addition to stable traits such as locus of control research in psychology has found that temporary changes in affect emotion can significantly impact individual performance on cognitive tasks we examine the relationship between fundamental visual judgement tasks and affect through a crowdsourced user_study that combines affective-priming techniques from psychology with longstanding graphical_perception experiments our results suggest that affective-priming can significantly influenceaccuracy in visual judgements and that some chart types may be more affected than others
mass and social media provide flows of images for real world events it is sometimes difficult to represent realities and impressions of events using only text however even a single photo might remind us complex events along with events in the real world there are representative images such as design of products and commercial pictures we can therefore recognize changes in trends of people's ideas experiences and interests through observing the flows of such representative images this paper presents a novel 3d_visualization system to explore temporal changes in trends using images associating with different topics called image bricks we show case studies using images extracted from our six-year blog archive we first extract clusters of images as topics related to given keywords we then visualize them on multiple timelines in a 3d space users can visually read stories of topics through exploring visualized images
datasets that are collected for research often contain millions of records and may carry hidden pitfalls that are hard to detect this work demonstrates how visual_analytics can be used for identifying problems in the spatial distribution of crawled photographic data in different datasets picasa web albums panoramio flickr and geograph chosen to be potential data sources for ongoing doctoral research this poster summary describes a number of problems found in the datasets using visual_analytics and suggests that greater attention should be paid to assessing the quality of data gathered from user-generated photographic content this work is the first part of a three-year phd project aimed at producing a pedestrian-routing system that can suggest attractive pathways extracted from user-generated photographic content
this poster describes general concepts of integrating the statistical computation package r into a coordinated multiple_views framework the integration is based on a cyclic analysis workflow in this model interactive selections are a key aspect to trigger and control computations in r dynamic updates of data columns are a generic mechanism to transfer computational results back to the interactive_visualization further aspects include the integration of the r console and an r object browser as views in our system we illustrate our approach by means of an interactive modeling process
event data can hold valuable decision_making information yet detecting interesting patterns in this type of data is not an easy task because the data is usually rich and contains spatial temporal as well as multivariate dimensions research into visual_analytics tools to support the discovery of patterns in event data often focuses on the spatiotemporal or spatiomultivariate dimension of the data only few research efforts focus on all three dimensions in one framework an integral view on all three dimensions is however required to unlock the full potential of event datasets in this poster we present an event visualization transition and interaction framework that enables an integral view on all dimensions of spatiotemporal multivariate event data the framework is built around the notion that the event data space can be considered a spatiotemporal multivariate hypercube results of a case_study we performed suggest that a visual_analytics tool based on the proposed framework is indeed capable to support users in the discovery of multidimensional spatiotemporal multivariate patterns in event data coordinated and multiple_views field studies multidimensional data visual_analytics visual_knowledge_discovery
recent research suggests that the personality trait locus of control loc can be a reliable predictor of performance when learning a new visualization tool while these results are compelling and have direct implications to visualization design the relationship between a user's loc measure and their performance is not well understood we hypothesize that there is a dependent relationship between loc and performance specifically a person's orientation on the loc scale directly influences their performance when learning new visualizations to test this hypothesis we conduct an experiment with 300 subjects using amazon's mechanical_turk we adapt techniques from personality psychology to manipulate a user's loc so that users are either primed to be more internally or externally oriented on the loc scale replicating previous studies investigating the effect of loc on performance we measure users' speed andaccuracy as they use visualizations with varying visual metaphors our findings demonstrate that changing a user's loc impacts their performance we find that a change in users' loc results in performance changes
visual_analysis of time_series_data is an important yet challenging task with many application examples in fields such as financial or news stream data_analysis many visual time_series analysis approaches consider a global perspective on the time_series fewer approaches consider visual_analysis of local patterns in time_series and often rely on interactive specification of the local area of interest we present initial results of an approach that is based on automatic detection of local interest points we follow an overview-first approach to find useful parameters for the interest point detection and details-on-demand to relate the found patterns we present initial results and detail possible extensions of the approach
the congressional budget office cbo is an agency of the federal government with about 240 employees that provides the us congress with timely nonpartisan analysis of important budgetary and economic issues recently cbo began producing static infographics to present its headline stories and to provide information to the congress in different ways data visualization federal budget federal government infographics
in this poster we track the evolution of cosmic structures and higher level host structures in cosmological simulation as they interact with each other the structures found in these simulations are made up of groups of dark_matter tracer particles called satellite halos and groups of satellite halos called host halos we implement a multilevel tracking model to track dark_matter tracer particles satellite halos and host halos to understand their behaviour and show how the different structures are formed over time we also represent the evolution of halos in the form of merger trees for detailed analysis by cosmologists group tracking merger tree multi-level tracking
data quality evaluation is one of the most critical steps during the data_mining processes data with poor quality often leads to poor performance in data_mining low efficiency in data_analysis wrong decision which bring great economic loss to users and organizations further although many researches have been carried out from various aspects of the extracting transforming and loading processes in data_mining most researches pay more attention to analysis automation than to data quality evaluation to address the data quality evaluation issues we propose an approach to combine human beings' powerful cognitive abilities in data quality evaluation with the high efficiency ability of computer and develop a visual_analysis method for data quality evaluation based on visual morphology database quality interactive_visualization visual_analysis
extracting information from text is challenging most current practices treat text as a bag of words or word clusters ignoring valuable linguistic information leveraging this linguistic information we propose a novel approach to visualize textual information the novelty lies in using state-of-the-art natural_language_processing nlp tools to automatically annotate text which provides a basis for new and powerful interactive_visualizations using nlp tools we built a web-based interactive visual browser for human history articles from wikipedia
the 2012 visual_analytics science and technology vast challenge posed two challenge problems for participants to solve using a combination of visual_analytics software and their own analytic_reasoning abilities challenge 1 c1 involved visualizing the network health of the fictitious bank of money to provide situation_awareness and identify emerging trends that could signify network issues challenge 2 c2 involved identifying the issues of concern within a region of the bank of money network experiencing operational difficulties utilizing the provided network logs participants were asked to analyze the data and provide solutions and explanations for both challenges the data sets were downloaded by nearly 1100 people by the close of submissions the vast challenge received 40 submissions with participants from 12 different countries and 14 awards were given visual_analytics contest evaluation human information interaction metrics sense making
netsecradar is a visual_analytics system to aid in monitoring the network_security in real time and perceiving the overall view of the security situation using radial graph at present we use this tool mainly for ids alerts to analyze the irregular behavioral patterns and synthesize interactions filtering and drill-down to detect the potential intrusions in conclusion we describe how this system was used to analyze the mini-challenges of the 2012 vast challenge intrusion detection real-time monitoring security visualization visual interfaces
sas® visual_analytics explorer is an advanced data visualization and exploratory_data_analysis application that is a component of the sas visual_analytics solution it excels at handling big data problems like the vast challenge with a wide range of visual_analytics features and the ability to scale to massive datasets sas visual_analytics explorer enables analysts to find patt er n s and relationships quickly and easily no matter the size of their data in this summary paper we explain how we used sas visual_analytics explorer to solve the vast challenge 2012 minichallenge 1 visual_analytics big data data visualization exploratory_data_analysis
we report the approach and results on the vast 2012 minichallenge 2 bank of money regional office network operations forensics using commercial data_mining visualization and database software such as knime tableau and mysql as well as a custom-written source vs destination ip pixel matrix our team of students identified suspicious irc traffic an attack on the firewall a drop in the firewall connections an attempt for sensitive information exchange and a possible distributed denial-of-service attack executed partly from a host within the bank network
we have developed a pixel-oriented treemap visualization intended for use on multiple displays with collaborating users it visualizes the health and status of about a million devices with a treemap layout in this paper we describe how we found useful pieces of the vast 2012 challenge mc1dataset and discuss how users interacted with this visualization during the analysis large display multiple displays physical navigation pixel-oriented visualization treemap
in this vast challenge log data coming from locations all over the bank of money facilities that contains close one million ip addresses since the geography of the data plays an important role for potential anomalies detection we present a particular visualization solution based on google earth that can provide measure and deal with geo-spatial_data by mapping three important attributes ie number of connections policy status and activity flag into 3d bars on top of physical locations coordinates anomaly distribution and trends can be efficiently visualized and analyzed the general kml file generator can be extended for further analysis on other gis systems
we present an extended version of targeted projection pursuit a high dimensional data_exploration tool adapted for producing graph_layouts using node-attributes attributes are generated based on detected events in the intrusion detection system and firewall logs and how often they occur for each ip address edges are the directed links between source and destination ips the layout is interactive and users can manipulate the points in order to find interesting layouts and then further analyse how these layouts are related to the events in the logs thus they first allow the user to detect anomalies and then gives them a platform to investigate why they occur
in this paper we describe the tool we developed to solve the vast 2012 challenge we present how an expert can online analyze huge amounts of multivariate data in space and time the visual components and the interaction between the system and the user are described as well as the setup to allow on the fly processing and retrieval of huge amounts of data we summarize how our tool helped us to solve the challenge human information interaction intelligence_analysis interactive_visualization visual_analytics
this paper describes the rapid development of a tailored cyber situational awareness and analysis application for the 2012 ieee vast mini-challenge 1 mc1 — cyber situation_awareness the novel aspect of this project was in the process of developing the tailored solution for a “big data” application aperture is an open adaptable and extensible web 20visualization_framework designed to produce visualizations for analysts and decision makers in any common web browser aperture utilizes a novel layer-based approach to visualization assembly and a data mapping api that simplifies the process of transformation of data or analytic results into visual forms and properties agile big data framework human information interaction toolkit visual_analytics visualization
to visualize the vast 2012 mini challenge 2 datasets we use the infovis toolkit ivtk custom visualizations as well as extra interaction capabilities have been added to the toolkit custom-made python scripts are used for data preprocessing purposes in this work we show how visualization tools may be combined to leverage network forensic analysis tasks h information interfaces and presentation user_interfaces — interaction systems
summary form only given in the past decades many new techniques have been developed to visualize and interact with abstract data but also many challenges remain in my talk i will reflect on how to make progress in our field how to identify interesting problems and next how to find effective solutions i will begin with an attempt to identify characteristics of interesting problems and discuss windows of opportunity for data tasks and users some problems have been solved some are too hard to deal with what is the range we should aim at and what impact can be obtained next i discuss strategies and approaches for finding novel solutions such as combining existing approaches and finding inspiration in other disciplines including art and design this talk is based on lessons we learned while developing new techniques and will be illustrated with a variety of cases and demos from our group at tu/e showing successes and failures
regression models play a key role in many application domains for analyzing or predicting a quantitative dependent variable based on one or more independent variables automated approaches for building regression models are typically limited with respect to incorporating domain knowledge in the process of selecting input variables also known as feature subset selection other limitations include the identification of local structures transformations and interactions between variables the contribution of this paper is a framework for building regression models addressing these limitations the framework combines a qualitative_analysis of relationship structures by visualization and a quantification of relevance for ranking any number of features and pairs of features which may be categorical or continuous a central aspect is the local approximation of the conditional target distribution by partitioning 1d and 2d feature domains into disjoint regions this enables a visual investigation of local patterns and largely avoids structural assumptions for the quantitative ranking we describe how the framework supports different tasks in model_building eg validation and comparison and we present an interactive workflow for feature subset selection a real-world case_study illustrates the step-wise identification of a five-dimensional model for natural gas consumption we also report feedback from domain experts after two months of deployment in the energy sector indicating a significant effort reduction for building and improving regression models complexity theory computational modeling feature_extraction frequency-domain analysis modeling regression regression analysis data partitioning feature_selection guided_visualization model_building visual_knowledge_discovery
we present a visual_analytics solution designed to address prevalent issues in the area of operational decision management odm in odm which has its roots in artificial intelligence expert systems and management science it is increasingly important to align business decisions with business goals in our work we consider decision models executable models of the business domain as ontologies that describe the business domain and production rules that describe the business logic of decisions to be made over this ontology executing a decision model produces an accumulation of decisions made over time for individual cases we are interested first to get insight in the decision logic and the accumulated facts by themselves secondly and more importantly we want to see how the accumulated facts reveal potential divergences between the reality as captured by the decision model and the reality as captured by the executed decisions we illustrate the motivation added value for visual_analytics and our proposed solution and tooling through a business case from the car insurance industry analytical models data visualization decision_making decision support systems model_validation_and_analysis multivariate statistics program_analysis statistical_analysis visual_analytics
for preserving the grotto wall_paintings and protecting these historic cultural icons from the damage and deterioration in nature environment a visual_analytics framework and a set of tools are proposed for the discovery of degradation patterns in comparison with the traditional analysis methods that used restricted scales our method provides users with multi-scale analytic support to study the problems on site cave wall and particular degradation area scales through the application of multidimensional visualization_techniques several case studies have been carried out using real-world wall painting data collected from a renowned world heritage site to verify the usability and effectiveness of the proposed method user studies and expert reviews were also conducted through by domain experts ranging from scientists such as microenvironment researchers archivists geologists chemists to practitioners such as conservators restorers and curators correlation cultural_heritage cultural differences data visualization degradation painting visual_analytics visual_analytics wall_paintings
topic_modeling has been widely used for analyzing text document collections recently there have been significant advancements in various topic_modeling techniques particularly in the form of probabilistic graphical modeling state-of-the-art techniques such as latent_dirichlet_allocation lda have been successfully applied in visual text_analytics however most of the widely-used methods based on probabilistic modeling have drawbacks in terms of consistency from multiple runs and empirical convergence furthermore due to the complicatedness in the formulation and the algorithm lda cannot easily incorporate various types of user feedback to tackle this problem we propose a reliable and flexible visual_analytics system for topic_modeling called utopian user-driven topic_modeling based on interactive nonnegative matrix factorization centered around its semi-supervised formulation utopian enables users to interact with the topic_modeling method and steer the result in a user-driven manner we demonstrate the capability of utopian via several usage scenarios with real-world document corpuses such as infovis/vast paper data set and product review data sets analytical models computational modeling context modeling interactive states latent_dirichlet_allocation visual_analytics interactive_clustering nonnegative matrix factorization text_analytics topic_modeling visual_analytics
analyzing large textual collections has become increasingly challenging given the size of the data available and the rate that more data is being generated topic-based text summarization methods coupled with interactive_visualizations have presented promising approaches to address the challenge of analyzing large text corpora as the text corpora and vocabulary grow larger more topics need to be generated in order to capture the meaningful latent themes and nuances in the corpora however it is difficult for most of current topic-based visualizations to represent large number of topics without being cluttered or illegible to facilitate the representation and navigation of a large number of topics we propose a visual_analytics system - hierarchicaltopic ht ht integrates a computational algorithm topic rose_tree with an interactive visual interface the topic rose_tree constructs a topic hierarchy based on a list of topics the interactive visual interface is designed to present the topic content as well as temporal evolution of topics in a hierarchical fashion user_interactions are provided for users to make changes to the topic hierarchy based on their mental_model of the topic space to qualitatively evaluate ht we present a case_study that showcases how hierarchicaltopics aid expert users in making sense of a large number of topics and discovering interesting patterns of topic groups we have also conducted a user_study to quantitatively evaluate the effect of hierarchical topic structure the study results reveal that the ht leads to faster identification of large number of relevant topics we have also solicited user feedback during the experiments and incorporated some suggestions into the current version of hierarchicaltopics algorithm design and analysis analytical models computational modeling hierarchical topic representation text mining visual_analytics vocabulary rose_tree topic_modeling visual_analytics
how do various topics compete for public attention when they are spreading on social media what roles do opinion leaders play in the rise and fall of competitiveness of various topics in this study we propose an expanded topic competition model to characterize the competition for public attention on multiple topics promoted by various opinion leaders on social media to allow an intuitive understanding of the estimated measures we present a timeline_visualization through a metaphoric interpretation of the results the visual_design features both topical and social aspects of the information_diffusion process by compositing themeriver with storyline style visualization themeriver shows the increase and decrease of competitiveness of each topic opinion leaders are drawn as threads that converge or diverge with regard to their roles in influencing the public agenda change over time to validate the effectiveness of the visual_analysis techniques we report the insights gained on two collections of tweets the 2012 united states presidential election and the occupy wall street movement_data visualization mathematical model recruitment social media visuaization social network services visual_analytics agenda-setting information_diffusion information_propagation topic competition
the number of microblog posts published daily has reached a level that hampers the effective retrieval of relevant messages and the amount of information conveyed through services such as twitter is still increasing analysts require new methods for monitoring their topic of interest dealing with the data volume and its dynamic nature it is of particular importance to provide situational awareness for decision_making in time-critical tasks current tools for monitoring microblogs typically filter messages based on user-defined keyword queries and metadata restrictions used on their own such methods can have drawbacks with respect to filteraccuracy and adaptability to changes in trends and topic structure we suggest scatterblogs2 a new approach to let analysts build task-tailored message filters in an interactive and visual manner based on recorded messages of well-understood previous events these message filters include supervised classification and query creation backed by the statistical distribution of terms and their co-occurrences the created filter methods can be orchestrated and adapted afterwards for interactive visual real-time monitoring and analysis of microblog feeds we demonstrate the feasibility of our approach for analyzing the twitter stream in emergency management scenarios blogs information_retrieval labeling microblog_analysis real-time systems social network services spatiotemporal phenomena twitter filter_construction information_visualization live_monitoring query_construction social_media_monitoring text_analytics text_classification visual_analytics
social_network_analysis sna is becoming increasingly concerned not only with actors and their relations but also with distinguishing between different types of such entities for example social scientists may want to investigate asymmetric relations in organizations with strict chains of command or incorporate non-actors such as conferences and projects when analyzing coauthorship patterns multimodal social_networks are those where actors and relations belong to different types or modes and multimodal social_network_analysis msna is accordingly sna for such networks in this paper we present a design_study that we conducted with several social scientist collaborators on how to support msna using visual_analytics tools based on an openended formative design process we devised a visual representation called parallel node-link bands pnlbs that splits modes into separate bands and renders connections between adjacent ones similar to the list view in jigsaw we then used the tool in a qualitative_evaluation involving five social scientists whose feedback informed a second design phase that incorporated additional network metrics finally we conducted a second qualitative_evaluation with our social scientist collaborators that provided further insights on the utility of the pnlbs representation and the potential of visual_analytics for msna complexity theory data visualization design methodology_design_study social network services user centered design visual_analytics interaction multimodal graphs node-link_diagrams qualitative_evaluation user-centered_design
this paper introduces an approach to exploration and discovery in high-dimensional_data that incorporates a user's knowledge and questions to craft sets of projection functions meaningful to them unlike most prior work that defines projections based on their statistical properties our approach creates projection functions that align with user-specified annotations therefore the resulting derived dimensions represent concepts defined by the user's examples these especially crafted projection functions or explainers can help find and explain relationships between the data variables and user-designated concepts they can organize the data according to these concepts sets of explainers can provide multiple perspectives on the data our approach considers tradeoffs in choosing these projection functions including their simplicity expressive power alignment with prior knowledge and diversity we provide techniques for creating collections of explainers the methods based on machine_learning optimization frameworks allow exploring the tradeoffs we demonstrate our approach on model problems and applications in text_analysis cities and towns high-dimensional spaces optimization quantization (signal) support_vector_machines text mining exploration support_vector_machines
when high-dimensional_data is visualized in a 2d plane by using parametric projection algorithms users may wish to manipulate the layout of the data points to better reflect their domain knowledge or to explore alternative structures however few users are well-versed in the algorithms behind the visualizations making parameter tweaking more of a guessing game than a series of decisive interactions translating user_interactions into algorithmic input is a key component of visual_to_parametric_interaction v2pi [13] instead of adjusting parameters users directly move data points on the screen which then updates the underlying statistical model however we have found that some data points that are not moved by the user are just as important in the interactions as the data points that are moved users frequently move some data points with respect to some other 'unmoved' data points that they consider as spatially contextual however in current v2pi interactions these points are not explicitly identified when directly manipulating the moved points we design a richer set of interactions that makes this context more explicit and a new algorithm and sophisticated weighting scheme that incorporates the importance of these unmoved data points into v2pi algorithm design and analysis cognitive science data visualization mathematical model semantics visual_to_parametric_interaction statistical_models visual_analytics
visual_exploration and analysis of multidimensional data becomes increasingly difficult with increasing dimensionality we want to understand the relationships between dimensions of data but lack flexible techniques for exploration beyond low-order relationships current visual techniques for multidimensional data_analysis focus on binary conjunctive relationships between dimensions recent techniques such as cross-filtering on an attribute relationship graph facilitate the exploration of some higher-order conjunctive relationships but require a great deal of care and precision to do so effectively this paper provides a detailed analysis of the expressive power of existing visual querying systems and describes a more flexible approach in which users can explore n-ary conjunctive inter- and intra- dimensional relationships by interactively constructing queries as visual hypergraphs in a hypergraph query nodes represent subsets of values and hyperedges represent conjunctive relationships analysts can dynamically build and modify the query using sequences of simple interactions the hypergraph serves not only as a query specification but also as a compact visual representation of the interactive state using examples from several domains focusing on the digital_humanities we describe the design considerations for developing the querying system and incorporating it into visual_analysis tools we analyze query expressiveness with regard to the kinds of questions it can and cannot pose and describe how it simultaneously expands the expressiveness of and is complemented by cross-filtering data_analysis data visualization database languages graph_search marine vehicles semantics visual_analytics attribute_relationship_graphs digital_humanities graph_query_language higher-order_conjunctive_queries multidimensional data multivariate data_analysis visual_query_language
many datasets such as scientific literature collections contain multiple heterogeneous facets which derive implicit relations as well as explicit relational references between data items the exploration of this data is challenging not only because of large_data scales but also the complexity of resource structures and semantics in this paper we present pivotslice an interactive_visualization_technique which provides efficient faceted_browsing as well as flexible capabilities to discover data relationships with the metaphor of direct_manipulation pivotslice allows the user to visually and logically construct a series of dynamic queries over the data based on a multi-focus and multi-scale tabular view that subdivides the entire dataset into several meaningful parts with customized semantics pivotslice further facilitates the visual_exploration and sensemaking process through features including live search and integration of online_data graphical interaction histories and smoothly animated visual state_transitions we evaluated pivotslice through a qualitative lab study with university researchers and report the findings from our observations and interviews we also demonstrate the effectiveness of pivotslice using a scenario of exploring a repository of information_visualization literature data visualization faceted_browsing faceted searches information filters market_research dynamic_query information_visualization interaction network_exploration visual_analytics
scientists engineers and analysts are confronted with ever larger and more complex sets of data whose analysis poses special challenges in many situations it is necessary to compare two or more datasets hence there is a need for comparative_visualization tools to help analyze differences or similarities among datasets in this paper an approach for comparative_visualization for sets of images is presented well-established techniques for comparing images frequently place them side-by-side a major drawback of such approaches is that they do not scale well other image comparison methods encode differences in images by abstract parameters like color in this case information about the underlying image data gets lost this paper introduces a new method for visualizing differences and similarities in large sets of images which preserves contextual information but also allows the detailed analysis of subtle variations our approach identifies local changes and applies cluster analysis techniques to embed them in a hierarchy the results of this process are then presented in an interactive web application which allows users to rapidly explore the space of differences and drill-down on particular features we demonstrate the flexibility of our approach by applying it to multiple distinct domains comparative_visualization data visualization image color analysis image_segmentation shape analysis visual_analytics focus+context_visualization image set comparison
spectral clustering is a powerful and versatile technique whose broad range of applications includes 3d image_analysis however its practical use often involves a tedious and time-consuming process of tuning parameters and making application-specific choices in the absence of training data with labeled clusters help from a human analyst is required to decide the number of clusters to determine whether hierarchical_clustering is needed and to define the appropriate distance measures parameters of the underlying graph and type of graph laplacian we propose to simplify this process via an open-box approach in which an interactive_system visualizes the involved mathematical quantities suggests parameter values and provides immediate feedback to support the required decisions our framework focuses on applications in 3d image_analysis and links the abstract high-dimensional feature space used in spectral clustering to the three-dimensional data space this provides a better understanding of the technique and helps the analyst predict how well specific parameter settings will generalize to similar tasks in addition our system supports filteringoutliers and labeling the final clusters in such a way that user actions can be recorded and transferred to different data in which the same structures are to be found our system supports a wide range of inputs including triangular_meshes regular grids and point clouds we use our system to develop segmentation protocols in chest ct and brain mri that are then successfully applied to other datasets in an automated manner clustering data visualization eigenvalues and eigenfunctions image_analysis image_segmentation laplace equations three-dimensional displays high-dimensional_embeddings linked_views programming_with_example spectral clustering
traditional sketch-based image or video search systems rely on machine_learning concepts as their core technology however in many applications machine_learning alone is impractical since videos may not be semantically annotated sufficiently there may be a lack of suitable training data and the search requirements of the user may frequently change for different tasks in this work we develop a visual_analytics systems that overcomes the shortcomings of the traditional approach we make use of a sketch-based interface to enable users to specify search requirement in a flexible manner without depending on semantic annotation we employ active machine_learning to train different analytical models for different types of search requirements we use visualization to facilitate knowledge_discovery at the different stages of visual_analytics this includes visualizing the parameter space of the trained model visualizing the search space to support interactive browsing visualizing candidature search results to support rapid interaction for active_learning while minimizing watching videos and visualizing aggregated information of the search results we demonstrate the system for searching spatiotemporal attributes from sports video to identify key instances of the team and player performance analytical models computational modeling data visualization machine_learning multimediacommunication visual_analytics data_clustering machine_learning multimedia_visualization visual_knowledge_discovery
we propose a novel video_visual_analytics system for interactive_exploration of surveillance video data our approach consists of providing analysts with various views of information related to moving objects in a video to do this we first extract each object's movement path we visualize each movement by a creating a single action shot image a still image that coalesces multiple frames b plotting its trajectory in a space-time cube and c displaying an overall timeline view of all the movements the action shots provide a still view of the moving object while the path view presents movement properties such as speed and location we also provide tools for spatial and temporal filtering based on regions of interest this allows analysts to filter out large amounts of movement activities while the action shot representation summarizes the content of each movement we incorporated this multi-part visual representation of moving objects in svisit a tool to facilitate browsing through the video content by interactive querying and retrieval of data based on our interaction with security personnel who routinely interact with surveillance video data we identified some of the most common tasks performed this resulted in designing a user_study to measure time-to-completion of the various tasks these generally required searching for specific events of interest targets in videos fourteen different tasks were designed and a total of 120 min of surveillance video were recorded indoor and outdoor locations recording movements of people and vehicles the time-to-completion of these tasks were compared against a manual fast forward video browsing guided with movement detection we demonstrate how our system can facilitate lengthy video exploration and significantly reduce browsing time to find events of interest reports from expert users identify positive aspects of our approach which we summarize in our recommendations for future video_visual_analytics systems data visualization image_segmentation interactive states navigation surveillance tracking video_visual_analytics visual_analytics surveillance video video_browsing_and_exploration video_summarization video_visualization
we introduce a visual_analytics method to analyze eye movement_data recorded for dynamic stimuli such as video or animated graphics the focus lies on the analysis of data of several viewers to identify trends in the general viewing behavior including time sequences of attentional synchrony and objects with strong attentional focus by using a space-time cube visualization in combination with clustering the dynamic stimuli and associated eye gazes can be analyzed in a static 3d representation shot-based spatiotemporal clustering of the data generates potential areas of interest that can be filtered interactively we also facilitate data drill-down the gaze points are shown with density-based color mapping and individual scan paths as lines in the space-time cube the analytical process is supported by multiple_coordinated_views that allow the user to focus on different aspects of spatial and temporal information in eye gaze data common eye-tracking visualization_techniques are extended to incorporate the spatiotemporal characteristics of the data for example heat maps are extended to motion-compensated heat maps and trajectories of scan paths are included in the space-time visualization our visual_analytics approach is assessed in a qualitative users study with expert users which showed the usefulness of the approach and uncovered that the experts applied different analysis strategies supported by the system clustering algorithms context awareness data visualization eye-tracking space-time codes spatiotemporal phenomena tracking visual_analytics dynamic_areas_of_interest motion-compensated heat map space-time cube spatiotemporal clustering
we describe and demonstrate an extensible framework that supports data_exploration and provenance in the context of human_terrain_analysis hta working closely with defence analysts we extract requirements and a list of features that characterise data analysed at the end of the hta chain from these we select an appropriate non-classified data source with analogous features and model it as a set of facets we develop proveml an xml-based extension of the open provenance model using these facets and augment it with the structures necessary to record the provenance of data analytical process and interpretations through an iterative process we develop and refine a prototype system for human terrain visual_analytics htva and demonstrate means of storing browsing and recalling analytical provenance and process through analytic bookmarks in proveml we show how these bookmarks can be combined to form narratives that link back to the live data throughout the process we demonstrate that through structured workshops rapid prototyping and structuredcommunication with intelligence analysts we are able to establish requirements and design schema techniques and tools that meet the requirements of the intelligence community we use the needs and reactions of defence analysts in defining and steering the methods to validate the framework context awareness data visualization human_factors human_terrain_analysis terrain mapping visual_analytics bookmarks framework narratives provenance
as increasing volumes of urban_data are captured and become available new opportunities arise for data-driven analysis that can lead to improvements in the lives of citizens through evidence-based decision_making and policies in this paper we focus on a particularly important urban_data set taxi trips taxis are valuable sensors and information associated with taxi trips can provide unprecedented insight into many different aspects of city life from economic activity and human behavior to mobility patterns but analyzing these data presents many challenges the data are complex containing geographical and temporal components in addition to multiple variables associated with each trip consequently it is hard to specify exploratory queries and to perform comparative analyses eg compare different regions over time this problem is compounded due to the size of the data-there are on average 500000 taxi trips each day in nyc we propose a new model that allows users to visually query taxi trips besides standard analytics queries the model supports origin-destination queries that enable the study of mobility across the city we show that this model is able to express a wide range of spatio-temporal_queries and it is also flexible in that not only can queries be composed but also different aggregations and visual representations can be applied allowing users to explore and compare results we have built a scalable system that implements this model which supports interactive response times makes use of an adaptive level-of-detail rendering strategy to generate clutter-free visualization for large results and shows hidden details to the users in a summary through the use of overlay heat maps we present a series of case studies motivated by traffic engineers and economists that show how our model and system enable domain experts to perform tasks that were previously unattainable for them analytical models cities and towns data models data visualization mathematical model nyc taxis spatio-temporal_queries timefactors visual_analytics urban_data visual_exploration
in this work we present an interactive_system for visual_analysis of urban traffic congestion based on gps trajectories for these trajectories we develop strategies to extract and derive traffic jam information after cleaning the trajectories they are matched to a road network subsequently traffic speed on each road segment is computed and traffic jam events are automatically detected spatially and temporally related events are concatenated in so-called traffic_jam_propagation graphs these graphs form a high-level description of a traffic jam and its propagation in time and space our system provides multiple_views for visually exploring and analyzing the traffic condition of a large city as a whole on the level of propagation graphs and on road segment level case studies with 24 days of taxi gps trajectories collected in beijing demonstrate the effectiveness of our system cities and towns data_mining data visualization global positioning system road traffic traffic control traffic visualization trajectory urban areas traffic_jam_propagation
we suggest a methodology for analyzing movement behaviors of individuals moving in a group group movement is analyzed at two levels of granularity the group as a whole and the individuals it comprises for analyzing the relative positions and movements of the individuals with respect to the rest of the group we apply space transformation in which the trajectories of the individuals are converted from geographical space to an abstract 'group space' the group space reference system is defined by both the position of the group center which is taken as the coordinate origin and the direction of the group's movement based on the individuals' positions mapped onto the group space we can compare the behaviors of different individuals determine their roles and/or ranks within the groups and possibly understand how group movement is organized the utility of the methodology has been evaluated by applying it to a set of real data concerning movements of wild social animals and discussing the results with experts in animal ethology behavioral science data models market_research trajectory visual_analytics collective_movement movement_data
we propose a novel approach of distance-based spatial_clustering and contribute a heuristic computation of input parameters for guiding users in the search of interesting cluster constellations we thereby combine computational geometry with interactive_visualization into one coherent framework our approach entails displaying the results of the heuristics to users as shown in figure 1 providing a setting from which to start the exploration and data_analysis addition interaction capabilities are available containing visual feedback for exploring further clustering options and is able to cope with noise in the data we evaluate and show the benefits of our approach on a sophisticated artificial dataset and demonstrate its usefulness on real-world data_clustering algorithms data visualization heuristic algorithms heuristic-based_spatial_clustering image color analysis noise measurement shape analysis visual_analytics iinteractive_visual_clustering k-order a-(alpha)-shapes
maintaining an awareness of collaborators' actions is critical during collaborative work including during collaborative_visualization activities particularly when collaborators are located at a distance it is important to know what everyone is working on in order to avoid duplication of effort share relevant results in a timely manner and build upon each other's results can a person's brushing actions provide an indication of their queries and interests in a data set can these actions be revealed to a collaborator without substantially disrupting their own independent work we designed a study to answer these questions in the context of distributed collaborative_visualization of tabular_data participants in our study worked independently to answer questions about a tabular_data set while simultaneously viewing brushing actions of a fictitious collaborator shown directly within a shared workspace we compared three methods of presenting the collaborator's actions brushing & linking ie highlighting exactly what the collaborator would see selection ie showing only a selected item and persistent selection ie showing only selected items but having them persist for some time our results demonstrated that persistent selection enabled some awareness of the collaborator's activities while causing minimal interference with independent work other techniques were less effective at providing awareness and brushing & linking caused substantial interference these findings suggest promise for the idea of exploiting natural brushing actions to provide awareness in collaborative work collaboration collaborative work context awareness data visualization attentionally_ambient_visualization awareness brushing_and_linking linked_views user_study
we present a system that lets analysts use paid crowd workers to explore data sets and helps analysts interactively examine and build upon workers' insights we take advantage of the fact that for many types of data independent crowd workers can readily perform basic analysis tasks like examining views and generating explanations for trends and patterns however workers operating in parallel can often generate redundant explanations moreover because workers have different competencies and domain knowledge some responses are likely to be more plausible than others to efficiently utilize the crowd's work analysts must be able to quickly identify and consolidate redundant responses and determine which explanations are the most plausible in this paper we demonstrate several crowd-assisted techniques to help analysts make better use of crowdsourced explanations 1 we explore crowd-assisted strategies that utilize multiple workers to detect redundant explanations we introduce color clustering with representative selection-a strategy in which multiple workers cluster explanations and we automatically select the most-representative result-and show that it generates clusterings that are as good as those produced by experts 2 we capture explanation provenance by introducing highlighting tasks and capturing workers' browsing behavior via an embedded web browser and refine that provenance information via source-review tasks we expose this information in an explanation-management interface that allows analysts to interactively filter and sort responses select the most plausible explanations and decide which to explore further clustering algorithms crowdsourcing data_analysis image color analysis market_research redundancy social_data_analysis social network services
spatial organization has been proposed as a compelling approach to externalizing the sensemaking process however there are two ways in which space can be provided to the user by creating a physical workspace that the user can interact with directly such as can be provided by a large high-resolution display or through the use of a virtual workspace that the user navigates using virtual navigation techniques such as zoom and pan in this study we explicitly examined the use of spatial sensemaking techniques within these two environments the results demonstrate that these two approaches to providing sensemaking space are not equivalent and that the greater embodiment afforded by the physical workspace changes how the space is perceived and used leading to increased externalization of the sensemaking process browsers image color analysis navigation sensemaking visual_analytics embodiment large high-resolution displays physical navigation visual_analytics
this research aims to develop design_guidelines for systems that support investigators and analysts in the exploration and assembly of evidence and inferences we focus here on the problem of identifying candidate 'influencers' within a community of practice to better understand this problem and its related cognitive and interaction needs we conducted a user_study using a system called invisque interactive visual_search and query environment loaded with content from the acm digital_library invisque supports search and manipulation of results over a freeform infinite 'canvas' the study focuses on the representations user create and their reasoning_process it also draws on some pre-established theories and frameworks related to sense-making and cognitive work in general which we apply as a 'theoretical lenses' to consider findings and articulate solutions analysing the user-study data in the light of these provides some understanding of how the high-level problem of identifying key players within a domain can translate into lower-level questions and interactions this in turn has informed our understanding of representation and functionality needs at a level of description which abstracts away from the specifics of the problem at hand to the class of problems of interest we consider the study outcomes from the perspective of implications for design design methodology query_processing user_interfaces visual_analytics analysis dataframe_mode evaluation interaction interface design reasoning sense-making
electronic_health_records ehrs have emerged as a cost-effective data source for conducting medical research the difficulty in using ehrs for research purposes however is that both patient selection and record analysis must be conducted across very large and typically very noisy datasets our previous work introduced eventflow a visualization tool that transforms an entire dataset of temporal event records into an aggregated display allowing researchers to analyze population-level patterns and trends as datasets become larger and more varied however it becomes increasingly difficult to provide a succinct summarizing display this paper presents a series of user-driven data simplifications that allow researchers to pare event records down to their core elements furthermore we present a novel metric for measuring visual complexity and a language for codifying disjoint strategies into an overarching simplification framework these simplifications were used by real-world researchers to gain new and valuable insights from initially overwhelming datasets complexity theory data_mining data visualization electronic medical records event sequences market_research electronic heath records simplification temporal_query
model_selection in time_series analysis is a challenging task for domain experts in many application areas such as epidemiology economy or environmental sciences the methodology used for this task demands a close combination of human judgement and automated computation however statistical software tools do not adequately support this combination through interactive visual interfaces we propose a visual_analytics process to guide domain experts in this task for this purpose we developed the timova prototype that implements this process based on user stories and iterative expert feedback on user_experience the prototype was evaluated by usage scenarios with an example dataset from epidemiology and interviews with two external domain experts in statistics the insights from the experts' feedback and the usage scenarios show that timova is able to support domain experts in model_selection tasks through interactive visual interfaces with short feedback cycles analytical models autoregressive processes data models mathematical model time_series analysis visual_analytics coordinated &amp amp  multiple_views model_selection time_series analysis visual_interaction
time-oriented data play an essential role in many visual_analytics scenarios such as extracting medical insights from collections of electronic_health_records or identifying emerging problems and vulnerabilities in network traffic however many software libraries for visual_analytics treat time as a flat numerical data type and insufficiently tackle the complexity of the time domain such as calendar granularities and intervals therefore developers of advanced visual_analytics designs need to implement temporal foundations in their application code over and over again we present timebench a software library that provides foundational data structures and algorithms for time-oriented data in visual_analytics its expressiveness and developer accessibility have been evaluated through application examples demonstrating a variety of challenges with time-oriented data and long-term developer studies conducted in the scope of research and student projects data models data structures data visualization time-domain analysis visual_analytics visual_analytics information_visualization software infrastructure temporal_data time toolkits
we present motionexplorer an exploratory_search and analysis system for sequences of human motion in large motion_capture_data collections this special type of multivariate time_series_data is relevant in many research fields including medicine sports and animation key tasks in working with motion data include analysis of motion states and transitions and synthesis of motion vectors by interpolation and combination in the practice of research and application of human motion data challenges exist in providing visual summaries and drill-down functionality for handling large motion data collections we find that this domain can benefit from appropriate visual retrieval and analysis support to handle these tasks in presence of large motion data to address this need we developed motionexplorer together with domain experts as an exploratory_search system based on interactive aggregation and visualization of motion states as a basis for data navigation exploration and search based on an overview-first type visualization users are able to search for interesting sub-sequences of motion based on a query-by-example metaphor and explore search results by details on demand we developed motionexplorer in close collaboration with the targeted users who are researchers working on human motion synthesis and analysis including a summative field study additionally we conducted a laboratory design_study to substantially improve motionexplorer towards an intuitive usable and robust design motionexplorer enables the search in human motion_capture_data with only a few mouse clicks the researchers unanimously confirm that the system can efficiently support their work data collection data visualization databases time_series analysis visual_analytics cluster glyph data_aggregation exploratory_search motion_capture_data multivariate time_series
the visual_analysis of dynamic_networks is a challenging task in this paper we introduce a new approach supporting the discovery of substructures sharing a similar trend over time by combining computation visualization and interaction with existing techniques their discovery would be a tedious endeavor because of the number of nodes edges as well as time points to be compared first on the basis of the supergraph we therefore group nodes and edges according to their associated attributes that are changing over time second the supergraph is visualized to provide an overview of the groups of nodes and edges with similar behavior over time in terms of their associated attributes third we provide specific interactions to explore and refine the temporal clustering allowing the user to further steer the analysis of the dynamic network we demonstrate our approach by the visual_analysis of a large wireless mesh network current measurement dynamic_networks image color analysis market_research power system dynamics time measurement time_series analysis supergraph_clustering visualization
rankings are a popular and universal approach to structuring otherwise unorganized collections of items by computing a rank for each item based on the value of one or more of its attributes this allows us for example to prioritize tasks or to evaluate the performance of products relative to each other while the visualization of a ranking itself is straightforward its interpretation is not because the rank of an item represents only a summary of a potentially complicated relationship between its attributes and those of the other items it is also common that alternative rankings exist which need to be compared and analyzed to gain insight into how multiple heterogeneous attributes affect the rankings advanced visual_exploration tools are needed to make this process efficient in this paper we present a comprehensive analysis of requirements for the visualization of multi-attribute_rankings based on these considerations we propose lineup - a novel and scalable_visualization_technique that uses bar_charts this interactive technique supports the ranking of items based on multiple heterogeneous attributes with different scales and semantics it enables users to interactively combine attributes and flexibly refine parameters to explore the effect of changes in the attribute combination this process can be employed to derive actionable insights as to which attributes of an item need to be modified in order for its rank to change additionally through integration of slope graphs lineup can also be used to compare multiple alternative rankings on the same set of items for example over time or across different attribute combinations we evaluate the effectiveness of the proposed multi-attribute_visualization_technique in a qualitative study the study shows that users are able to successfully solve complex ranking tasks in a short period of time data visualization encoding histograms ranking_visualization rankings scalability multi-attribute multi-faceted multifactorial ranking scoring stacked_bar_charts
many application domains deal with multi-variate_data that consist of both categorical and numerical information small-multiple displays are a powerful concept for comparing such data by juxtaposition for comparison by overlay or by explicit encoding of computed differences however a specification of references is necessary in this paper we present a formal model for defining semantically meaningful comparisons between many categories in a small-multiple display based on pivotized data that are hierarchically partitioned by the categories assigned to the x and y axis of the display we propose two alternatives for structure-based comparison within this hierarchy with an absolute reference specification categories are compared to a fixed reference category with a relative reference specification in contrast a semantic ordering of the categories is considered when comparing them either to the previous or subsequent category each both reference specifications can be defined at multiple levels of the hierarchy including aggregated summaries enabling a multitude of useful comparisons we demonstrate the general applicability of our model in several application examples using different visualizations that compare data by overlay or explicit encoding of differences comparative_visualization computational modeling data visualization displays encoding categorical_data small-multiple displays trellis_displays
visualizations are great tools ofcommunications-they summarize findings and quickly convey main messages to our audience as designers of charts we have to make sure that information is shown with a minimum of distortion we have to also consider illusions and other perceptual limitations of our audience in this paper we discuss the effect and strength of the line width illusion a muller-lyer type illusion on designs related to displaying associations between categorical variables parallel_sets and hammock_plots are both affected by line width illusions we introduce the common-angle plot as an alternative method for displaying categorical_data in a manner that minimizes the effect from perceptual illusions results from user studies both highlight the need for addressing line-width illusions in displays and provide evidence that common angle charts successfully resolve this issue biochemistry biological cells data visualization linewidth_illusion muller-lyer_illusion parallel_processing data visualization hammock_plots high-dimensional_displays parallel_sets
an ongoing debate in the visualization community concerns the role that visualization types play in data understanding in human cognition understanding and memorability are intertwined as a first step towards being able to ask questions about impact and effectiveness here we ask 'what makes a visualization memorable' we ran the largest scale visualization study to date using 2070 single-panel visualizations categorized with visualization type eg bar_chart line graph etc collected from news media sites government reports scientific journals and infographic sources each visualization was annotated with additional attributes including ratings for data-ink ratios and visual densities using amazon's mechanical_turk we collected memorability scores for hundreds of these visualizations and discovered that observers are consistent in which visualizations they find memorable and forgettable we find intuitive results eg attributes like color and the inclusion of a human recognizable object enhance memorability and less intuitive results eg common graphs are less memorable than unique visualization types altogether our findings suggest that quantifying memorability is a general metric of the utility of information an essential step towards determining how to design effective visualizations data visualization encoding information technology taxonomy visualization_taxonomy information_visualization memorability
the visual system can make highly efficient aggregate judgements about a set of objects with speed roughly independent of the number of objects considered while there is a rich literature on these mechanisms and their ramifications for visual summarization tasks this prior work rarely considers more complex tasks requiring multiple judgements over long periods of time and has not considered certain critical aggregation types such as the localization of the mean value of a set of points in this paper we explore these questions using a common visualization task as a case_study relative mean value judgements within multi-class scatterplots we describe how the perception literature provides a set of expected constraints on the task and evaluate these predictions with a large-scale perceptual_study with crowd-sourced participants judgements are no harder when each set contains more points redundant and conflicting encodings as well as additional sets do not strongly affect performance and judgements are harder when using less salient encodings these results have concrete ramifications for the design of scatterplots color imaging encoding psychophysics shape analysis visual systems information_visualization perceptual_study
scatter plots are diagrams that visualize two-dimensional data as sets of points in the plane they allow users to detect correlations and clusters in the data whether or not a user can accomplish these tasks highly depends on the aspect_ratio selected for the plot ie the ratio between the horizontal and the vertical extent of the diagram we argue that an aspect_ratio is good if the delaunay_triangulation of the scatter plot at this aspect_ratio has some nice geometric property eg a large minimum angle or a small total edge length more precisely we consider the following optimization problem given a set q of points in the plane find a scale factor s such that scaling the x-coordinates of the points in q by s and the y-coordinates by 1=s yields a point set ps that optimizes a property of the delaunay_triangulation of ps over all choices of s we present an algorithm that solves this problem efficiently and demonstrate its usefulness on real-world instances moreover we discuss an empirical test in which we asked 64 participants to choose the aspect_ratios of 18 scatter plots we tested six different quality measures that our algorithm can optimize in conclusion minimizing the total edge length and minimizing what we call the 'uncompactness' of the triangles of the delaunay_triangulation yielded the aspect_ratios that were most similar to those chosen by the participants in the test approximation algorithms approximation methods atmospheric measurements data visualization delaunay_triangulation market_research particle measurements scatter plot aspect_ratio
in controlled experiments on the relation of display size ie the number of pixels and the usability of visualizations the size of theinformation_space can either be kept constant or varied relative to display size both experimental approaches have limitations if theinformation_space is kept constant then the scale ratio between an overview of the entireinformation_space and the lowest zoom level varies which can impact performance if theinformation_space is varied then the scale ratio is kept constant but performance cannot be directly compared in other words display sizeinformation_space and scale ratio are interrelated variables we investigate this relation in two experiments with interfaces that implement classic information_visualization_techniques-focus+context overview+detail and zooming-for multi-scale navigation in maps display size varied between 017 15 and 138 megapixelsinformation_space varied relative to display size in one experiment and was constant in the other results suggest that for tasks where users navigate targets that are visible at all map scales the interfaces do not benefit from a large display with a constant map size a larger display does not improve performance with the interfaces with map size varied relative to display size participants found interfaces harder to use with a larger display and task completion times decrease only when they are normalized to compensate for the increase in map size the two experimental approaches show different interaction effects between display size and interface in particular focus+context performs relatively worse at a large display size with variable map size and relatively worse at a small display size with a fixed map size based on a theoretical analysis of the interaction with the visualization_techniques we examine individual task actions empirically so as to understand the relative impact of display size and scale ratio on the visualization_techniques' p- rformance and to discuss differences between the two experimental approaches aerospace electronics data visualization information_visualization interactive_systems monitoring navigation experimental_method interaction techniques multi-scale navigation user studies
we present a first investigation into hybrid-image visualization for data_analysis in large-scale viewing environments hybrid-image visualizations blend two different visual representations into a single static view such that each representation can be perceived at a different viewing distance our work is motivated by data_analysis scenarios that incorporate one or more displays with sufficiently large size and resolution to be comfortably viewed by different people from various distances hybrid-image visualizations can be used in particular to enhance overview tasks from a distance and detail-in-context tasks when standing close to the display by using a perception-based blending approach hybrid-image visualizations make two full-screen visualizations accessible without tracking viewers in front of a display we contribute a design space discuss the perceptual rationale for our work provide examples and introduce a set of techniques and tools to aid the design of hybrid-image visualizations data visualization encoding frequency-domain analysis image color analysis multi-scale collaboration hybrid_images large displays visualization
proposals to establish a 'science_of_interaction' have been forwarded from information_visualization and visual_analytics as well as cartography geo visualization and giscience this paper reports on two studies to contribute to this call for an interaction science with the goal of developing a functional taxonomy of interaction_primitives for map-based visualization a semi-structured interview study first was conducted with 21 expert interactive map users to understand the way in which map-based visualizations currently are employed the interviews were transcribed and coded to identify statements representative of either the task the user wished to accomplish ie objective primitives or the interactive functionality included in the visualization to achieve this task ie operator primitives a card sorting study then was conducted with 15 expert interactive map designers to organize these example statements into logical structures based on their experience translating client requests into interaction_designs example statements were supplemented with primitive definitions in the literature and were separated into two sorting exercises objectives and operators the objective sort suggested five objectives that increase in cognitive sophistication identify compare rank associate & delineate but exhibited a large amount of variation across participants due to consideration of broader user goals procure predict & prescribe and interaction operands space-alone attributes-in-space & space-in-time elementary & general the operator sort suggested five enabling operators import export save edit & annotate and twelve work operators reexpress arrange sequence resymbolize overlay pan zoom reproject search filter retrieve & calculate this taxonomy offers an empirically-derived and ecologically-valid structure to inform future research and design on interaction cartography geophysical measurements object recognition science_of_interaction search problems geo visualization interaction_primitives interaction techniques interactive maps
knowledge about visualization tasks plays an important role in choosing or building suitable visual representations to pursue them yet tasks are a multi-faceted concept and it is thus not surprising that the many existing task taxonomies and models all describe different aspects of tasks depending on what these task descriptions aim to capture this results in a clear need to bring these different aspects together under the common hood of a general design space of visualization tasks which we propose in this paper our design space consists of five design dimensions that characterize the main aspects of tasks and that have so far been distributed across different task descriptions we exemplify its concrete use by applying our design space in the domain of climate_impact_research to this end we propose interfaces to our design space for different user roles developers authors and end users that allow users of different levels of expertise to work with it data visualization market_research meteorology task_taxonomy taxonomy climate_impact_research design space visualization_recommendation
the considerable previous work characterizing visualization usage has focused on low-level tasks or interactions and high-level tasks leaving a gap between them that is not addressed this gap leads to a lack of distinction between the ends and means of a task limiting the potential for rigorous analysis we contribute a multi-level typology of visualization tasks to address this gap distinguishing why and how a visualization task is performed as well as what the task inputs and outputs are our typology allows complex tasks to be expressed as sequences of interdependent simpler tasks resulting in concise and flexible descriptions for tasks of varying complexity and scope it provides abstract rather than domain-specific descriptions of tasks so that useful comparisons can be made between visualization_systems targeted at different application domains this descriptive power supports a level of analysis required for the generation of new designs by guiding the translation of domain-specific problems into abstract tasks and for the qualitative_evaluation of visualization usage we demonstrate the benefits of our approach in a detailed case_study comparing task descriptions from our typology to those derived from related work we also discuss the similarities and differences between our typology and over two dozen extant classification systems and theoretical frameworks from the literatures of visualization human-computer_interaction information_retrievalcommunications and cartography encoding modeling qualitative_evaluations topology typology qualitative_evaluation task_and_requirements_analysis visualization_models
people typically interact with information_visualizations using a mouse their physical movement orientation and distance to visualizations are rarely used as input we explore how to use such spatial relations among people and visualizations ie proxemics to drive interaction with visualizations focusing here on the spatial relations between a single user and visualizations on a large display we implement interaction techniques that zoom and pan query and relate and adapt visualizations based on tracking of users' position in relation to a large high-resolution display alternative prototypes are tested in three user studies and compared with baseline conditions that use a mouse our aim is to gain empirical data on the usefulness of a range of design possibilities and to generate more ideas among other things the results show promise for changing zoom level or visual representation with the user's physical distance to a large display we discuss possible benefits and potential issues to avoid when designing information_visualizations that use proxemics data visualization encoding information filters navigation proxemics distance information_visualization large displays movement orientation user_study user_tracking
we present an interaction_model for beyond-desktop visualizations that combines the visualization reference model with the instrumental interaction paradigm beyond-desktop visualizations involve a wide range of emerging technologies such as wall-sized displays 3d and shape-changing displays touch and tangible input and physical information_visualizations while these technologies allow for new forms of interaction they are often studied in isolation new conceptual_models are needed to build a coherent picture of what has been done and what is possible we describe a modified pipeline_model where raw data is processed into a visualization and then rendered into the physical world users can explore or change data by directly manipulating visualizations or through the use of instruments interactions can also take place in the physical world outside the visualization system such as when using locomotion to inspect a large scale visualization through case studies we illustrate how this model can be used to describe both conventional and unconventional interactive_visualization_systems and compare different design alternatives data visualization information_visualization pipelines rendering (computer_graphics) three-dimensional displays interaction_model notational_system physical_visualization
conveying a narrative with visualizations often requires choosing an order in which to present visualizations while evidence exists that narrative sequencing in traditional stories can affect comprehension and memory little is known about how sequencing choices affect narrative_visualization we consider the forms and reactions to sequencing in narrative_visualization presentations to provide a deeper understanding with a focus on linear 'slideshow-style' presentations we conduct a qualitative_analysis of 42 professional narrative_visualizations to gain empirical knowledge on the forms that structure and sequence take based on the results of this study we propose a graph-driven approach for automatically identifying effective sequences in a set of visualizations to be presented linearly our approach identifies possible transitions in a visualization set and prioritizes local visualization-to-visualization transitions based on an objective function that minimizes the cost of transitions from the audience perspective we conduct two studies to validate this function we also expand the approach with additional knowledge of user preferences for different types of local transitions and the effects of global sequencing strategies on memory preference and comprehension our results include a relative ranking of types of visualization transitions by the audience perspective and support for memory and subjective rating benefits of visualization sequences that use parallelism as a structural device we discuss how these insights can guide the design of narrative_visualization and systems that support optimization of visualization sequence data_storytelling data visualization encoding linear programming parallel_processing sequential analysis narrative_structure narrative_visualization
presenting and communicating insights to an audience-telling a story-is one of the main goals of data_exploration even though visualization as a storytelling medium has recently begun to gain attention storytelling is still underexplored in information_visualization and little research has been done to help people tell their stories with data to create a new more engaging form of storytelling with data we leverage and extend the narrative storytelling attributes of whiteboard animation with pen_and_touch interactions we present sketchstory a data-enabled digital whiteboard that facilitates the creation of personalized and expressive data charts quickly and easily sketchstory recognizes a small set of sketch gestures for chart invocation and automatically completes charts by synthesizing the visuals from the presenter-provided example icon and binding them to the underlying data furthermore sketchstory allows the presenter to move and resize the completed data charts with touch and filter the underlying data to facilitate interactive_exploration we conducted a controlled experiment for both audiences and presenters to compare sketchstory with a traditional presentation system microsoft powerpoint results show that the audience is more engaged by presentations done with sketchstory than powerpoint eighteen out of 24 audience participants preferred sketchstory to powerpoint four out of five presenter participants also favored sketchstory despite the extra effort required for presentation animation data visualization filtering real-time systems rendering (computer_graphics) storytelling data_presentation interaction pen_and_touch sketch visualization
from financial statistics to nutritional values we are frequently exposed to quantitative information expressed in measures of either extreme magnitudes or unfamiliar units or both a common practice used to comprehend such complex measures is to relate re-express and compare them through visual depictions using magnitudes and units that are easier to grasp through this practice we create a new graphic_composition that we refer to as a concrete scale to the best of our knowledge there are no design_guidelines that exist for concrete scales despite their common use incommunication educational and decision-making settings we attempt to fill this void by introducing a novel framework that would serve as a practical guide for their analysis and design informed by a thorough analysis of graphic_compositions involving complex measures and an extensive literature review of scale_cognition mechanisms our framework outlines the design space of various measure relations-specifically relations involving the re-expression of complex measures to more familiar concepts-and their visual representations as graphic_compositions complexity theory computer_graphics concrete scale measurement graphic_composition scale_cognition visual_comparison visual_notation
storyline_visualizations which are useful in many applications aim to illustrate the dynamic relationships between entities in a story however the growing complexity and scalability of stories pose great challenges for existing approaches in this paper we propose an efficient optimization approach to generating an aesthetically appealing storyline_visualization which effectively handles the hierarchical relationships between entities over time the approach formulates the storyline layout as a novel hybrid optimization approach that combines discrete and continuous optimization the discrete method generates an initial layout through the ordering and alignment of entities and the continuous method optimizes the initial layout to produce the optimal one the efficient approach makes real-time interactions eg bundling and straightening possible thus enabling users to better understand and track how the story evolves experiments and case studies are conducted to demonstrate the effectiveness and usefulness of the optimization approach heuristic algorithms layout motion pictures optimization storylines white spaces level-of-detail optimization story-telling visualization user_interactions
we introduce visual sedimentation a novel design metaphor for visualizing data_streams directly inspired by the physical process of sedimentation visualizing data_streams e g tweets rss emails is challenging as incoming data arrive at unpredictable rates and have to remain readable for data_streams clearly expressing chronological order while avoiding clutter and keeping aging data visible are important the metaphor is drawn from the real-world sedimentation processes objects fall due to gravity and aggregate into strata over time inspired by this metaphor data is visually depicted as falling objects using a force model to land on a surface aggregating into strata over time in this paper we discuss how this metaphor addresses the specific challenge of smoothing the transition between incoming and aging data we describe the metaphor's design space a toolkit developed to facilitate itsimplementation and example applications to a range of case studies we then explore the generative capabilities of the design space through our toolkit we finally illustrate creative extensions of the metaphor when applied to real streams of data data visualization design design methodology real-time systems sediments data_stream dynamic_data dynamic_visualization information_visualization_metaphor real time
consider real-time exploration of large multidimensional spatiotemporal_datasets with billions of entries each defined by a location a time and other attributes are certain attributes correlated spatially or temporally are there trends oroutliers in the data answering these questions requires aggregation over arbitrary regions of the domain and attributes of the data many relational_databases implement the well-known data_cube aggregation operation which in a sense precomputes every possible aggregate query over the database data_cubes are sometimes assumed to take a prohibitively large amount of space and to consequently require disk storage in contrast we show how to construct a data_cube that fits in a modern laptop's main memory even for billions of entries we call this data structure a nanocube we present algorithms to compute and query a nanocube and show how it can be used to generate well-known visual_encodings such as heatmaps histograms and parallel coordinate plots when compared to exact visualizations created by scanning an entire dataset nanocube plots have bounded screen error across a variety of scales thanks to a hierarchical structure in space and time we demonstrate the effectiveness of our technique on a variety of real-world datasets and present memory timing and network bandwidth measurements we find that the timings for the queries in our examples are dominated by network and user-interaction latencies androids data_cube data visualization encoding humanoid robots nanostructured materials spatiotemporal phenomena data structures interactive_exploration
distributed_systems are complex to develop and administer and performance problem_diagnosis is particularly challenging when performance degrades the problem might be in any of the system's many components or could be a result of poor interactions among them recent research efforts have created tools that automatically localize the problem to a small number of potential culprits but research is needed to understand what visualization_techniques work best for helping distributed_systems developers understand and explore their results this paper compares the relative merits of three well-known visualization approaches side-by-side diff and animation in the context of presenting the results of one proven automated localization technique called request-flow comparison via a 26-person user_study which included real distributed_systems developers we identify the unique benefits that each approach provides for different problem types and usage modes distributed processing distributed_systems human_factors layout training human_factors problem_diagnosis visualization
having effective visualizations of filesystem provenance_data is valuable for understanding its complex hierarchical structure the most common visual representation of provenance_data is the node-link diagram while effective for understanding local activity the node-link diagram fails to offer a high-level summary of activity and inter-relationships within the data we present a new tool inprov which displays filesystem provenance with an interactive radial-based tree layout the tool also utilizes a new time-based hierarchical node grouping method for filesystem provenance_data we developed to match the user's mental_model and make data_exploration more intuitive we compared inprov to a conventional node-link based tool orbiter in a quantitative_evaluation with real users of filesystem provenance_data including provenance_data experts it professionals and computational scientists we also compared in the evaluation our new node grouping method to a conventional method the results demonstrate that inprov results in higheraccuracy in identifying system activity than orbiter with large complex data sets the results also show that our new time-based hierarchical node grouping method improves performance in both tools and participants found both tools significantly easier to use with the new time-based node grouping method subjective measures show that participants found inprov to require less mental activity less physical activity less work and is less stressful to use our study also reveals one of the first cases of gender differences in visualization both genders had comparable performance with inprov but women had a significantly lower averageaccuracy 56% compared to men 70% with orbiter context awareness data visualization encoding layout provenance_data gender differences graph/network_data hierarchy_data quantitative_evaluation
an important feature of networks for many application domains is their community structure this is because objects within the same community usually have at least one property in common the investigation of community structure can therefore support the understanding of object attributes from the network topology alone in real-world systems objects may belong to several communities at the same time ie communities can overlap analyzing fuzzy community memberships is essential to understand to what extent objects contribute to different communities and whether some communities are highly interconnected we developed a visualization approach that is based on node-link_diagrams and supports the investigation of fuzzy communities in weighted undirected graphs at different levels of detail starting with the network of communities the user can continuously drill down to the network of individual nodes and finally analyze the membership distribution of nodes of interest our approach uses layout strategies and further visual_mappings to graphically encode the fuzzy community memberships the usefulness of our approach is illustrated by two case studies analyzing networks of different domains social networking and biological interactions the case studies showed that our layout and visualization approach helps investigate fuzzy overlapping communities fuzzy vertices as well as the different communities to which they belong can be easily identified based on node color and position communities data visualization fuzzy methods image color analysis layout overlapping community visualization uncertainty fuzzy_clustering graph_visualization uncertainty_visualization
in many applications data tables contain multi-valued_attributes that often store the memberships of the table entities to multiple sets such as which languages a person masters which skills an applicant documents or which features a product comes with with a growing number of entities the resulting element-set membership matrix becomes very rich of information about how these sets overlap many analysis tasks targeted at set-typed_data are concerned with these overlaps as salient features of such data this paper presents radial sets a novel visual technique to analyze set memberships for a large number of elements our technique uses frequency-based representations to enable quickly finding and analyzing different kinds of overlaps between the sets and relating these overlaps to other attributes of the table entities furthermore it enables various interactions to select elements of interest find out if they are over-represented in specific sets or overlaps and if they exhibit a different distribution for a specific attribute compared to the rest of the elements these interactions allow formulating highly-expressive visual_queries on the elements in terms of their set memberships and attribute values as we demonstrate via two usage scenarios radial sets enable revealing and analyzing a multitude of overlapping patterns between large sets beyond the limits of state-of-the-art techniques color imaging data visualization histograms interactive_systems multi-valued_attributes scalability overlapping sets scalability set-typed_data visualization_technique
this article presents soccerstories a visualization_interface to support analysts in exploring soccer data and communicating interesting insights currently most analyses on such data relate to statistics on individual players or teams however soccer analysts we collaborated with consider that quantitative analysis alone does not convey the right picture of the game as context player positions and phases of player actions are the most relevant aspects we designed soccerstories to support the current practice of soccer analysts and to enrich it both in the analysis andcommunication stages our system provides an overview+detail interface of game phases and their aggregation into a series of connected visualizations each visualization being tailored for actions such as a series of passes or a goal attempt to evaluate our tool we ran two qualitative user studies on recent games using soccerstories with data from one of the world's leading live sports data providers the first study resulted in a series of four articles on soccer tactics by a tactics analyst who said he would not have been able to write these otherwise the second study consisted in an exploratory follow-up to investigate design alternatives for embedding soccer phases into word-sized graphics for both experiments we received a very enthusiastic feedback and participants consider further use of soccerstories to enhance their current workflow data visualization games layout navigation visual_knowledge_discovery sport_analytics visual_aggregation visual_knowledge_representation
we enhance a user-centered_design process with techniques that deliberately promote creativity to identify opportunities for the visualization of data generated by a major energy supplier visualization prototypes developed in this way prove effective in a situation whereby data sets are largely unknown and requirements open - enabling successful exploration of possibilities for visualization in smart_home data_analysis the process gives rise to novel designs and design metaphors including data sculpting it suggests that the deliberate use of creativity_techniques with data stakeholders is likely to contribute to successful novel and effective solutions that being explicit about creativity may contribute to designers developing creative solutions that using creativity_techniques early in the design process may result in a creative approach persisting throughout the process the work constitutes the first systematic visualization design for a data rich source that will be increasingly important to energy suppliers and consumers as smart meter technology is widely deployed it is novel in explicitly employing creativity_techniques at the requirements stage of visualization design and development paving the way for further use and study of creativity methods in visualization design creativity_techniques data models data visualization home appliances prototypes smart_homes data visualization energy_consumption smart_home user-centered_design
business_ecosystems are characterized by large complex and global networks of firms often from many different market segments all collaborating partnering and competing to create and deliver new products and services given the rapidly increasing scale complexity and rate of change of business_ecosystems as well as economic and competitive pressures analysts are faced with the formidable task of quickly understanding the fundamental characteristics of these interfirm networks existing tools however are predominantly query- or list-centric with limited interactive exploratory capabilities guided by a field study of corporate analysts we have designed and implemented dotlink360 an interactive_visualization system that provides capabilities to gain systemic insight into the compositional temporal and connective characteristics of business_ecosystems dotlink360 consists of novel multiple connected views enabling the analyst to explore discover and understand interfirm networks for a focal firm specific market segments or countries and the entire business ecosystem system evaluation by a small group of prototypical users shows supporting evidence of the benefits of our approach this design_study contributes to the relatively unexplored but promising area of exploratory information_visualization in market_research and business strategy business_ecosystems companies data visualization ecosystems interactive_systems mobilecommunication design_study interaction market_research network_visualization strategic_analysis
biological pathway maps are highly relevant tools for many tasks in molecular biology they reduce the complexity of the overall biological network by partitioning it into smaller manageable parts while this reduction of complexity is their biggest strength it is at the same time their biggest weakness by removing what is deemed not important for the primary function of the pathway biologists lose the ability to follow and understand cross-talks between pathways considering these cross-talks is however critical in many analysis scenarios such as judging effects of drugs in this paper we introduce entourage a novel visualization_technique that provides contextual information lost due to the artificial partitioning of the biological network but at the same time limits the presented information to what is relevant to the analyst's task we use one pathway map as the focus of an analysis and allow a larger set of contextual pathways for these context pathways we only show the contextual subsets ie the parts of the graph that are relevant to a selection entourage suggests related pathways based on similarities and highlights parts of a pathway that are interesting in terms of mapped experimental data we visualize interdependencies between pathways using stubs of visual_links which we found effective yet not obtrusive by combining this approach with visualization of experimental data we can provide domain experts with a highly valuable tool we demonstrate the utility of entourage with case studies conducted with a biochemist who researches the effects of drugs on pathways we show that the technique is well suited to investigate interdependencies between pathways and to analyze understand and predict the effect that drugs have on different cell types bioinformatics biological system modeling context awareness data visualization drugs pathway_visualization portals biological_networks biomolecular_data graphs subsets
scientists use dna_sequence differences between an individual's genome and a standard reference genome to study the genetic basis of disease such differences are called sequence variants and determining their impact in the cell is difficult because it requires reasoning about both the type and location of the variant across several levels of biological context in this design_study we worked with four analysts to design a visualization tool supporting variant impact assessment for three different tasks we contribute data and task abstractions for the problem of variant impact assessment and the carefully justified design andimplementation of the variant view tool variant view features an information-dense visual_encoding that provides maximal information at the overview level in contrast to the extensive navigation required by currently-prevalent genome browsers we provide initial evidence that the tool simplified and accelerated workflows for these three tasks through three case studies finally we reflect on the lessons learned in creating and refining data and task abstractions that allow for concise overviews of sprawlinginformation_spaces that can reduce or remove the need for the memory-intensive use of navigation bioinformatics browsers context awareness databases design methodology genomics information_visualization sequential analysis bioinformatics design_study genetic_variants
visualization of dynamically changing networks graphs is a significant challenge for researchers previous work has experimentally compared animation small_multiples and other techniques and found trade-offs between these one potential way to avoid such trade-offs is to combine previous techniques in a hybrid_visualization we present two taxonomies of visualizations of dynamic graphs one of non-hybrid techniques and one of hybrid techniques we also describe a prototype called diffani that allows a graph to be visualized as a sequence of three kinds of tiles diff tiles that show difference_maps over some time interval animation tiles that show the evolution of the graph over some time interval and small multiple tiles that show the graph state at an individual time slice this sequence of tiles is ordered by time and covers all time slices in the data an experimental evaluation of diffani shows that our hybrid approach has advantages over non-hybrid techniques in certain cases animation computer_graphics dynamic_networks prototypes animation difference_map evolution hybrid_visualization_taxonomy
to analyze data such as the us federal budget or characteristics of the student population of a university it is common to look for changes over time this task can be made easier and more fruitful if the analysis is performed by grouping by attributes such as by agencies bureaus and accounts for the budget or ethnicity gender and major in a university we present treeversity2 a web based interactive data visualization tool that allows users to analyze change in datasets by creating dynamic hierarchies based on the data attributes treeversity2 introduces a novel space filling visualization stemview to represent change in trees at multiple levels - not just at the leaf level with this visualization users can explore absolute and relative changes created and removed nodes and each node's actual values while maintaining the context of the tree in addition treeversity2 provides overviews of change over the entire time period and a reporting tool that listsoutliers in textual form which helps users identify the major changes in the data without having to manually setup filters we validated treeversity2 with 12 case studies with organizations as diverse as the national cancer institute federal drug administration department of transportation office of the bursar of the university of maryland or ebay our case studies demonstrated that treeversity2 is flexible enough to be used in different domains and provide useful insights for the data owners a treeversity2 demo can be found at https//treeversitycattlabumdedu context awareness data visualization image color analysis information_visualization topology tree_comparison
this paper is concerned with the creation of 'macros' in workflow_visualization as a support tool to increase the efficiency of data_curation tasks we propose computation of candidate macros based on their usage in large collections of workflows in data repositories we describe an efficient algorithm for extracting macro motifs from workflow graphs we discovered that the state transition information used to identify macro candidates characterizes the structural pattern of the macro and can be harnessed as part of the visual_design of the corresponding macro glyph this facilitates partial automation and consistency in glyph_design applicable to a large set of macro glyphs we tested this approach against a repository of biological_data holding some 9670 workflows and found that the algorithmically generated candidate macros are in keeping with domain expert expectations algorithm design and analysis biological system modeling data visualization semantics workflow_visualization glyph_generation glyph-based_visualization motif_detection state-transition-based algorithm
domain-specific database applications tend to contain a sizable number of table- form- and report-style views that must each be designed and maintained by a software developer a significant part of this job is the necessary tweaking of low-level presentation details such as label_placements text field dimensions list or table styles and so on in this paper we present a horizontally constrained layout_management algorithm that automates the display of structured hierarchical_data using the traditional visual idioms of hand-designed database uis tables multi-column forms and outline-style indented lists we compare our system with pure outline and nested table layouts with respect to space efficiency and readability the latter with an online user_study on 27 subjects our layouts are 39 and 16 times more compact on average than outline layouts and horizontally unconstrained table layouts respectively and are as readable as table layouts even for large_datasets data visualization hierarchy_data layout xml layout_management nested_relations tabular_data
we explore the effectiveness of visualizing dense directed graphs by replacing individual edges with edges connected to 'modules'-or groups of nodes-such that the new edges imply aggregate connectivity we only consider techniques that offer a lossless_compression that is where the entire graph can still be read from the compressed version the techniques considered are a simple grouping of nodes with identical neighbor sets modular_decomposition which permits internal structure in modules and allows them to be nested and power_graph_analysis which further allows edges to cross module boundaries these techniques all have the same goal-to compress the set of edges that need to be rendered to fully convey connectivity-but each successive relaxation of the module definition permits fewer edges to be drawn in the rendered graph each successive technique also we hypothesize requires a higher degree of mental effort to interpret we test this hypothetical trade-off with two studies involving human participants for power_graph_analysis we propose a novel optimal technique based on constraint programming this enables us to explore the parameter space for the technique more precisely than could be achieved with a heuristic although applicable to many domains we are motivated by-and discuss in particular-the application to software dependency analysis computer_graphics directed graphs edge detection modular construction modular_decomposition networks power_graph_analysis
scatterplot matrices sploms parallel_coordinates and glyphs can all be used to visualize the multiple continuous variables ie dependent variables or measures in multidimensional multivariate data however these techniques are not well suited to visualizing many categorical variables ie independent variables or dimensions to visualize multiple categorical variables 'hierarchical axes' that 'stack dimensions' have been used in systems like polaris and tableau however this approach does not scale well beyond a small number of categorical variables emerson et al [8] extend the matrix paradigm of the splom to simultaneously visualize several categorical and continuous variables displaying many kinds of charts in the matrix depending on the kinds of variables involved we propose a variant of their technique called the generalized plot matrix gplom the gplom restricts emerson et al's technique to only three kinds of charts scatterplots for pairs of continuous variables heatmaps for pairs of categorical variables and barcharts for pairings of categorical and continuous variable in an effort to make it easier to understand at the same time the gplom extends emerson et al's work by demonstrating interactive techniques suited to the matrix of charts we discuss the visual_design and interactive features of our gplom prototype including a textual search feature allowing users to quickly locate values or variables by name we also present a user_study that compared performance with tableau and our gplom prototype that found that gplom is significantly faster in certain cases and not significantly slower in other cases data visualization multidimensional data prototypes visual databases business_intelligence database_visualization databaseoverview high-dimensional_data mdmv parallel_coordinates relational_data scatterplot_matrix tabular_data user_interfaces
star_coordinates is a popular projection technique from an nd data space to a 2d/3d_visualization domain it is defined by setting n coordinate axes in the visualization domain since it generally defines an affine projection strong distortions can occur an nd sphere can be mapped to an ellipse of arbitrary size and aspect_ratio we propose to restrict star_coordinates to orthographic_projections which map an nd sphere of radius r to a 2d circle of radius r we achieve this by formulating conditions for the coordinate axes to define orthographic_projections and by running a repeated non-linear optimization in the background of every modification of the coordinate axes this way we define a number of orthographic interaction concepts as well as orthographic data tour sequences a scatterplot tour a principle component tour and a grand_tour all concepts are illustrated and evaluated with synthetic and real data data visualization minimization nonlinear distortion principal_component_analysis start plot three-dimensional displays multivariate visualization visual_analytics
for high-dimensional_data this work proposes two novel visual_exploration methods to gain insights into the data aspect and the dimension aspect of the data the first is a dimension projection matrix as an extension of a scatterplot_matrix in the matrix each row or column represents a group of dimensions and each cell shows a dimension projection such as mds of the data with the corresponding dimensions the second is a dimension projection tree where every node is either a dimension projection plot or a dimension projection matrix nodes are connected with links and each child node in the tree covers a subset of the parent node's dimensions or a subset of the parent node's data items while the tree nodes visualize the subspaces of dimensions or subsets of the data items under exploration the matrix nodes enable cross-comparison between different combinations of subspaces both dimension projection matrix and dimension project tree can be constructed algorithmically through automation or manually through user_interaction ourimplementation enables interactions such as drilling down to explore different levels of the data merging or splitting the subspaces to adjust the matrix and applying brushing to select data clusters our method enables simultaneously exploring data correlation and dimension correlation for data with high dimensions algorithm design and analysis clustering algorithms correlation data visualization high dimensional data image color analysis hierarchical visualization matrix sub-dimensional_space subspace tree user_interaction
to verify cluster separation in high-dimensional_data analysts often reduce the data with a dimension reduction dr technique and then visualize it with 2d scatterplots interactive 3d scatterplots or scatterplot matrices sploms with the goal of providing guidance between these visual_encoding choices we conducted an empirical data study in which two human coders manually inspected a broad set of 816 scatterplots derived from 75 datasets 4 dr techniques and the 3 previously mentioned scatterplot techniques each coder scored all color-coded classes in each scatterplot in terms of their separability from other classes we analyze the resulting quantitative data with a heatmap approach and qualitatively discuss interesting scatterplot examples our findings reveal that 2d scatterplots are often 'good enough' that is neither splom nor interactive 3d adds notably more cluster separability with the chosen dr technique if 2d is not good enough the most promising approach is to use an alternative dr technique in 2d beyond that splom occasionally adds additional value and interactive 3d rarely helps but often hurts in terms of poorer class separation and usability we summarize these results as a workflow model and implications for design our results offer guidance to analysts during the dr exploration process data_analysis data visualization dimensionality_reduction encoding principal_component_analysis three-dimensional displays quantitative study scatterplots
analysis of dynamic object deformations such as cardiac motion is of great importance especially when there is a necessity to visualize and compare the deformation behavior across subjects however there is a lack of effective techniques for comparative_visualization and assessment of a collection of motion data due to its 4-dimensional nature ie timely varying three-dimensional shapes from the geometric point of view the motion change can be considered as a function defined on the 2d manifold of the surface this paper presents a novel classification and visualization method based on a medial_surface shape_space in which two novel shape descriptors are defined for discriminating normal and abnormal human heart deformations as well as localizing the abnormal motion regions in our medial_surface shape_space the geodesic distance connecting two points in the space measures the similarity between their corresponding medial_surfaces which can quantify the similarity and disparity of the 3d heart motions furthermore the novel descriptors can effectively localize the inconsistently deforming myopathic regions on the left ventricle an easy visualization of heart motion sequences on the projected space allows users to distinguish the deformation differences our experimental results on both synthetic and real imaging data show that this method can automatically classify the healthy and myopathic subjects and accurately detect myopathic regions on the left ventricle which outperforms other conventional cardiac diagnostic methods atomic measurements biomedical monitoring cardiology data visualization heart level measurement medial_surface shape analysis comparative_visualization left_ventricle_diagnosis shape_space
representation of molecular surfaces is a well established way to study the interaction of molecules the state-of-theart molecular representation is the ses model which provides a detailed surface visualization nevertheless it is computationally expensive so the less accurate gaussian model is traditionally preferred we introduce a novel surface_representation that resembles the ses and approaches the rendering performance of the gaussian model our technique is based on the iterative blending of implicit functions and avoids any pre-computation additionally we propose a gpu-based ray-casting algorithm that efficiently visualize our molecular representation a qualitative and quantitative comparison of our model with respect to the gaussian and ses models is presented as showcased in the paper our technique is a valid and appealing alternative to the gaussian representation this is especially relevant in all the applications where the cost of the ses is prohibitive atomic measurements computational modeling mathematical model molecular_visualization rendering (computer_graphics) solvents geometry-based_techniques implicit_surfaces
visualizing symmetric patterns in the data often helps the domain scientists make important observations and gain insights about the underlying experiment detecting symmetry in scalar_fields is a nascent area of research and existing methods that detect symmetry are either not robust in the presence of noise or computationally costly we propose a data structure called the augmented extremum_graph and use it to design a novel symmetry_detection method based on robust estimation of distances the augmented extremum_graph captures both topological and geometric information of the scalar field and enables robust and computationally efficient detection of symmetry we apply the proposed method to detect symmetries in cryo-electron microscopy datasets and the experiments demonstrate that the algorithm is capable of detecting symmetry even in the presence of significant noise we describe novel applications that use the detected symmetry to enhance visualization of scalar field data and facilitate their exploration computer_graphics feature_extraction geometry histograms morse decomposition robustness scalar_field_visualization symmetric matrices data_exploration extremum_graph symmetry_detection
we propose a novel gpu-based approach to render virtual x-ray projections of deformable tetrahedral_meshes these meshes represent the shape and the internal density distribution of a particular anatomical structure and are derived from statistical_shape_and_intensity_models ssims we apply our method to improve the geometric reconstruction of 3d anatomy eg pelvic bone from 2d x-ray images for that purpose shape and density of a tetrahedral mesh are varied and virtual x-ray projections are generated within an optimization process until the similarity between the computed virtual x-ray and the respective anatomy depicted in a given clinical x-ray is maximized the openglimplementation presented in this work deforms and projects tetrahedral_meshes of high resolution 200000+ tetrahedra at interactive rates it generates virtual x-rays that accurately depict the density distribution of an anatomy of interest compared to existing methods that accumulate x-ray attenuation in deformable meshes our novel approach significantly boosts the deformation/projection performance the proposed projection algorithm scales better with respect to mesh resolution and complexity of the density distribution and the combined deformation and projection on the gpu scales better with respect to the number of deformation parameters the gain in performance allows for a larger number of cycles in the optimization process consequently it reduces the risk of being stuck in a local optimum we believe that our approach will improve treatments in orthopedics where 3d anatomical information is essential attenuation deformable models digitally_reconstructed_radiographs gpu_acceleration graphics processing units image reconstruction shape analysis three-dimensional displays x-ray imaging image registration mesh_deformation statistical_shape_and_intensity_models volume_rendering
information_theory provides a theoretical framework for measuring information content for an observed variable and has attracted much attention from visualization researchers for its ability to quantify saliency and similarity among variables in this paper we present a new approach towards building an exploration framework based on information_theory to guide the users through the multivariate data_exploration process in our framework we compute the total entropy of the multivariate data set and identify the contribution of individual variables to the total entropy the variables are classified into groups based on a novel graph model where a node represents a variable and the links encode the mutual information shared between the variables the variables inside the groups are analyzed for their representativeness and an information based importance is assigned we exploit specific information metrics to analyze the relationship between the variables and use the metrics to choose isocontours of selected variables for a chosen group of points parallel_coordinates plots pcp are used to show the states of the variables and provide an interface for the user to select values of interest experiments with different data sets reveal the effectiveness of our proposed framework in depicting the interesting regions of the data sets taking into account the interaction among the variables entropy information technology information_theory isosurfaces layout mutual information uncertainty framework isosurface multivariate uncertainty
histograms computed from local regions are commonly used in many visualization_applications and allowing the user to query histograms interactively in regions of arbitrary locations and sizes plays an important role in feature identification and tracking computing histograms in regions with arbitrary location and size nevertheless can be time consuming for large_data sets since it involves expensive i/o and scan of data elements to achieve both performance- and storage-efficient query of local histograms we present a new algorithm called waveletsat which utilizes integral_histograms an extension of the summed area tables sat and discrete wavelet transform dwt similar to sat an integral histogram is the histogram computed from the area between each grid point and the grid origin which can be be pre-computed to support fast query nevertheless because one histogram contains multiple bins it will be very expensive to store one integral histogram at each grid point to reduce the storage cost for large integral_histograms waveletsat treats the integral_histograms of all grid points as multiple sats each of which can be converted into a sparse representation via dwt allowing the reconstruction of axis-aligned region histograms of arbitrary sizes from a limited number of wavelet coefficients besides we present an efficient wavelet transform algorithm for sats that can operate on each grid point separately in logarithmic time complexity which can be extended to parallel gpu-basedimplementation with theoretical and empirical demonstration we show that waveletsat can achieve fast preprocessing and smaller storage overhead than the conventional integral histogram approach with close query performance discrete wavelet transforms histograms integral equations statistical_analysis time complexity waveletsat discrete wavelet transform integral_histograms
numerical ensemble forecasting is a powerful tool that drives many risk analysis efforts and decision_making tasks these ensembles are composed of individual simulations that each uniquely model a possible outcome for a common event of interest eg the direction and force of a hurricane or the path of travel and mortality rate of a pandemic this paper presents a new visual strategy to help quantify and characterize a numerical ensemble's predictive uncertainty ie the ability for ensemble constituents to accurately and consistently predict an event of interest based on ground truth observations our strategy employs a bayesian framework to first construct a statistical aggregate from the ensemble we extend the information obtained from the aggregate with a visualization strategy that characterizes predictive uncertainty at two levels at a global level which assesses the ensemble as a whole as well as a local level which examines each of the ensemble's constituents through this approach modelers are able to better assess the predictive strengths and weaknesses of the ensemble as a whole as well as individual models we apply our method to two datasets to demonstrate its broad applicability bayes methods data visualization mathematical model numerical models predictive models uncertainty_visualization numerical_ensembles statistical visualization
ensembles of numerical simulations are used in a variety of applications such as meteorology or computational solid mechanics in order to quantify the uncertainty or possible error in a model or simulation deriving robust statistics and visualizing the variability of an ensemble is a challenging task and is usually accomplished through direct visualization of ensemble members or by providing aggregate representations such as an average or pointwise probabilities in many cases the interesting quantities in a simulation are not dense fields but are sets of features that are often represented as thresholds on physical or derived quantities in this paper we introduce a generalization of boxplots called contour boxplots for visualization and exploration of ensembles of contours or level_sets of functions conventional boxplots have been widely used as an exploratory or communicative tool for data_analysis and they typically show the median mean confidence intervals andoutliers of a population the proposed contour boxplots are a generalization of functional boxplots which build on the notion of data depth data depth approximates the extent to which a particular sample is centrally located within its density function this produces a center-outward ordering that gives rise to the statistical quantities that are essential to boxplots here we present a generalization of functional data depth to contours and demonstrate methods for displaying the resulting boxplots for two-dimensional simulation data in weather forecasting and computational fluid dynamics boxplots computational modeling data visualization numerical models shape analysis statistical_analysis uncertainty uncertainty_visualization weather forecasting band_depth ensemble_visualization order statistics
we present a study of linear_interpolation when applied to uncertain data linear_interpolation is a key step for isosurface_extraction algorithms and the uncertainties in the data lead to non-linear variations in the geometry of the extracted isosurface we present an approach for deriving the probability density function of a random variable modeling the positional uncertainty in the isosurface_extraction when the uncertainty is quantified by a uniform distribution our approach provides a closed-form characterization of the mentioned random variable this allows us to derive in closed form the expected value as well as the variance of the level-crossing position while the former quantity is used for constructing a stable isosurface for uncertain data the latter is used for visualizing the positional uncertainties in the expected isosurface level crossings on the underlying grid data models interpolation isosurfaces marching_cubes probability density function random variables uncertainty uncertainty_quantification isosurface_extraction linear_interpolation
ensemble run simulations are becoming increasingly widespread in this work we couple particle advection with pathline analysis to visualize and reveal the differences among the flow_fields of ensemble runs our method first constructs a variation field using a lagrangian-based distance metric the variation field characterizes the variation between vector_fields of the ensemble runs by extracting and visualizing the variation of pathlines within ensemble parallelism in a mapreduce style is leveraged to handle data processing and computing at scale using our prototype system we demonstrate how scientists can effectively explore and investigate differences within ensemble simulations computational modeling data visualization ensemble analysis scalability spatiotemporal phenomena uncertainty field_line_advection parallel_processing
sets of simulation runs based on parameter and model variation so-called ensembles are increasingly used to model physical behaviors whose parameter space is too large or complex to be explored automatically visualization plays a key role in conveying important properties in ensembles such as the degree to which members of the ensemble agree or disagree in their behavior for ensembles of time-varying vector_fields there are numerous challenges for providing an expressive comparative_visualization among which is the requirement to relate the effect of individual flow divergence to joint transport characteristics of the ensemble yet techniques developed for scalar ensembles are of little use in this context as the notion of transport induced by a vector field cannot be modeled using such tools we develop a lagrangian framework for the comparison of flow_fields in an ensemble our techniques evaluate individual and joint transport variance and introduce a classification space that facilitates incorporation of these properties into a common ensemble_visualization variances of lagrangian neighborhoods are computed using pathline integration and principal components analysis this allows for an inclusion of uncertainty measurements into the visualization and analysis approach our results demonstrate the usefulness and expressiveness of the presented method on several practical examples computational modeling data visualization ensemble lagrangian principal_component_analysis trajectory visual_analytics comparison flow_field principal components analysis time-varying variance visualization
we present a new efficient and scalable method for the high_quality reconstruction of the flow map from sparse samples the flow map describes the transport of massless particles along the flow as such it is a fundamental concept in the analysis of transient flow phenomena and all so-called lagrangian_flow_visualization_techniques require its approximation the flow map is generally obtained by integrating a dense 1d 2d or 3d set of particles across the domain of definition of the flow despite its embarrassingly parallel nature this computation creates a performance bottleneck in the analysis of large-scale_datasets that existing adaptive techniques alleviate only partially our iterative approximation method significantly improves upon the state of the art by precisely modeling the flow behavior around automatically detected geometric structures embedded in the flow thus effectively restricting the sampling effort to interesting regions our data reconstruction is based on a modified version of sibson's scattered_data_interpolation and allows us at each step to offer an intermediate dense approximation of the flow map and to seamlessly integrate regions that will be further refined in subsequent steps we present a quantitative and qualitative_evaluation of our method on different types of flow datasets and offer a detailed comparison with existing techniques approximation error image edge detection interpolation lagrangian_flow_visualization least squares approximations surface_reconstruction trajectory adaptive_refinement edge_features flow map parallel reconstruction scattered_data_interpolation sparse_sampling
recent advances in vector_field_topologymake it possible to compute its multi-scale graph representations for autonomous 2d vector_fields in a robust and efficient manner one of these representations is a morse connection graph mcg a directed graph whose nodes correspond to morse sets generalizing stationary points and periodic trajectories and arcs - to trajectories connecting them while being useful for simple vector_fields the mcg can be hard to comprehend for topologically rich vector_fields containing a large number of features this paper describes a visual representation of the mcg inspired by previous work on graph_visualization our approach aims to preserve the spatial relationships between the mcg arcs and nodes and highlight the coherent behavior of connecting trajectories using simulations of ocean flow we show that it can provide useful information on the flow structure this paper focuses specifically on mcgs computed for piecewise constant pc vector_fields in particular we describe extensions of the pc framework that make it more flexible and better suited for analysis of data on complex shaped domains with a boundary we also describe a topology simplification scheme that makes our mcg visualizations less ambiguous despite the focus on the pc framework our approach could also be applied to graph representations or topological skeletons computed using different methods computer_graphics corporate acquisitions morse connection graph topology trajectory two dimensional displays vector_field_topology
cardiovascular diseases cvd are the leading cause of death worldwide their initiation and evolution depends strongly on the blood flow characteristics in recent years advances in 4d_pc-mri acquisition enable reliable and time-resolved 3d flow measuring which allows a qualitative and quantitative analysis of the patient-specific hemodynamics currently medical researchers investigate the relation between characteristic flow patterns like vortices and different pathologies the manual extraction and evaluation is tedious and requires expert knowledge standardized semi-automatic and reliable techniques are necessary to make the analysis of 4d_pc-mri applicable for the clinical routine in this work we present an approach for the extraction of vortex flow in the aorta and pulmonary artery incorporating line predicates we provide an extensive comparison of existent vortex_extraction methods to determine the most suitable vortex criterion for cardiac_blood_flow and apply our approach to ten datasets with different pathologies like coarctations tetralogy of fallot and aneurysms for two cases we provide a detailed discussion how our results are capable to complement existent diagnosis information to ensure real-time feedback for the domain experts we implement our method completely on the gpu d pc-mri arteries biomedical monitoring blood flow cardiovascular system data_mining heart pathology smoothing methods cardiac_blood_flow hemodynamics line predicates vortex_extraction
we present an interface for exploring large design spaces as encountered in simulation-based engineering design of visual effects and other tasks that require tuning parameters of computationally-intensive simulations and visually evaluating results the goal is to enable a style of design with simulations that feels as-direct-as-possible so users can concentrate on creative design tasks the approach integrates forward design via direct_manipulation of simulation inputs eg geometric properties applied forces in the same visual space with inverse design via 'tugging' and reshaping simulation outputs eg scalar_fields from finite_element analysis fea or computational_fluid_dynamics_(cfd) the interface includes algorithms for interpreting the intent of users' drag operations relative to parameterized models morphing arbitrary scalar_fields output from fea and cfd simulations and in-place interactive ensemble_visualization the inverse design strategy can be extended to use multi-touch input in combination with an as-rigid-as-possible shape manipulation to support rich visual_queries the potential of this new design approach is confirmed via two applications medical device engineering of a vacuum-assisted biopsy device and visual effects design using a physically based flame simulation biological system modeling computational modeling data models data visualization design real-time systems direct_manipulation multi-touch simulation
we present an integrated camera motion design and path generation system for building volume data animations creating animations is an essential task in presenting complex scientific_visualizations existing visualization_systems use an established animation function based on keyframes selected by the user this approach is limited in providing the optimal in-between views of the data alternatively computer_graphics and virtual_reality camera_motion_planning is frequently focused on collision free movement in a virtual walkthrough for semi-transparent fuzzy or blobby volume data the collision free objective becomes insufficient here we provide a set of essential criteria focused on computing camera paths to establish effective animations of volume data our dynamic multi-criteria solver coupled with a force-directed routing algorithm enables rapid generation of camera paths once users review the resulting animation and evaluate the camera motion they are able to determine how each criterion impacts path generation in this paper we demonstrate how incorporating this animation approach with an interactive_volume_visualization system reduces the effort in creating context-aware and coherent animations this frees the user to focus on visualization tasks with the objective of gaining additional insight from the volume data animation camera_motion_planning cameras data visualization motion control rendering (computer_graphics) three-dimensional displays animation visualization volume_rendering
we present a prop-based tangible interface for 3d interactive_visualization of thin fiber structures these data are commonly found in current bioimaging datasets for example second-harmonic generation microscopy of collagen fibers in tissue our approach uses commodity visualization technologies such as a depth sensing camera and low-cost 3d display unlike most current uses of these emerging technologies in the games and graphics communities we employ the depth sensing camera to create a fish-tank stereposcopic virtual_reality system at the scientist's desk that supports tracking of small-scale gestures with objects already found in the work space we apply the new interface to the problem of interactive exploratory_visualization of three-dimensional thin fiber data a critical task for the visual_analysis of these data is understanding patterns in fiber orientation throughout a volumethe interface enables a new fluid style of data_exploration and fiber orientation analysis by using props to provide needed passive-haptic feedback making 3d_interactions with these fiber structures more controlled we also contribute a low-level algorithm for extracting fiber centerlines from volumetric imaging the system was designed and evaluated with two biophotonic experts who currently use it in their lab as compared to typical practice within their field the new visualization system provides a more effective way to examine and understand the 3d bioimaging datasets they collect d interaction data visualization microscopy scientific_visualization three dimensional displays microscopy_visualization tangible interaction
conflicting results are reported in the literature on whether dynamic_visualizations are more effective than static visualizations for learning and mastering 3-d tasks and only a few investigations have considered the influence of the spatial abilities of the learners in a study with 117 participants we compared the benefit of static vs dynamic_visualization training tools on learners with different spatial abilities performing a typical 3-d task specifically creating orthographic_projections of a 3-d object we measured the spatial abilities of the participants using the mental rotation test mrt and classified participants into two groups high and low abilities to examine how the participants' abilities predicted change in performance after training with static versus dynamic training tools our results indicate that 1visualization training programs can help learners to improve 3-d task_performance 2 dynamic_visualizations provide no advantages over static visualizations that show intermediate steps 3 training programs are more beneficial for individuals with low spatial abilities than for individuals with high spatial abilities and 4 training individuals with high spatial abilities using dynamic_visualizations provides little benefit d visualization animation atmospheric measurements cad design automation optimized production technology particle measurements spatial_ability spatial resolution evaluation orthographic_projection training
we present an assessment of the state and historic development of evaluation practices as reported in papers published at the ieee visualization conference our goal is to reflect on a meta-level about evaluation in our community through a systematic understanding of the characteristics and goals of presented evaluations for this purpose we conducted a systematic review of ten years of evaluations in the published papers using and extending a coding scheme previously established by lam et al [2012] the results of our review include an overview of the most common evaluation goals in the community how they evolved over time and how they contrast or align to those of the ieee information_visualization conference in particular we found that evaluations specific to assessing resulting images and algorithm performance are the most prevalent with consistently 80-90% of all papers since 1997 however especially over the last six years there is a steady increase in evaluation methods that include participants either by evaluating their performances and subjective feedback or by evaluating their work practices and their improved analysis and reasoning capabilities using visual tools up to 2010 this trend in the ieee visualization conference was much more pronounced than in the ieee information_visualization conference which only showed an increasing percentage of evaluation through user performance and experience testing since 2011 however also papers in ieee information_visualization show such an increase of evaluations of work practices and analysis as well as reasoning using visual tools further we found that generally the studies reporting requirements analyses and domain-specific work practices are too informally reported which hinders cross-comparison and lowers external validity data visualization encoding evaluation history mathematical model systematics information_visualization scientific_visualization systematic review validation visualization
the precise modeling of vascular structures plays a key role in medical imaging applications such as diagnosis therapy planning and blood flow simulations for the simulation of blood flow in particular high-precision models are required to produce accurate results it is thus common practice to perform extensive manual data polishing on vascular segmentations prior to simulation this usually involves a complex tool chain which is highly impractical for clinical on-site application to close this gap in current blood flow simulation pipelines we present a novel technique for interactive vascular modeling which is based on implicit sweep surfaces our method is able to generate and correct smooth high-quality models based on geometric centerline descriptions on the fly it supports complex vascular free-form contours and consequently allows for an accurate and fast modeling of pathological structures such as aneurysms or stenoses we extend the concept of implicit sweep surfaces to achieve increased robustness and applicability as required in the medical field we finally compare our method to existing techniques and provide case studies that confirm its contribution to current simulation pipelines biomedical imaging computational modeling image_segmentation interpolation splines (mathematics) surface_modeling vascular structures centerline-based modeling vascular_visualization
we propose a new colon_flattening algorithm that is efficient shape-preserving and robust to topological_noise unlike previous approaches which require a mandatory topological denoising to remove fake handles our algorithm directly flattens the colon surface without any denoising in our method we replace the original euclidean metric of the colon surface with a heat_diffusion metric that is insensitive to topological_noise using this heat_diffusion metric we then solve a laplacian equation followed by an integration step to compute the final flattening we demonstrate that our method is shape-preserving and the shape of the polyps are well preserved the flattened colon also provides an efficient way to enhance the navigation and inspection in virtual_colonoscopy we further show how the existing colon registration pipeline is made more robust by using our colon_flattening we have tested our method on several colon wall surfaces and the experimental results demonstrate the robustness and the efficiency of our method biomedical measurement colon_flattening colonoscopy harmonic analysis heating volume_rendering heat_diffusion shape-preserving mapping topological_noise virtual_colonoscopy volume_rendering
visualizations of vascular structures are frequently used in radiological investigations to detect and analyze vascular diseases obstructions of the blood flow through a vessel are one of the main interests of physicians and several methods have been proposed to aid the visual assessment of calcifications on vessel walls curved_planar_reformation cpr is a wide-spread method that is designed for peripheral arteries which exhibit one dominant direction to analyze the lumen of arbitrarily oriented vessels centerline reformation cr has been proposed both methods project the vascular structures into 2d image space in order to reconstruct the vessel lumen in this paper we propose curved surface reformation csr a technique that computes the vessel lumen fully in 3d this offers high-quality interactive_visualizations of vessel lumina and does not suffer from problems of earlier methods such as ambiguous visibility cues or premature discretization of centerline_data our method maintains exact visibility information until the final query of the 3d lumina data we also present feedback from several domain experts data visualization radiology reformation rendering (computer_graphics) surface treatment three-dimensional displays vascular structures surface approximation volume_rendering
this paper presents connectomeexplorer an application for the interactive_exploration and query-guided visual_analysis of large volumetric electron microscopy em data sets in connectomics research our system incorporates a knowledge-based query_algebra that supports the interactive specification of dynamically evaluated queries which enable neuroscientists to pose and answer domain-specific questions in an intuitive manner queries are built step by step in a visual query builder building more complex queries from combinations of simpler queries our application is based on a scalable volume_visualization_framework that scales to multiple volumes of several teravoxels each enabling the concurrent_visualization and querying of the original em volume additional segmentation volumes neuronal connectivity and additional meta data comprising a variety of neuronal data attributes we evaluate our application on a data set of roughly one terabyte of em data and 750 gb of segmentation data containing over 4000 segmented structures and 1000 synapses we demonstrate typical use-case scenarios of our collaborators in neuroscience where our system has enabled them to answer specific scientific questions using interactive querying and analysis on the full-size data for the first time connectomics data visualization nerve fibers neuroscience query_processing three-dimensional displays neuroscience petascale_volume_analysis query_algebra visual_knowledge_discovery
as the visualization field matures an increasing number of general toolkits are developed to cover a broad range of applications however no general tool can incorporate the latest capabilities for all possible applications nor can the user_interfaces and workflows be easily adjusted to accommodate all user communities as a result users will often chose either substandard solutions presented in familiar customized tools or assemble a patchwork of individual applications glued through ad-hoc scripts and extensive manual intervention instead we need the ability to easily and rapidly assemble the best-in-task tools into custom interfaces and workflows to optimally serve any given application community unfortunately creating such meta-applications at the api or sdk level is difficult time consuming and often infeasible due to the sheer variety of data models design philosophies limits in functionality and the use of closed commercial systems in this paper we present the manyvis framework which enables custom solutions to be built both rapidly and simply by allowing coordination andcommunication across existing unrelated applications manyvis allows users to combine software tools with complementary characteristics into one virtual application driven by a single custom-designed interface data visualization graphical user_interfaces image color analysis programming shape analysis visualization_environments integrated_applications linked_views macros
we present a framework for acuity-driven visualization of super-high resolution image data on gigapixel displays tiled display walls offer a large workspace that can be navigated physically by the user based on head tracking information the physical characteristics of the tiled display and the formulation of visual_acuity we guide an out-of-core gigapixel rendering scheme by delivering high levels of detail only in places where it is perceivable to the user we apply this principle to gigapixel image rendering through adaptive level of detail selection additionally we have developed an acuity-driven tessellation scheme for high-quality focus-and-context f+c lenses that significantly reduces visual artifacts while accurately capturing the underlying lens function we demonstrate this framework on the reality_deck an immersive gigapixel display we present the results of a user_study designed to quantify the impact of our acuity-driven rendering optimizations in the visual_exploration process we discovered no evidence suggesting a difference in search task_performance between our framework and naive rendering of gigapixel resolution data while realizing significant benefits in terms of data transfer overhead additionally we show that our acuity-driven tessellation scheme offers substantially increased frame rates when compared to naive pre-tessellation while providing indistinguishable image quality context awareness data visualization gigapixel_visualization image resolution lenses pixels reality_deck rendering (computer_graphics) focus_and_context gigapixel display visual_acuity
we describe a framework to explore and visualize the movement of cloud systems using techniques from computational_topology and computer vision our framework allows the user to study this movement at various scales in space and time such movements could have large temporal and spatial scales such as the madden julian oscillation mjo which has a spatial scale ranging from 1000 km to 10000 km and time of oscillation of around 40 days embedded within these larger scale oscillations are a hierarchy of cloud_clusters which could have smaller spatial and temporal scales such as the nakazawa cloud_clusters these smaller cloud_clusters while being part of the equatorial mjo sometimes move at speeds different from the larger scale and in a direction opposite to that of the mjo envelope hitherto one could only speculate about such movements by selectively analysing data and a priori knowledge of such systems our framework automatically delineates such cloud_clusters and does not depend on the prior experience of the user to define cloud_clusters analysis using our framework also shows that most tropical systems such as cyclones also contain multi-scale_interactions between clouds and cloud systems we show the effectiveness of our framework to track organized cloud system during one such rainfall event which happened at mumbai india in july 2005 and for cyclone aila which occurred in bay of bengal during may 2009 brightness temperature cloud_clusters clouds data visualization level set meteorology optical imaging tracking computational_topology split_tree tracking weather and climate simulations
this paper describes an advanced visualization method for the analysis of defects in industrial 3d_x-ray_computed_tomography xct data we present a novel way to explore a high number of individual objects in a dataset eg pores inclusions particles fibers and cracks demonstrated on the special application area of pore extraction in carbon_fiber_reinforced_polymers cfrp after calculating the individual object properties volume dimensions and shapefactors all objects are clustered into a mean object mobject the resulting mobject parameter space can be explored interactively to do so we introduce the visualization of mean object sets mobject sets in a radial and a parallel arrangement each mobject may be split up into sub-classes by selecting a specific property eg volume or shape factor and the desired number of classes applying this interactive selection iteratively leads to the intended classifications and visualizations of mobjects along the selected analysis path hereby the given different scalingfactors of the mobjects down the analysis path are visualized through a visual linking approach furthermore the representative mobjects are exported as volumetric_datasets to serve as input for successive calculations and simulations in the field of porosity determination in cfrp non-destructive_testing practitioners use representative mobjects to improve ultrasonic calibration curves representative pores also serve as input for heat conduction simulations in active thermography for a fast overview of the pore properties in a dataset we propose a local mobjects visualization in combination with a color-coded homogeneity visualization of cells the advantages of our novel approach are demonstrated using real world cfrp specimens the results were evaluated through a questionnaire in order to determine the practicality of the mobjects visualization as a supportive tool for domain specialists d x-ray computed_tomography biomedical imaging data visualization mobjects three-dimensional displays transfer_functions x-ray tomography carbon_fiber_reinforced_polymers parameter_space_analysis porosity
we present the design of a novel framework for the visual integration comparison and exploration of correlations in spatial and non-spatial geriatric research data these data are in general high-dimensional and span both the spatial volumetric domain - through magnetic_resonance_imaging volumes - and the non-spatial domain through variables such as age gender or walking speed the visual_analysis framework blends medical imaging mathematical analysis and interactive_visualization_techniques and includes the adaptation of sparse partial least squares and iterated tikhonov regularization algorithms to quantify potential neurologymobility connections a linked-view design geared specifically at interactive visual_comparison integrates spatial and abstract visual representations to enable the users to effectively generate and refine hypotheses in a large multidimensional and fragmented space in addition to the domain analysis and design description we demonstrate the usefulness of this approach on two case studies last we report the lessons learned through the iterative design and evaluation of our approach in particular those relevant to the design of comparative_visualization of spatial and non-spatial_data algorithm design and analysis biomedical imaging brain modeling data visualization design studies geriatrics rendering (computer_graphics) applications_of_visualization high-dimensional_data integrating_spatial_and_non-spatial_data_visualization methodology_design task_and_requirements_analysis visual_comparison
analysis of multivariate data is of great importance in many scientific disciplines however visualization of 3d spatially-fixed multivariate volumetric_data is a very challenging task in this paper we present a method that allows simultaneous real-time_visualization of multivariate data we redistribute the opacity within a voxel to improve the readability of the color defined by a regular transfer_function and to maintain the see-through capabilities of volume_rendering we use predictable procedural noise - random-phase gabor noise - to generate a high-frequency redistribution pattern and construct an opacity mapping function which allows to partition the available space among the displayed data attributes this mapping function is appropriately filtered to avoid aliasing while maintaining transparent regions we show the usefulness of our approach on various data sets and with different example applications furthermore we evaluate our method by comparing it to other visualization_techniques in a controlled user_study overall the results of our study indicate that users are much more accurate in determining exact data values with our novel 3d volume_visualization method significantly lower error rates for reading data values and high subjective ranking of our method imply that it has a high chance of being adopted for the purpose of visualization of multivariate 3d data colored noise data visualization image color analysis rendering (computer_graphics) three-dimensional displays transfer_functions volume_rendering multi-variate_data visualization multi-volume_rendering scientific_visualization
we present ambient_scattering as a preintegration method for scattering on mesoscopic scales in direct_volume_rendering far-range scattering effects usually provide negligible contributions to a given location due to the exponential attenuation with increasing distance this motivates our approach to preintegrating multiple scattering within a finite spherical region around any given sample point to this end we solve the full light_transport with a monte-carlo simulation within a set of spherical regions where each region may have different material parameters regarding anisotropy and extinction this precomputation is independent of the data set and the transfer_function and results in a small preintegration table during rendering the look-up table is accessed for each ray sample point with respect to the viewing direction phase function and material properties in the spherical neighborhood of the sample our rendering technique is efficient and versatile because it readily fits in existing ray marching algorithms and can be combined with local_illumination and volumetric ambient_occlusion it provides interactive volumetric scattering and soft shadows with interactive control of the transfer_function anisotropy parameter of the phase function lighting conditions and viewpoint a gpuimplementation demonstrates the benefits of ambient_scattering for the visualization of different types of data sets with respect to spatial_perception high-quality illumination translucency and rendering speed computational modeling direct_volume_rendering light sources lighting rendering (computer_graphics) scattering solid modeling transfer_functions ambient_scattering gradient-free_shading preintegrated_light_transport volume_illumination
with the evolution of graphics_hardware high_quality global_illumination becomes available for real-time volume_rendering compared to local_illumination global_illumination can produce realistic shading effects which are closer to real world scenes and has proven useful for enhancing volume data visualization to enable better depth and shape perception however setting up optimal lighting could be a nontrivial task for average users there were lighting_design works for volume_visualization but they did not consider global light_transportation in this paper we present a lighting_design method for volume_visualization employing global_illumination the resulting system takes into account view and transfer-function dependent content of the volume data to automatically generate an optimized three-point lighting environment our method fully exploits the back light which is not used by previous volume_visualization_systems by also including global shadow and multiple scattering our lighting system can effectively enhance the depth and shape perception of volumetric features of interest in addition we propose an automatic tone_mapping operator which recovers visual details from overexposed areas while maintaining sufficient contrast in the dark areas we show that our method is effective for visualizing volume datasets with complex structures the structural information is more clearly and correctly presented under the automatically generated light sources data visualization global_illumination image color analysis light sources lighting rendering (computer_graphics) volume_rendering lighting_design tone_mapping volume_rendering
