Paper ID	Conference	Year	Title	DOI	Link	First Page	Last Page	IEEE Xplore Article Number	INSPEC Controlled	INSPEC Non-Controlled	IEEE Terms	Author Keywords	Abstract	Author Names	Author Affiliations	Author IDs
	InfoVis	19-20 Oct 1998	Algorithm visualization for distributed environments	10.1109/INFVIS.1998.729561	http://dx.doi.org/10.1109/INFVIS.1998.729561	71	78, 154	729561	Internet;client-server systems;computer animation;distributed algorithms;program visualisation	VADE;Web page;algorithm visualization;animations;asynchronous distributed systems;client machine;conceptual model;distributed algorithms;distributed environments;server machines	Algorithm design and analysis;Animation;Code standards;Displays;Distributed algorithms;Education;Parallel algorithms;Programming profession;Visualization;World Wide Web		The paper investigates the visualization of distributed algorithms. We present a conceptual model and a system, VADE, that realizes this model. Since in asynchronous distributed systems there is no way of knowing (let alone, visualizing) the “real” execution, we show how to generate a visualization which is consistent with the execution of the distributed algorithm. We also present the design and implementation of our system. VADE is designed so that the algorithm runs on the server's machines while the visualization is executed on a Web page on the client's machine. Programmers can write animations quickly and easily with the assistance of VADE's libraries	Moses, Y.;Polunsky, Z.;Tal, A.;Ulitsky, L.	Dept. of Electr. Eng., Technion-Israel Inst. of Technol., Haifa, Israel|c|;;;	37371861000;37374449900;37337567000;37374450900
	InfoVis	19-20 Oct 1998	Geographic visualization: designing manipulable maps for exploring temporally varying georeferenced statistics	10.1109/INFVIS.1998.729563	http://dx.doi.org/10.1109/INFVIS.1998.729563	87	94, 156	729563	cartography;computer animation;data analysis;data visualisation;geographic information systems;health care;medical information systems;statistical databases;time series	cartographic visualization;cartography;data analysis strategies;data extremes;experts;exploratory data analysis;geographic information systems;geographic pattern changes;geographic visualization;georeferenced information;georeferenced variable similarity;hypothesis generation;information visualization;manipulable dynamic GVis tools;manipulable map design;pattern noticing;qualitative exploratory analysis;spatiotemporal patterns;system use;temporally varying georeferenced statistics exploration;time series multivariate georeferenced health statistics;transaction logs;verbal protocols;visual thinking	Animation;Data analysis;Data visualization;Electronic design automation and methodology;Geographic Information Systems;Information analysis;Protocols;Prototypes;Spatiotemporal phenomena;Statistics		Geographic visualization, sometimes called cartographic visualization, is a form of information visualization in which principles from cartography, geographic information systems (GIS), exploratory data analysis (EDA), and information visualization more generally are integrated in the development and assessment of visual methods that facilitate the exploration, analysis, synthesis, and presentation of georeferenced information. The authors report on development and use of one component of a prototype GVis environment designed to facilitate exploration, by domain experts, of time series multivariate georeferenced health statistics. Emphasis is on how manipulable dynamic GVis tools may facilitate visual thinking, pattern noticing, and hypothesis generation. The prototype facilitates the highlighting of data extremes, examination of change in geographic patterns over time, and exploration of similarity among georeferenced variables. A qualitative exploratory analysis of verbal protocols and transaction logs is used to characterize system use. Evidence produced through the characterization highlights differences among experts in data analysis strategies (particularly in relation to the use of attribute “focusing” combined with time series animation) and corresponding differences in success at noticing spatiotemporal patterns	MacEachren, A.M.;Boscoe, F.P.;Haug, D.;Pickle, L.W.	Dept. of Geogr., Pennsylvania State Univ., University Park, PA, USA|c|;;;	37374699000;37374699200;37374702200;37374700700
	InfoVis	19-20 Oct 1998	The shape of Shakespeare: visualizing text using implicit surfaces	10.1109/INFVIS.1998.729568	http://dx.doi.org/10.1109/INFVIS.1998.729568	121	129, 160	729568	data visualisation;text analysis	3D arrangement;connected shape;corpus meta-data;document content;document content mapping;document corpus;document relationships;free-form text;global content differences;global content similarities;global contextual information;implicit surfaces;information visualization;inter-document relationships;local contextual information;nonvisual information;shape;similarity scoring;text visualization;transparent clusters;visual clustering method	Amorphous materials;Animation;Humans;Information analysis;Navigation;Neural networks;Scattering;Self organizing feature maps;Shape control;Visualization		Information visualization focuses on the use of visual means for exploring non-visual information. While free-form text is a rich, common source of information, visualization of text is a challenging problem since text is inherently non-spatial. The paper explores the use of implicit surface models for visualizing text. The authors describe several techniques for text visualization that aid in understanding document content and document relationships. A simple method is defined for mapping document content to shape. By comparing the shapes of multiple documents, global content similarities and differences may be noted. In addition, they describe a visual clustering method in which documents are arranged in 3D based upon similarity scoring. Documents deemed closely related blend together as a single connected shape. Hence, a document corpus becomes a collection of shapes that reflect inter-document relationships. These techniques provide methods to visualize individual documents as well as corpus meta-data. They then combine the two techniques to produce transparent clusters enclosing individual document shapes. This provides a way to visualize both local and global contextual information. Finally, they elaborate on several potential applications of these methods	Rohrer, R.M.;Ebert, D.S.;Sibert, J.L.	Dept. of Electr. Eng. & Comput. Sci., George Washington Univ., Washington, DC, USA|c|;;	37373506600;38472155600;37265573100
	InfoVis	19-20 Oct 1998	Dynamic aggregation with circular visual designs	10.1109/INFVIS.1998.729557	http://dx.doi.org/10.1109/INFVIS.1998.729557	35	43, 151	729557	data visualisation;interactive systems;spatial data structures;user interfaces	aggregate levels;aggregation level control;aggregation methods;aggregation process;automatic aggregation;binning;circular visual designs;context maintenance;continuous change;dynamic aggregation;feedback;interactive manipulation;large data set management;spatially based aggregates;visual representation	Aggregates;Computer science;Data visualization;Displays;Electrical capacitance tomography;Encoding;Usability		One very effective method for managing large data sets is aggregation or binning. We consider two aggregation methods that are tightly coupled with interactive manipulation and the visual representation of the data. Through this integration we hope to provide effective support for the aggregation process, specifically by enabling: 1) automatic aggregation, 2) continuous change and control of the aggregation level, 3) spatially based aggregates, 4) context maintenance across different aggregate levels, and 5) feedback on the level of aggregation	Chuah, M.C.	Sch. of Comput. Sci., Carnegie Mellon Univ., Pittsburgh, PA, USA|c|	37343565300
	InfoVis	19-20 Oct 1998	LensBar-visualization for browsing and filtering large lists of data	10.1109/INFVIS.1998.729567	http://dx.doi.org/10.1109/INFVIS.1998.729567	113	120, 159	729567	data visualisation;graphical user interfaces;information retrieval;list processing	LensBar;browsing;filtering features;graphical interface tool;information retrieval;large data list filtering;large data list visualization;querying;scroll window;slide;textual data;zooming features	Computer science;Control systems;Data visualization;Information filtering;Information filters;Information retrieval;Laboratories;Motion pictures;Pattern matching;Production systems		The author proposes a simple and powerful graphical interface tool called the LensBar for filtering and visualizing large lists of data. Browsing and querying are the most important tasks in retrieving information and LensBar integrates the two techniques into a simple scroll window with slider. While it looks familiar to users of conventional graphical interface tools, its filtering and zooming features offer sophisticated handling of large lists of textual data	Masui, T.	Sony Comput. Sci. Labs. Inc., Tokyo, Japan|c|	37357771900
	InfoVis	19-20 Oct 1998	Comparative visualization of protein structure-sequence alignments	10.1109/INFVIS.1998.729566	http://dx.doi.org/10.1109/INFVIS.1998.729566	106	110, 158	729566	biology computing;data visualisation;molecular biophysics;molecular configurations;proteins	3D shape prediction;abstract shapes;amino acid side chain rotation;comparative streamline visualization;comparative visualization;glyphs;high resolution display;low resolution molecular graphics tool;low resolution representation;protein alignment information mapping;protein fold recognition;protein similarity;protein structure-sequence alignments;spatial orientations;target sequence;threading	Amino acids;Bioinformatics;Computer science;Data visualization;Displays;Prediction methods;Protein engineering;Shape;Spatial resolution;Target recognition			Hansen, M.;Meads, D.;Pang, A.	Dept. of Comput. Sci., California Univ., CA, USA|c|;;	37374024300;37374564600;37267352000
	InfoVis	19-20 Oct 1998	Visualizing decision table classifiers	10.1109/INFVIS.1998.729565	http://dx.doi.org/10.1109/INFVIS.1998.729565	102	105, 157	729565	data visualisation;decision tables;learning (artificial intelligence);pattern classification	attributes;classification models;decision table classifier visualization;dimensional stacking;hierarchical table;interaction;machine learning algorithms;prediction	Classification tree analysis;Data mining;Data visualization;Decision trees;Displays;Graphics;Layout;Predictive models;Stacking;Training data		Decision tables, like decision trees or neural nets, are classification models used for prediction. They are induced by machine learning algorithms. A decision table consists of a hierarchical table in which each entry in a higher level table gets broken down by the values of a pair of additional attributes to form another table. The structure is similar to dimensional stacking. A visualization method is presented that allows a model based on many attributes to be understood even by those unfamiliar with machine learning. Various forms of interaction are used to make this visualization more useful than other static designs	Becker, B.G.	Silicon Graphics Inc., Mountain View, CA, USA|c|	37363099200
	InfoVis	19-20 Oct 1998	Similarity clustering of dimensions for an enhanced visualization of multidimensional data	10.1109/INFVIS.1998.729559	http://dx.doi.org/10.1109/INFVIS.1998.729559	52	60, 153	729559	computational complexity;data mining;data visualisation	NP complete;data dimensions;enhanced visualization;global similarity;hard problems;heuristic algorithms;multidimensional data;parallel coordinates;recursive pattern;scatterplots;similar behavior;similarity clustering;similarity measures;systematic approach;two dimensional arrangement;visualization techniques	Data visualization;Euclidean distance;Humans;Laboratories;Multidimensional systems;Shape		The order and arrangement of dimensions (variates) is crucial for the effectiveness of a large number of visualization techniques such as parallel coordinates, scatterplots, recursive pattern, and many others. We describe a systematic approach to arrange the dimensions according to their similarity. The basic idea is to rearrange the data dimensions such that dimensions showing a similar behavior are positioned next to each other. For the similarity clustering of dimensions, we need to define similarity measures which determine the partial or global similarity of dimensions. We then consider the problem of finding an optimal one- or two-dimensional arrangement of the dimensions based on their similarity. Theoretical considerations show that both, the one- and the two-dimensional arrangement problem are surprisingly hard problems, i.e. they are NP complete. Our solution of the problem is therefore based on heuristic algorithms. An empirical evaluation using a number of different visualization techniques shows the high impact of our similarity clustering of dimensions on the visualization results	Ankerst, M.;Berchtold, S.;Keim, D.A.	Munich Univ., Germany|c|;;	37371609900;37374589200;37283138700
	InfoVis	19-20 Oct 1998	WEBPATH-a three dimensional Web history	10.1109/INFVIS.1998.729553	http://dx.doi.org/10.1109/INFVIS.1998.729553	3	10, 148	729553	Internet;data visualisation;information retrieval;online front-ends;real-time systems;virtual reality	WEBPATH;WWW browsers;WWW users;browse history;browsing;history mechanisms;searching techniques;tailorable real time visualisation;three dimensional Web history;traditional Web browsers;usability studies;virtual reality based application	Chromium;Computer graphics;Computer science;Electrical capacitance tomography;History;Navigation;Usability;Virtual reality;Visualization;World Wide Web		A number of usability studies report that many users of the WWW cannot find pages already visited, additionally many users cannot visualise where they are, or where they have been browsing. Currently, readily available WWW browsers provide history mechanisms that offer little or no support in the presentation and manipulation of visited sites. Manipulation and presentation of usage data, such as a browse history has been used in a number of cases to aid users in searching for previously attained data, and to teach or assist other users in their browse or searching techniques. The paper presents a virtual reality (VR) based application to be used alongside traditional Web browsers, which provides them with a flexibly tailorable real time visualisation of their history	Frecon, E.;Smith, G.	Swedish Inst. of Comput. Sci., Sweden|c|;	37265491900;37365189200
	InfoVis	19-20 Oct 1998	IVORY-an object-oriented framework for physics-based information visualization in Java	10.1109/INFVIS.1998.729562	http://dx.doi.org/10.1109/INFVIS.1998.729562	79	86, 155	729562	Java;client-server systems;data visualisation;interactive systems;physics computing;virtual reality languages	IVML;IVORY;Java;NT 4;VRML 2 exports;VRML plugged-in WWW browser;advanced plug-in mechanism;client server setup;haptic interface;information visualization applications;interactive visualization examples;multidimensional graph layout;object oriented framework;physics based visualization;platform-independent framework;script language;thin clients;visual metaphors	Application software;Computer science;Data mining;Data visualization;Haptic interfaces;Java;Libraries;Multidimensional systems;Object oriented modeling;Plugs		We present IVORY a newly developed, platform-independent framework for physics based visualization. IVORY is especially designed for information visualization applications and multidimensional graph layout. It is fully implemented in Java 1.1 and its architecture features client server setup, which allows us to run the visualization even on thin clients. In addition, VRML 2.0 exports can be viewed by any VRML plugged-in WWW browser. Individual visual metaphors are invoked into IVORY via an advanced plug-in mechanism, where plug-ins can be implemented by any experienced user. The configuration of IVORY is accomplished using a script language, called IVML. Some interactive visualization examples, such as the integration of a haptic interface illustrate the performance and versatility of our system. Our current implementation supports NT 4.0	Sprenger, T.C.;Gross, M.H.;Bielser, D.	Dept. of Comput. Sci., Fed. Inst. of Technol., Zurich, Switzerland|c|;;	37374656700;37275694700;37374656400
	InfoVis	19-20 Oct 1998	Multi-faceted insight through interoperable visual information analysis paradigms	10.1109/INFVIS.1998.729570	http://dx.doi.org/10.1109/INFVIS.1998.729570	137	144, 161	729570	data visualisation;information analysis	complex information collections;information exploration;information overload;information visualization;integrated methods;interoperable visual information analysis paradigms;large information collection;multi-faceted insight	Data visualization;Filters;Government;Information analysis;Information retrieval;Laboratories;Ontologies;Prototypes;Taxonomy;Text analysis		To gain insight and understanding of complex information collections, users must be able to visualize and explore many facets of the information. The paper presents several novel visual methods from an information analyst's perspective. The authors present a sample scenario, using the various methods to gain a variety of insights from a large information collection. They conclude that no single paradigm or visual method is sufficient for many analytical tasks. Often a suite of integrated methods offers a better analytic environment in today's emerging culture of information overload and rapidly changing issues. They also conclude that the interactions among these visual paradigms are equally as important as, if not more important than, the paradigms themselves	Hetzler, B.;Whitney, P.;Martucci, L.;Thomas, J.	Pacific Northwest Lab., Richland, WA, USA|c|;;;	37374614800;37363311600;37372798600;37273308900
	InfoVis	19-20 Oct 1998	BiblioMapper: a cluster-based information visualization technique	10.1109/INFVIS.1998.729569	http://dx.doi.org/10.1109/INFVIS.1998.729569	130	136	729569	Internet;bibliographic systems;data visualisation;document image processing;information needs;information retrieval	BiblioMapper;CISI collections;Internet;bibliographic databases;cluster-based information visualization technique;document space;document space visualization;information needs;semantic similarities	Clustering algorithms;Data visualization;Information retrieval;Information science;Libraries;Navigation;Neural networks;Self organizing feature maps;Testing;Visual databases		The purpose of the paper is to develop a visualization system of a document space, called BiblioMapper, for CISI collections, one of the bibliographic databases available on the Internet. The major function of BiblioMapper is to visualize the document space with a cluster-based visualization technique. The cluster-based visualization technique assembles a set of documents according to semantic similarities. One advantage of this technique is that users are able to focus on and assess each cluster and the documents which the cluster comprises according to their information needs	Min Song	Sch. of Libr. & Inf. Sci., Indiana Univ., Bloomington, IN, USA|c|	37366020600
	InfoVis	19-20 Oct 1998	An interactive view for hierarchical clustering	10.1109/INFVIS.1998.729556	http://dx.doi.org/10.1109/INFVIS.1998.729556	26	31, 150	729556	data analysis;interactive systems;pattern clustering;program visualisation	cluster numbers;clustering method;clustering tree;compact representation;hierarchical clustering;interactive control;interactive view;linked views environment;radical re-drawing;rectangular area;space filling recursive division;visual method;visualization	Algorithm design and analysis;Animals;Binary trees;Clustering algorithms;Data analysis;Electronic mail;Identity-based encryption;Iris;Leg;Length measurement		The paper describes a visualization of a general hierarchical clustering algorithm that allows the user to manipulate the number of classes produced by the clustering method without requiring a radical re-drawing of the clustering tree. The visual method used, a space filling recursive division of a rectangular area, keeps the items under consideration at the same screen position, even while the number of classes is under interactive control. As well as presenting a compact representation of the clustering with different cluster numbers, this method is particularly useful in a linked views environment where additional information can be added to a display to encode other information, without this added level of detail being perturbed when changes are made to the number of clusters	Wills, G.J.	Lucent Technol., AT&T Bell Labs., Naperville, IL, USA|c|	37844196100
	InfoVis	19-20 Oct 1998	Saying it in graphics: from intentions to visualizations	10.1109/INFVIS.1998.729564	http://dx.doi.org/10.1109/INFVIS.1998.729564	97	101	729564	data visualisation;multimedia computing	assertions;automatic communicative goal realization;content structures;graphics;intentions;multimedia argument;rhetorical structures;task model;textual argument redesign;visualizations	Auditory system;Computer industry;Data visualization;Design methodology;Ear;Electronic switching systems;Graphics;Tellurium;Testing;Vehicles		The authors propose a methodology for automatically realizing communicative goals in graphics. It features a task model that mediates the communicative intent and the selection of graphical techniques. The methodology supports the following functions: isolating assertions presentable in graphics; mapping such assertions into tasks for the potential reader, and selecting graphical techniques that support those tasks. They illustrate the methodology by redesigning a textual argument into a multimedia one with the same rhetorical and content structures but employing graphics to achieve some of the intentions	Kerpedjiev, S.;Carenini, G.;Green, N.;Moore, J.;Roth, S.	Carnegie Mellon Univ., Pittsburgh, PA, USA|c|;;;;	37374604500;37374602500;37368889100;37359522500;37357418100
	InfoVis	19-20 Oct 1998	Reconfigurable disc trees for visualizing large hierarchical information space	10.1109/INFVIS.1998.729555	http://dx.doi.org/10.1109/INFVIS.1998.729555	19	25, 149	729555	data visualisation;tree data structures;trees (mathematics);user interfaces	2D layouts;3D depth;3D layouts;RDT;Reconfigurable Disc Tree;VISIT;Visual Information System for reconfigurable dIsc tree;animation;browsing;children;compact disc tree;cone trees;displayed nodes;foreground objects;large hierarchical information space visualization;large hierarchies;occluded region;plane disc tree;reconfigurable disc trees;tree reconfiguration;tree shading;tree transformation;user interface features;user navigation capabilities;user perception;viewing transformation;visual overlapping;visualization system;visualization technique	Animation;Computer science;Information systems;Lapping;Maintenance engineering;Shape;Space technology;Tree graphs;User interfaces;Visualization		We present a new visualization technique, called RDT (Reconfigurable Disc Tree) which can alleviate the disadvantages of cone trees significantly for large hierarchies while maintaining its context of using 3D depth. In RDT, each node is associated with a disc, around which its children are placed. Using discs instead of cones as the basic shape in RDT has several advantages: significant reduction of occluded region, sharp increase in number of displayed nodes, and easy projection onto plane without visual overlapping. We show that RDT can greatly enhance user perception by transforming its shapes dynamically in several ways: (1) disc tree which can significantly reduce the occluded region by the foreground objects; (2) compact disc tree which can increase the number of nodes displayed on the screen; and (3) plane disc tree which can be mapped onto the plane without visual overlapping. We describe an implementation of our visualization system called VISIT (Visual Information System for reconfigurable dIsc tree). It provides 2D and 3D layouts for RDT and various user interface features such as tree reconfiguration, tree transformation, tree shading, viewing transformation, animation, selection and browsing which can enhance the user perception and navigation capabilities. We also evaluate our system using the following three metrics: percentage of occlusion, density of displayed nodes on a screen, and number of identifiable nodes	Chang-Sung Jeong;Pang, A.	Dept. of Electron. Eng., Korea Univ., Seoul, South Korea|c|;	37360232800;37267352000
	InfoVis	19-20 Oct 1998	An operator interaction framework for visualization systems	10.1109/INFVIS.1998.729560	http://dx.doi.org/10.1109/INFVIS.1998.729560	63	70	729560	data visualisation;human factors;interactive systems;spreadsheet programs;user interfaces	analysis tasks;data domains;design space;end users;graphic representations;information visualization;interactive techniques;operator interaction framework;representation methods;visualization community;visualization operators;visualization spreadsheet;visualization systems;visualization techniques	Computer science;Couplings;Data visualization;Displays;File systems;Filters;Graphics;Humans;Pipelines;Scattering		Information visualization encounters a wide variety of different data domains. The visualization community has developed representation methods and interactive techniques. As a community, we have realized that the requirements in each domain are often dramatically different. In order to easily apply existing methods, researchers have developed a semiology of graphic representations. We have extended this research into a framework that includes operators and interactions in visualization systems, such as a visualization spreadsheet. We discuss properties of this framework and use it to characterize operations spanning a variety of different visualization techniques. The framework developed in the paper enables a new way of exploring and evaluating the design space of visualization operators, and helps end users in their analysis tasks	Ed Huai-Hsin Chi;Riedl, J.T.	Dept. of Comput. Sci. & Eng., Minnesota Univ., MN, USA|c|;	37362911700;37282873000
	InfoVis	19-20 Oct 1998	The generalized detail in-context problem	10.1109/INFVIS.1998.729558	http://dx.doi.org/10.1109/INFVIS.1998.729558	44	51, 152	729558	data visualisation;interactive systems;user interfaces	continuous nonlinear magnification system;general formulation;generalized detail in-context problem;information content;information space;multiple global views;nonlinear magnification systems;seamless multi level views	Data visualization;Ear;Electrical capacitance tomography;Laboratories;Nonlinear distortion;Scattering;Terminology;Two dimensional displays		The paper describes a general formulation of the “detail-in-context” problem, which is a central issue of fundamental importance to a wide variety of nonlinear magnification systems. A number of tools are described for dealing with this problem effectively. These tools can be applied to any continuous nonlinear magnification system, and are not tied to specific implementation features of the system that produced the original transformation. Of particular interest is the development of “seamless multi level views”, which allow multiple global views of an information space (each having different information content) to be integrated into a single view without discontinuity	Keahey, T.A.	Los Alamos Nat. Lab., NM, USA|c|	37349418300
	InfoVis	19-20 Oct 1998	Traversal-based visualization of data structures	10.1109/INFVIS.1998.729554	http://dx.doi.org/10.1109/INFVIS.1998.729554	11	18	729554	computer animation;data structures;formal specification;graphical user interfaces;high level languages;program debugging;program visualisation	algorithm animation systems;algorithm animations;data structure visualization;declarative language;graphical debuggers;graphical user interface;informative visualizations;on-screen representation;program state;semantic content;symbolic information;traversal based visualization;traversal specifications;user augmented source code;user supplied patterns;visual representations	Animation;Chromium;Computer science;Data structures;Data visualization;Debugging;Displays;Graphical user interfaces;Image generation;Software algorithms		Algorithm animation systems and graphical debuggers perform the task of translating program state into visual representations. While algorithm animations typically rely on user augmented source code to produce visualizations, debuggers make use of symbolic information in the target program. As a result, visualizations produced by debuggers often lack important semantic content, making them inferior to algorithm animation systems. The paper presents a method to provide higher level, more informative visualizations in a debugger using a technique called traversal based visualization. The debugger traverses a data structure using a set of user supplied patterns to identify parts of the data structure to be drawn a similar way. A declarative language is used to specify the patterns and the actions to take when the patterns are encountered. Alternatively, the user can construct traversal specifications through a graphical user interface to the declarative language. Furthermore, the debugger supports modification of data. Changes made to the on-screen representation are reflected in the underlying data	Korn, J.L.;Appel, A.W.	Dept. of Comput. Sci., Princeton Univ., NJ, USA|c|;	38179814200;37339549500
	SciVis	24-24 Oct. 1998	Large scale terrain visualization using the restricted quadtree triangulation	10.1109/VISUAL.1998.745280	http://dx.doi.org/10.1109/VISUAL.1998.745280	19	26	745280	data visualisation;quadtrees;real-time systems;rendering (computer graphics);visual databases	adaptive surface triangulation;algorithms;all-in-one visualization system;dynamic scene management;exact error approximation;graphics load;interactive visualization;large scale terrain visualization;levels of detail;performance enhancement;progressive meshing;real-time rendering;restricted quadtree triangulation;spatial access;spatial data handling;terrain database access;triangulated surfaces;very large scale grid digital elevation models;visible scene management	Adaptive control;Digital elevation models;Graphics;Large-scale systems;Layout;Programmable control;Rendering (computer graphics);Spatial databases;Visual databases;Visualization		Real-time rendering of triangulated surfaces has attracted growing interest in the last few years. However, interactive visualization of very large scale grid digital elevation models is still difficult. The graphics load must be controlled by adaptive surface triangulation and by taking advantage of different levels of detail. Furthermore, management of the visible scene requires efficient access to the terrain database. We describe an all-in-one visualization system which integrates adaptive triangulation, dynamic scene management and spatial data handling. The triangulation model is based on the restricted quadtree triangulation. Furthermore, we present new algorithms of restricted quadtree triangulation. These include among others exact error approximation, progressive meshing, performance enhancements and spatial access.	Pajarola, Renato	Inst. of Theor. Comput. Sci., Eidgenossische Tech. Hochschule, Zurich, Switzerland|c|	37282193800
	SciVis	24-24 Oct. 1998	Contour interpolation and surface reconstruction of smooth terrain models	10.1109/VISUAL.1998.745281	http://dx.doi.org/10.1109/VISUAL.1998.745281	27	33	745281	CAD;computational geometry;computer graphics;geographic information systems;interpolation;partial differential equations	C1 continuity;CAD systems;computer graphics;contour gradient;contour height;contour interpolation;contour locations;contour map;geographic information systems;globally smooth surface;gradient controlled partial differential equation surfaces;rational interpolated sub-contours;rational surface;smooth saddle shapes;smooth terrain models;surface branching;surface reconstruction;surface shapes	Chromium;Computer graphics;Differential equations;Extrapolation;Geographic Information Systems;Interpolation;Partial differential equations;Shape control;Solid modeling;Surface reconstruction			Chai, J.;Miyoshi, T.;Nakamae, E.	Sanei Co., Japan|c|;;	37361678300;37276340700;37271893200
	SciVis	24-24 Oct. 1998	Smooth view-dependent level-of-detail control and its application to terrain rendering	10.1109/VISUAL.1998.745282	http://dx.doi.org/10.1109/VISUAL.1998.745282	35	42	745282	computational geometry;data structures;data visualisation;geomorphology;geophysics computing;interpolation;rendering (computer graphics);topography (Earth)	accurate approximating meshes;arbitrary triangle mesh;block refinements;block-based simplification scheme;changing view parameters;flyover;geometrically optimized refinement transformations;graphics workstation;huge terrain grids;large-scale surfaces;local surface geometric complexity adaptation;memory use;output-sensitive data structures;real-time rendering;run-time geomorph creation;smooth geometry interpolation;smooth view-dependent level-of-detail control;temporal coherence;terrain rendering;view-dependent progressive mesh framework	Data structures;Data visualization;Geometry;Graphics;Large-scale systems;Layout;Pipelines;Rain;Rendering (computer graphics);Runtime;Workstations		"The key to real-time rendering of large-scale surfaces is to locally adapt surface geometric complexity to changing view parameters. Several schemes have been developed to address this problem of view-dependent level-of-detail control. Among these, the view-dependent progressive mesh (VDPM) framework represents an arbitrary triangle mesh as a hierarchy of geometrically optimized refinement transformations, from which accurate approximating meshes can be efficiently retrieved. In this paper we extend the general VDPM framework to provide temporal coherence through the run-time creation of geomorphs. These geomorphs eliminate ""popping"" artifacts by smoothly interpolating geometry. Their implementation requires new output-sensitive data structures, which have the added benefit of reducing memory use. We specialize the VDPM framework to the important case of terrain rendering. To handle huge terrain grids, we introduce a block-based simplification scheme that constructs a progressive mesh as a hierarchy of block refinements. We demonstrate the need for an accurate approximation metric during simplification. Our contributions are highlighted in a real-time flyover of a large, rugged terrain. Notably, the use of geomorphs results in visually smooth rendering even at 72 frames/sec on a graphics workstation."	Hoppe, H.	Microsoft Res., USA|c|	37360111200
	SciVis	24-24 Oct. 1998	Efficient implementation of multi-triangulations	10.1109/VISUAL.1998.745283	http://dx.doi.org/10.1109/VISUAL.1998.745283	43	50	745283	computational geometry;data structures;data visualisation;virtual reality	data structures;encoding;large triangle meshes;level of detail management;multi-triangulation;point location;querying;selective refinement;space/performance trade-offs;spatial interference queries;vertex decimation;windowing	Application software;Chromium;Compaction;Computer graphics;Data structures;Encoding;Image coding;Information theory;Interference;Object oriented modeling;Solid modeling;Three dimensional displays;Virtual reality		Multi-triangulation (MT) is a general framework for managing the level-of-detail in large triangle meshes, which we have introduced in our previous work. In this paper, we describe an efficient implementation of an MT based on vertex decimation. We present general techniques for querying an MT, which are independent of a specific application, and which can be applied for solving problems, such as selective refinement, windowing, point location, and other spatial interference queries. We describe alternative data structures for encoding an MT, which achieve different trade-offs between space and performance. Experimental results are discussed.	De Floriani, L.;Magillo, P.;Puppo, E.	Genoa Univ., Italy|c|;;	37283084000;37371970000;37355451700
	SciVis	24-24 Oct. 1998	Visualization of scalar topology for structural enhancement	10.1109/VISUAL.1998.745284	http://dx.doi.org/10.1109/VISUAL.1998.745284	51	58	745284	computational geometry;data visualisation;image processing	global scalar structure;image co-registration;image processing;isocontouring;mesh compression;numerical structure detection;scalar fields;scalar topology visualization;scientific application;structural enhancement	Application software;Data mining;Data visualization;Displays;Image coding;Image processing;Information analysis;Laboratories;Medical services;Scientific computing;Topology		Scalar fields arise in every scientific application. Existing scalar visualization techniques require that the user infers the global scalar structure from what is frequently an insufficient display of information. We present a visualization technique which numerically detects the structure at all scales, removing from the user the responsibility of extracting information implicit in the data, and presenting the structure explicitly for analysis. We further demonstrate how scalar topology detection proves useful for correct visualization and image processing applications such as image co-registration, isocontouring, and mesh compression.	Bajaj, C.L.;Schikore, D.R.	Dept. of Comput. Sci., Texas Univ., Austin, TX, USA|c|;	37282899200;37355637500
	SciVis	24-24 Oct. 1998	A general method for preserving attribute values on simplified meshes	10.1109/VISUAL.1998.745285	http://dx.doi.org/10.1109/VISUAL.1998.745285	59	66	745285	computational geometry;image texture;rendering (computer graphics)	2D textures;3D meshes;3D models;attribute value preservation;bump maps;detail preservation;rendering;simplified mesh;simplified mesh face;texture maps;triangular texture patches	Application software;Chromium;Computer displays;Computer graphics;Degradation;Frequency;Image converters;Image generation;Merging;Rendering (computer graphics);Shape control;Surface texture;Visualization		Many sophisticated solutions have been proposed to reduce the geometric complexity of 3D meshes. A problem studied less often is how to preserve on a simplified mesh the detail (e.g., color, high frequency shape detail, scalar fields, etc.) which is encoded in the original mesh. We present a general approach for preserving detail on simplified meshes. The detail (or high frequency information) lost after simplification is encoded through texture or bump maps. The original contribution is that preservation is performed after simplification, by building set of triangular texture patches that are then packed in a single texture map. Each simplified mesh face is sampled to build the associated triangular texture patch; a new method for storing this set of texture patches into a standard rectangular texture is presented and discussed. Our detail preserving approach makes no assumptions about the simplification process adopted to reduce mesh complexity and allows highly efficient rendering. The solution is very general, allowing preservation of any attribute value defined on the high resolution mesh. We also describe an alternative application: the conversion of 3D models with 3D static procedural textures into standard 3D models with 2D textures.	Cignoni, P.;Montani, C.;Rocchini, C.;Scopigno, R.	Ist. di Elaborazione dell''Inf., Italy|c|;;;	37265783400;37371973400;37333160000;37270887900
	SciVis	24-24 Oct. 1998	Surface reconstruction with anisotropic density-scaled alpha shapes	10.1109/VISUAL.1998.745286	http://dx.doi.org/10.1109/VISUAL.1998.745286	67	72	745286	computational geometry;computer graphics;image reconstruction;mesh generation	α-ball form modulation;3D model generation;anisotropic density-scaled alpha shapes;anisotropic shaping;computer graphics;density scaling;discontinuous regions;point normals;surface reconstruction;unorganized point set	Anisotropic magnetoresistance;Computer science;Image reconstruction;Laboratories;Rough surfaces;Sampling methods;Shape;Surface reconstruction;Surface roughness;Tensile stress		Generation of a three-dimensional model from an unorganized set of points is an active area of research in computer graphics. Alpha shapes can be employed to construct a surface which most closely reflects the object described by the points. However, no α-shape, for any value of α, can properly detail discontinuous regions of a model. We introduce herein two methods of improving the results of reconstruction using α-shapes: density-scaling, which modulates the value of a depending on the density of points in a region; and anisotropic shaping, which modulates the form of the α-ball based on point normals. We give experimental results that show the successes and limitations of our method.	Teichmann, M.;Capps, M.	Lab. for Comput. Sci., MIT, Cambridge, MA, USA|c|;	37369233200;37347604500
	SciVis	24-24 Oct. 1998	Level of detail visualization of scalar data sets on irregular surface meshes	10.1109/VISUAL.1998.745287	http://dx.doi.org/10.1109/VISUAL.1998.745287	73	77	745287	computational geometry;data visualisation;piecewise linear techniques;wavelet transforms	approximation error;base mesh simplification;continuous piecewise linear functions;decimation sequence;detail coefficients;exact reconstruction;irregular surface meshes;level of detail visualization;multi-resolution framework;non-nested approximation spaces;removed vertices;scalar data sets;spherical meshes;triangular planar meshes;wavelet-based techniques	Approximation error;Continuous wavelet transforms;Cost function;Data visualization;Greedy algorithms;Piecewise linear approximation;Piecewise linear techniques;Reconstruction algorithms;Surface reconstruction;Tail;Tin		In this article, we build a multi-resolution framework intended to be used for the visualization of continuous piecewise linear functions defined over triangular planar or spherical meshes. In particular, the data set can be viewed at different level of detail, that's to say as a piecewise linear function defined over any simplification of the base mesh. In his multi-resolution form, the function requires strictly the same volume of data than the original input: It is then possible to go through consecutive levels by the use of so-called detail coefficients, with exact reconstruction if desired. We also show how to choose a decimation sequence that leads to a good compromise between the resulting approximation error and the number of removed vertices. The theoretical tools used here are inspired from wavelet-based techniques and extended in the sense that they can handle non-nested approximation spaces.	Bonneau, G.-P.;Gerussi, A.	CNRS, Grenoble, France|c|;	37368609100;37371998500
	SciVis	24-24 Oct. 1998	Tracking scalar features in unstructured data sets	10.1109/VISUAL.1998.745288	http://dx.doi.org/10.1109/VISUAL.1998.745288	79	86	745288	computer vision;data structures;data visualisation;feature extraction;tracking	3D time-varying structured data sets;3D time-varying unstructured data sets;adaptive grids;algorithm;amorphous regions;connected regions;curvilinear grids;data structure;hybrid grids;regions of interest;scalar feature tracking;structured grids;visualization;volume feature tracking	Amorphous materials;Computational fluid dynamics;Computational modeling;Computer vision;Data analysis;Data engineering;Data structures;Data visualization;Displays;Feature extraction;Silver		3D time-varying unstructured and structured data sets are difficult to visualize and analyze because of the immense amount of data involved. These data sets contain many evolving amorphous regions, and standard visualization techniques provide no facilities to aid the scientist to follow regions of interest. In this paper, we present a basic framework for the visualization of time-varying data sets, and a new algorithm and data structure to track volume features in unstructured scalar data sets. The algorithm and data structure are general and can be used for structured, curvilinear, adaptive and hybrid grids as well. The features tracked can be any type of connected regions. Examples are shown from ongoing research.	Silver, D.;Wang, X.	Dept. of Electr. & Comput. Eng., Rutgers Univ., Piscataway, NJ, USA|c|;	37274132700;37367862600
	SciVis	24-24 Oct. 1998	Feature detection in linked derived spaces	10.1109/VISUAL.1998.745289	http://dx.doi.org/10.1109/VISUAL.1998.745289	87	94	745289	computational fluid dynamics;data visualisation;digital simulation;feature extraction;flow simulation;flow visualisation	brushing;coordinate systems;data interaction;data plotting;exploratory visualization;feature detection;highlighted subsets;linked derived spaces;linking;logical operators;multiple metric spaces;multivariate data;portraits;spatial connectivity;temporal variability;time-varying computational fluid dynamics data sets	Computational fluid dynamics;Computational modeling;Computer interfaces;Computer vision;Data visualization;Design engineering;Information retrieval;Information systems;NASA;Software tools;Space technology		This paper describes by example a strategy for plotting and interacting with data in multiple metric spaces. The example system was designed for use with time-varying computational fluid dynamics (CFD) data sets, but the methodology is directly applicable to other types of field data. The central objects embodied by the tool are portraits, which show the data in various coordinate systems, while preserving their spatial connectivity and temporal variability. The coordinates are derived in various ways from the field data, and an important feature is that new and derived portraits can be created interactively. The primary operations supported by the tool are brushing and linking: the user can select a subset of a given portrait, and this subset is highlighted in all portraits. The user can combine highlighted subsets from an arbitrary number of portraits with the usual logical operators, thereby indicating where an arbitrarily complex set of conditions holds. The system is useful for exploratory visualization and feature detection in multivariate data.	Henze, C.	NASA Ames Res. Center, Moffett Field, CA, USA|c|	37373380100
	SciVis	24-24 Oct. 1998	Extremal feature extraction from 3-D vector and noisy scalar fields	10.1109/VISUAL.1998.745290	http://dx.doi.org/10.1109/VISUAL.1998.745290	95	102	745290	curve fitting;data visualisation;dentistry;feature extraction;mesh generation;noise;shock waves;surface fitting;tensors;terrain mapping;vectors;vortices	3D space curves;abrupt direction changes;abrupt velocity changes;anatomical lines;coherent features;coherent surfaces;complex surfaces;connected oriented nonintersecting polyline segments;crest-lines;dense vector fields;digital 3D potential vector field;digital terrain map;extremal curve algorithm;extremal feature extraction;extremal surface algorithm;field visualization;flow field;grooves;hole-free triangulation mesh;inaccurate scalar field;inaccurate vector field;interacting vortex cores;iso-surface techniques;local extrema;low-resolution data;marching cubes algorithm;noisy dental data;noisy scalar field;regular 3D grid;ridges;sampling density;shock waves;spurious noisy samples;surface fitting;tensor voting;velocity field;volume data;volume densification;volume vectorization;vorticity lines;voxel	Data mining;Data visualization;Dentistry;Feature extraction;Intelligent robots;Noise shaping;Shape;Shock waves;Surface fitting;Surface waves;Tensile stress;Voting		"We are interested in feature extraction from volume data in terms of coherent surfaces and 3D space curves. The input can be an inaccurate scalar or vector field, sampled densely or sparsely on a regular 3D grid, in which poor resolution and the presence of spurious noisy samples make traditional iso-surface techniques inappropriate. In this paper, we present a general-purpose methodology to extract surfaces or curves from a digital 3D potential vector field {(s,v~)}, in which each voxel holds a scalar s designating the strength and a vector v~ indicating the direction. For scalar, sparse or low-resolution data, we ""vectorize"" and ""densify"" the volume by tensor voting to produce dense vector fields that are suitable as input to our algorithms, the extremal surface and curve algorithms. Both algorithms extract, with sub-voxel precision, coherent features representing local extrema in the given vector field. These coherent features are a hole-free triangulation mesh (in the surface case), and a set of connected, oriented and non-intersecting polyline segments (in the curve case). We demonstrate the general usefulness of both extremal algorithms on a variety of real data by properly extracting their inherent extremal properties, such as (a) shock waves induced by abrupt velocity or direction changes in a flow field, (b) interacting vortex cores and vorticity lines in a velocity field, (c) crest-lines and ridges implicit in a digital terrain map, and (d) grooves, anatomical lines and complex surfaces from noisy dental data."	Chi-Keung Tang;Medioni, G.	Univ. of Southern California, Los Angeles, CA, USA|c|;	37281400600;37280043300
	SciVis	24-24 Oct. 1998	Feature comparisons of vector fields using Earth mover&#39;s distance	10.1109/VISUAL.1998.745291	http://dx.doi.org/10.1109/VISUAL.1998.745291	103	109	745291	computational fluid dynamics;critical points;data compression;data visualisation;natural sciences computing;topology;vectors	Earth mover's distance;closed set;closeness measure;critical points;data compression;database searching;feature comparisons;flow fields;higher-order nonlinear vector fields;nearest-neighbor query;similar topologies;time-varying data sets;unique patterns;unit circle;vector field topology	Earth;Electric variables measurement;Extraterrestrial measurements;Fluid flow measurement;Humans;Navigation;Physics;Spatial databases;Stress;Topology		"A novel approach is introduced to define a quantitative measure of closeness between vector fields. The usefulness of this measurement can be seen when comparing computational and experimental flow fields under the same conditions. Furthermore, its applicability can be extended to more cumbersome tasks, such as navigating through a large database, searching for similar topologies. This new measure relies on the use of critical points, which are a key feature in vector field topology. In order to characterize critical points, α and β parameters are introduced. They are used to form a closed set of eight unique patterns for simple critical points. These patterns are also basic building blocks for higher-order nonlinear vector fields. In order to study and compare a given set of vector fields, a measure of distance between different patterns of critical points is introduced. The basic patterns of critical points are mapped onto a unit circle in α-β space. The concept of the ""Earth mover's distance"" is used to compute the closeness between various pairs of vector fields, and a nearest-neighbor query is thus produced to illustrate the relationship between the given set of vector fields. This approach quantitatively measures the similarity and dissimilarity between vector fields. It is ideal for data compression of a large flow field, since only the number and types of critical points along with their corresponding α and β parameters are necessary to reconstruct the whole field. It can also be used to better quantify the changes in time-varying data sets."	Lavin, Y.;Batra, R.;Hesselink, Lambertus	Dept. of Phys., Stanford Univ., CA, USA|c|;;	37372012600;37282555600;37274095200
	SciVis	24-24 Oct. 1998	Building perceptual textures to visualize multidimensional datasets	10.1109/VISUAL.1998.745292	http://dx.doi.org/10.1109/VISUAL.1998.745292	111	118	745292	computer vision;data visualisation;geophysics computing;human factors;image texture;oceanography;psychology;storms;visual perception	3D height field;cognitive psychology;computer graphics;computer vision;density;experimental design;hue;human vision;intensity;multidimensional dataset visualization;northern Pacific ocean;ocean conditions analysis;oceanography;perceptual texture patterns;pexels;preattentive processing;regularity;scientific visualization;simultaneously displayed attribute values;southeast Asia;texture dimensions;typhoon activity tracking;visual interference	Asia;Computer graphics;Computer vision;Data visualization;Displays;Interference;Multidimensional systems;Psychology;Sea measurements;Typhoons		Presents a new method for using texture to visualize multi-dimensional data elements arranged on an underlying 3D height field. We hope to use simple texture patterns in combination with other visual features like hue and intensity to increase the number of attribute values we can display simultaneously. Our technique builds perceptual texture elements (or pexels) to represent each data element. Attribute values encoded in the data element are used to vary the appearance of a corresponding pexel. Texture patterns that form when the pexels are displayed can be used to rapidly and accurately explore the dataset. Our pexels are built by controlling three separate texture dimensions: height, density and regularity. Results from computer graphics, computer vision and cognitive psychology have identified these dimensions as important for the formation of perceptual texture patterns. We conducted a set of controlled experiments to measure the effectiveness of these dimensions, and to identify any visual interference that may occur when all three are displayed simultaneously at the same spatial location. Results from our experiments show that these dimensions can be used in specific combinations to form perceptual textures for visualizing multidimensional datasets. We demonstrate the effectiveness of our technique by applying it to two real-world visualization environments: tracking typhoon activity in southeast Asia, and analyzing ocean conditions in the northern Pacific.	Healey, Christopher G.;Enns, J.T.	North Carolina State Univ., Raleigh, NC, USA|c|;	37298406100;37372253300
	SciVis	24-24 Oct. 1998	Efficient co-triangulation of large data sets	10.1109/VISUAL.1998.745293	http://dx.doi.org/10.1109/VISUAL.1998.745293	119	126	745293	approximation theory;computational complexity;computational geometry;error analysis;interpolation;mesh generation;spatial data structures	Delaunay triangulation;adapted dynamic data structures;approximation algorithm;asymptotic time complexity;caching;co-triangulation;computational geometry;dependent variables;error tolerance values;higher-dimensional approximation;independently sampled variables;iterative algorithm;large data sets;linear interpolation;multidimensional approximation;multivariate function reconstruction;running times;scattered data;vertices	Acoustic scattering;Algorithm design and analysis;Approximation algorithms;Biomedical imaging;Computational geometry;Heuristic algorithms;Humidity;Image reconstruction;Interpolation;Iterative algorithms;Land surface temperature;Magnetic resonance imaging;Multidimensional systems		Presents an efficient algorithm for the reconstruction of a multivariate function from multiple sets of scattered data. Given N sets of scattered data representing N distinct dependent variables that have been sampled independently over a common domain and N error tolerance values, the algorithm constructs a triangulation of the domain of the data and associates multivariate values with the vertices of the triangulation. The resulting linear interpolation of these multivariate values yields a multivariate function, called a co-triangulation, that represents all of the dependent data up to the given error tolerance. A simple iterative algorithm for the construction of a co-triangulation from any number of data sets is presented and analyzed. The main contribution of this paper lies in the description of a highly efficient framework for the realization of this approximation algorithm. While the asymptotic time complexity of the algorithm certainly remains within the theoretical bounds, we demonstrate that it is possible to achieve running times that depend only linearly on the number of data even for very large problems with more than two million samples. This efficient realization of the algorithm uses adapted dynamic data structures and careful caching in an integrated framework.	Weimer, H.;Warren, Joe;Troutner, J.;Wiggins, W.;Shrout, J.	Rice Univ., Houston, TX, USA|c|;;;;	37359801600;37272377400;37372002000;37372001300;37371998300
	SciVis	24-24 Oct. 1998	Visualizing diffusion tensor images of the mouse spinal cord	10.1109/VISUAL.1998.745294	http://dx.doi.org/10.1109/VISUAL.1998.745294	127	134	745294	Brownian motion;biodiffusion;biology computing;biomedical MRI;data visualisation;diseases;neurophysiology;tensors	H2O;anatomical scalar field;anisotropic diffusion rate;brush strokes;diagnostic value;diffusion tensor image visualization;diffusion-weighted MRI;ellipsoid array;experimental allergic encephalomyelitis;histology;molecular stochastic Brownian motion;mouse spinal cord;multi-valued visualization;neurodegenerative diseases;noninvasive imaging methodology;normalization;oil painting;tensor fields;tensor values;underlying tissue structure	Anisotropic magnetoresistance;Biological systems;Displays;Ellipsoids;Magnetic resonance imaging;Mice;Spinal cord;Stochastic systems;Tensile stress;Visualization		Within biological systems, water molecules undergo continuous stochastic Brownian motion. The diffusion rate can give clues to the structure of the underlying tissues. In some tissues, the rate is anisotropic. Diffusion-rate images can be calculated from diffusion-weighted MRI. A 2D diffusion tensor image (DTI) and an associated anatomical scalar field define seven values at each spatial location. We present two new methods for visually representing DTIs. The first method displays an array of ellipsoids, where the shape of each ellipsoid represents one tensor value. The ellipsoids are all normalized to approximately the same size so that they can be displayed simultaneously in context. The second method uses concepts from oil painting to represent the seven-valued data with multiple layers of varying brush strokes. Both methods successfully display most or all of the information in DTIs and provide exploratory methods for understanding them. The ellipsoid method has a simpler interpretation and explanation than the painting-motivated method; the painting-motivated method displays more of the information and is easier to read quantatively. We demonstrate the methods on images of the mouse spinal cord. The visualizations show significant differences between spinal cords from mice suffering from experimental allergic encephalomyelitis and spinal cords from wild-type mice. The differences are consistent with differences shown histologically and suggest that our new non-invasive imaging methodology and visualization of the results could have early diagnostic value for neurodegenerative diseases.	Laidlaw, D.H.;Ahrens, E.T.;Kremers, D.;Avalos, M.J.;Jacobs, R.E.;Readhead, C.	California Inst. of Technol., Pasadena, CA, USA|c|;;;;;	37275712600;37359190700;37371978600;37371978400;37368212100;37371977900
	SciVis	24-24 Oct. 1998	Image-guided streamline placement on curvilinear grid surfaces	10.1109/VISUAL.1998.745295	http://dx.doi.org/10.1109/VISUAL.1998.745295	135	142	745295	computational fluid dynamics;data visualisation;flow visualisation;image processing;low-pass filters;optimisation;vectors	3D parametric surfaces;automatic seed point placement;computational space;curvilinear grid surfaces;energy function;flow visualization;image-guided streamline placement;local densities;low-pass filtered streamline image;mapping distortion;optimization process;uneven grid density;uniform distribution;vector field visualization	Density measurement;Grid computing;Low pass filters;Optimization methods;Physics computing;Space shuttles;Streaming media;Surface treatment;Topology;Visualization		The success of using a streamline technique for visualizing a vector field usually depends largely on the choice of adequate seed points. G. Turk and D. Banks (1996) developed an elegant technique for automatically placing seed points to achieve a uniform distribution of streamlines on a 2D vector field. Their method uses an energy function calculated from the low-pass filtered streamline image to guide the optimization process of the streamline distribution. This paper proposes a new technique for creating evenly distributed streamlines on 3D parametric surfaces found in curvilinear grids. We make use of Turk and Banks's 2D algorithm by first mapping the vectors on a 3D surface into the computational space of the curvilinear grid. To take into the consideration the mapping distortion caused by the uneven grid density in a curvilinear grid, a new energy function is designed and used for guiding the placement of streamlines in the computational space with desired local densities.	Xiaoyang Mao;Hatanaka, Y.;Higashida, H.;Imamiya, A.	Dept. of Comput. & Media Eng., Yamanashi Univ., Kofu, Japan|c|;;;	37275999300;37361235200;37372000800;37272095800
	SciVis	24-24 Oct. 1998	A higher-order method for finding vortex core lines	10.1109/VISUAL.1998.745296	http://dx.doi.org/10.1109/VISUAL.1998.745296	143	150	745296	computational fluid dynamics;data visualisation;feature extraction;flow visualisation;mechanical engineering computing;turbines;vortices	3D vector fields;bent vortex location;computational fluid dynamics;feature extraction;feature-based visualization;flow fields;higher-order derivatives;quality attribute;recognition procedure;strength attribute;turbomachinery;vortex core lines;vortical structure extraction	Acceleration;Computational fluid dynamics;Data analysis;Data mining;Data visualization;Displays;Eigenvalues and eigenfunctions;Feature extraction;Fluid dynamics;Geometry;Isosurfaces;Jacobian matrices;Navier-Stokes equations;Scientific computing;Topology;Turbomachinery		This paper presents a novel method to extract vortical structures from 3D CFD (computational fluid dynamics) vector fields automatically. It discusses the underlying theory and some aspects of the implementation. Making use of higher-order derivatives, the method is able to locate bent vortices. In order to structure the recognition procedure, we distinguish locating the core line from calculating attributes of strength and quality. Results are presented on several flow fields from the field of turbomachinery.	Roth, M.;Peikert, R.	Center for Sci. Comput., Eidgenossische Tech. Hochschule, Zurich, Switzerland|c|;	37365755100;37282541100
	SciVis	24-24 Oct. 1998	Automatic detection of open and closed separation and attachment lines	10.1109/VISUAL.1998.745297	http://dx.doi.org/10.1109/VISUAL.1998.745297	151	158	745297	computational fluid dynamics;data visualisation;feature extraction;flow separation;flow visualisation;vectors	2D phase plane analysis;3D numerical flow fields;automatic feature detection algorithm;automatic line detection;closed separation lines;flow attachment lines;flow separation;linear vector fields;local analytic tests;open separation lines;visualization techniques	Aircraft;Algorithm design and analysis;Computer graphics;Computer vision;Data analysis;Data visualization;Detection algorithms;Partitioning algorithms;Performance analysis;Topology		A fully automatic feature detection algorithm is presented that locates and distinguishes lines of flow separation and attachment on surfaces in 3D numerical flow fields. The algorithm is based on concepts from 2D phase-plane analysis of linear vector fields. Unlike prior visualization techniques based on particle tracing or flow topology, the phase-plane algorithm detects separation using local analytic tests. The results show that it not only detects the standard closed separation lines but also the illusive open separation lines which are not captured by flow topology methods.	Kenwright, D.N.	MRJ Technol Solutions, NASA Ames Res. Center, Moffett Field, CA, USA|c|	37355295400
	SciVis	24-24 Oct. 1998	Isosurface extraction in time-varying fields using a temporal hierarchical index tree	10.1109/VISUAL.1998.745298	http://dx.doi.org/10.1109/VISUAL.1998.745298	159	166	745298	computational fluid dynamics;data visualisation;feature extraction;flow visualisation;indexing;time-varying systems;tree data structures	I/O requirements;adaptive coalescence;computational fluid dynamics simulations;data structure;disk-space savings;extreme value coalescence;isosurface cell location;isosurface cell search index;isosurface extraction;marching cubes algorithm;scalar field visualization;search index;span space;storage overhead;temporal coherence;temporal hierarchical index tree;time-varying fields;time-varying scalar data set;volume visualization	Acceleration;Computational fluid dynamics;Data analysis;Data mining;Data visualization;Isosurfaces;Large-scale systems;NASA;Space technology;Tree data structures		Many high-performance isosurface extraction algorithms have been proposed in the past several years as a result of intensive research efforts. When applying these algorithms to large-scale time-varying fields, the storage overhead incurred from storing the search index often becomes overwhelming. This paper proposes an algorithm for locating isosurface cells in time-varying fields. We devise a new data structure, called the temporal hierarchical index tree, which utilizes the temporal coherence that exists in a time-varying field and adaptively coalesces the cells' extreme values over time; the resulting extreme values are then used to create the isosurface cell search index. For a typical time-varying scalar data set, not only does this temporal hierarchical index tree require much less storage space, but also the amount of I/O required to access the indices from the disk at different time steps is substantially reduced. We illustrate the utility and speed of our algorithm with data from several large-scale time-varying CFD simulations. Our algorithm can achieve more than 80% of disk-space savings when compared with the existing techniques, while the isosurface extraction time is nearly optimal.	Shen, H.-W.	MRJ Technol. Solutions, NASA Ames Res. Center, Moffett Field, CA, USA|c|	37366741500
	SciVis	24-24 Oct. 1998	Interactive out-of-core isosurface extraction	10.1109/VISUAL.1998.745299	http://dx.doi.org/10.1109/VISUAL.1998.745299	167	174	745299	computational geometry;data visualisation;database indexing;interactive systems;query processing;tree data structures;trees (mathematics);visual databases	I/O-efficient approach;I/O-optimal data structure;active meta-cells;binary blocked I/O interval tree;cell information;dataset;disk space;disk space requirements;indexing data structure;interactive computation;interactive out-of-core isosurface extraction;isosurface extraction queries;isosurface query;k-d-tree-like partition;meta-cell pointers;meta-cell technique;meta-intervals;query time;query value;stabbing queries;two-level indexing scheme;visualization workstation;volume data	Clustering algorithms;Costs;Data mining;Data structures;Data visualization;Indexing;Isosurfaces;Partitioning algorithms;Tree data structures;Workstations		We present a novel out-of-core technique for the interactive computation of isosurfaces from volume data. Our algorithm minimizes the main memory and disk space requirements on the visualization workstation, while speeding up isosurface extraction queries. Our overall approach is a two-level indexing scheme. First, by our meta-cell technique, we partition the original dataset into clusters of cells, called meta-cells. Secondly, we produce meta-intervals associated with the meta-cells, and build an indexing data structure on the meta-intervals. We separate the cell information, kept only in meta-cells on disk, from the indexing structure, which is also on disk and only contains pointers to meta-cells. Our meta-cell technique is an I/O-efficient approach for computing a k-d-tree-like partition of the dataset. Our indexing data structure, the binary blocked I/O interval tree, is a new I/O-optimal data structure to perform stabbing queries that report from a set of meta-intervals (or intervals) those containing a query value q. Our tree is simpler to implement, and is also more space-efficient in practice than existing structures. To perform an isosurface query, we first query the indexing structure, and then use the reported meta-cell pointers to read from disk the active meta-cells intersected by the isosurface. The isosurface itself can then be generated from active meta-cells. Rather than being a single cost indexing approach, our technique exhibits a smooth trade-off between query time and disk space.	Yi-Jen Chiang;Silva, C.T.;Schroeder, W.J.	;;	37362725400;38183059100;37282730100
	SciVis	24-24 Oct. 1998	View dependent isosurface extraction	10.1109/VISUAL.1998.745300	http://dx.doi.org/10.1109/VISUAL.1998.745300	175	180	745300	computational geometry;data visualisation;rendering (computer graphics)	graphics hardware;hierarchical tiles;large datasets;polygonal isosurface extraction;remote visualization applications;rendering;search phase bottleneck;shear-warp factorization;view dependent isosurface extraction;visibility tests	Costs;Graphics;Hardware;Isosurfaces;Performance evaluation;Rendering (computer graphics);Software performance;Software testing;Tiles;Visualization		We propose a new approach to polygonal isosurface extraction that is based on extracting only the visible portion of the isosurface. The visibility tests are done in two phases. First, coarse visibility tests are performed in software to determine the visible cells. These tests are based on hierarchical tiles and shear-warp factorization. The second phase resolves the visible portions of the extracted triangles and is accomplished by the graphics hardware. While the latest isosurface extraction methods have effectively eliminated the search phase bottleneck, the cost of constructing and rendering the isosurface remains high. Many of today's large datasets contain very large and complex isosurfaces that can easily overwhelm even state-of-the-art graphics hardware. The proposed approach is output sensitive and is thus well suited for remote visualization applications where the extraction and rendering phases are done on a separate machines.	Livnat, Y.;Hansen, C.	Utah Univ., Salt Lake City, UT, USA|c|;	37282553200;37266777200
	SciVis	24-24 Oct. 1998	The Gridfit algorithm: an efficient and effective approach to visualizing large amounts of spatial data	10.1109/VISUAL.1998.745301	http://dx.doi.org/10.1109/VISUAL.1998.745301	181	188	745301	data visualisation;spatial data structures;very large databases;visual databases	Gridfit algorithm;Hilbert-curve;distance-preservation;hierarchical partitioning;limited-size screen display;overplotting;pixels;position-preservation;screen-filling curve;spatial data visualization;spatial locations;spatially referenced data	Air pollution;Atmospheric measurements;Cities and towns;Clustering algorithms;Computer science;Data visualization;Displays;Euclidean distance;Large screen displays;NP-complete problem;Partitioning algorithms;Pollution measurement;Position measurement;Sea measurements;Spirals;Temperature measurement		In a large number of applications, data is collected and referenced by their spatial locations. Visualizing large amounts of spatially referenced data on a limited-size screen display often results in poor visualizations due to the high degree of overplotting of neighboring datapoints. We introduce a new approach to visualizing large amounts of spatially referenced data. The basic idea is to intelligently use the unoccupied pixels of the display instead of overplotting data points. After formally describing the problem, we present two solutions which are based on: placing overlapping data points on the nearest unoccupied pixel; and shifting data points along a screen-filling curve (e.g., Hilbert-curve). We then develop a more sophisticated approach called Gridfit, which is based on a hierarchical partitioning of the data space. We evaluate all three approaches with respect to their efficiency and effectiveness and show the superiority of the Gridfit approach. For measuring the effectiveness, we not only present the resulting visualizations but also introduce mathematical effectiveness criteria measuring properties of the generated visualizations with respect to the original data such as distance- and position-preservation.	Keim, D.A.;Herrmann, A.	Inst. of Comput. Sci., Halle-Wittenberg Univ., Halle, Germany|c|;	37283138700;37372264500
	SciVis	24-24 Oct. 1998	TOPIC ISLANDSTM-a wavelet-based text visualization system	10.1109/VISUAL.1998.745302	http://dx.doi.org/10.1109/VISUAL.1998.745302	189	196	745302	data visualisation;document handling;statistical analysis;wavelet transforms	TOPIC ISLANDS;TOPIC-O-GRAPHY;document;fuzzy document outlines;multiresolution wavelet energy;query processing;statistical methods;text visualization system;unstructured text;user interests;wavelet transforms	Energy resolution;Frequency domain analysis;Fuzzy systems;Prototypes;Signal resolution;Statistical analysis;Visualization;Wavelet analysis;Wavelet domain;Wavelet transforms			Miller, N.E.;Pak Chung Wong;Brewster, M.;Foote, H.	Pacific Northwest Lab., Richland, WA, USA|c|;;;	37361698400;37280665600;37371985000;37372586800
	SciVis	24-24 Oct. 1998	Continuous cartogram construction	10.1109/VISUAL.1998.745303	http://dx.doi.org/10.1109/VISUAL.1998.745303	197	204	745303	cartography;data visualisation;geographic information systems;optimisation	area cartograms;constrained dynamics;constrained optimization problem;continuous cartogram construction;geographically distributed data visualization;hierarchical resolution;map topology;measurements;region shape recognition cues;relaxation method	Area measurement;Constraint optimization;Data visualization;Demography;Graphics;Joining processes;Laboratories;Nominations and elections;Relaxation methods;Shape;Topology;Voting		Area cartograms are used for visualizing geographically distributed data by attaching measurements to regions of a map and scaling the regions such that their areas are proportional to the measured quantities. A continuous area cartogram is a cartogram that is constructed without changing the underlying map topology. We present a new algorithm for the construction of continuous area cartograms that was developed by viewing their construction as a constrained optimization problem. The algorithm uses a relaxation method that exploits hierarchical resolution, constrained dynamics, and a scheme that alternates goals of achieving correct region areas and adjusting region shapes. It is compared favorably to existing methods in its ability to preserve region shape recognition cues, while still achieving high accuracy.	House, D.H.;Kocmoud, C.J.	;	37284216700;37372172300
	SciVis	24-24 Oct. 1998	A concept for virtual reality tools for design reviews	10.1109/VISUAL.1998.745304	http://dx.doi.org/10.1109/VISUAL.1998.745304	205	210	745304	CAD/CAM;mechanical engineering computing;solid modelling;user interfaces;virtual reality	3D interface;3D visualization;CAD/CAM;design reviews;mechanical products;three dimensional visualization;virtual reality	Computer graphics;Computer interfaces;Design automation;Physics computing;Prototypes;User interfaces;Virtual environment;Virtual prototyping;Virtual reality;Visualization		The paper discusses a concept for virtual reality tools for use in design reviews of mechanical products. In this discussion, the special requirements of a virtual environment are given consideration. The focus of this paper is on suggestions for the visualization and arrangement of a product, its structure, its components and their alternatives together in one environment. The realization of these concepts results in a 3D-interface that allows users, especially engineers, to evaluate different configurations of a product and gives them direct access to the product structure. By applying various visualization techniques, product components and their attributes, e.g., their price, can be brought together into one visualization. Thus, in contrast to state-of-the-art software, the product structure, three-dimensional, real-sized components, and attribute values can be combined together in 3D-visualizations. This research was done in cooperation with Christoph Brandt, member of the Heinz Nixdorf Institute's virtual reality group.	Kremer, K.	Heinz Nixdorf Inst., Paderborn Univ., Germany|c|	37371920800
	SciVis	24-24 Oct. 1998	Efficient warping for architectural walkthroughs using layered depth images	10.1109/VISUAL.1998.745305	http://dx.doi.org/10.1109/VISUAL.1998.745305	211	215	745305	image morphing;parallel processing;rendering (computer graphics);resource allocation	architectural walkthroughs;exposure errors;image preprocessing;image warping;image-based rendering;layered depth images;load balancing;occlusion compatible ordering;parallelization scheme;performance;reference image space clipping algorithm	Computer errors;Computer graphics;Geometry;Hardware;Layout;Load management;Pixel;Portals;Rendering (computer graphics);Solid modeling		This paper presents efficient image-based rendering techniques used in the context of an architectural walkthrough system. Portals (doors and windows) are rendered by warping layered depth images (LDIs). In a preprocessing phase, for every portal, a number of pre-rendered images are combined into an LDI. The resulting LDI stores, exactly once, all surfaces visible in at least one of the images used in the construction, so most of the exposure errors are efficiently eliminated. The LDI can be warped in the McMillan occlusion compatible ordering. A substantial increase in performance is obtained by warping in parallel. Our parallelization scheme achieves good load balancing, scales with the number of processors, and preserves the occlusion compatible ordering. A fast, conservative reference-image-space clipping algorithm also reduces the warping effort.	Popescu, V.;Lastra, A.;Aliaga, D.;de Oliveira Neto, M.	North Carolina Univ., Chapel Hill, NC, USA|c|;;;	37272425500;37274005100;37270561900;37267049500
	SciVis	24-24 Oct. 1998	Visualizing differences in movies of cortical activity	10.1109/VISUAL.1998.745306	http://dx.doi.org/10.1109/VISUAL.1998.745306	217	224	745306	biology computing;computer animation;data visualisation;image sampling;principal component analysis	animation;cortical activity;data sets;electrical response;high-speed photodiode array;low-dimensional subspace;movie sampling;neurobiology;optical stimulation;principal component analysis;time snapshots;vectors;video data;visual cortex;visualization	Data visualization;High speed optical techniques;Image coding;Image sampling;Motion pictures;Optical arrays;Optical noise;Photodiodes;Principal component analysis;Stimulated emission		This paper discusses techniques for visualizing structure in video data and other data sets that represent time snapshots of physical phenomena. Individual frames of a movie are treated as vectors and projected onto a low-dimensional subspace spanned by principal components. Movies can be compared and their differences visualized by analyzing the nature of the subspace and the projections of multiple movies onto the same subspace. The approach is demonstrated on an application in neurobiology in which the electrical response of a visual cortex to optical stimulation is imaged onto a high-speed photodiode array to produce a cortical movie. Techniques for sampling movies over a single trial and multiple trials are discussed. The approach provides the traditional benefits of principal component analysis (compression, noise reduction and classification) and also allows the visual separation of spatial and temporal behavior.	Robbins, K.A.;Senseman, D.M.	Div. of Comput. Sci., Texas Univ., San Antonio, TX, USA|c|;	37282602100;37282602800
	SciVis	24-24 Oct. 1998	A distributed blackboard architecture for interactive data visualization	10.1109/VISUAL.1998.745307	http://dx.doi.org/10.1109/VISUAL.1998.745307	225	231	745307	blackboard architecture;data visualisation;distributed processing;interactive systems	distributed blackboard architecture;interactive data visualization;qualitative analysis;quantitative analysis;simulations;system design	Analytical models;Application software;Computational modeling;Computer architecture;Computer science;Data models;Data visualization;High performance computing;Inspection;Mathematics;Problem-solving		In this paper the motivation, design and application of a distributed blackboard architecture for interactive data visualization is discussed. The main advantages of the architecture are twofold. First, it allows visualization tools to be tightly integrated with simulations. Second, it allows qualitative and quantitative analysis to be combined during the visualization process.	van Liere, R.;Harkes, J.;Wim de Leeuw	Center for Math. & Comput. Sci., CWI, Amsterdam, Netherlands|c|;;	37282925600;37300843200;37327212900
	SciVis	24-24 Oct. 1998	Eliminating popping artifacts in sheet buffer-based splatting	10.1109/VISUAL.1998.745309	http://dx.doi.org/10.1109/VISUAL.1998.745309	239	245	745309	colour graphics;data visualisation;interpolation;rendering (computer graphics)	animated viewing;bleeding artifacts;cache-sheets;compositing sheet orientation;fast splat access;fast volume rendering algorithm;hidden objects;image plane;image screen;list-based volume traversal scheme;partial splats;popping artifact elimination;pre-integrated interpolation kernels;ray divergence;sheet buffer-based splatting;splat decomposition;splatting kernels;visible color popping artifacts;volumetric objects;voxel projection	Animation;Clouds;Color;Displays;Facial animation;Filters;Hemorrhaging;Information science;Interpolation;Isosurfaces;Kernel;Medical simulation;Rendering (computer graphics);Sampling methods;Transfer functions;Visualization		Splatting is a fast volume rendering algorithm which achieves its speed by projecting voxels in the form of pre-integrated interpolation kernels, or splats. Presently, two main variants of the splatting algorithm exist: (i) the original method, in which all splats are composited back-to-front, and (ii) the sheet-buffer method, in which the splats are added in cache-sheets, aligned with the volume face most parallel to the image plane, which are subsequently composited back-to-front. The former method is prone to cause bleeding artifacts from hidden objects, while the latter method reduces bleeding, but causes very visible color popping artifacts when the orientation of the compositing sheets changes suddenly as the image screen becomes more parallel to another volume face. We present a new variant of the splatting algorithm in which the compositing sheets are always parallel to the image plane, eliminating the condition for popping, while maintaining the insensitivity to color bleeding. This enables pleasing animated viewing of volumetric objects without temporal color and lighting discontinuities. The method uses a hierarchy of partial splats and employs an efficient list-based volume traversal scheme for fast splat access. It also offers more accuracy for perspective splatting as the decomposition of the individual splats facilitates a better approximation to the diverging nature of the rays that traverse the splatting kernels.	Mueller, K.;Crawfis, R.	Dept. of Comput. & Inf. Sci., Ohio State Univ., Columbus, OH, USA|c|;	37273119700;37284273900
	SciVis	24-24 Oct. 1998	Accelerated ray-casting for curvilinear volumes	10.1109/VISUAL.1998.745310	http://dx.doi.org/10.1109/VISUAL.1998.745310	247	253	745310	data visualisation;parallel algorithms;ray tracing;rendering (computer graphics);structural engineering computing	accelerated ray-casting;arbitrarily-shaped cells;curvilinear data sets;curvilinear volumes;dynamic simulation;parallel rendering;projection paradigm;ray traversal;rendering	Acceleration;Aerodynamics;Computational fluid dynamics;Computational modeling;Computer science;Data visualization;Grid computing;Pixel;Production;Robustness;Space technology		We present an efficient and robust ray-casting algorithm for directly rendering a curvilinear volume of arbitrarily-shaped cells. We designed the algorithm to alleviate the consumption of CPU power and memory space. By incorporating the essence of the projection paradigm into the ray-casting process, we have successfully accelerated the ray traversal through the grid and data interpolations at sample points. Our algorithm also overcomes the conventional limitation requiring the cells to be convex. Application of this algorithm to several commonly-used curvilinear data sets has produced a favorable performance when compared with recently reported algorithms.	Lichan Hong;Kaufman, A.	Lucent Technol., AT&T Bell Labs., Naperville, IL, USA|c|;	37347459100;37268052800
	SciVis	24-24 Oct. 1998	High quality rendering of attributed volume data	10.1109/VISUAL.1998.745311	http://dx.doi.org/10.1109/VISUAL.1998.745311	255	262	745311	computerised tomography;image segmentation;medical image processing;ray tracing;rendering (computer graphics)	Visible-Human-Data;aliasing artifacts;animations;attributed volume data;gray-level-gradient;high quality rendering;magnified views;nearly photo-realistic images;object boundary;partial-volume-effect;ray-casting approach;subvoxel resolution;tomographic volume data	Computer graphics;Data visualization;Image reconstruction;Interpolation;Isosurfaces;Magnetic resonance imaging;Rendering (computer graphics);Spatial resolution;Surface fitting;Surface morphology;Surface reconstruction;Tomography		For high quality rendering of objects segmented from tomographic volume data the precise location of the boundaries of adjacent objects in subvoxel resolution is required. We describe a new method that determines the membership of a given sample point to an object by reclassifying the sample point using interpolation of the original intensity values and searching for the best fitting object in the neighbourhood. Using a ray-casting approach we then compute the surface location between successive sample points along the viewing-ray by interpolation or bisection. The accurate calculation of the object boundary enables a much more precise computation of the gray-level-gradient yielding the surface normal. Our new approach significantly improves the quality of reconstructed and shaded surfaces and reduces aliasing artifacts for animations and magnified views. We illustrate the results on different cases including the Visible-Human-Data, where we achieve nearly photo-realistic images.	Tiede, U.;Schiemann, T.;Hohne, K.H.	Inst. of Math. & Comput. Sci. in Med., Eppendorf Univ. Hosp., Hamburg, Germany|c|;;	37372275900;37372275700;37372285500
	SciVis	24-24 Oct. 1998	Simplifying surfaces with color and texture using quadric error metrics	10.1109/VISUAL.1998.745312	http://dx.doi.org/10.1109/VISUAL.1998.745312	263	269	745312	computer graphics;image representation;image texture;iterative methods	color;complex polygonal surface models;discontinuity preservation;high quality approximations;iterative edge contraction;multiresolution modeling;quadric error metric;quadric error metrics;surface normals;surface simplification algorithm;texture;vertex attributes	Application software;Chromium;Computer errors;Computer graphics;Costs;Iterative algorithms;Material properties;Solid modeling;Surface texture;Visualization		There are a variety of application areas in which there is a need for simplifying complex polygonal surface models. These models often have material properties such as colors, textures, and surface normals. Our surface simplification algorithm, based on iterative edge contraction and quadric error metrics, can rapidly produce high quality approximations of such models. We present a natural extension of our original error metric that can account for a wide range of vertex attributes.	Garland, M.;Heckbert, P.S.	Carnegie Mellon Univ., Pittsburgh, PA, USA|c|;	37272036400;37372245200
	SciVis	24-24 Oct. 1998	A unified approach for simplifying polygonal and spline models	10.1109/VISUAL.1998.745313	http://dx.doi.org/10.1109/VISUAL.1998.745313	271	278	745313	computational geometry;polynomials;splines (mathematics)	batch connectivity information;curved levels-of-detail;dynamic tessellation schemes;geometric operations;model simplification;polygonal approximations;polygonal models;spline models;static tessellation schemes;surface approximation;surface deviation error;surface fitting;triangular Bezier patches	Approximation algorithms;Clustering algorithms;Computational geometry;Computer graphics;Computer science;Heuristic algorithms;Merging;Solid modeling;Spline;Surface fitting;Topology;Vehicle dynamics		We present a new approach for simplifying models composed of polygons or spline patches. Given an input model, the algorithm computes a new representation of the model in terms of triangular Bezier patches. It performs a series of geometric operations, consisting of patch merging and swapping diagonals, and makes use of batch connectivity information to generate C-LODs (curved levels-of-detail). Each C-LOD is represented using cubic triangular Bezier patches. The C-LODs provide a compact representation for storing the model. The algorithm tries to minimize the surface deviation error and maintains continuity at patch boundaries. Given the CLODs, the algorithm can generate their polygonal approximations using static and dynamic tessellation schemes. It has been implemented and we highlight its performance on a number of polygonal and spline models.	Gopi, M.;Manocha, D.	Dept. of Comput. Sci., North Carolina Univ., Chapel Hill, NC, USA|c|;	37271691400;37267825600
	SciVis	24-24 Oct. 1998	Fast and memory efficient polygonal simplification	10.1109/VISUAL.1998.745314	http://dx.doi.org/10.1109/VISUAL.1998.745314	279	286	745314	computational geometry	edge collapse priorities;edge collapses;face connectivity;high-quality simplified polygonal models;mean geometric error;memory efficient polygonal simplification;per-triangle change	Application software;Computer graphics;Computer vision;Data acquisition;Geometry;History;Information geometry;Magnetic resonance imaging;Satellites;Shape;Solid modeling;Spaceborne radar		Conventional wisdom says that in order to produce high-quality simplified polygonal models, one must retain and use information about the original model during the simplification process. We demonstrate that excellent simplified models can be produced without the need to compare against information from the original geometry while performing local changes to the model. We use edge collapses to perform simplification, as do a number of other methods. We select the position of the new vertex so that the original volume of the model is maintained and we minimize the per-triangle change in volume of the tetrahedra swept out by those triangles that are moved. We also maintain surface area near boundaries and minimize the per-triangle area changes. Calculating the edge collapse priorities and the positions of the new vertices requires only the face connectivity and the the vertex locations in the intermediate model. This approach is memory efficient, allowing the simplification of very large polygonal models, and it is also fast. Moreover, simplified models created using this technique compare favorably to a number of other published simplification methods in terms of mean geometric error.	Lindstrom, P.;Turk, G.	Georgia Inst. of Technol., Atlanta, GA, USA|c|;	37269320000;37334922800
	SciVis	24-24 Oct. 1998	Simplification of tetrahedral meshes	10.1109/VISUAL.1998.745315	http://dx.doi.org/10.1109/VISUAL.1998.745315	287	295	745315	computational geometry;mesh generation;spatial data structures;splines (mathematics)	hierarchical representation;high-resolution triangulation;linear spline function;multiresolution method;simplification process;tetrahedral meshes;trivariate function;use-specified threshold	Acoustic scattering;Chemicals;Computer science;Data visualization;Image processing;Large-scale systems;Legged locomotion;Mesh generation;Spline;Temperature dependence		We present a method for the construction of multiple levels of tetrahedral meshes approximating a trivariate function at different levels of detail. Starting with an initial, high-resolution triangulation of a three-dimensional region, we construct coarser representation levels by collapsing tetrahedra. Each triangulation defines a linear spline function, where the function values associated with the vertices are the spline coefficients. Based on predicted errors, we collapse tetrahedron in the grid that do not cause the maximum error to exceed a use-specified threshold. Bounds are stored for individual tetrahedra and are updated as the mesh is simplified. We continue the simplification process until a certain error is reached. The result is a hierarchical data description suited for the efficient visualization of large data sets at varying levels of detail.	Trotts, I.J.;Hamann, B.;Joy, K.I.;Wiley, D.F.	Center for Image Process. & Integrated Comput., California Univ., Davis, CA, USA|c|;;;	37372274900;37282068700;37267811400;37282066500
	SciVis	24-24 Oct. 1998	Interactive deformations from tensor fields	10.1109/VISUAL.1998.745316	http://dx.doi.org/10.1109/VISUAL.1998.745316	297	304	745316	computational geometry;data visualisation;interactive systems;tensors	CFD strain rate;Deviator-Isotropic tensor decomposition;compressive force;deformation visualizations;directional flow techniques;idealized objects;interactive deformations;interactive visualization;strain;stress;tensile force;tensor fields	Capacitive sensors;Compressive stress;Computational fluid dynamics;Data engineering;Data visualization;Fluid flow measurement;Force measurement;Geologic measurements;Liquid crystals;Mechanical variables measurement;Tensile stress		This paper presents techniques for interactively visualizing tensor fields using deformations. The conceptual idea behind this approach is to allow the tensor field to manifest its influence on idealized objects placed within the tensor field. This is similar, though not exactly the same, to surfaces deforming under load in order to relieve built up stress and strain. We illustrate the effectiveness of the Deviator-Isotropic tensor decomposition in deformation visualizations of CFD strain rate. We also investigate how directional flow techniques can be extended to distinguish between regions of tensile versus compressive forces.	Boring, E.;Pang, A.	Dept. of Comput. Sci., California Univ., Santa Cruz, CA, USA|c|;	37372278100;37267352000
	SciVis	24-24 Oct. 1998	Real-time techniques for 3D flow visualization	10.1109/VISUAL.1998.745317	http://dx.doi.org/10.1109/VISUAL.1998.745317	305	312	745317	computer animation;data visualisation;flow visualisation;image texture;real-time systems	3D flow visualization;dashtubes;magic boxes;magic lenses;occlusion;opacity-mapped streamlines;real-time graphic representations;texture mapping;three dimensional flow visualization;virtual environments;visualization icon	Animation;Computer graphics;Convolution;Data visualization;Focusing;Hair;Hardware;Lenses;Streaming media;Virtual environment		Visualization of three-dimensional steady flow has to overcome a lot of problems to be effective. Among them are occlusion of distant details, lack of directional and depth hints and occlusion. We present methods which address these problems for real-time graphic representations applicable in virtual environments. We use dashtubes, i.e., animated, opacity-mapped streamlines, as a visualization icon for 3D-flow visualization. We present a texture mapping technique to keep the level of texture detail along a streamline nearly constant even when the velocity of the flow varies considerably. An algorithm is described which distributes the dashtubes evenly in space. We apply magic lenses and magic boxes as interaction techniques for investigating densely filled areas without overwhelming the observer with visual detail. Implementation details of these methods and their integration in our virtual environment conclude the paper.	Fuhrmann, A.;Groller, E.	Inst. of Comput. Graphics, Wien Univ., Austria|c|;	37267442100;38471589100
	SciVis	24-24 Oct. 1998	Wavelets over curvilinear grids	10.1109/VISUAL.1998.745318	http://dx.doi.org/10.1109/VISUAL.1998.745318	313	317	745318	computational geometry;data visualisation;flow visualisation;piecewise linear techniques;wavelet transforms	2D flow visualization;Haar wavelets;curvilinear grids;decomposition equations;geometry;lifting techniques;multiresolution models;nested domains;piecewise defined functions;refinement equations;two-dimensional flow visualization	Computer graphics;Computer science;Data visualization;Equations;Geometry;Image reconstruction;Maintenance engineering;Mathematical model;Piecewise linear techniques;Wavelet domain		We develop multiresolution models for analyzing and visualizing two-dimensional flows over curvilinear grids. Our models are based upon nested spaces of piecewise defined functions defined over nested curvilinear grid domains. The nested domains are selected so as to maintain the original geometry of the inner boundary. We first give the refinement and decomposition equations for Haar wavelets over these domains. Next, using lifting techniques we develop and show examples of piecewise linear wavelets over curvilinear grids.	Nielson, G.M.;Jung, Il.-H.;Sung, J.	Dept. of Comput. Sci. & Eng., Arizona State Univ., Tempe, AZ, USA|c|;;	37283754100;37366444500;37362401700
	SciVis	24-24 Oct. 1998	Image-based transfer function design for data exploration in volume visualization	10.1109/VISUAL.1998.745319	http://dx.doi.org/10.1109/VISUAL.1998.745319	319	326	745319	data visualisation;image enhancement;optical microscopy;rendering (computer graphics);transfer functions	3D image enhancement;3D image processing procedures;3D image processing tools;3D microscopy data exploration;boundary detection tools;common trial-and-error approach;data exploration;descriptive parameters;image based transfer function;image based transfer function design;image based transfer function model;integrated component;integration methods;parameterized transfer function model;subjective visualization goals;time consuming process;transfer function searching;transfer function searching process;visualization results;volume data visualization;volume visualization;volume visualization algorithms;volume visualization pipeline	Biomedical imaging;Computed tomography;Data analysis;Data visualization;Displays;Image processing;Material properties;Medical diagnostic imaging;Microscopy;Optical microscopy;Rendering (computer graphics);Transfer functions		Transfer function design is an integrated component in volume visualization and data exploration. The common trial-and-error approach for transfer function searching is a very difficult and time consuming process. A goal oriented and parameterized transfer function model is therefore crucial in guiding the transfer function searching process for better and more meaningful visualization results. The paper presents an image based transfer function model that integrates 3D image processing tools into the volume visualization pipeline to facilitate the search for an image based transfer function in volume data visualization and exploration. The model defines a transfer function as a sequence of 3D image processing procedures, and allows the users to adjust a set of qualitative and descriptive parameters to achieve their subjective visualization goals. 3D image enhancement and boundary detection tools, and their integration methods with volume visualization algorithms are described. The application of this approach for 3D microscopy data exploration and analysis is also discussed.	Shiaofen Fang;Biddlecome, T.;Tuceryan, M.	Dept. of Comput. & Inf. Sci., Indiana Univ., Indianapolis, IN, USA|c|;;	37289749900;37372276300;37372277000
	SciVis	24-24 Oct. 1998	Image-based rendering with occlusions via cubist images	10.1109/VISUAL.1998.745320	http://dx.doi.org/10.1109/VISUAL.1998.745320	327	334	745320	art;data compression;interpolation;ray tracing;rendering (computer graphics)	3D intercepts;animation sequence;camera motions;camera rays;cubist art;cubist images;data compression;distorted multiperspective images;general viewpoint scene renderings;image based rendering;minimal data storage;multiperspective image space;multiple viewpoint photometry;occlusions;scene geometry;singular interpolation functions;sparse time key rays;sparse time keyframes;spatial discontinuities;spatially sparse sequences;warped multiperspective image space	Animation;Cameras;Data compression;Geometry;Interpolation;Layout;Painting;Photometry;Rendering (computer graphics);Spatial resolution		We attack the problem of image based rendering with occlusions and general camera motions by using distorted multiperspective images; such images provide multiple viewpoint photometry similar to the paintings of cubist artists. We take scene geometry, in contrast, to be embodied in mappings of viewing rays from their original 3D intercepts into the warped multiperspective image space. This approach allows us to render approximations of scenes with occlusions using time dense and spatially sparse sequences of camera rays, which is a significant improvement over the storage requirements of an equivalent animation sequence. Additional data compression can be achieved using sparse time keyframes as well. Interpolating the paths of sparse time key rays correctly in image space requires singular interpolation functions with spatial discontinuities. While there are many technical questions yet to be resolved, the employment of these singular interpolation functions in the multiperspective image space appears to be of potential interest for generating general viewpoint scene renderings with minimal data storage.	Hanson, A.J.;Wernert, E.A.	Indiana Univ., Bloomington, IN, USA|c|;	37333439100;37371964800
	SciVis	24-24 Oct. 1998	Hierarchical volume analysis and visualization based on morphological operators	10.1109/VISUAL.1998.745321	http://dx.doi.org/10.1109/VISUAL.1998.745321	335	341	745321	data visualisation;interactive systems;mathematical morphology;rendering (computer graphics);transfer functions	distortional influence;hierarchical analysis;hierarchical volume analysis;interactive process;medical data sets;morphological operator decomposition;morphology based hierarchical analysis;nonlinear filters;one dimensional computation;optical properties;segmentation algorithms;spatial information;technical data sets;tensor product based linear filters;three dimensional analysis;transfer functions;volume visualization;wavelet analysis	Biomedical optical imaging;Morphology;Nonlinear distortion;Nonlinear filters;Nonlinear optics;Optical distortion;Optical filters;Transfer functions;Visualization;Wavelet analysis		One common problem in the practical application of volume visualization is the proper choice of transfer functions in order to color different parts of the volume meaningfully. This interactive process can be very complicated and time consuming. An alternative to the adjustment of transfer functions is the application of segmentation algorithms. These algorithms are often dedicated to a limited range of data sets and tend to be very compute intensive. We propose a morphology based hierarchical analysis to estimate the optical properties of the volume to be rendered. This approach requires fewer parameters and incorporates also spatial information, but it is far less compute intensive than most of the segmentation methods. The hierarchical analysis is constructed in analogy to the wavelet analysis, except for the fact, that nonlinear filters are used in our case. These morphological operators have a lower distortional influence on the analyzed structures than the usual linear filters. A special decomposition of the morphological operators is discussed, which leads to an efficient implementation of this approach. This technique reduces the three dimensional analysis to a one dimensional computation, as it is done in tensor product based linear filters. The resulting decomposition may also be parallelized easily. We demonstrate the usefulness of the proposed technique by applying it to medical and technical data sets.	Lurig, C.;Ertl, T.	Comput. Graphics Group, Erlangen-Nurnberg Univ., Germany|c|;	37372245300;38356649400
	SciVis	24-24 Oct. 1998	Interactive display of very large textures	10.1109/VISUAL.1998.745322	http://dx.doi.org/10.1109/VISUAL.1998.745322	343	350	745322	cache storage;image texture;interactive systems;real-time systems	bandwidth limited resource;caching problem;finite resource;interactive display;interactivity;large textures;memory swapping;network transfer;performance bottlenecks;primary cache;real time applications;real time performance;secondary cache;tertiary cache;texture memory;texture translation;very large textures	Application software;Chromium;Computer displays;Computer science;Graphics;Hardware;Image coding;Image generation;Layout;Surface fitting;Workstations		Large textures cause bottlenecks in real time applications that often lead to a loss of interactivity. These performance bottlenecks occur because of disk and network transfer, texture translation, and memory swapping. We present a software solution that alleviates the problems associated with large textures by treating texture as a bandwidth limited resource rather than a finite resource. As a result the display of large textures is reduced to a caching problem in which texture memory serves as the primary cache for texture data, main memory the secondary cache, and local disk the tertiary cache. By using this cache hierarchy, applications are able to maintain real time performance while displaying textures hundreds of times larger than can fit into texture memory.	Cline, D.;Egbert, P.K.	Dept. of Comput. Sci., Brigham Young Univ., Provo, UT, USA|c|;	37305635000;37298442300
	SciVis	24-24 Oct. 1998	Pixel masks for screen-door transparency	10.1109/VISUAL.1998.745323	http://dx.doi.org/10.1109/VISUAL.1998.745323	351	358	745323	data visualisation;realistic images;rendering (computer graphics)	alpha blending;distracting patterns;dynamic scenes;incorrect opacities;intersecting polygons;multiple transparent objects;object rendering;overlapping structures;pixel masks;screen door transparency	Application software;Computational modeling;Computer displays;Computer graphics;Computer science;Data visualization;Layout;Mathematics;Optical refraction;Rendering (computer graphics);Sorting		Rendering objects transparently gives additional insight in complex and overlapping structures. However, traditional techniques for the rendering of transparent objects such as alpha blending are not very well suited for the rendering of multiple transparent objects in dynamic scenes. Screen door transparency is a technique to render transparent objects in a simple and efficient way: no sorting is required and intersecting polygons can be handled without further preprocessing. With this technique, polygons are rendered through a mask: only where the mask is present, pixels are set. However, artifacts such as incorrect opacities and distracting patterns can easily occur if the masks are not carefully designed. The requirements on the masks are considered. Next, three algorithms are presented for the generation of pixel masks. One algorithm is designed for the creation of small (e.g. 4×4) masks. The other two algorithms can be used for the creation of larger masks (e.g. 32×32). For each of these algorithms, results are presented and discussed.	Mulder, J.D.;Groen, F.C.A.;van Wijk, J.J.	Center for Math. & Comput. Sci., CWI, Amsterdam, Netherlands|c|;;	38263707600;37266134600;37267249200
	SciVis	24-24 Oct. 1998	Comparing LIC and spot noise	10.1109/VISUAL.1998.745324	http://dx.doi.org/10.1109/VISUAL.1998.745324	359	365	745324	convolution;data visualisation;image texture	LIC;continuous directional convolution;line integral convolution;mathematical concept;spot noise;texture synthesis techniques;vector field visualization;visual appearance	Analytical models;Animation;Computational modeling;Computer graphics;Computer simulation;Convolution;Data visualization;Filters;Kernel;Shape		Spot noise and line integral convolution (LIC) are two texture synthesis techniques for vector field visualization. The two techniques are compared. Continuous directional convolution is used as a common basis for comparing the techniques. It is shown that the techniques are based on the same mathematical concept. Comparisons of the visual appearance of the output and performance of the algorithms are made.	Wim de Leeuw;van Liere, R.	Center for Math. & Comput. Sci., CWI, Amsterdam, Netherlands|c|;	37327212900;37282925600
	SciVis	24-24 Oct. 1998	Size preserving pattern mapping	10.1109/VISUAL.1998.745325	http://dx.doi.org/10.1109/VISUAL.1998.745325	367	373	745325	computational geometry;image texture;rendering (computer graphics)	2D image mapping;C1 continuous normal;cavity mapping;global shape;high resolution images;local attributes;low resolution volumes;parametric surfaces;rendered volume data;size preserving pattern mapping;surface types;texture mapping;texture maps;topology;volumetric iso-surfaces	Art;Image resolution;Information science;Painting;Paints;Rendering (computer graphics);Shape;Skin;Surface texture;Topology			Kurzion, Y.;Moller, T.;Yagel, R.	Dept. of Comput. & Inf. Sci., Ohio State Univ., Columbus, OH, USA|c|;;	37372244200;37275858700;37342422500
	SciVis	24-24 Oct. 1998	Constrained optimal framings of curves and surfaces using quaternion Gauss maps	10.1109/VISUAL.1998.745326	http://dx.doi.org/10.1109/VISUAL.1998.745326	375	382	745326	computational geometry;data visualisation;minimisation	3D coordinate frame fields;axial rotational freedom;constrained optimal framings;constrained space;curve/surface visualization;frame axis;heuristic generalization;minimal quaternion measure;minimal tangential acceleration approaches;optimal coordinate frame fields;optimization process;parallel transport;quaternion Gauss maps;quaternion frame fields;quaternion space;sliding ring constraints;standard optimization tools;tangent map	Application software;Chromium;Computer graphics;Computer science;Constraint optimization;Displays;Gaussian processes;Quaternions;Robustness;Visualization		"We propose a general paradigm for computing optimal coordinate frame fields that may be exploited to visualize curves and surfaces. Parallel transport framings, which work well for open curves, generally fail to have desirable properties for cyclic curves and for surfaces. We suggest that minimal quaternion measure provides an appropriate heuristic generalization of parallel transport. Our approach differs from minimal tangential acceleration approaches due to the addition of ""sliding ring"" constraints that fix one frame axis, but allow an axial rotational freedom whose value is varied in the optimization process. Our fundamental tool is the quaternion Gauss map, a generalization to quaternion space of the tangent map for curves and of the Gauss map for surfaces. The quaternion Gauss map takes 3D coordinate frame fields for curves and surfaces into corresponding curves and surfaces constrained to the space of possible orientations in quaternion space. Standard optimization tools provide application specific means of choosing optimal, e.g., length- or area-minimizing, quaternion frame fields in this constrained space."	Hanson, A.J.	Dept. of Comput. Sci., Indiana Univ., Bloomington, IN, USA|c|	37333439100
	SciVis	24-24 Oct. 1998	Converting sets of polygons to manifold surfaces by cutting and stitching	10.1109/VISUAL.1998.745327	http://dx.doi.org/10.1109/VISUAL.1998.745327	383	390	745327	CAD;computational complexity;computational geometry;topology	cutting;edge pinching;edge snapping;linear complexity;manifold polygonal surfaces;manifold surfaces;non manifold sets;optional stitching phase;polygon vertex indices;polygons;real world polygonal surfaces;singular vertices;stitching strategies;surface boundary edges;surface geometry;surface topology;topological singularities;vertex coordinates	Biomedical imaging;Computer bugs;Error correction;Facial animation;Fans;Geometry;Smoothing methods;Software algorithms;Solid modeling;Topology;Visualization		"Many real world polygonal surfaces contain topological singularities that represent a challenge for processes such as simplification, compression, smoothing, etc. We present an algorithm for removing such singularities, thus converting non manifold sets of polygons to manifold polygonal surfaces (orientable if necessary). We identify singular vertices and edges, multiply singular vertices, and cut through singular edges. In an optional stitching phase, we join surface boundary edges that were cut, or whose endpoints are sufficiently close, while guaranteeing that the surface is a manifold. We study two different stitching strategies called ""edge pinching"" and ""edge snapping""; when snapping, special care is required to avoid re-creating singularities. The algorithm manipulates the polygon vertex indices (surface topology) and essentially ignores vertex coordinates (surface geometry). Except for the optional stitching, the algorithm has a linear complexity in the number of vertices edges and faces, and require no floating point operation."	Gueziec, A.;Taubin, Gabriel;Lazarus, F.;Horn, W.	IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA|c|;;;	37372281700;37282608000;37372282900;37361807200
	SciVis	24-24 Oct. 1998	Interpolation of triangle hierarchies	10.1109/VISUAL.1998.745328	http://dx.doi.org/10.1109/VISUAL.1998.745328	391	396	745328	computational geometry;computer animation;interpolation	Rivara element bisection algorithm;animation;conforming hierarchy;conforming triangulations;constraint satisfaction;hierarchical triangulations;intermediate connectivity;intermediate hierarchy;interpolation process;keyframe hierarchies;n-parameter family;root elements;smooth interpolation;triangle hierarchy interpolation;weak compatibility constraint;weak constraints	Animation;Chromium;Computational geometry;Computer graphics;Data structures;Energy resolution;Interpolation;Polynomials;Shape;Solid modeling		We consider interpolation between keyframe hierarchies. We impose a set of weak constraints that allows smooth interpolation between two keyframe hierarchies in an animation or, more generally, allows the interpolation in an n-parameter family of hierarchies. We use hierarchical triangulations obtained by the Rivara element bisection algorithm (M. Rivara, 1984) and impose a weak compatibility constraint on the set of root elements of all keyframe hierarchies. We show that the introduced constraints are rather weak. The strength of our approach is that the interpolation works in the class of conforming triangulations and simplifies the task of finding the intermediate hierarchy, which is the union of the two (or more) keyframe hierarchies involved in the interpolation process. This allows for an efficient generation of the intermediate connectivity and additionally ensures that the intermediate hierarchy is again a conforming hierarchy satisfying the same constraints.	Friedrich, A.;Polthier, K.;Schmies, M.	Tech. Univ. Berlin, Germany|c|;;	37359495500;37282271000;37371960500
	SciVis	24-24 Oct. 1998	Progressive tetrahedralizations	10.1109/VISUAL.1998.745329	http://dx.doi.org/10.1109/VISUAL.1998.745329	397	402	745329	computational geometry;interpolation;mesh generation;rendering (computer graphics)	collapsing edges;cost functions;edge collapses;finite element meshing;progressive simplicial complexes;progressive tetrahedralizations;progressively refined tetrahedralizations;rendering;robust implementations;scattered data interpolation;unstructured volume data	Application software;Computational fluid dynamics;Computer graphics;Computer science;Cost function;Data visualization;Finite element methods;Interpolation;Robustness;Scattering;Testing		The paper describes some fundamental issues for robust implementations of progressively refined tetrahedralizations generated through sequences of edge collapses. We address the definition of appropriate cost functions and explain on various tests which are necessary to preserve the consistency of the mesh when collapsing edges. Although considered a special case of progressive simplicial complexes (J. Popovic and H. Hoppe, 1997), the results of our method are of high practical importance and can be used in many different applications, such as finite element meshing, scattered data interpolation, or rendering of unstructured volume data.	Staadt, O.G.;Gross, M.H.	Comput. Graphics Res. Group, Fed.. Inst. of Technol., Zurich, Switzerland|c|;	37355334600;37275694700
	SciVis	24-24 Oct. 1998	Task-specific visualization design: a case study in operational weather forecasting	10.1109/VISUAL.1998.745330	http://dx.doi.org/10.1109/VISUAL.1998.745330	405	409	745330	data visualisation;geophysics computing;weather forecasting	common framework;generic solutions;highly generic visualizations;operational activities;operational weather forecasting;task-specific visualization design;visualization tools	Atmospheric modeling;Chromium;Computational modeling;Data visualization;Graphics;Hardware;Hybrid integrated circuits;Meteorology;Packaging;Predictive models;Surface topography;Taxonomy;Testing;User interfaces;Weather forecasting;Wind speed;Workstations		Efforts to create highly generic visualizations, both content and interface, often when applied to non research oriented or operational activities are composed of several goals. Although these goals may appear to be related, they are often composed of distinct tasks. Generic solutions, even if domain-specific, may lack sufficient focus to be effective for such purposes. The design of different visualization tools matched to a set of tasks but built on top of a common framework with a similar approach to content is a promising alternative. This hypothesis is tested in detail by application to a demanding problem-operational weather forecasting.	Treinish, L.A.	IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA|c|	37372175500
	SciVis	24-24 Oct. 1998	Development of a multi-source visualization prototype	10.1109/VISUAL.1998.745331	http://dx.doi.org/10.1109/VISUAL.1998.745331	411	414	745331	aerospace computing;aerospace simulation;data visualisation;digital simulation;software reusability;software tools	VISOR;Visual Integration of Simulated and Observed Results;aerospace engineering design;data analysis;data visualization;disparate sources;multi source visualization prototype;reusable software tools	Aerospace engineering;Analytical models;Computational modeling;Data analysis;Data models;Data visualization;Displays;Layout;Libraries;NASA;Network servers;Probes;Prototypes;Software libraries;Space technology;Testing		This case study describes the design and development of VISOR (Visual Integration of Simulated and Observed Results), a tool which supports the visualization and analysis of a wide variety of data relevant to aerospace engineering design. Integrating data from such disparate sources is challenging; overcoming the obstacles results in a powerful tool. The process has also been valuable in exposing requirements for the libraries of reusable software tools for visualization and data analysis being developed at NASA Ames.	Keely, L.;Uselton, S.	NASA Ames Res. Center, Moffett Field, CA, USA|c|;	37372280900;37372247300
	SciVis	24-24 Oct. 1998	Data level comparison of wind tunnel and computational fluid dynamics data	10.1109/VISUAL.1998.745332	http://dx.doi.org/10.1109/VISUAL.1998.745332	415	418	745332	computational fluid dynamics;data visualisation;flow simulation;wind tunnels	computational fluid dynamics data;data level comparative visualization system;data level comparison;data sets;experimental wind tunnel data;fluid flow;mathematical models;user feedback;wind tunnel	Computational fluid dynamics;Computer architecture;Computer science;Data flow computing;Data visualization;Feedback;Fluid flow;Image analysis;Mathematical model;Streaming media		The paper describes the architecture of a data level comparative visualization system and experiences using it to study computational fluid dynamics data and experimental wind tunnel data. We illustrate how the system can be used to compare data sets from different sources, data sets with different resolutions and data sets computed using different mathematical models of fluid flow. Suggested improvements to the system based on user feedback are also discussed.	Shen, Q.;Pang, A.;Uselton, S.	Dept. of Comput. Sci., California Univ., CA, USA|c|;;	37362345800;37267352000;37372247300
	SciVis	24-24 Oct. 1998	Selective visualization of vortices in hydrodynamic flows	10.1109/VISUAL.1998.745333	http://dx.doi.org/10.1109/VISUAL.1998.745333	419	422	745333	computational geometry;data visualisation;hydrodynamics;physics computing;vortices	curve based geometric criteria;hydrodynamic flows;point based scalar quantities;selective visualization;streamlines;vortex detection criteria;vortices	Aerodynamics;Computer science;Design engineering;Design optimization;Eigenvalues and eigenfunctions;Fluid dynamics;Hydrodynamics;Physics;Region 4;Tensile stress;Turbomachinery;Visualization		Vortices are important features in many research and engineering fields. Visualization is an important step in gaining more understanding and control of vortices. Vortex detection criteria fall into two categories: point based scalar quantities, calculated at single points, and curve based geometric criteria, calculated for, e.g., streamlines. The first category is easy to compute, but does not work in all cases. The second category is more intuitive and should work in all cases, but currently only works in 2D (or 3D projected) flows. We show applications of both approaches in hydrodynamic flows.	Sadarjoen, I.A.;Post, F.H.;Bing Ma;Banks, D.C.;Pagendarm, H.-G.	Dept. of Comput. Sci., Delft Univ. of Technol., Netherlands|c|;;;;	37372009600;37295045800;37363531700;37356983700;37372011800
	SciVis	24-24 Oct. 1998	Visual presentation of magnetic resonance images	10.1109/VISUAL.1998.745334	http://dx.doi.org/10.1109/VISUAL.1998.745334	423	426	745334	biomedical MRI;human factors;interactive systems;medical image processing;user interfaces	MRI analysis;computational environment;computational presentation possibilities;computer environments;computer screen;dynamic grouping;field study;human computer interaction studies;image groups;image retrieval;layout adjustment;magnetic resonance image presentation;magnification techniques;medical image analysis;radiologist interactions;radiologists;traditional light screen environment;visual environment;visual presentation	Biomedical imaging;Focusing;Hospitals;Image analysis;Image processing;Large screen displays;Magnetic analysis;Magnetic resonance;Magnetic resonance imaging;Medical diagnostic imaging		Medical image analysis is shifting from current film oriented light screen environments to computer environments that involve viewing and analyzing large sets of images on a computer screen. Magnetic resonance imaging (MRI) studies, in particular, can involve many images. The paper examines how best to meet the needs of radiologists in a computational environment. To this end, a field study was conducted to observe radiologists' interactions during MRI analysis in the traditional light screen environment. Key issues uncovered involve control over focus and context, dynamic grouping of images and retrieval of images and image groups. To address the problem of focus and context, existing layout adjustment and magnification techniques are explored to provide the most appropriate solution. Our interest is in combining the methodologies of human computer interaction studies with computational presentation possibilities to design a visual environment for the crucial field of medical image analysis.	van der Heyden, J.E.;Carpendale, M.S.T.;Atkins, M.S.	Sch. of Comput. Sci., Simon Fraser Univ., Burnaby, BC, Canada|c|;;	37372192800;37326090900;37284302300
	SciVis	24-24 Oct. 1998	Visualization in corneal topography	10.1109/VISUAL.1998.745335	http://dx.doi.org/10.1109/VISUAL.1998.745335	427	430	745335	biology computing;colour graphics;data visualisation;eye	anterior surface;colour mapped representation;corneal topography visualization;direct data representation;eye;height lines;height surface;instantaneous radius;mean normal radius;rainbow colour scale;second order surface properties;shape deviations;surface characteristics;three part display;visualization techniques;well documented artifacts	Charge coupled devices;Charge-coupled image sensors;Cornea;Data visualization;Ellipsoids;Extraterrestrial measurements;Image reconstruction;Instruments;Lenses;Optical reflection;Physics;Shape measurement;Surface fitting;Surface reconstruction;Surface topography		The anterior surface of the eye ('cornea') is extremely important for good sight. Instruments measuring corneal shape conventionally visualize the surface characteristics by mapping the instantaneous radius of curvature onto a rainbow colour scale. This technique is known to have important drawbacks. Firstly, not corneal shape itself is visualized, but rather second order surface properties. Secondly, the type of colouring produces well documented artifacts. We discuss visualization techniques for a more direct representation of the data. In a three part display, shape deviations are presented as a height surface in one window, height lines superimposed over the input image in another, and a colour mapped representation of the mean normal radius of curvature in a third. With the aid of some typical examples, it is shown that these visualizations are easy to interpret by the physician and overcome the limitations of the conventional techniques.	Vos, F.M.;Spoelder, H.J.W.	Delft Univ. of Technol., Netherlands|c|;	37345045400;37284264400
	SciVis	24-24 Oct. 1998	A case study using the virtual environment for reconstructive surgery	10.1109/VISUAL.1998.745336	http://dx.doi.org/10.1109/VISUAL.1998.745336	431	434	745336	computerised tomography;data visualisation;medical computing;mesh generation;surgery;virtual reality	VERS;VR technology;case study;computed tomography scans;custom surgical template;data segmentation;mesh generation;patient-specific mesh;preoperative visualization;reconstructive surgery;severe facial defect;soft tissue tumor;virtual environment	Computed tomography;Data visualization;Image reconstruction;NASA;Neoplasms;Plastics;Radiology;Space technology;Surgery;Virtual environment		The paper details the use of a Virtual Environment for Reconstructive Surgery (VERS) in the case of a 17 year-old boy with a severe facial defect, arising from the removal of a soft tissue tumor. Computed tomography (CT) scans were taken of the patient, the data were segmented, a mesh was generated, and this patient-specific mesh was used in a virtual environment by the surgeons for preoperative visualization of the defect, planning of the surgery, and production of a custom surgical template to aid in repairing the defect. The paper details the case of this patient, provides a background on the virtual environment technology used, discusses the difficulties encountered, and describes the lessons learned.	Montgomery, K.;Stephanides, M.;Schendel, S.;Ross, M.	Nat. Biocomput. Center, Stanford Univ., CA, USA|c|;;;	37329570500;37372282800;37372280500;37364193600
	SciVis	24-24 Oct. 1998	Interactive virtual angioscopy	10.1109/VISUAL.1998.745337	http://dx.doi.org/10.1109/VISUAL.1998.745337	435	438	745337	angiocardiography;interactive systems;medical image processing;real-time systems;rendering (computer graphics);virtual reality	dynamic endoscopic camera control;human carotid artery;human data sets;human vascular system;interactive inspection;interactive point picking;interactive tissue classification;interactive tool;interactive virtual angioscopy;morphological feature measurement;non invasive medical procedure;real time navigation;standard medical imaging modalities;stereoscopic direct volume rendering;virtual environment	Biomedical imaging;Cameras;Control systems;Humans;Medical control systems;Navigation;Real time systems;Standards development;Virtual environment;Volume measurement		Virtual angioscopy is a non invasive medical procedure for exploring parts of the human vascular system. We have developed an interactive tool that takes as input, data acquired with standard medical imaging modalities and regards it as a virtual environment to be interactively inspected. The system supports real time navigation with stereoscopic direct volume rendering and dynamic endoscopic camera control, interactive tissue classification, and interactive point picking for morphological feature measurement. We provide an overview of the system, discuss the techniques used in our prototype, and present experimental results on human data sets.	Gobbetti, E.;Pili, P.;Zorcolo, A.	Center for Adv. Studies, Cagliari, Italy|c|;;	37346861300;37372178500;37372176300
	SciVis	24-24 Oct. 1998	Volumetric modeling of acoustic fields in CNMAT&#39;s sound spatialization theatre	10.1109/VISUAL.1998.745338	http://dx.doi.org/10.1109/VISUAL.1998.745338	439	442	745338	acoustic field;audio signal processing;data visualisation;humanities;real-time systems;signal processing	3D modeling software;CNMAT sound spatialization theatre;acoustic simulation methods;acoustic sound fields;impulse response form;real time visualization;reverberant field;room database;sound engineers;sound spatialization theatre;spatial sound researchers;visualization system;volume visualization strategies;volumetric modeling;volumetric modeling software	Acoustic applications;Acoustic materials;Application software;Art;Audio databases;Instruments;Loudspeakers;Music;Signal design;Space technology;Spatial databases;Visual databases;Visualization		A new tool for real time visualization of acoustic sound fields has been developed for a new sound spatialization theatre. The theatre is described and several applications of the acoustic and volumetric modeling software are presented. The visualization system described is a valuable tool for spatial sound researchers, sound engineers and composers using CNMAT's sound spatialization theatre. Further work is in progress on the adaptation of better acoustic simulation methods (M. Monks et al., 1996) for more accurate display of the quality of the reverberant field. The room database will be automatically extracted from a model built with 3D modeling software. Volume visualization strategies are being explored to display sounds in spectral and impulse response form.	Khoury, S.;Freed, A.;Wessel, D.	CNMAT, Berkeley, CA, USA|c|;;	37362454900;37372286400;37353973700
	SciVis	24-24 Oct. 1998	Supporting detail-in-context for the DNA representation, H-curves	10.1109/VISUAL.1998.745339	http://dx.doi.org/10.1109/VISUAL.1998.745339	443	446	745339	DNA;biology computing;data visualisation;molecular biophysics	DNA representation;DNA sequences;H-curves;detail-in-context viewing;geometric continuity;micro-features;nonoccluding orthogonal technique;three-dimensional distortion algorithm	Bioinformatics;DNA computing;Data visualization;Displays;Genetics;Genomics;Information analysis;Sequences;Spatial databases;Usability;Visual databases		This paper presents a tool for the visual exploration of DNA sequences represented as H-curves. Although very long sequences can be plotted using H-curves, micro-features are lost as sequences get longer. We present a new three-dimensional distortion algorithm to allow the magnification of a sub-segment of an H-curve while preserving a global view of the curve. This is particularly appropriate for H-curves as they provide useful visual information at several resolutions. Our approach also extends the current possibilities of detail-in-context viewing in 3D. It provides a non-occluding, orthogonal technique that preserves uniform scaling within regions and maintains geometric continuity between regions.	Lantin, M.L.;Carpendale, M.S.T.	Sch. of Comput. Sci., Simon Fraser Univ., Burnaby, BC, Canada|c|;	37371967400;37326090900
	SciVis	24-24 Oct. 1998	Visualizing Hilbert curves	10.1109/VISUAL.1998.745340	http://dx.doi.org/10.1109/VISUAL.1998.745340	447	450	745340	Hilbert spaces;computer animation;physics computing;rendering (computer graphics)	Hilbert curves visualisation;computer animated movie;volume rendering	Animation;Data compression;Image coding;Laboratories;Motion pictures;Pixel;Production;Switches;Tiles;Visualization		A computer animated movie was produced, illustrating both 2D and 3D Hilbert curves, and showing the transition from 2D to 3D with the help of volume rendering.	Max, N.	Lawrence Livermore Nat. Lab., CA, USA|c|	37267387800
	SciVis	24-24 Oct. 1998	Rear-projecting virtual data onto physical terrain: an exercise in two senses being better than one	10.1109/VISUAL.1998.745341	http://dx.doi.org/10.1109/VISUAL.1998.745341	451	454	745341	data visualisation;geophysics computing;terrain mapping	Alaska;Prince of Wales Island;USGS terrain information;combined physical model fabrication and virtual computer-based data display;deforestation data;physical terrain;rear-projecting virtual data;terrain features;visualization presentation	Computer displays;Data visualization;Fabrication;Geographic Information Systems;Physics computing;Prototypes;Space shuttles;Supercomputers;Terrain mapping;Wildlife		This paper describes a project that combined physical model fabrication and virtual computer-based data display to create a unique visualization presentation. USGS terrain information on Prince of Wales Island, Alaska was used to create a physical prototype in SDSC's TeleManufacturing Facility. This model was then used as a mold to create a translucent plate of the terrain. Finally, deforestation data from the island was color mapped and rear-projected onto the translucent plate within a light box. The result is a very compelling display in which both the senses of sight and touch are used to make relationships between terrain features and the data more readily apparent.	Clark, D.;Marciano, R.;McKeon, R.;Bailey, M.	Supercomput. Center, California Univ., San Diego, La Jolla, CA, USA|c|;;;	37364046500;37294917000;37372250000;37280473500
	SciVis	24-24 Oct. 1998	Intent, perception, and out-of-core visualization applied to terrain	10.1109/VISUAL.1998.745342	http://dx.doi.org/10.1109/VISUAL.1998.745342	455	458	745342	data visualisation;geophysics computing;terrain mapping	interactive terrain visualization;interactive visualization;multiresolution data;out-of-core visualization;terrain datasets;visual perception	Analytical models;Data visualization;Graphics;Large-scale systems;Memory management;Performance analysis;Spatial resolution;Three dimensional displays;Usability;Visual perception		This paper considers how out-of-core visualization applies to terrain datasets, which are among the largest now presented for interactive visualization and can range to sizes of 20 GB and more. It is found that a combination of out-of-core visualization, which tends to focus on 3D data, and visual simulation, which places an emphasis on visual perception and real-time display of multiresolution data, results in interactive terrain visualization with significantly improved data access and quality of presentation. Further, the visual simulation approach provides qualities that are useful for general data, not just terrain.	Davis, D.;Jiang, T.Y.;Ribarsky, W.;Faust, N.	Graphics, Visualization & Usability Center, Georgia Inst. of Technol., Atlanta, GA, USA|c|;;;	37358762700;37366640700;37300425000;37326280000
	SciVis	24-24 Oct. 1998	Production visualization for the ASCI One TeraFLOPS machine	10.1109/VISUAL.1998.745343	http://dx.doi.org/10.1109/VISUAL.1998.745343	459	462	745343	data visualisation;parallel machines;parallel programming;physics computing;pipeline processing	64 bit addressing;ASCI One TeraFLOPS machine;data transfer;fully parallel data visualization pipeline;physics simulation code;post processing;production data visualization;production visualization;rendering;terascale computing	Acceleration;Analytical models;Computational modeling;Concurrent computing;Data visualization;Physics computing;Pipelines;Production;Productivity;Systems engineering and theory		"The delivery of the first one tera-operations/sec computer has significantly impacted production data visualization, affecting data transfer, post processing, and rendering. Terascale computing has motivated a need to consider the entire data visualization system; improving a single algorithm is not sufficient. This paper presents a systems approach to decrease by a factor of four the time required to prepare large data sets for visualization. For daily production use, all stages in the processing pipeline from physics simulation code to pixels on a screen, must be balanced to yield good overall performance. Performance of the initial visualization system is compared with recent improvements. ""Lessons learned"" from the coordinated deployment of improved algorithms also are discussed, including the need for 64 bit addressing and a fully parallel data visualization pipeline."	Heermann, P.D.	Sandia Nat. Labs., Albuquerque, NM, USA|c|	37346784400
	SciVis	24-24 Oct. 1998	Battlefield visualization on the responsive workbench	10.1109/VISUAL.1998.745344	http://dx.doi.org/10.1109/VISUAL.1998.745344	463	466	745344	command and control systems;data visualisation;virtual reality	Dragon;Hunter Warrior advanced warfighting experiment;Joint Counter Mine advanced concept tactical demonstration;battlefield visualization;real-world deployments;virtual reality responsive workbench	Command and control systems;Computer errors;Counting circuits;Data visualization;Laboratories;Large-scale systems;Manuals;Military computing;Virtual environment;Virtual reality		In this paper we describe a battlefield visualization system, called Dragon, which we have implemented on a virtual reality responsive workbench. The Dragon system has been successfully deployed as part of two large military exercises: the Hunter Warrior advanced warfighting experiment, in March 1997, and the Joint Counter Mine advanced concept tactical demonstration, in August and September 1997. We describe battlefield visualization, the Dragon system, and the workbench, and we describe our experiences as part of these two real-world deployments, with an emphasis on lessons learned and needed future work.	Durbin, J.;Swan, J.E.;Colbert, B.;Crowe, J.;King, R.;Scannell, C.;Wartell, Z.;Welsh, T.	Naval Res. Lab., Washington, DC, USA|c|;;;;;;;	37371968700;37295140400;37371969500;37359546500;37360767800;37371957500;37297344900;37371958200
	SciVis	24-24 Oct. 1998	Scientific visualization and data modeling of scattered sediment contaminant data in New York/New Jersey estuaries	10.1109/VISUAL.1998.745345	http://dx.doi.org/10.1109/VISUAL.1998.745345	467	470	745345	data models;data visualisation;environmental science computing;sediments;water pollution	IBM Data Explorer;New York/New Jersey estuaries;data modeling;routine shipping channel dredging;scattered sediment contaminant data;scientific data visualization;scientific visualization;spatial characteristics;spectral domain-decomposition scattered data model;toxic inorganic compounds;toxic organic compounds	Concurrent computing;Data models;Data visualization;Geophysics computing;Inorganic compounds;Pollution;Protection;Region 2;Sampling methods;Scattering;Sediments;Software packages		Sediments in many parts of the New York and New Jersey estuary system are contaminated with toxic organic and inorganic compounds by different sources. Because of the potential environmental consequences, detailed information on the spatial distribution of sediment contaminants is essential in order to carry out routine shipping channel dredging in an environmentally responsible way, and to remediate hot spots cost-effectively and safely. Scientific visualization and scatter data modeling techniques have been successfully applied in analyzing the sparse sampling data of sediment contaminants in New York and New Jersey estuaries, the underlying spatial characteristics of which are otherwise difficult to comprehend. Continuous realizations of contaminant concentrations in the region were obtained by using a spectral domain-decomposition scattered data model and IBM Data Explorer which is a software package for scientific data visualization.	Ma, H.;Jones, K.W.;Stern, E.A.	Brookhaven Nat. Lab., Upton, NY, USA|c|;;	37365761300;37363102200;37371812900
	SciVis	24-24 Oct. 1998	POPTEX: Interactive ocean model visualization using texture mapping hardware	10.1109/VISUAL.1998.745346	http://dx.doi.org/10.1109/VISUAL.1998.745346	471	474	745346	data visualisation;geophysics computing;image texture;oceanographic techniques	POP ocean model;POPTEX;data exploration;global circulation models;high speed graphics hardware;interactive ocean model visualization;ocean circulation simulation;texture mapping hardware;texture memory	Acceleration;Atmospheric modeling;Computational modeling;Data visualization;Graphics;Hardware;Laboratories;Marine technology;Ocean temperature;Spatial resolution		Global circulation models are used to gain an understanding of the processes that affect the Earth's climate and may ultimately be used to assess the impact of humanity's activities on it. The POP ocean model developed at Los Alamos is an example of such a global circulation model that is being used to investigate the role of the ocean in the climate system. Data output from POP has traditionally been visualized using video technology which precludes rapid modification of visualization parameters and techniques. This paper describes a visualization system that leverages high speed graphics hardware, specifically texture mapping hardware, to accelerate data exploration to interactive rates. We describe the design of the system, the specific hardware features used, and provide examples of its use. The system is capable of viewing ocean circulation simulation results at up to 60 frames per second while loading texture memory at approximately 72 million texels per second.	McPherson, A.;Maltrud, M.	Adv. Comput. Lab., Los Alamos Nat. Lab., NM, USA|c|;	37360217100;37299279900
	SciVis	24-24 Oct. 1998	Acoustic imaging and visualization of plumes discharging from black smoker vents on the deep seafloor	10.1109/VISUAL.1998.745347	http://dx.doi.org/10.1109/VISUAL.1998.745347	475	478	745347	backscatter;data visualisation;geophysical signal processing;oceanographic techniques;seafloor phenomena;sonar imaging;underwater sound	acoustic images;acoustic imaging;backscattering;black smoker vents;centerline location;cross sectional area;deep seafloor;hydrothermal plumes;isointensity surfaces;maximum backscatter intensity;metallic mineral particles;multiple representations;plume visualization;plume volume;seawater;thermal plumes	Acoustic imaging;Acoustic measurements;Backscatter;Image analysis;Sea floor;Surface reconstruction;Thermal pollution;Vents;Visualization;Volume measurement		Visualization and quantification methods are being developed to analyze our acoustic images of thermal plumes containing metallic mineral particles that discharge from hot springs on the deep seafloor. The acoustic images record intensity of backscattering from the particulate matter suspended in the plumes. The visualization methods extract, classify, visualize, measure and track reconstructions of the plumes, depicted by isointensity surfaces as 3D volume objects and 2D slices. The parameters measured, including plume volume, cross sectional area, centerline location (trajectory), surface area and isosurfaces at percentages of maximum backscatter intensity, are being used to derive elements of plume behavior including expansion with height, dilution, and mechanisms of entrainment of surrounding seawater. Our aim is to compare the observational data with predictions of plume theory to test and advance models of the behavior of hydrothermal plumes through the use of multiple representations.	Rona, P.;Bemis, K.;Kenchammana-Hosekote, D.;Silver, D.	Inst. of Marine & Coastal Sci., Rutgers Univ., New Brunswick, NJ, USA|c|;;;	37282733200;37282733100;37372284000;37274132700
	SciVis	24-24 Oct. 1998	Seabed visualization	10.1109/VISUAL.1998.745348	http://dx.doi.org/10.1109/VISUAL.1998.745348	479	481	745348	bathymetry;data visualisation;geophysics computing;oceanographic techniques;sonar imaging	annual survey inspections;harbour wall;high resolution bathymetric data;high speed multi-frequency continuous scan sonar;seabed visualization;shipwreck visualization	Acoustic signal detection;Computer graphics;Computer science;Data acquisition;Data visualization;Inspection;Sonar detection;Sonar navigation;Surfaces;Tides		The development of a high speed multi-frequency continuous scan sonar at Sonar Research & Development Ltd has resulted in the acquisition of extremely accurate, high resolution bathymetric data. This rich underwater data provides new challenges and possibilities within the field of seabed visualization. This paper introduces the reader to seabed visualization by describing two example case studies which use the Seabed Visualization System developed at SRD. Both case studies, harbour wall and shipwreck visualization, are implemented using real survey data. The high resolution of the data obtained means slight changes in the seabed topography are easily distinguishable. Annual survey inspections in both case studies enable comparisons to be made between the data sets making the visualization system an important tool for management and planning.	Chapman, P.;Stevens, P.;Wills, D.;Brookes, G.	Dept. of Comput. Sci., Hull Univ., UK|c|;;;	37364789000;37373315000;37368030400;37372177500
	SciVis	24-24 Oct. 1998	Configuration space visualization for mechanical design	10.1109/VISUAL.1998.745349	http://dx.doi.org/10.1109/VISUAL.1998.745349	483	486	745349	CAD;computational geometry;data visualisation;engineering graphics;mechanical contact;mechanical engineering computing	computer-aided mechanical design;configuration space visualization;contact analysis;degrees of freedom;general planar systems;geometric problems;geometric representation;motion constraints;part motion paths;qualitative information;quantitative information;rigid-body interaction;system failure modes;three-dimensional spaces;topology	Biomedical computing;Cameras;Computer science;Mechanical systems;Motion pictures;Pervasive computing;Shape measurement;Software design;Software prototyping;Visualization		We are studying difficult geometric problems in computer-aided mechanical design where visualization plays a key role. The research addresses the fundamental design task of contact analysis: deriving the part contacts and the ensuing motion constraints in a mechanical system. We have automated contact analysis of general planar systems via configuration space computation. Configuration space is a geometric representation of rigid-body interaction that encodes quantitative information, such as part motion paths, and qualitative information, such as system failure modes. The configuration space dimension equals the number of degrees of freedom in the system. Three-dimensional spaces are most important, but higher-dimensions are often useful. The qualitative aspects, which relate to the topology of the configuration space, are best understood by visualization. We explain what configuration space is, how it encodes contact information, and what research challenges it poses for visualization.	Sacks, E.;Joskowicz, L.	Purdue Univ., West Lafayette, IN, USA|c|;	37282563500;37371987800
	SciVis	24-24 Oct. 1998	Three-dimensional visualization of microstructures	10.1109/VISUAL.1998.745350	http://dx.doi.org/10.1109/VISUAL.1998.745350	487	490	745350	data visualisation;engineering graphics;image reconstruction;mechanical engineering computing;optical microscopy;scanning electron microscopy;steel industry	3D reconstruction;3D visualization;alloy steel;case study;cementite precipitates;chemical etching;internal microscopic structure;microstructures;morphology;optical microscopy;polishing;reference marks;scanning electron microscopy;thermal processing;three-dimensional visualization	Chemicals;Electron optics;Etching;Microstructure;Optical materials;Optical microscopy;Scanning electron microscopy;Surface morphology;Surface reconstruction;Visualization		This case study describes a technique for the three-dimensional analysis of the internal microscopic structure (microstructure) of materials. This technique consists of incrementally polishing through a thin layer (approximately 0.2 μm) of material, chemically etching the polished surface, applying reference marks, and performing optical or scanning electron microscopy on selected areas. The series of images are then processed employing AVS and other visualization software to obtain a 3D reconstruction of the material. We describe how we applied this technique to an alloy steel to study the morphology, connectivity, and distribution of cementite precipitates formed during thermal processing. The results showed microstructural features not previously identified with traditional 2D techniques.	Lanzagorta, M.;Swan, J.E.;Spanos, G.;Rosenberg, R.;Kuo, E.	Naval Res. Lab., Washington, DC, USA|c|;;;;	37295678700;37295140400;37372261800;37285510700;37372263000
	SciVis	24-24 Oct. 1998	Visualization for multiparameter aircraft designs	10.1109/VISUAL.1998.745351	http://dx.doi.org/10.1109/VISUAL.1998.745351	491	494	745351	CAD;aerospace computing;aircraft;constraint theory;data visualisation;engineering graphics;mechanical engineering computing;minimisation	constraint satisfaction;high dimensional space;minimization;multidimensional visualization;multiparameter aircraft design;optimization	Aerodynamics;Aircraft propulsion;Computational fluid dynamics;Computer science;Constraint optimization;Costs;Design optimization;Fuels;Mathematics;Multidimensional systems;Visualization		We describe an aircraft design problem in high dimensional space, with D typically being 10 to 30. In some respects this is a classic optimization problem, where the goal is to find the point that minimizes an objective function while satisfying a set of constraints. However, evaluating an individual point is expensive, and the high dimensionality makes many approaches to solving the problem infeasible. The difficulty of the problem means that aircraft designers would benefit from any insights that can be provided. We discuss how simple visualizations have already proved beneficial, and then describe how visualization might be of further help in the future.	Shaffer, C.A.;Knill, D.L.;Watson, L.T.	Dept. of Comput. Sci., Virginia Polytech. Inst. & State Univ., Blacksburg, VA, USA|c|;;	37273199900;37371928100;37273212300
	SciVis	24-24 Oct. 1998	Why is Real-Time Volume Rendering No Longer a Year Away?	10.1109/VISUAL.1998.745352	http://dx.doi.org/10.1109/VISUAL.1998.745352	497	499	745352			Acceleration;Biomedical imaging;Computer architecture;Computer graphics;Hardware;Pipelines;Rendering (computer graphics);Three dimensional displays;Visualization;Workstations			Kaufman, A.;Brady, M.;Kitson, F.;Pfister, H.	General Electric Corporation|c|;;;	37268052800;37291463600;37295399300;37275698100
	SciVis	24-24 Oct. 1998	Multi-Source Data Analysis Challenges	10.1109/VISUAL.1998.745353	http://dx.doi.org/10.1109/VISUAL.1998.745353	501	504	745353			Acceleration;Application software;Atmospheric modeling;Computational modeling;Data analysis;Data visualization;Laboratories;Medical simulation;Nuclear weapons;Testing			Uselton, S.	University of North Carolina at Chapel Hill|c|	
	SciVis	24-24 Oct. 1998	Key Problems and Thorny Issues in Multidimensional Visualization	10.1109/VISUAL.1998.745354	http://dx.doi.org/10.1109/VISUAL.1998.745354	505	506	745354			Computer displays;Data visualization;Guidelines;Humans;Mirrors;Multidimensional systems;NIST;Roads;Usability;Visual databases			Grinstein, G.	University of Massachusetts Lowell|c|	
	SciVis	24-24 Oct. 1998	Art and Visualization: Oil and Water?	10.1109/VISUAL.1998.745355	http://dx.doi.org/10.1109/VISUAL.1998.745355	507	509	745355			Art;Bandwidth;Biology computing;Concrete;Concurrent computing;Humans;Mathematics;Painting;Petroleum;Visualization			Laidlaw, D.	University of Minnesota Computer Scientist|c|	
	SciVis	24-24 Oct. 1998	Interactive ray tracing for isosurface rendering	10.1109/VISUAL.1998.745713	http://dx.doi.org/10.1109/VISUAL.1998.745713	233	238	745713	colour graphics;computational geometry;distributed shared memory systems;interactive systems;optimisation;ray tracing;rendering (computer graphics);very large databases	SGI Reality Monster;Visible Woman dataset;analytic isosurface intersection computation;color image;computational cost;distributed shared-memory multiprocessor;geometry;interactive isosurfacing;interactive ray tracing;isosurface rendering;optimization;pixel;scalability;shallow hierarchy;very large rectilinear datasets;volume bricking;z-buffer	Computational efficiency;Computational geometry;Computer graphics;Computer science;Displays;Hardware;Isosurfaces;Ray tracing;Rendering (computer graphics);Scalability		We show that it is feasible to perform interactive isosurfacing of very large rectilinear datasets with brute-force ray tracing on a conventional (distributed) shared-memory multiprocessor machine. Rather than generate geometry representing the isosurface and render with a z-buffer, for each pixel we trace a ray through a volume and do an analytic isosurface intersection computation. Although this method has a high intrinsic computational cost, its simplicity and scalability make it ideal for large datasets on current high-end systems. Incorporating simple optimizations, such as volume bricking and a shallow hierarchy, enables interactive rendering (i.e. 10 frames per second) of the 1 GByte full resolution Visible Woman dataset on an SGI Reality Monster. The graphics capabilities of the Reality Monster are used only for display of the final color image.	Parker, S.;Shirley, P.;Livnat, Y.;Hansen, C.;Sloan, P.-P.	Dept. of Comput. Sci., Utah Univ., Salt Lake City, UT, USA|c|;;;;	37361048100;37266808400;37282553200;37266777200;37373213900
