Paper ID	Conference	Year	Title	DOI	Link	First Page	Last Page	IEEE Xplore Article Number	INSPEC Controlled	INSPEC Non-Controlled	IEEE Terms	Author Keywords	Abstract	Author Names	Author Affiliations	Author IDs
	InfoVis	21-21 Oct. 1997	H3: laying out large directed graphs in 3D hyperbolic space	10.1109/INFVIS.1997.636718	http://dx.doi.org/10.1109/INFVIS.1997.636718	2	10	636718	data visualisation;diagrams;directed graphs;optimisation;trees (mathematics)	3D hyperbolic space;H3 layout technique;cone tree layout algorithm;data visualization;domain-specific knowledge;euclidean 3-space;graph drawing;hierarchical data;hyperbolic navigation;large directed graphs;node-link diagrams;optimization;spanning tree;subtree pruning;visual clutter	Computer science;File systems;History;Information systems;Mouth;Navigation;Tree graphs;Visualization;Web page design;Web sites		We present the H3 layout technique for drawing large directed graphs as node-link diagrams in 3D hyperbolic space. We can lay out much larger structures than can be handled using traditional techniques for drawing general graphs because we assume a hierarchical nature of the data. We impose a hierarchy on the graph by using domain-specific knowledge to find an appropriate spanning tree. Links which are not part of the spanning tree do not influence the layout but can be selectively drawn by user request. The volume of hyperbolic 3-space increases exponentially, as opposed to the familiar geometric increase of euclidean 3-space. We exploit this exponential amount of room by computing the layout according to the hyperbolic metric. We optimize the cone tree layout algorithm for 3D hyperbolic space by placing children on a hemisphere around the cone mouth instead of on its perimeter. Hyperbolic navigation affords a Focus+Context view of the structure with minimal visual clutter. We have successfully laid out hierarchies of over 20,000 nodes. Our implementation accommodates navigation through graphs too large to be rendered interactively by allowing the user to explicitly prune or expand subtrees.	Munzner, T.	Stanford Univ., CA, USA|c|	37349490300
	InfoVis	21-21 Oct. 1997	Visualizing information on a sphere	10.1109/INFVIS.1997.636759	http://dx.doi.org/10.1109/INFVIS.1997.636759	11	16	636759	bank data processing;data encapsulation;data visualisation;object-oriented languages;object-oriented programming;risk management	IVORY;Java;banking industry;blobby clustering mechanism;encapsulation;energy minimum;financial service providers;information visualization;large data sets;mass-spring system;multidimensional relations;prototype system;risk analysis;semantic analysis;similarity measures;spatial neighborhood;sphere;stock prediction;topological arrangements	Banking;Computer industry;Computer science;Data visualization;Encapsulation;Energy measurement;Fingers;Information retrieval;Multidimensional systems;Risk analysis;Springs;World Wide Web		We describe a method for the visualization of information units on spherical domains which is employed in the banking industry for risk analysis, stock prediction and other tasks. The system is based on a quantification of the similarity of related objects that governs the parameters of a mass-spring system. Unlike existing approaches we initialize all information units onto the inner surface of two concentric spheres and attach them with springs to the outer sphere. Since the spring stiffnesses correspond to the computed similarity measures, the system converges into an energy minimum which reveals multidimensional relations and adjacencies in terms of spatial neighborhoods. Depending on the application scenario our approach supports different topological arrangements of related objects. In order to cope with large data sets we propose a blobby clustering mechanism that enables encapsulation of similar objects by implicit shapes. In addition, we implemented various interaction techniques allowing semantic analysis of the underlying data sets. Our prototype system IVORY is written in Java, and its versatility is illustrated by an example from financial service providers.	Gross, M.H.;Sprenger, T.C.	Dept. of Comput. Sci., Zurich Univ., Switzerland|c|;	37275694700;37374656700
	InfoVis	21-21 Oct. 1997	A spreadsheet approach to information visualization	10.1109/INFVIS.1997.636761	http://dx.doi.org/10.1109/INFVIS.1997.636761	17	24	636761	data structures;data visualisation;spreadsheet programs;user interfaces	complex data set;data complexity;data operations;data types;domain-specific operators;information visualization;inter-cell operations;multidimensional datasets;prototype systems;selection criteria;spreadsheet approach;view operations;viewing parameters;viewing specifications	Arithmetic;Computer science;Costs;Data visualization;Displays;Filtering;Multidimensional systems;Navigation;Programming environments;Prototypes;Software maintenance;Tires		In information visualization, as the volume and complexity of the data increases, researchers require more powerful visualization tools that enable them to more effectively explore multidimensional datasets. We discuss the general utility of a novel visualization spreadsheet framework. Just as a numerical spreadsheet enables exploration of numbers, a visualization spreadsheet enables exploration of visual forms of information. We show that the spreadsheet approach facilitates certain information visualization tasks that are more difficult using other approaches. Unlike traditional spreadsheets, which store only simple data elements and formulas in each cell, a visualization spreadsheet cell can hold an entire complex data set, selection criteria, viewing specifications, and other information needed for a full-fledged information visualization. Similarly, inter-cell operations are far more complex, stretching beyond simple arithmetic and string operations to encompass a range of domain-specific operators. We have built two prototype systems that illustrate some of these research issues. The underlying approach in our work allows domain experts to define new data types and data operations, and enables visualization experts to incorporate new visualizations, viewing parameters, and view operations.	Ed Huai-Hsin Chi;Barry, P.;Riedl, J.;Konstan, J.	Dept. of Comput. Sci., Minnesota Univ., Minneapolis, MN, USA|c|;;;	37362911700;37610041700;37282873000;37282872500
	InfoVis	21-21 Oct. 1997	Adaptive information visualization based on the user&#39;s multiple viewpoints-interactive 3D visualization of the WWW	10.1109/INFVIS.1997.636778	http://dx.doi.org/10.1109/INFVIS.1997.636778	25	28	636778	Internet;data visualisation;graphical user interfaces;human factors;hypermedia;information retrieval;interactive systems	CVI;Internet;RF-Cone;World Wide Web;adaptive information visualization method;graphical user interfaces;hypermedia;information retrieval;interactive 3D visualization;interactive viewpoint selection;multiple viewpoints;semantic relationship;structural relationship	Character generation;Computer graphics;Hardware;Information retrieval;Navigation;Personal communication networks;Research and development;Visualization;Workstations;World Wide Web		We introduce the adaptive information visualization method for hypermedia and the WWW based on the user's multiple viewpoints. We propose two graphical interfaces, the CVI and the RF-Cone. The CVI is the interface for interactive viewpoint selection. We can select a viewpoint reflecting our interests by using the CVI. According to the given viewpoint, the RF-Cone adaptively organizes the 3D representation of the hypermedia so that we can understand the semantic and structural relationship of the hypermedia and easily retrieve the information. Combining these methods, we have developed the WWW visualization system which can provide highly efficient navigation.	Teraoka, T.;Maruyama, M.	Adv. Technol. R&D Center, Mitsubishi Electr. Corp., Amagasaki, Japan|c|;	37361571100;37274551000
	InfoVis	21-21 Oct. 1997	Managing software with new visual representations	10.1109/INFVIS.1997.636782	http://dx.doi.org/10.1109/INFVIS.1997.636782	30	37	636782	data visualisation;project management;scheduling;software development management;statistical analysis;visual programming	data visualization;diverse data types;large data volume management;large project management;multi-million line software project;project resource tracking;scheduling;software development management;statistics;time dependent data viewing;visual programming;visual representations	Computer science;Data visualization;Engineering management;Human resource management;Processor scheduling;Production;Project management;Resource management;Software engineering;Statistics		Managing large projects is a very challenging task requiring the tracking and scheduling of many resources. Although new technologies have made it possible to automatically collect data on project resources, it is very difficult to access this data because of its size and lack of structure. We present three novel glyphs for simplifying this process and apply them to visualizing statistics from a multi-million line software project. These glyphs address four important needs in project management: viewing time dependent data; managing large data volumes; dealing with diverse data types; and correspondence of data to real-world concepts.	Chuah, M.C.;Eick, S.G.	Sch. of Comput. Sci., Carnegie Mellon Univ., Pittsburgh, PA, USA|c|;	;37282570100
	InfoVis	21-21 Oct. 1997	On integrating visualization techniques for effective software exploration	10.1109/INFVIS.1997.636784	http://dx.doi.org/10.1109/INFVIS.1997.636784	38	45	636784	data visualisation;graph theory;graphical user interfaces;hypermedia;reverse engineering;software tools;visual programming	SHriMP;Simple Hierarchical MultiPerspective visualization;animated panning;fisheye-view visualization;hypertext link-following metaphor;multiple focal points;nested graph view;program comprehension;program understanding;reverse engineering;software exploration;software structure;source code browsing;visualization techniques;zooming	Animation;Data structures;Data visualization;Documentation;Information retrieval;Navigation;Programming profession;Software maintenance;Software systems;Software tools;Switches		This paper describes the SHriMP visualization technique for seamlessly exploring software structure and browsing source code, with a focus on effectively assisting hybrid program comprehension strategies. The technique integrates both pan+zoom and fisheye-view visualization approaches for exploring a nested graph view of software structure. The fisheye-view approach handles multiple focal points, which are necessary when examining several subsystems and their mutual interconnections. Source code is presented by embedding code fragments within the nodes of the nested graph. Finer connections among these fragments are represented by a network that is navigated using a hypertext link-following metaphor. SHriMP combines this hypertext metaphor with animated panning and zooming motions over the nested graph to provide continuous orientation and contextual cues for the user. The SHriMP tool is being evaluated in several user studies. Observations of users performing program understanding tasks with the tool are discussed.	Storey, M.-A.D.;Wong, K.;Fracchia, F.D.;Muller, H.A.	Sch. of Comput. Sci., Simon Fraser Univ., Burnaby, BC, Canada|c|;;;	37268043300;37278683200;37611726800;37271730100
	InfoVis	21-21 Oct. 1997	Cacti: a front end for program visualization	10.1109/INFVIS.1997.636785	http://dx.doi.org/10.1109/INFVIS.1997.636785	46	49	636785	data visualisation;program diagnostics;reverse engineering;software tools;visual programming	Cacti;data model;data sources;dynamic analysis;high-quality;multiple data sources;program visualization front end;queries;resource files;software understanding;static analysis;visual universal-relation front end	Computer science;Data models;Data visualization;Displays;History;Information analysis;Programming profession;Software maintenance;Software quality;Software standards;Timing		We describe a system that allows the user to rapidly construct program visualizations over a variety of data sources. Such a system is a necessary foundation for using visualization as an aid to software understanding. The system supports an arbitrary set of data sources so that information from both static and dynamic analysis can be combined to offer meaningful software visualizations. It provides the user with a visual universal-relation front end that supports the definition of queries over multiple data sources without knowledge of the structure or contents of the sources. It uses a flexible back end with a range of different visualizations, most geared to the efficient display of large amounts of data. The result is a high-quality, easy-to-define program visualization that can address specific problems and hence is useful for software understanding. The overall system is flexible and extensible in that both the underlying data model and the set of visualizations are defined in resource files.	Reiss, S.P.	Dept. of Comput. Sci., Brown Univ., Providence, RI, USA|c|	37269712400
	InfoVis	21-21 Oct. 1997	Nonlinear magnification fields	10.1109/INFVIS.1997.636786	http://dx.doi.org/10.1109/INFVIS.1997.636786	51	58	636786	data visualisation;user interfaces	abstract representation;data visualization;data-driven magnifications;explicit foci;magnification brushing;magnification value specification;manipulation;nonlinear magnification fields;transformation routines;user interfaces	Chromium;Computer interfaces;Computer science;Data visualization;Displays;Ear;Focusing;Information systems;Lenses;Nonlinear distortion;Piecewise linear approximation;Piecewise linear techniques;Rubber		We introduce nonlinear magnification fields as an abstract representation of nonlinear magnification, providing methods for converting transformation routines to magnification fields and vice-versa. This new representation provides ease of manipulation and power of expression. By removing the restrictions of explicit foci and allowing precise specification of magnification values, we can achieve magnification effects which were not previously possible. Of particular interest are techniques we introduce for expressing complex and subtle magnification effects through magnification brushing, and allowing intrinsic properties of the data being visualized to create data-driven magnifications.	Keahey, T.A.;Robertson, E.L.	Dept. of Comput. Sci., Indiana Univ., Bloomington, IN, USA|c|;	;37340580700
	InfoVis	21-21 Oct. 1997	Managing multiple focal levels in Table Lens	10.1109/INFVIS.1997.636787	http://dx.doi.org/10.1109/INFVIS.1997.636787	59	63	636787	data analysis;data visualisation;user interfaces;very large databases	Table Lens;context visualization;data values;design enhancements;exploratory data analysis;focus visualization;graphical representations;large data tables;multiple focal level management;screen space;single pixel row;spreadsheet;user interface	Data analysis;Data visualization;Displays;Lenses;Navigation;Optical design;Rendering (computer graphics)		The Table Lens, focus+context visualization for large data tables, allows users to see 100 times as many data values as a spreadsheet in the same screen space in a manner that enables an extremely immediate form of exploratory data analysis. In the original Table Lens design, data are shown in the context area using graphical representations in a single pixel row. Scaling up the Table Lens technique beyond approximately 500 cases (rows) by 40 variables (columns) requires not showing every value individually and thus raises challenges for preserving the exploratory and navigational ease and power of the original design. We describe two design enhancements for introducing regions of less than a pixel row for each data value and discuss the issues raised by each.	Tenev, T.;Rao, R.	Xerox Palo Alto Res. Center, CA, USA|c|;	37621651000;37616700600
	InfoVis	21-21 Oct. 1997	Coordinating declarative queries with a direct manipulation data exploration environment	10.1109/INFVIS.1997.636788	http://dx.doi.org/10.1109/INFVIS.1997.636788	65	72	636788	data analysis;data visualisation;database management systems;graphical user interfaces;interactive systems;query languages;query processing;visual languages	VQE;Visage;consistency;data analysis;data exploration environment;database systems;dataset;declarative queries;direct manipulation;graphical user interfaces;information centric paradigm;interactive visualization;multiple objects;visual query language	Data analysis;Data visualization;Database languages;Database systems;Feedback;Joining processes;Object oriented modeling;Robot kinematics;User interfaces;Visual databases		Interactive visualization techniques allow data exploration to be a continuous process, rather than a discrete sequence of queries and results as in traditional database systems. However limitations in expressive power of current visualization systems force users to go outside the system and form a new dataset in order to perform certain operations, such as those involving the relationship among multiple objects. Further, there is no support for integrating data from the new dataset into previous visualizations, so users must recreate them. Visage's information centric paradigm provides an architectural hook for linking data across multiple queries, removing this overhead. This paper describes the addition to Visage of a visual query language, called VQE, which allows users to express more complicated queries than in previous interactive visualization systems. Visualizations can be created from queries and vice versa. When either is updated, the other changes to maintain consistency.	Derthick, M.;Roth, S.F.;Kolojejchick, J.	Robotics Inst., Carnegie Mellon Univ., Pittsburgh, PA, USA|c|;;	37444609500;37357418100;37352245400
	InfoVis	21-21 Oct. 1997	Domesticating Bead: adapting an information visualization system to a financial institution	10.1109/INFVIS.1997.636789	http://dx.doi.org/10.1109/INFVIS.1997.636789	73	80	636789	data analysis;data visualisation;financial data processing;graphical user interfaces;spreadsheet programs	2D visualization tool;3D visualizations;3D web browsers;Bead visualization system;UBS;data analysis;data types;financial institution;high-dimensional data layout;information visualization system;low-dimensional space;similarity metrics;spreadsheets	Collaboration;Data analysis;Data visualization;Databases;Financial management;Guidelines;Humans;Information systems;Layout;Marketing management;Multidimensional systems;Navigation;Scattering;Stochastic processes;Three dimensional displays;Two dimensional displays;Virtual environment		The Bead visualization system employs a fast algorithm for laying out high-dimensional data in a low-dimensional space, and a number of features added to 3D visualizations to improve imageability. We describe recent work on both aspects of the system, in particular a generalization of the data types laid out and the implementation of imageability features in a 2D visualization tool. The variety of data analyzed in a financial institution such as UBS, and the ubiquity of spreadsheets as a medium for analysis, led us to extend our layout tools to handle data in a generic spreadsheet format. We describe the metrics of similarity used for this data type, and give examples of layouts of sets of records of financial trades. Conservatism and scepticism with regard to 3D visualization, along with the lack of functionality of widely available 3D web browsers, led to the development of a 2D visualization tool with refinements of a number of our imageability features.	Brodbeck, D.;Chalmers, M.;Lunzer, A.;Cotture, P.	Ubilab, UBS, Zurich, Switzerland|c|;;;	37611663700;37283487400;37546729500;37611663200
	InfoVis	21-21 Oct. 1997	Design and evaluation of incremental data structures and algorithms for dynamic query interfaces	10.1109/INFVIS.1997.636790	http://dx.doi.org/10.1109/INFVIS.1997.636790	81	86	636790	data structures;data visualisation;graphical user interfaces;query formulation;query languages;query processing;real-time systems;very large databases;visual languages	DQI algorithms;continuous real-time feedback;database access mechanism;dynamic query interfaces;incremental data structures;information visualization;large databases;query formulation;small databases;user interfaces	Algorithm design and analysis;Bars;Computer science;Data structures;Data visualization;Displays;Feedback;Heuristic algorithms;Manipulator dynamics;NASA;Visual databases		A dynamic query interface (DQI) is a database access mechanism that provides continuous real-time feedback to the user during query formulation. Previous work shows that DQIs are elegant and powerful interfaces to small databases. Unfortunately, when applied to large databases, previous DQI algorithms slow to a crawl. We present a new incremental approach to DQI algorithms and display updates that work well with large databases, both in theory and in practice.	Tanin, E.;Beigel, R.;Shneiderman, B.	Dept. of Comput. Sci., Maryland Univ., College Park, MD, USA|c|;;	38269384800;37354669000;37283016400
	InfoVis	21-21 Oct. 1997	Volume rendering for relational data	10.1109/INFVIS.1997.636791	http://dx.doi.org/10.1109/INFVIS.1997.636791	87	90	636791	data structures;data visualisation;query processing;relational databases;rendering (computer graphics);user interfaces	categorical variables;data points;data structure;dense scatterplots;external query sliders;information visualization;multivariate data;nonaxis dimensions;plotting;relational data;table;unknown values;volume rendering;voxel	Data compression;Data structures;Data visualization;Displays;Graphics;Relational databases;Rendering (computer graphics);Sampling methods;Scattering;Shape;Silicon;Stacking		A method for efficiently volume rendering dense scatterplots of relational data is described. Plotting difficulties that arise from large numbers of data points, categorical variables, interaction with non-axis dimensions, and unknown values, are addressed by this method. The domain of the plot is voxelized using binning and then volume rendering. Since a table is used as the underlying data structure, no storage is wasted on regions with no data. The opacity of each voxel is a function of the number of data points in a corresponding bin. A voxel's color is derived by averaging the value of one of the variables for all the data points that fall in a bin. Other variables in the data may be mapped to external query sliders. A dragger object permits a user to select regions inside the volume.	Becker, B.G.	Silicon Graphics Comput. Syst., Mountain View, CA, USA|c|	37363099200
	InfoVis	21-21 Oct. 1997	The structure of the information visualization design space	10.1109/INFVIS.1997.636792	http://dx.doi.org/10.1109/INFVIS.1997.636792	92	99	636792	data visualisation;user interfaces	information visualization design space;information visualization literature;morphological analysis;point designs;research;taxonomy;user interfaces	Arithmetic;Color;Data communication;Data visualization;Filters;Graphics;Information analysis;Space technology;Taxonomy;Visual communication		Research on information visualization has reached the point where a number of successful point designs have been proposed and a variety of techniques have been discovered. It is now appropriate to describe and analyze portions of the design space so as to understand the differences among designs and to suggest new possibilities. This paper proposes an organization of the information visualization literature and illustrates it with a series of examples. The result is a framework for designing new visualizations and augmenting existing designs.	Card, S.K.;Mackinlay, J.	Xerox Palo Alto Res. Center, CA, USA|c|;	37444594200;37372036700
	InfoVis	21-21 Oct. 1997	Multidimensional detective	10.1109/INFVIS.1997.636793	http://dx.doi.org/10.1109/INFVIS.1997.636793	100	107	636793	data structures;data visualisation;economics;knowledge acquisition;pattern recognition;very large databases	2D pattern recognition problem;VLSI chip;approximate optimization;competition;decision support;economic model;economic policies;economic sectors;knowledge discovery;modeling relations;monitoring;multidimensional detective;multivariate dataset display;parallel coordinates;trade-off analyses;visual data mining	Application software;Computer displays;Computer science;Data mining;Guidelines;Monitoring;Multidimensional systems;Pattern recognition;Production;Very large scale integration		The display of multivariate datasets in parallel coordinates, transforms the search for relations among the variables into a 2-D pattern recognition problem. This is the basis for the application to visual data mining. The knowledge discovery process together with some general guidelines are illustrated on a dataset from the production of a VLSI chip. The special strength of parallel coordinates is in modeling relations. As an example, a simplified economic model is constructed with data from various economic sectors of a real country. The visual model shows the interelationship and dependencies between the sectors, circumstances where there is competition for the same resource, and feasible economic policies. Interactively, the model can be used to do trade-off analyses, discover sensitivities, do approximate optimization, monitor (as in a process) and provide decision support.	Inselberg, A.	Dept. of Comput. Sci., Tel Aviv Univ., Israel|c|	37294162600
	InfoVis	21-21 Oct. 1997	Metrics for effective information visualization	10.1109/INFVIS.1997.636794	http://dx.doi.org/10.1109/INFVIS.1997.636794	108	111	636794	data visualisation;software metrics	3D information visualization;cognitive overhead;data density;data points;design;dimensions;identifiable points;information visualization metrics;occlusion;reference context	Data visualization;Graphics;Guidelines;Information geometry;Layout;Multidimensional systems;Navigation;Scattering;Testing;Three dimensional displays;Two dimensional displays;Visualization		Metrics for information visualization will help designers create and evaluate 3D information visualizations. Based on experience from 60+ 3D information visualizations, the metrics we propose are: number of data points and data density; number of dimensions and cognitive overhead; occlusion percentage; and reference context and percentage of identifiable points.	Brath, R.	Visible Decision Inc., Toronto, Ont., Canada|c|	
	SciVis	24-24 Oct. 1997	A comparison of normal estimation schemes	10.1109/VISUAL.1997.663848	http://dx.doi.org/10.1109/VISUAL.1997.663848	19	26	663848	rendering (computer graphics)	continuous derivative filter;derivative filters;discrete function;high order methods;image processing;inexpensive schemes;interpolation;interpolation filter;normal estimation schemes;numerical accuracy;theoretical computational cost;volume rendering	Art;Computational modeling;Convolution;Costs;Digital filters;Equations;Filtering;Frequency;Image analysis;Image processing;Image reconstruction;Information science;Interpolation;Low pass filters;Rendering (computer graphics);Sampling methods;State estimation;Taylor series		The task of reconstructing the derivative of a discrete function is essential for its shading and rendering as well as being widely used in image processing and analysis. We survey the possible methods for normal estimation in volume rendering and divide them into two classes based on the delivered numerical accuracy. The three members of the first class determine the normal in two steps by employing both interpolation and derivative filters. Among these is a new method which has never been realized. The members of the first class are all equally accurate. The second class has only one member and employs a continuous derivative filter obtained through the analytic derivation of an interpolation filter. We use the new method to analytically compare the accuracy of the first class with that of the second. As a result of our analysis we show that even inexpensive schemes can in fact be more accurate than high order methods. We describe the theoretical computational cost of applying the schemes in a volume rendering application and provide guidelines for helping one choose a scheme for estimating derivatives. In particular we find that the new method can be very inexpensive and can compete with the normal estimations which pre-shade and pre-classify the volume (M. Levoy, 1988).	Mo&#x0308;ller, T.;Machiraju, R.;Mueller, K.;Yagel, R.	Dept. of Comput. & Inf. Sci., Ohio State Univ., Columbus, OH, USA|c|;;;	37275858700;37269516700;37273119700;37342422500
	SciVis	24-24 Oct. 1997	Collision detection for volumetric objects	10.1109/VISUAL.1997.663851	http://dx.doi.org/10.1109/VISUAL.1997.663851	27	34	663851	rendering (computer graphics)	collision probabilities;complicated interactions;distance map;hierarchical collision detection algorithms;intersecting point;intersecting regions;local volume properties;probability map;probability model;space point;surface crossing probability;uniform structure;volumetric datasets;volumetric objects	Application software;Computational modeling;Computer graphics;Constraint optimization;Detection algorithms;Object detection;Probability;Road accidents;Solid modeling;Virtual reality;Visualization		"We propose a probability model for the handling of complicated interactions between volumetric objects. In our model each volume is associated with a ""probability map"" that assigns a ""surface crossing"" probability to each space point according to local volume properties. The interaction between two volumes is then described by finding the intersecting regions between the volumes, and calculating the ""collision probabilities"" at each intersecting point from the surface crossing probabilities. To enable fast and efficient calculations, we introduce the concept of a distance map and develop two hierarchical collision detection algorithms, taking advantage of the uniform structure of volumetric datasets."	Taosong He;Kaufman, A.	AT&T Bell Labs., Naperville, IL, USA|c|;	37350343500;37268052800
	SciVis	24-24 Oct. 1997	The VSBUFFER: visibility ordering of unstructured volume primitives by polygon drawing	10.1109/VISUAL.1997.663853	http://dx.doi.org/10.1109/VISUAL.1997.663853	35	42	663853	rendering (computer graphics)	VSBUFFER;arbitrary grid topologies;arbitrary scattered convex polyhedra;connectivity information;data types;direct volume rendering;hardware assisted polygon drawing;polygon drawing;polygon mode;sorting procedure;two pass rendering approach;unstructured topologies;unstructured volume primitives;visibility ordering;volume integration;volume primitives;volumetric scalar data sets;voluminous polyhedra	Computer graphics;Data visualization;Hardware;Image quality;Interpolation;Light scattering;Mesh generation;Physics;Rendering (computer graphics);Sorting;Topology		Different techniques have been proposed for rendering volumetric scalar data sets. Usually these approaches are focusing on orthogonal cartesian grids, but in the last years research did also concentrate on arbitrary structured or even unstructured topologies. In particular, direct volume rendering of these data types is numerically complex and mostly requires sorting the whole database. We present a new approach to direct rendering of convex, voluminous polyhedra on arbitrary grid topologies, which efficiently use hardware assisted polygon drawing to support the sorting procedure. The key idea of this technique lies in a two pass rendering approach. First, the volume primitives are drawn in polygon mode to obtain their cross sections in the VSBUFFER orthogonal to the viewing plane. Second, this buffer is traversed in front to back order and the volume integration is performed. Thus, the complexity of the sorting procedure is reduced. Furthermore, any connectivity information can be completely neglected, which allows for the rendering of arbitrary scattered, convex polyhedra.	Westermann, R.;Ertl, T.	Comput. Graphics Group, Erlangen-Nurnberg Univ., Germany|c|;	37444424000;37268023800
	SciVis	24-24 Oct. 1997	Volume rendering of abdominal aortic aneurysms	10.1109/VISUAL.1997.663855	http://dx.doi.org/10.1109/VISUAL.1997.663855	43	50	663855	medical computing	2D greyscale slices;3D reconstructed volumes;CT scans;abdominal aortic aneurysms;classification method;colourized volume;computed tomography;experimental post operative environment;human visual system;individual intensity values;intensity values;medical images;medical scanners;perceptual colour selection technique;radiologists;spatial interactions;surgery;volume rendering;volume visualization	Abdomen;Aneurysm;Biomedical imaging;Computed tomography;Humans;Image reconstruction;Image segmentation;Rendering (computer graphics);Visual system;Visualization		One well known application area of volume rendering is the reconstruction and visualization of output from medical scanners like computed tomography (CT). 2D greyscale slices produced by these scanners can be reconstructed and displayed onscreen as a 3D model. Volume visualization of medical images must address two important issues. First, it is difficult to segment medical scans into individual materials based only on intensity values. Second, although greyscale images are the normal method for displaying medical volumes, these types of images are not necessarily appropriate for highlighting regions of interest within the volume. Studies of the human visual system have shown that individual intensity values are difficult to detect in a greyscale image. In these situations colour is a more effective visual feature. We addressed both problems during the visualization of CT scans of abdominal aortic aneurysms. We have developed a classification method that empirically segments regions of interest in each of the 2D slices. We use a perceptual colour selection technique to identify each region of interest in both the 2D slices and the 3D reconstructed volumes. The result is a colourized volume that the radiologists are using to rapidly and accurately identify the locations and spatial interactions of different materials from their scans. Our technique is being used in an experimental post operative environment to help to evaluate the results of surgery designed to prevent the rupture of the aneurysm. In the future, we hope to use the technique during the planning of placement of support grafts prior to the actual operation.	Tam, R.C.;Flak, B.;Cahoon, P.	Dept. of Comput. Sci., British Columbia Univ., Vancouver, BC, Canada|c|;;	37267823200;37612079700;37612081300
	SciVis	24-24 Oct. 1997	Auralization of streamline vorticity in computational fluid dynamics data	10.1109/VISUAL.1997.663856	http://dx.doi.org/10.1109/VISUAL.1997.663856	51	57	663856	flow visualisation	apparently continuously decreasing pitch;apparently continuously increasing pitch;audible frequency range;composite tone;computational fluid dynamics data;continuous streamline rotations;sine wave superimposition;streamline vorticity auralization;vector field	Audio user interfaces;Auditory displays;Auditory system;Clocks;Computational fluid dynamics;Computer science;Data visualization;Frequency;Streaming media;Temperature;Timbre		Presents a new method for auralization of the vorticity of a streamline in a vector field. This technique involves using a composite tone formed by superimposing sine waves of various amplitudes whose frequency and amplitude vary in such a way as to give the perception that the resulting sound increases or decreases endlessly in pitch without ever extending beyond the listener's range of audible frequencies. Continuous clockwise or counterclockwise rotations of a streamline resulting from vorticity can then be displayed aurally as an apparently continuous increase or decrease in pitch.	Volpe, C.R.;Glinert, E.P.	GE Corp. Res. & Dev., Schenectady, NY, USA|c|;	37378427600;37347783300
	SciVis	24-24 Oct. 1997	Singularities in nonuniform tensor fields	10.1109/VISUAL.1997.663857	http://dx.doi.org/10.1109/VISUAL.1997.663857	59	66	663857	tensors	2nd-order symmetric tensor field topology;Newtonian flow;anisotropy;compressible flow;control functions;deformation tensor;degenerate point distribution;deviator;incompressible flow;isotropic pressure contribution;isotropic tensor;nonuniform tensor fields;physical phenomena;singularities;stationary flow;stress tensor;topological structure;vectors;viscosity;viscous flow;vortex core	Aerospace engineering;Anisotropic magnetoresistance;Computer science;Data engineering;Data visualization;Heart;Numerical simulation;Physics;Skeleton;Tensile stress;Topology;Viscosity		Studies the topology of 2nd-order symmetric tensor fields. Degenerate points are basic constituents of tensor fields. From the set of degenerate points, an experienced researcher can reconstruct a whole tensor field. We address the conditions for the existence of degenerate points and, based on these conditions, we predict the distribution of degenerate points inside the field. Every tensor can be decomposed into a deviator and an isotropic tensor. A deviator determines the properties of a tensor field, while the isotropic part provides a uniform bias. Deviators can be 3D or locally 2D. The triple-degenerate points of a tensor field are associated with the singular points of its deviator and the double-degenerate points of a tensor field have singular local 2D deviators. This provides insights into the similarity of topological structure between 1st-order (or vectors) and 2nd-order tensors. Control functions are in charge of the occurrences of a singularity of a deviator. These singularities can further be linked to important physical properties of the underlying physical phenomena. For a deformation tensor in a stationary flow, the singularities of its deviator actually represent the area of the vortex core in the field; for a stress tensor, the singularities represent the area with no stress; for a Newtonian flow, compressible flow and incompressible flow as well as stress and deformation tensors share similar topological features due to the similarity of their deviators; for a viscous flow, removing the large, isotropic pressure contribution dramatically enhances the anisotropy due to viscosity.	Lavin, Y.;Levy, Y.;Hesselink, Lambertus	Dept. of Phys., Stanford Univ., CA, USA|c|;;	37372012600;37610569700;37274095200
	SciVis	24-24 Oct. 1997	Visualization of higher order singularities in vector fields	10.1109/VISUAL.1997.663858	http://dx.doi.org/10.1109/VISUAL.1997.663858	67	74	663858	vectors	Clifford algebra;critical points;execution speed;higher-order singularity visualization;linear approximation;polynomial approximation;vector field topology visualization algorithm	Algebra;Computer science;Data visualization;H infinity control;Interpolation;Linear approximation;Physics;Piecewise linear approximation;Polynomials;Spirals;Topology;Vectors;Visualization		Presents an algorithm for the visualization of vector field topology based on Clifford algebra. It allows the detection of higher-order singularities. This is accomplished by first analysing the possible critical points and then choosing a suitable polynomial approximation, because conventional methods based on piecewise linear or bilinear approximation do not allow higher-order critical points and destroy the topology in such cases. The algorithm is still very fast, because of using linear approximation outside the areas with several critical points.	Scheuermann, G.;Hagen, H.;Kruger, H.;Menzel, M.;Rockwood, A.	Dept. of Comput. Sci., Kaiserslautern Univ., Germany|c|;;;;	37282574800;37282578800;37420744100;37375936800;37372689000
	SciVis	24-24 Oct. 1997	Principal stream surfaces	10.1109/VISUAL.1997.663859	http://dx.doi.org/10.1109/VISUAL.1997.663859	75	80	663859	physics computing	filtering;flow topology;fluid field velocity direction;irrotational flow;normal vectors;principal stream function;principal stream surface algorithm;scalar field;starting point placement;streamlines;vector visualization;velocity points;volume rendering	Acceleration;Chromium;Computational fluid dynamics;Computer graphics;Computer science;Convolution;Face detection;Image generation;Rendering (computer graphics);Streaming media;Surface reconstruction;Topology;Visualization		The use of stream surfaces and streamlines is well established in vector visualization. However, the proper placement of starting points is critical for these constructs to clearly illustrate the flow topology. In this paper, we present the principal stream surface algorithm, which automatically generates stream surfaces that properly depict the topology of an irrotational flow. For each velocity point in the fluid field, we construct the normal to the principal stream surface through the point. The set of all such normal vectors is used to construct the principal stream function, which is a scalar field describing the direction of velocity in the fluid field. Volume rendering can then be used to visualize the principal stream function, which is directly related to the flow topology. Thus, topology in a fluid field can be easily modeled and rendered.	Wenli Cai	Fraunhofer Inst. for Comput. Graphics, Germany|c|	37613897400
	SciVis	24-24 Oct. 1997	ROAMing terrain: Real-time Optimally Adapting Meshes	10.1109/VISUAL.1997.663860	http://dx.doi.org/10.1109/VISUAL.1997.663860	81	88	663860	data visualisation	ROAM method;accurate images;continuous triangulations;dynamic terrain morphing;dynamic view-dependent triangle meshes;error bounds;execution time;flexible view-dependent error metrics optimization;flight simulation;frame rate;frame-to-frame coherence;graphics;greedy algorithm;ground-based aircraft testing;incremental triangle stripping;large datasets;merging operation;mesh size;performance optimization;pre-processed bintree triangles;priority queues;priority-computation deferral lists;real-time optimally adapting meshes;resolution;splitting operation;synthetic sensor simulation;terrain visualization;texture maps;triangle changes;triangle counts;vertex morphing	Aerospace simulation;Aircraft manufacture;Chromium;Computer graphics;Geometry;Hardware;Image sensors;Laboratories;Runtime;Testing;Visualization		Terrain visualization is a difficult problem for applications requiring accurate images of large datasets at high frame rates, such as flight simulation and ground-based aircraft testing using synthetic sensor simulation. On current graphics hardware, the problem is to maintain dynamic, view-dependent triangle meshes and texture maps that produce good images at the required frame rate. We present an algorithm for constructing triangle meshes that optimizes flexible view-dependent error metrics, produces guaranteed error bounds, achieves specified triangle counts directly and uses frame-to-frame coherence to operate at high frame rates for thousands of triangles per frame. Our method, dubbed Real-time Optimally Adapting Meshes (ROAM), uses two priority queues to drive split and merge operations that maintain continuous triangulations built from pre-processed bintree triangles. We introduce two additional performance optimizations: incremental triangle stripping and priority-computation deferral lists. ROAM's execution time is proportional to the number of triangle changes per frame, which is typically a few percent of the output mesh size; hence ROAM's performance is insensitive to the resolution and extent of the input terrain. Dynamic terrain and simple vertex morphing are supported.	Duchaineau, M.;Wolinsky, M.;Sigeti, D.E.;Aldrich, C.;Mineev-Weinstein, M.B.	Los Alamos Nat. Lab., NM, USA|c|;;;;	37267813100;37443252200;37443253500;37608683600;37612202100
	SciVis	24-24 Oct. 1997	Visualization of height field data with physical models and texture photomapping	10.1109/VISUAL.1997.663862	http://dx.doi.org/10.1109/VISUAL.1997.663862	89	94	663862	data visualisation;engineering graphics;manufacturing processes;solid modelling	3D photographic plate;3D solid representation;height field data visualization;laminated object manufacturing;physical models;scalar data;scalar information display;solid fabricated parts;solid freeform fabrication techniques;texture photomapping	Data visualization;Fabrication;Glass;Humans;Prototypes;Research and development;Solid modeling;Supercomputers;Three dimensional displays;Workstations		"The paper discusses a unique way to visualize height field data-the use of solid fabricated parts with a photomapped texture to display scalar information. In this process, the data in a height field are turned into a 3D solid representation through solid freeform fabrication techniques, in this case laminated object manufacturing. Next, that object is used as a 3D ""photographic plate"" to allow a texture image representing scalar data to be permanently mapped onto it. The paper discusses this process and how it can be used in different visualization situations."	Clark, D.;Bailey, M.	California Univ., San Diego, La Jolla, CA, USA|c|;	37364046500;37280473500
	SciVis	24-24 Oct. 1997	Visualization of large terrains in resource-limited computing environments	10.1109/VISUAL.1997.663863	http://dx.doi.org/10.1109/VISUAL.1997.663863	95	102	663863	data visualisation	CPU constraints;Delaunay triangulation;Internet;algorithmic techniques;bandwidth constraints;dynamic terrain data point subset;frame rate;geometric data;graphics pipeline;interactive visualization;large terrain visualization;low-bandwidth client/server data-streaming scenario;low-end computer;minimized data I/O;optimized polygon number;optimized texture pixel number;paging scheme;progressive wavelet scheme;rendering performance;resource-limited computing environments;software system;terrain database;texture data	Bandwidth;Computer networks;Geometry;Network servers;Read-write memory;Rendering (computer graphics);Software systems;Spatial databases;Visual databases;Visualization		"The authors describe a software system supporting interactive visualization of large terrains in a resource-limited environment, i.e. a low-end client computer accessing a large terrain database server through a low-bandwidth network. By ""large"", they mean that the size of the terrain database is orders of magnitude larger than the computer RAM. Superior performance is achieved by manipulating both geometric and texture data at a continuum of resolutions, and, at any given moment, using the best resolution dictated by the CPU and bandwidth constraints. The geometry is maintained as a Delaunay triangulation of a dynamic subset of the terrain data points, and the texture compressed by a progressive wavelet scheme. A careful blend of algorithmic techniques enables the system to achieve superior rendering performance on a low-end computer by optimizing the number of polygons and texture pixels sent to the graphics pipeline. It guarantees a frame rate depending only on the size and quality of the rendered image, independent of the viewing parameters and scene database size. An efficient paging scheme minimizes data I/O, thus enabling the use of the system in a low-bandwidth client/server data-streaming scenario, such as on the Internet."	Rabinovich, B.;Gotsman, C.	Dept. of Comput. Sci., Technion-Israel Inst. of Technol., Haifa, Israel|c|;	37612158500;37281769000
	SciVis	24-24 Oct. 1997	Building and traversing a surface at variable resolution	10.1109/VISUAL.1997.663865	http://dx.doi.org/10.1109/VISUAL.1997.663865	103	110	663865	computational geometry	heuristics;interruptible algorithm;multi-triangulation;point location;surface approximation extraction;surface building;surface traversal;triangle meshes;variable resolution	Algorithm design and analysis;Buildings;Data compression;Data mining;Data visualization;Encoding;Geographic Information Systems;Solid modeling;Spatial resolution;Virtual reality		The authors consider the multi-triangulation, a general model for representing surfaces at variable resolution based on triangle meshes. They analyse characteristics of the model that make it effective for supporting basic operations such as extraction of a surface approximation, and point location. An interruptible algorithm for extracting a representation at a resolution variable over the surface is presented. Different heuristics for building the model are considered and compared. Results on both the construction and the extraction algorithm are presented.	De Fioriani, L.;Magillo, P.;Puppo, E.	Dipt. di Inf. e Sci. dell''Inf., Genoa Univ., Italy|c|;;	;37371970000;37355451700
	SciVis	24-24 Oct. 1997	Multivariate visualization using metric scaling	10.1109/VISUAL.1997.663866	http://dx.doi.org/10.1109/VISUAL.1997.663866	111	118	663866	data visualisation;graph theory	data record;data semantics;display coordinates;dissimilarities;graph configuration algorithm;graphical summary;low dimensional data overview;metric scaling;multidimensional data brushing;multivariate data exploration;multivariate visualization;principal components;reduced data dimensions;reduced data size	Animation;Automobiles;Computer science;Data analysis;Data visualization;Displays;Extraterrestrial measurements;Graphics;Layout;Motion analysis;Multidimensional systems;Scattering		The authors present an efficient visualization approach to support multivariate data exploration through a simple but effective low dimensional data overview based on metric scaling. A multivariate dataset is first transformed into a set of dissimilarities between all pairs of data records. A graph configuration algorithm based on principal components is then wed to determine the display coordinates of the data records in the low dimensional data overview. This overview provides a graphical summary of the multivariate data with reduced data dimensions, reduced data size, and additional data semantics. It can be used to enhance multidimensional data brushing, or to arrange the layout of other conventional multivariate visualization techniques. Real life data is used to demonstrate the approach.	Pak Chung Wong;Bergeron, R.D.	Dept. of Comput. Sci., New Hampshire Univ., Durham, NH, USA|c|;	;37342054500
	SciVis	24-24 Oct. 1997	Visualizing the behaviour of higher dimensional dynamical systems	10.1109/VISUAL.1997.663867	http://dx.doi.org/10.1109/VISUAL.1997.663867	119	125	663867	data visualisation	high-dimensional data set visualization;high-dimensional dynamical systems;higher dimensional dynamical systems;interactive visualization tool;parallel coordinates;scientific visualization;smooth n-dimensional flow;statistical features;trajectory visualization	Computer graphics;Data structures;Data visualization;Design methodology;Differential equations;Equations;Finance;Lattices;Market research;Mathematical model;Mathematics;Space technology;Topology			Wegenkittl, R.;Loffelmann, H.;Groller, E.	Inst. of Comput. Graphics, Wien Univ. of Technol., Austria|c|;;	37267822600;37427428300;38471589500
	SciVis	24-24 Oct. 1997	Displaying data in multidimensional relevance space with 2D visualization maps	10.1109/VISUAL.1997.663868	http://dx.doi.org/10.1109/VISUAL.1997.663868	127	134	663868	data visualisation;information retrieval;simulated annealing	2D relevance map;2D visualization maps;RMAP prototype system;Web search visualization;data display;features;map layout;multidimensional relevance space visualization;optimized layout;simulated annealing algorithm	Computer science;Data visualization;Displays;Information analysis;Information retrieval;Multidimensional systems;Prototypes;Simulated annealing;System testing;Web search		"The paper introduces a tool for visualizing a multidimensional relevance space. Abstractly, the information to be displayed consists of a large number of objects, a set of features that are likely to be of interest to the user, and some function that measures the relevance level of every object to the various features. The goal is to provide the user with a concise and comprehensible visualization of that information. For the type of applications concentrated on, the exact relevance measures of the objects are not significant. This enables accuracy to be traded for a clearer display. The idea is to ""flatten"" the multidimensionality of the feature space into a 2D ""relevance map"", capturing the inter-relations among the features, without causing too many ambiguous interpretations of the results. To better reflect the nature of the data and to resolve the ambiguity the authors refine the given set of features and introduce the notion of composed features. The layout of the map is then obtained by grading it according to a set of rules and using a simulated annealing algorithm which optimizes the layout with respect to these rules. The technique proposed has been implemented and tested, in the context of visualizing the result of a Web search, in the RMAP (Relevance Map) prototype system."	Assa, J.;Cohen-Or, D.;Milo, T.	Dept. of Comput. Sci., Tel Aviv Univ., Israel|c|;;	37612084200;38312451600;37409090400
	SciVis	24-24 Oct. 1997	Multiresolution tetrahedral framework for visualizing regular volume data	10.1109/VISUAL.1997.663869	http://dx.doi.org/10.1109/VISUAL.1997.663869	135	142	663869	data visualisation	Multi-Tetra framework;binary trees;continuous generated isosurface distribution;continuous rendered intensity distribution;crack problem;direct volume projection;error-based model;marching cubes;multiresolution tetrahedral framework;recursive subdivision;recursive tetrahedra fusing;regular volume data visualization;rendering;splatting;topology;volume data approximation;voxels	Binary trees;Chromium;Computer graphics;Computer science;Data mining;Data visualization;Displays;Image resolution;Isosurfaces;Rendering (computer graphics);Topology		The authors present a multiresolution framework, called Multi-Tetra framework, that approximates volume data with different levels-of-detail tetrahedra. The framework is generated through a recursive subdivision of the volume data and is represented by binary trees. Instead of using a certain level of the Multi-Tetra framework for approximation, an error-based model (EBM) is generated by recursively fusing a sequence of tetrahedra from different levels of the Multi-Tetra framework. The EBM significantly reduces the number of voxels required to model an object, while preserving the original topology. The approach provides continuous distribution of rendered intensity or generated isosurfaces along boundaries of different levels-of-detail thus solving the crack problem. The model supports typical rendering approaches, such as marching cubes, direct volume projection, and splatting. Experimental results demonstrate the strengths of the approach.	Yong Zhou;Chen, B.;Kaufman, A.	Center for Visual Comput., State Univ. of New York, Stony Brook, NY, USA|c|;;	37363214300;37367422700;37268052800
	SciVis	24-24 Oct. 1997	Haar wavelets over triangular domains with applications to multiresolution models for flow over a sphere	10.1109/VISUAL.1997.663871	http://dx.doi.org/10.1109/VISUAL.1997.663871	143	149	663871	wavelet transforms	convergence;flow;fully orthogonal Haar wavelets;multiresolution analysis;nested triangulated domains;piecewise constant wavelets;spherical domain	Application software;Computer science;Data structures;Fluid flow;Labeling;Multiresolution analysis;Wavelet analysis;Wavelet domain		Some new piecewise constant wavelets defined over nested triangulated domains are presented and applied to the problem of multiresolution analysis of flow over a spherical domain. These new, nearly orthogonal wavelets have advantages over the existing weaker biorthogonal wavelets. In the planar case of uniform areas, the wavelets converge to one of two fully orthogonal Haar wavelets. These new, fully orthogonal wavelets are proven to be the only possible wavelets of this type.	Nielson, G.M.;Jung, Il.-H.;Sung, J.	Dept. of Comput. Sci. & Eng., Arizona State Univ., Tempe, AZ, USA|c|;;	37283754100;37366444500;37362401700
	SciVis	24-24 Oct. 1997	Wavelet-based multiresolutional representation of computational field simulation datasets	10.1109/VISUAL.1997.663872	http://dx.doi.org/10.1109/VISUAL.1997.663872	151	158	663872	wavelet transforms	bit stream;computational field simulation datasets;information localization;progressive access;progressive transmission;regions of interest;visual information content;wavelet coefficients;wavelet transform;wavelet-based multiresolutional representation	Chromium;Computational modeling;Computer graphics;Computer science;Computer simulation;Data visualization;Displays;Energy resolution;Energy storage;Grid computing;Image coding;Spatial resolution;Wavelet transforms		The paper addresses multiresolutional representation of datasets arising from a computational field simulation. The approach determines the regions of interest, breaks the volume into variable size blocks to localize the information, and then codes each block using a wavelet transform. The blocks are then ranked by visual information content so that the most informative wavelet coefficients can be embedded in a bit stream for progressive transmission or access. The technique is demonstrated on a widely-used computational field simulation dataset.	Zhifan Zhu;Machiraju, R.;Fry, B.;Moorhead, R.	NSF Eng. Res. Center for Comput. Field Simulation, Mississippi State Univ., MS, USA|c|;;;	37616769800;37269516700;37619005500;37282559500
	SciVis	24-24 Oct. 1997	Dynamic color mapping of bivariate qualitative data	10.1109/VISUAL.1997.663874	http://dx.doi.org/10.1109/VISUAL.1997.663874	159	166	663874	data visualisation	bivariate qualitative data;dynamic color mapping;dynamic parameter control;dynamic representation;feature shape;location;magnitude;multivariate data display;multivariate spatial distributions;random noise;relative positions;shape;single scalar variable value display;static representations	Computer displays;Data visualization;Information science;Manipulator dynamics;Motion pictures;Noise shaping;Psychology;Shape;Sparks;Working environment noise		"Color is widely and reliably used to display the value of a single scalar variable. It is more rarely, and far less reliably, used to display multivariate data. Dynamic control over the parameters of the color mapping results in a more effective environment for the exploration of multivariate spatial distributions. The paper describes an empirical study comparing the effectiveness of static versus dynamic representations for the exploration of qualitative aspects of bivariate distributions. In this experiment, subjects made judgments about the correspondence of the shape, location, and magnitude of two patterns under conditions with varying amounts of random noise. Subjects made significantly more correct judgements (p
		"	Rheingans, P.	Dept. of Comput. & Inf. Sci., Mississippi Univ., MS, USA|c|	37282292000
	SciVis	24-24 Oct. 1997	The contour spectrum	10.1109/VISUAL.1997.663875	http://dx.doi.org/10.1109/VISUAL.1997.663875	167	173	663875	data visualisation	2D interface;area;contour attributes;contour spectrum;gradient integral;informative visualization;isocontour visualization;isovalue selection;multi-dimensional unstructured triangular grids;qualitative user interaction;real-time exact quantification;scalar data;signature graphs;surface;time step;time-varying data;univariate B-spline functions;user interface component;volume	Computer displays;Computer interfaces;Data acquisition;Data visualization;Image analysis;Postal services;Spline;User interfaces		The authors introduce the contour spectrum, a user interface component that improves qualitative user interaction and provides real-time exact quantification in the visualization of isocontours. The contour spectrum is a signature consisting of a variety of scalar data and contour attributes, computed over the range of scalar values /spl omega//spl isin/R. They explore the use of surface, area, volume, and gradient integral of the contour that are shown to be univariate B-spline functions of the scalar value /spl omega/ for multi-dimensional unstructured triangular grids. These quantitative properties are calculated in real-time and presented to the user as a collection of signature graphs (plots of functions of /spl omega/) to assist in selecting relevant isovalues /spl omega//sub 0/ for informative visualization. For time-varying data, these quantitative properties can also be computed over time, and displayed using a 2D interface, giving the user an overview of the time-varying function, and allowing interaction in both isovalue and time step. The effectiveness of the current system and potential extensions are discussed.	Bajaj, C.L.;Pascucci, V.;Schikore, D.R.	Shastra Lab., Purdue Univ., West Lafayette, IN, USA|c|;;	37282899200;37284312600;37355637500
	SciVis	24-24 Oct. 1997	Constrained 3D navigation with 2D controllers	10.1109/VISUAL.1997.663876	http://dx.doi.org/10.1109/VISUAL.1997.663876	175	182	663876	virtual reality	2D controllers;2D device;3D world display;complex molecules;constrained 3D navigation;context-dependent constraints;controls smooth movements;designer-supplied constraint modes;interactive graphics;interior architectural spaces;mouse;terrain models;unified mathematical framework;viewpoints;virtual reality	Animation;Application software;Cameras;Computer graphics;Control systems;Interpolation;Layout;Mice;Navigation;Virtual reality		"Navigation through 3D spaces is required in many interactive graphics and virtual reality applications. The authors consider the subclass of situations in which a 2D device such as a mouse controls smooth movements among viewpoints for a ""through the screen"" display of a 3D world. Frequently, there is a poor match between the goal of such a navigation activity, the control device, and the skills of the average user. They propose a unified mathematical framework for incorporating context-dependent constraints into the generalized viewpoint generation problem. These designer-supplied constraint modes provide a middle ground between the triviality of a single camera animation path and the confusing excess freedom of common unconstrained control paradigms. They illustrate the approach with a variety of examples, including terrain models, interior architectural spaces, and complex molecules."	Hanson, A.J.;Wernert, E.A.	Dept. of Comput. Sci., Indiana Univ., Bloomington, IN, USA|c|;	37333439100;37371964800
	SciVis	24-24 Oct. 1997	Two-phase perspective ray casting for interactive volume navigation	10.1109/VISUAL.1997.663878	http://dx.doi.org/10.1109/VISUAL.1997.663878	183	189	663878	data visualisation;interpolation;ray tracing;rendering (computer graphics)	PC platform;accelerated 3D graphics hardware;alpha blending;approximate animated frame sequence;coherence;dynamic data swapping;frame;interactive volume data set exploration;interactive volume navigation;local depth-limited frustum;main memory;perspective projections;ray casting algorithm;resampling;sample points;trilinear interpolation;two-phase perspective ray casting;view point;volume rendered view;volumetric compositing	Acceleration;Animation;Biomedical imaging;Casting;Design methodology;Graphics;Hardware;Interpolation;Navigation;Rendering (computer graphics)		"Volume navigation is the interactive exploration of volume data sets by ""flying"" the view point through the data, producing a volume rendered view at each frame. The authors present an inexpensive perspective volume navigation method designed to run on a PC platform with accelerated 3D graphics hardware. They compute perspective projections at each frame, allow trilinear interpolation of sample points, and render both gray scale and RGB volumes by volumetric compositing. The implementation handles arbitrarily large volumes, by dynamically swapping data within the local depth-limited frustum into main memory as the viewpoint moves through the volume. They describe a new ray casting algorithm that takes advantage of the coherence inherent in adjacent frames to generate a sequence of approximate animated frames much faster than they could be computed individually. They also take advantage of the 3D graphics acceleration hardware to offload much of the alpha blending and resampling from the CPU."	Brady, M.;Jung, K.;Nguyen, H.T.;Nguyen, T.	Microcomput. Res. Labs., Intel Corp., USA|c|;;;	37291463600;37360575900;37366032400;38184427400
	SciVis	24-24 Oct. 1997	Accelerated volume rendering using homogeneous region encoding	10.1109/VISUAL.1997.663880	http://dx.doi.org/10.1109/VISUAL.1997.663880	191	196	663880	rendering (computer graphics)	3D distance transforms;accelerated volume rendering;back projection algorithms;empty regions;homogeneous region encoding;homogeneous region preprocessing;volume ray-casting algorithms	Acceleration;Automatic control;Clouds;Encoding;Graphics;Image databases;Projection algorithms;Rendering (computer graphics);Silicon;Spatial databases;Tree data structures		Previous accelerated volume rendering techniques have used auxiliary hierarchical datastructures to skip empty and homogeneous regions. Although some recent research has taken advantage of more efficient direct encoding techniques to skip empty regions, no work has been done to directly encode homogeneous but not empty regions. 3D distance transforms previously used to encode empty space can be extended to preprocess homogeneous regions as well, and these regions can be efficiently encoded and incorporated into volume ray-casting and back projection algorithms with a high degree of flexibility.	Freund, J.;Sloan, K.	Silicon Graphics Inc., USA|c|;	37621721400;37611960100
	SciVis	24-24 Oct. 1997	An anti-aliasing technique for splatting	10.1109/VISUAL.1997.663882	http://dx.doi.org/10.1109/VISUAL.1997.663882	197	204	663882	rendering (computer graphics)	anti-aliasing technique;direct volume rendering algorithm;efficient hardware implementation;high-quality splatted images;high-resolution volumes;image sampling rate;orthographic projections;perspective projections;spatial aliasing artifacts;splatting;temporal aliasing artifacts;volume sampling rate	Algorithm design and analysis;Art;Image quality;Image reconstruction;Image resolution;Image sampling;Information science;Kernel;Laboratories;Pixel;Rendering (computer graphics);Shape		Splatting is a popular direct volume rendering algorithm. However, the algorithm does not correctly render cases where the volume sampling rate is higher than the image sampling rate (e.g. more than one voxel maps into a pixel). This situation arises with orthographic projections of high-resolution volumes, as well as with perspective projections of volumes of any resolution. The result is potentially severe spatial and temporal aliasing artifacts. Some volume ray-casting algorithms avoid these artifacts by employing reconstruction kernels which vary in width as the rays diverge. Unlike ray-casting algorithms, existing splatting algorithms do not have an equivalent mechanism for avoiding these artifacts. The authors propose such a mechanism, which delivers high-quality splatted images and has the potential for a very efficient hardware implementation.	Swan, J.E.;Mueller, K.;Moller, T.;Crawfis, R.;Yagel, R.	Adv. Comput. Center for the Arts & Design, Ohio State Univ., Columbus, OH, USA|c|;;;;	37295140400;37273119700;38341221100;37284273900;38335291800
	SciVis	24-24 Oct. 1997	A topology modifying progressive decimation algorithm	10.1109/VISUAL.1997.663883	http://dx.doi.org/10.1109/VISUAL.1997.663883	205	212	663883	topology	data storage;data transmission;fast local decimation technique;guaranteed reduction level;holes;interactive rendering performance;mesh splitting operations;reconstruction;reduction ratios;topological constraints;topology modifying progressive decimation algorithm;triangle decimation techniques	Algorithm design and analysis;Animation;Computer graphics;Computer simulation;Data acquisition;Hardware;Laser modes;Memory;Merging;Research and development;Satellites;Topology		Triangle decimation techniques reduce the number of triangles in a mesh, typically to improve interactive rendering performance or reduce data storage and transmission requirements. Most of these algorithms are designed to preserve the original topology of the mesh. Unfortunately, this characteristic is a strong limiting factor in overall reduction capability, since objects with a large number of holes or other topological constraints cannot be effectively reduced. The author presents an algorithm that yields a guaranteed reduction level, modifying topology as necessary to achieve the desired result. In addition, the algorithm is based on a fast local decimation technique, and its operations can be encoded for progressive storage, transmission, and reconstruction. He describes the new progressive decimation algorithm, introduces mesh splitting operations and shows how they can be encoded as a progressive mesh. He also demonstrates the utility of the algorithm on models ranging in size from 1,132 to 1.68 million triangles and reduction ratios of up to 200:1.	Schroeder, W.J.		37282730100
	SciVis	24-24 Oct. 1997	Efficient subdivision of finite-element datasets into consistent tetrahedra	10.1109/VISUAL.1997.663885	http://dx.doi.org/10.1109/VISUAL.1997.663885	213	219	663885	finite element analysis	consistent tetrahedra;discontinuous behaviour;efficient algorithms;efficient subdivision;element faces;face adjacency graphs;finite-element datasets;finite-element simulations;hexahedra;iso-contouring;mesh topology preservation;particle advection;prisms;pyramids;tetrahedra;unstructured mesh topology;visible artifacts;volume rendering	Computational fluid dynamics;Computational modeling;Data visualization;Finite element methods;Information science;Interpolation;Isosurfaces;Lead compounds;Robustness;Solids;Topology		The paper discusses the problem of subdividing unstructured mesh topologies containing hexahedra, prisms, pyramids and tetrahedra into a consistent set of only tetrahedra, while preserving the overall mesh topology. Efficient algorithms for volume rendering, iso-contouring and particle advection exist for mesh topologies comprised solely of tetrahedra. General finite-element simulations however, consist mainly of hexahedra, and possibly prisms, pyramids and tetrahedra. Arbitrary subdivision of these mesh topologies into tetrahedra can lead to discontinuous behaviour across element faces. This will show up as visible artifacts in the iso-contouring and volume rendering algorithms, and lead to impossible face adjacency graphs for many algorithms. The authors present various properties of tetrahedral subdivisions, and an algorithm SOP determining a consistent subdivision containing a minimal set of tetrahedra.	Albertelli, G.;Crawfis, R.A.	Dept. of Comput. & Inf. Sci., Ohio State Univ., Columbus, OH, USA|c|;	38325377700;37284273900
	SciVis	24-24 Oct. 1997	Interval volume tetrahedrization	10.1109/VISUAL.1997.663886	http://dx.doi.org/10.1109/VISUAL.1997.663886	221	228	663886	computational geometry	3D rectilinear grid;algorithm;interval volume;interval volume tetrahedrization;isosurface;marching cubes algorithm;polyhedral approximation;tetrahedrization;triangular approximation	Approximation algorithms;Computer science;Grid computing;Inference algorithms;Isosurfaces;Lead compounds;Magnetic resonance imaging;Shape;X-ray tomography		The interval volume is a generalization of the isosurface commonly associated with the marching cubes algorithm. Based upon samples at the locations of a 3D rectilinear grid, the algorithm produces a triangular approximation to the surface defined by F(x,y,z)=c. The interval volume is defined by a=F(x,y,z)=. The authors describe an algorithm for computing a tetrahedrization of a polyhedral approximation to the interval volume.	Nielson, G.M.;Sung, J.	Dept. of Comput. Sci., Arizona State Univ., Tempe, AZ, USA|c|;	37283754100;37362401700
	SciVis	24-24 Oct. 1997	Computing the separating surface for segmented data	10.1109/VISUAL.1997.663887	http://dx.doi.org/10.1109/VISUAL.1997.663887	229	233	663887	computational geometry	algorithm;data function values;data points;segmented data;separating surface computation;triangulated surface computation	Computer science;Data engineering;Data structures;Isosurfaces;Material properties;Mathematics;Medical simulation;Microwave integrated circuits;Solid modeling		An algorithm for computing a triangulated surface which separates a collection of data points that have been segmented into a number of different classes is presented. The problem generalizes the concept of an isosurface which separates data points that have been segmented into only two classes: those for which data function values are above the threshold and those which are below the threshold value. The algorithm is very simple, easy to implement and applies without limit to the number of classes.	Nielson, G.M.;Franke, R.	Dept. of Comput. Sci. & Eng., Arizona State Univ., Tempe, AZ, USA|c|;	37283754100;37413313200
	SciVis	24-24 Oct. 1997	Application-controlled demand paging for out-of-core visualization	10.1109/VISUAL.1997.663888	http://dx.doi.org/10.1109/VISUAL.1997.663888	235	244	663888	data visualisation	application-controlled demand paging;computational fluid dynamics;graphics workstations;input data sets;memory capacity;memory management;operating system virtual memory;out-of-core visualization;paged segments;scientific visualization;segmentation;supercomputers;virtual memory	Computational fluid dynamics;Computer graphics;Data visualization;Memory management;Microcomputers;NASA;Operating systems;Power engineering and energy;Size control;Software tools;Workstations		In the area of scientific visualization, input data sets are often very large. In visualization of computational fluid dynamics (CFD) in particular, input data sets today can surpass 100 Gbytes, and are expected to scale with the ability of supercomputers to generate them. Some visualization tools already partition large data sets into segments, and load appropriate segments as they are needed. However, this does not remove the problem for two reasons: 1) there are data sets for which even the individual segments are too large for the largest graphics workstations, 2) many practitioners do not have access to workstations with the memory capacity required to load even a segment, especially since the state-of-the-art visualization tools tend to be developed by researchers with much more powerful machines. When the size of the data that must be accessed is larger than the size of memory, some form of virtual memory is simply required. This may be by segmentation, paging, or by paged segments. The authors demonstrate that complete reliance on operating system virtual memory for out-of-core visualization leads to egregious performance. They then describe a paged segment system that they have implemented, and explore the principles of memory management that can be employed by the application for out-of-core visualization. They show that application control over some of these can significantly improve performance. They show that sparse traversal can be exploited by loading only those data actually required.	Cox, M.;Ellsworth, D.	NASA Ames Res. Center, Moffett Field, CA, USA|c|;	37381630700;37282594500
	SciVis	24-24 Oct. 1997	GADGET: goal-oriented application design guidance for modular visualization environments	10.1109/VISUAL.1997.663889	http://dx.doi.org/10.1109/VISUAL.1997.663889	245	252	663889	data visualisation	GADGET;accuracy requirements;application networks;data visualization problems;dataset dimension;dataset mesh type;extendability;goal-oriented application design guidance;graphical primitives;heuristics;knowledge base design;modular visualization environments;numerical data mapping;reusability;scientific data visualization;system architecture;temporal efficiency;visual programming;visualization goal	Artificial intelligence;Chromium;Computer architecture;Data engineering;Data visualization;Filtering;Problem-solving;Prototypes;Software engineering;Software reusability;Taxonomy;User interfaces		Modular visualization environments (MVEs) have recently been regarded as the de facto standard for scientific data visualization, mainly due to adoption of the visual programming style, reusability, and extendability. However, since scientists and engineers as the MVE principal user are not always familiar with how to map numerical data to proper graphical primitives, the set of built-in modules is not fully used to construct necessary application networks. Therefore, a certain mechanism needs to be incorporated into MVEs, which makes use of heuristics and expertise of visualization specialists (visineers), and which supports the user in designing his/her applications with MVEs. The Wehrend's goal-oriented taxonomy of visualization techniques is adopted as the basic philosophy to develop a system, called GADGET, for application design guidance for MVEs. The GADGET system interactively helps the user design appropriate applications according to the specific visualization goals, temporal efficiency versus accuracy requirements, and such properties as dimension and mesh type of a given target dataset. Also the GADGET system is capable of assisting the user in customizing a prototype modular network for his/her desired applications by showing execution examples involving datasets of the same type. The paper provides an overview of the GADGET guidance mechanism and system architecture, with an emphasis on its knowledge base design. Sample data visualization problems are used to demonstrate the usefulness of the GADGET system.	Fujishiro, I.;Takeshima, Y.;Ichikawa, Y.;Nakamura, K.	Dept. of Inf. Sci., Ochanomizu Univ., Tokyo, Japan|c|;;;	37282596600;37282600100;37618383200;37364307400
	SciVis	24-24 Oct. 1997	Collaborative visualization	10.1109/VISUAL.1997.663890	http://dx.doi.org/10.1109/VISUAL.1997.663890	253	259	663890	data visualisation	IRIS Explorer;collaborative visualization;collective large data set analysis;data flow visualization system;system architecture;user model;visualization pipeline	Collaboration;Collaborative work;Computer architecture;Data analysis;Data engineering;Data visualization;Distributed computing;Iris;Pipelines;Workstations		Current visualization systems are designed around a single user model, making it awkward for large research teams to collectively analyse large data sets. The paper shows how the popular data flow approach to visualization can be extended to allow multiple users to collaborate-each running their own visualization pipeline but with the opportunity to connect in data generated by a colleague, Thus collaborative visualizations are 'programmed' in exactly the same 'plug-and-play' style as is now customary for single-user mode. The paper describes a system architecture that can act as a basis for the collaborative extension of any data flow visualization system, and the ideas are demonstrated through a particular implementation in terms of IRIS Explorer.	Wood, J.;Wright, H.	Sch. of Comput. Studies, Leeds Univ., UK|c|;	37273103500;37609618400
	SciVis	24-24 Oct. 1997	VizWiz: a Java applet for interactive 3D scientific visualization on the Web	10.1109/VISUAL.1997.663891	http://dx.doi.org/10.1109/VISUAL.1997.663891	261	267	663891	Internet;data visualisation;security of data;subroutines	2D datasets;3D datasets;3D graphics;Java AWT API;Java applet;VizWiz;applet Web server;cutting planes;design tradeoffs;elevation plots;graphics performance figures;interactive 3D scientific visualization;isosurfaces;platform independent scientific visualization tool;security limitations;user data file uploading	Computer science;Data engineering;Data security;Data visualization;Drives;Graphics;Java;Testing;User interfaces;Web server		VizWiz is a Java applet that provides basic interactive scientific visualization functionality, such as isosurfaces, cutting planes, and elevation plots, for 2D and 3D datasets that can be loaded into the applet by the user via, the applet's Web server. VizWiz is unique in that it is a completely platform independent scientific visualization tool, and is usable over the Web, without being manually downloaded or installed. Its 3D graphics are implemented using only the Java AWT API, making them portable across all Java supporting platforms. The paper describes the implementation of VizWiz, including design tradeoffs. Graphics performance figures are provided for a number of different platforms. A solution to the problem of uploading user data files into a Java applet, working around security limitations, is demonstrated. The lessons learned from this project are discussed.	Michaels, C.;Bailey, M.	Dept. of Comput. Sci. & Eng., California Univ., San Diego, La Jolla, CA, USA|c|;	37623369100;37280473500
	SciVis	24-24 Oct. 1997	Image synthesis from a sparse set of views	10.1109/VISUAL.1997.663892	http://dx.doi.org/10.1109/VISUAL.1997.663892	269	275	663892	rendering (computer graphics)	duplicate information;image synthesis;multiple views;occlusion;perspective;polygon-based photo-realistic rendering process;real time texture mapping;sparse photograph set;sparse view set;unknown viewpoints	Cameras;Computer vision;Geometry;Image generation;Image reconstruction;Intelligent robots;Intelligent systems;Interpolation;Layout;Noise robustness;Rendering (computer graphics);Solid modeling		The authors present an image synthesis methodology and a system built around it. Given a sparse set of photographs taken from unknown viewpoints, the system generates images from new, different viewpoints with correct perspective, and handles occlusion. It achieves this without requiring any knowledge about the 3D structure of the scene nor the intrinsic camera parameters. The photo-realistic rendering process is polygon based and can be potentially implemented as real time texture mapping. The system is robust to noise by taking advantage of duplicate information from multiple views. They present results on several example scenes.	Qian Chen;Medioni, G.	Univ. of Southern California, Los Angeles, CA, USA|c|;	37364793300;37280043300
	SciVis	24-24 Oct. 1997	Virtualized reality: constructing time-varying virtual worlds from real world events	10.1109/VISUAL.1997.663893	http://dx.doi.org/10.1109/VISUAL.1997.663893	277	283	663893	image sequences;rendering (computer graphics);stereo image processing;video signal processing;virtual reality	3D scene manipulation;dynamic events;full 3D virtual representation construction;global 3D surface model;global modeling;image pair;image-based rendering;image-based stereo;intensity image;modeling technique;multiple video streams;range image;real world events;static scene sequence;time instant;time-varying virtual world construction;view synthesis;viewpoint;virtual representation;virtualized reality;visible surface model	Calibration;Cameras;Chromium;Computational geometry;Data mining;Error correction;Image analysis;Intelligent robots;Interpolation;Layout;Pixel;Platform virtualization;Rendering (computer graphics);Robots;Solid modeling;Streaming media;Virtual reality		Virtualized reality is a modeling technique that constructs full 3D virtual representations of dynamic events from multiple video streams. Image-based stereo is used to compute a range image corresponding to each intensity image in each video stream. Each range and intensity image pair encodes the scene structure and appearance of the scene visible to the camera at that moment, and is therefore called a visible surface model (VSM). A single time instant of the dynamic event can be modeled as a collection of VSMs from different viewpoints, and the full event can be modeled as a sequence of static scenes-the 3D equivalent of video. Alternatively, the collection of VSMs at a single time can be fused into a global 3D surface model, thus creating a traditional virtual representation out of real world events. Global modeling has the added benefit of eliminating the need to hand-edit the range images to correct errors made in stereo, a drawback of previous techniques. Like image-based rendering models, these virtual representations can be used to synthesize nearly any view of the virtualized event. For this reason, the paper includes a detailed comparison of existing view synthesis techniques with the authors' own approach. In the virtualized representations, however, scene structure is explicitly represented and therefore easily manipulated, for example by adding virtual objects to (or removing virtualized objects from) the model without interfering with real event. Virtualized reality, then, is a platform not only for image-based rendering but also for 3D scene manipulation.	Rander, P.;Narayanan, P.J.;Kanade, T.	Robotics Inst., Carnegie Mellon Univ., Pittsburgh, PA, USA|c|;;	37371914000;37298506200;37283735000
	SciVis	24-24 Oct. 1997	Extracting feature lines from 3D unstructured grids	10.1109/VISUAL.1997.663894	http://dx.doi.org/10.1109/VISUAL.1997.663894	285	292	663894	computational geometry;data visualisation;feature extraction;rendering (computer graphics)	3D unstructured grids;algorithms;dense meshes;feature line extraction;interactive manipulation;large meshes;solution data visualization;view-independent features;viewpoint-dependent features	Computer applications;Data mining;Displays;Feature extraction;NASA;Numerical simulation;Rendering (computer graphics);Shape;Solid modeling;Surface treatment;Switches		The paper discusses techniques for extracting feature lines from three-dimensional unstructured grids. The twin objectives are to facilitate the interactive manipulation of these typically very large and dense meshes, and to clarify the visualization of the solution data that accompanies them. The authors describe the perceptual importance of specific viewpoint-dependent and view-independent features, discuss the relative advantages and disadvantages of several alternative algorithms for identifying these features (taking into consideration both local and global criteria), and demonstrate the results of these methods on a variety of different data sets.	Kwan-Liu Ma;Interrante, V.	Inst. for Comput. Applications in Sci. & Eng., NASA Langley Res. Center, Hampton, VA, USA|c|;	37275869400;37282637800
	SciVis	24-24 Oct. 1997	I/O optimal isosurface extraction	10.1109/VISUAL.1997.663895	http://dx.doi.org/10.1109/VISUAL.1997.663895	293	300	663895	data visualisation;feature extraction;input-output programs;query processing;storage management;tree data structures;tree searching	I/O filter;I/O optimal isosurface extraction;I/O-optimal interval tree;active cells;algorithms;data set preprocessing;disk access;efficient search structure;main memory space;output-sensitive query;query operation;search time;unstructured grids;very large data set visualization;volumetric data;workstations	Computer displays;Data mining;Filters;Information retrieval;Isosurfaces;Mathematics;Packaging;Phase estimation;Statistics;Visualization;Workstations		The authors give I/O-optimal techniques for the extraction of isosurfaces from volumetric data, by a novel application of the I/O-optimal interval tree of Arge and Vitter (1996). The main idea is to preprocess the data set once and for all to build an efficient search structure in disk, and then each time one wants to extract an isosurface, they perform an output-sensitive query on the search structure to retrieve only those active cells that are intersected by the isosurface. During the query operation, only two blocks of main memory space are needed, and only those active cells are brought into the main memory, plus some negligible overhead of disk accesses. This implies that one can efficiently visualize very large data sets on workstations with just enough main memory to hold the isosurfaces themselves. The implementation is delicate but not complicated. They give the first implementation of the I/O-optimal interval tree, and also implement their methods as an I/O filter for Vtk's isosurface extraction for the case of unstructured grids. They show that, in practice, the algorithms improve the performance of isosurface extraction by speeding up the active-cell searching process so that it is no longer a bottleneck. Moreover, this search time is independent of the main memory available. The practical efficiency of the techniques reflects their theoretical optimality.	Yi-Jen Chiang;Silva, Claudio T.	;	37362725400;38183059100
	SciVis	24-24 Oct. 1997	CAVEvis: distributed real-time visualization of time-varying scalar and vector fields using the CAVE virtual reality theater	10.1109/VISUAL.1997.663896	http://dx.doi.org/10.1109/VISUAL.1997.663896	301	308	663896	data visualisation;distributed processing;real-time systems;rendering (computer graphics);synchronisation;virtual reality	CAVE virtual reality theater;CAVEvis;asynchronously running modules;distributed real-time visualization;interactive exploration;interactive visualization;interactivity bottlenecks;module synchronization;multiple machines;rendering;time-dependent data management;time-varying scalar fields;time-varying vector fields	Computational geometry;Computational modeling;Data visualization;Displays;Distributed computing;Geometry;Isosurfaces;Layout;Navigation;Probes;Rendering (computer graphics);Tornadoes;Virtual reality		The paper discusses CAVEvis and a related set of tools for the interactive visualization and exploration of large sets of time-varying scalar and vector fields using the CAVE virtual reality environment. Since visualization of large data sets can be very time-consuming in both computation and rendering time, the task is distributed over multiple machines, each of which is specialized for some aspect of the visualization process. All modules must run asynchronously to maintain the highest level of interactivity. A model of distributed visualization is introduced that addresses important issues related to the management of time-dependent data, module synchronization, and interactivity bottlenecks.	Jaswa, V.	Nat. Center for Supercomput. Applications, Champaign, IL, USA|c|	
	SciVis	24-24 Oct. 1997	Fast oriented line integral convolution for vector field visualization via the Internet	10.1109/VISUAL.1997.663897	http://dx.doi.org/10.1109/VISUAL.1997.663897	309	316	663897	Internet;computer animation;convolution;data visualisation;flow visualisation;image texture;integral equations;physics computing;rendering (computer graphics)	2D vector fields;Internet;Java applet;animation techniques;anisotropic convolution kernel;convolution operations;disks;fast oriented line integral convolution;fast rendering;flow fields;overlapping streamlets;sparse texture;streamlet approximation;vector field visualization	Animation;Anisotropic magnetoresistance;Computer graphics;Convolution;Filtering;Filters;Internet;Kernel;Low-frequency noise;Noise shaping;Streaming media;Visualization		Oriented line integral convolution (OLIC) illustrates flow fields by convolving a sparse texture with an anisotropic convolution kernel. The kernel is aligned to the underlying flow of the vector field. OLIC does not only show the direction of the flow but also its orientation. The paper presents fast rendering of oriented line integral convolution (FROLIC), which is approximately two orders of magnitude faster than OLIC. Costly convolution operations as done in OLIC are replaced in FROLIC by approximating a streamlet through a set of disks with varying intensity. The issue of overlapping streamlets is discussed. Two efficient animation techniques for animating FROLIC images are described. FROLIC has been implemented as a Java applet. This allows researchers from various disciplines (typically with inhomogenous hardware environments) to conveniently explore and investigate analytically defined 2D vector fields.	Wegenkittl, R.;Groller, E.	Inst. of Comput. Graphics, Wien Univ. of Technol., Austria|c|;	37267822600;38471589400
	SciVis	24-24 Oct. 1997	UFLIC: a line integral convolution algorithm for visualizing unsteady flows	10.1109/VISUAL.1997.663898	http://dx.doi.org/10.1109/VISUAL.1997.663898	317	322	663898	computer animation;convolution;data visualisation;digital simulation;feedforward;flow instability;flow simulation;flow visualisation;integral equations;physics computing	UFLIC algorithm;animation frames;flow advection;global feature tracing;line integral convolution algorithm;successive feedforward method;time-accurate highly coherent flow animation;time-accurate value depositing scheme;unsteady flow field;unsteady flow visualization;vector data visualization	Aerodynamics;Animation;Computational fluid dynamics;Computational modeling;Computer displays;Computer graphics;Convolution;Data visualization;Feedforward systems;NASA;Postal services		The paper presents an algorithm, UFLIC (Unsteady Flow LIC), to visualize vector data in unsteady flow fields. Using line integral convolution (LIC) as the underlying method, a new convolution algorithm is proposed that can effectively trace the flow's global features over time. The new algorithm consists of a time-accurate value depositing scheme and a successive feedforward method. The value depositing scheme accurately models the flow advection, and the successive feedforward method maintains the coherence between animation frames. The new algorithm can produce time-accurate, highly coherent flow animations to highlight global features in unsteady flow fields. CFD scientists, for the first time, are able to visualize unsteady surface flows using the algorithm.	Shen, H.-W.;Kao, D.L.	NASA Ames Res. Center, Moffett Field, CA, USA|c|;	37366741500;37339406400
	SciVis	24-24 Oct. 1997	The motion map: efficient computation of steady flow animations	10.1109/VISUAL.1997.663899	http://dx.doi.org/10.1109/VISUAL.1997.663899	323	328	663899	computer animation;data structures;data visualisation;flow visualisation;physics computing	2D steady flow field animation;cyclical variable-speed animations;data structure;dense representation;efficiency;efficient computation;frames;memory cost;memory requirements;motion information;motion map	Animation;Computational efficiency;Costs;Data structures;Fading;Filters;Image coding;Kernel;Rendering (computer graphics);Topology;Visualization		The paper presents a new approach for animating 2D steady flow fields. It is based on an original data structure called the motion map. The motion map contains not only a dense representation of the flow field but also all the motion information required to animate the flow. An important feature of this method is that it allows, in a natural way, cyclical variable-speed animations. As far as efficiency is concerned, the advantage of this method is that computing the motion map does not take more time than computing a single still image of the flow and the motion map has to be computed only once. Another advantage is that the memory requirements for a cyclical animation of an arbitrary number of frames amounts to the memory cost of a single still image.	Jobard, B.;Lefer, W.	Lab., Calais, France|c|;	37267249300;37612167300
	SciVis	24-24 Oct. 1997	Integrated volume compression and visualization	10.1109/VISUAL.1997.663900	http://dx.doi.org/10.1109/VISUAL.1997.663900	329	336	663900	Fourier analysis;Hartley transforms;data compression;data visualisation;medical image processing;rendering (computer graphics)	2D image rendering;Fourier projection theorem;compressed 3D data sets;data manipulation;data movement;frequency-domain representation;integrated volume compression/visualization;local coherency;medical diagnostic data;occlusion effect;projected 2D images;storage capacity;subcubes;viewing angles;volume rendering;volumetric data sets;volumetric data visualization scheme;whole-volume frequency-domain rendering schemes	Communication systems;Computer science;Data visualization;Fourier transforms;Frequency conversion;Helium;Image coding;Laboratories;Nuclear magnetic resonance;Rendering (computer graphics);Stress		Volumetric data sets require enormous storage capacity even at moderate resolution levels. The excessive storage demands not only stress the capacity of the underlying storage and communications systems, but also seriously limit the speed of volume rendering due to data movement and manipulation. A novel volumetric data visualization scheme is proposed and implemented in this work that renders 2D images directly from compressed 3D data sets. The novelty of this algorithm is that rendering is performed on the compressed representation of the volumetric data without pre-decompression. As a result, the overheads associated with both data movement and rendering processing are significantly reduced. The proposed algorithm generalizes previously proposed whole-volume frequency-domain rendering schemes by first dividing the 3D data set into subcubes, transforming each subcube to a frequency-domain representation, and applying the Fourier projection theorem to produce the projected 2D images according to given viewing angles. Compared to the whole-volume approach, the subcube-based scheme not only achieves higher compression efficiency by exploiting local coherency, but also improves the quality of resultant rendering images because it approximates the occlusion effect on a subcube by subcube basis.	Tzi-cker Chiueh;Chuan-Kai Yang;Taosong He;Pfister, H.;Kaufman, A.	Center for Visual Comput., State Univ. of New York, Stony Brook, NY, USA|c|;;;;	37273336700;37615351500;37350343500;37275698100;37268052800
	SciVis	24-24 Oct. 1997	Multiresolution compression and reconstruction	10.1109/VISUAL.1997.663901	http://dx.doi.org/10.1109/VISUAL.1997.663901	337	346	663901	computational geometry;data compression;image reconstruction;piecewise-linear techniques;pipeline processing;rendering (computer graphics);wavelet transforms	B-spline wavelet precoding;arbitrarily dimensioned data;bit stream;distributed applications;geometric constraint compression;geometric reconstruction;global oracles;interactive control;large scale data sets;lines;local client;local oracles;multiresolution compression;piecewise adaptive linear approximation;pipeline;point removal strategy;real world elements;remote server;rendering;surfaces;triangulation technique;uniform sampled data;volumes	Application software;Computational geometry;Computer graphics;Image coding;Image reconstruction;Isosurfaces;Piecewise linear approximation;Pipelines;Surface reconstruction;Web server;World Wide Web		The paper presents a framework for multiresolution compression and geometric reconstruction of arbitrarily dimensioned data designed for distributed applications. Although being restricted to uniform sampled data, the versatile approach enables the handling of a large variety of real world elements. Examples include nonparametric, parametric and implicit lines, surfaces or volumes, all of which are common to large scale data sets. The framework is based on two fundamental steps: compression is carried out by a remote server and generates a bit-stream transmitted over the underlying network. Geometric reconstruction is performed by the local client and renders a piecewise linear approximation of the data. More precisely, the compression scheme consists of a newly developed pipeline starting from an initial B-spline wavelet precoding. The fundamental properties of wavelets allow progressive transmission and interactive control of the compression gain by means of global and local oracles. In particular the authors discuss the problem of oracles in semiorthogonal settings and propose sophisticated oracles to remove unimportant coefficients. In addition, geometric constraints such as boundary lines can be compressed in a lossless manner and are incorporated into the resulting bit-stream. The reconstruction pipeline performs a piecewise adaptive linear approximation of data using a fast and easy to use point removal strategy which works with any subsequent triangulation technique.	Staadt, O.G.;Gross, M.H.;Weber, R.	Dept. of Comput. Sci., Eidgenossische Tech. Hochschule, Zurich, Switzerland|c|;;	37355334600;37275694700;37273480100
	SciVis	24-24 Oct. 1997	Optimized geometry compression for real-time rendering	10.1109/VISUAL.1997.663902	http://dx.doi.org/10.1109/VISUAL.1997.663902	347	354	663902	computational geometry;data compression;data visualisation;real-time systems;rendering (computer graphics)	24 byte;30 Hz;3D geometry;3D rendering hardware;720 MB/s;ASCII encoded formats;binary encoded triangle strips;complex data sets;compression ratios;generalized triangle meshes;large 3D model retrieval;large 3D model storage;memory bus bandwidth bottleneck;memory requirements;mesh vertices;off-line pre-process;optimized geometry compression;processor to graphics pipeline interface;real-time decompression;real-time rendering;transfer rate;visualization	Bandwidth;Data visualization;Encoding;Geometry;Graphics;Hardware;Information retrieval;Pipelines;Rendering (computer graphics);Solid modeling		Most existing visualization applications use 3D geometry as their basic rendering primitive. As users demand more complex data sets, the memory requirements for retrieving and storing large 3D models are becoming excessive. In addition, the current 3D rendering hardware is facing a large memory bus bandwidth bottleneck at the processor to graphics pipeline interface. Rendering 1 million triangles with 24 bytes per triangle at 30 Hz requires as much as 720 MB/sec memory bus bandwidth. This transfer rate is well beyond the current low-cost graphics systems. A solution is to compress the static 3D geometry as an off-line pre-process. Then, only the compressed geometry needs to be stored in main memory and sent down to the graphics pipeline for real-time decompression and rendering. The author presents several new techniques for compression of 3D geometry that produce 2 to 3 times better compression ratios than existing methods. They first introduce several algorithms for the efficient encoding of the original geometry as generalized triangle meshes. This encoding allows most of the mesh vertices to be reused when forming new triangles. Their second contribution allows various parts of a geometric model to be compressed with different precision depending on the level of details present. Together, the meshifying algorithms and the variable compression method achieve compression ratios of 30 and 37 to one over ASCII encoded formats and 10 and 15 to one over binary encoded triangle strips. The experimental results show a dramatically lowered memory bandwidth required for real-time visualization of complex data sets.	Chow, M.M.	MIT, Cambridge, MA, USA|c|	37609292100
	SciVis	24-24 Oct. 1997	Architectural walkthroughs using portal textures	10.1109/VISUAL.1997.663903	http://dx.doi.org/10.1109/VISUAL.1997.663903	355	362	663903	architectural CAD;computational geometry;image texture;rendering (computer graphics)	adjacent cells;architectural walkthroughs;cell-partitioned model;depth complexity;doors;interactive performance;portal textures;rendering complexity;windows	Buildings;Computer science;Displays;Geometry;Marine vehicles;Portals;Rendering (computer graphics);Runtime;Solid modeling;Visualization		This paper outlines a method to dynamically replace portals with textures in a cell-partitioned model. The rendering complexity is reduced to the geometry of the current cell thus increasing interactive performance. A portal is a generalization of windows and doors. It connects two adjacent cells (or rooms). Each portal of the current cell that is some distance away from the viewpoint is rendered as a texture. The portal texture (smoothly) returns to geometry when the viewpoint gets close to the portal. This way all portal sequences (not too close to the viewpoint) have a depth complexity of one. The size of each texture and distance at which the transition occurs is configurable for each portal.	Aliaga, Daniel G.;Lastra, A.A.	Dept. of Comput. Sci., North Carolina Univ., Chapel Hill, NC, USA|c|;	37270561900;37274005100
	SciVis	24-24 Oct. 1997	Repairing CAD models	10.1109/VISUAL.1997.663904	http://dx.doi.org/10.1109/VISUAL.1997.663904	363	370	663904	CAD;computational geometry;error analysis;solid modelling	B-REP;CAD model repair;boundary edge;dangling wall;designer errors;fault-repair algorithm;finite element analysis;imprecise arithmetic;model transformations;most appropriate edge;polygonal hole;polyhedral CAD models;programming bugs;radiosity computation;rapid prototyping;shared-vertex representation;solid models;visualization	Arithmetic;Computer bugs;Computer errors;Computer science;Design automation;Error correction;Geometry;Military computing;Prototypes;Solid modeling;Surface cracks;Uniform resource locators;Visualization		We describe an algorithm for repairing polyhedral CAD models that have errors in their B-REP. Errors like cracks, degeneracies, duplication, holes and overlaps are usually introduced in solid models due to imprecise arithmetic, model transformations, designer errors, programming bugs, etc. Such errors often hamper further processing such as finite element analysis, radiosity computation and rapid prototyping. Our fault-repair algorithm converts an unordered collection of polygons to a shared-vertex representation to help eliminate errors. This is done by choosing, for each polygon edge, the most appropriate edge to unify it with. The two edges are then geometrically merged into one, by moving vertices. At the end of this process, each polygon edge is either coincident with another or is a boundary edge for a polygonal hole or a dangling wall and may be appropriately repaired. Finally, in order to allow user-inspection of the automatic corrections, we produce a visualization of the repair and let the user mark the corrections that conflict with the original design intent. A second iteration of the correction algorithm then produces a repair that is commensurate with the intent. This, by involving the users in a feedback loop, we are able to refine the correction to their satisfaction.	Barequet, G.;Subodh Kumar	Dept. of Comput. Sci., Johns Hopkins Univ., Baltimore, MD, USA|c|;	37326555000;37277183300
	SciVis	24-24 Oct. 1997	Dynamic smooth subdivision surfaces for data visualization	10.1109/VISUAL.1997.663905	http://dx.doi.org/10.1109/VISUAL.1997.663905	371	377	663905	computational geometry;data visualisation;differential equations;finite element analysis;interactive systems;real-time systems	Catmull-Clark subdivision scheme;Lagrangian mechanics;complex shape recovery;complicated objects;computer graphics;control vertices;data visualization;degrees of freedom;dynamic differential equation;dynamic smooth subdivision surfaces;dynamic surface model;finite element discretization;geometric shape design;geometric shape manipulation;interactive deformation;physics-based modeling paradigm;range data;real-time synthesized forces;recursive subdivision schemes;scientific visualization;smooth surface modelling;topology;user-specified polygonal mesh;volume data	Application software;Computer graphics;Data visualization;Deformable models;Differential equations;Mesh generation;Robust control;Shape control;Solid modeling;Topology		Recursive subdivision schemes have been extensively used in computer graphics and scientific visualization for modeling smooth surfaces of arbitrary topology. Recursive subdivision generates a visually pleasing smooth surface in the limit from an initial user-specified polygonal mesh through the repeated application of a fixed set of subdivision rules. In this paper, we present a new dynamic surface model based on the Catmull-Clark (1978) subdivision scheme, which is a very popular method to model complicated objects of arbitrary genus because of many of its nice properties. Our new dynamic surface model inherits the attractive properties of the Catmull-Clark subdivision scheme as well as that of the physics-based modeling paradigm. This new model provides a direct and intuitive means of manipulating geometric shapes, a fast, robust and hierarchical approach for recovering complex geometric shapes from range and volume data using very few degrees of freedom (control vertices). We provide an analytic formulation and introduce the physical quantities required to develop the dynamic subdivision surface model which can be interactively deformed by applying synthesized forces in real time. The governing dynamic differential equation is derived using Lagrangian mechanics and a finite element discretization. Our experiments demonstrate that this new dynamic model has a promising future in computer graphics, geometric shape design and scientific visualization.	Mandal, C.;Hong Qin;Vemuri, B.C.	Dept. of Comput. & Inf. Sci. & Eng., Florida Univ., Gainesville, FL, USA|c|;;	37369631900;37276553900;37285145200
	SciVis	24-24 Oct. 1997	Smooth hierarchical surface triangulations	10.1109/VISUAL.1997.663906	http://dx.doi.org/10.1109/VISUAL.1997.663906	379	386	663906	approximation theory;data visualisation;mesh generation;sequences	data visualization;function generation;level-of-detail blending;level-of-detail representation;mesh sequence;mesh simplification;mesh transformation;ordering operation;shape approximation;smooth hierarchical surface triangulations;triangle meshes;triangle-collapse operations;triangulated surface model;underlying surface approximation;weight assignment	Area measurement;Chromium;Computer graphics;Computer science;Data visualization;Image generation;Image processing;Large-scale systems;Legged locomotion;Rendering (computer graphics);Shape			Gieng, T.S.;Hamann, B.;Joy, K.I.;Schussman, G.L.;Trotts, I.J.	Center for Image Process. & Integrated Comput., California Univ., Davis, CA, USA|c|;;;;	37612257400;37282068700;37267811400;37283639900;37372274900
	SciVis	24-24 Oct. 1997	The multilevel finite element method for adaptive mesh optimization and visualization of volume data	10.1109/VISUAL.1997.663907	http://dx.doi.org/10.1109/VISUAL.1997.663907	387	394	663907	adaptive estimation;conjugate gradient methods;data visualisation;error analysis;function approximation;mesh generation;optimisation;piecewise-linear techniques;ray tracing;rendering (computer graphics)	adaptive mesh generation;adaptive mesh optimization;adaptively compressed datasets;coarse triangulation;global approximation error;highly nonuniform tetrahedral meshes;highly nonuniform triangular meshes;iso-surface extractor;large datasets;local error control;local refinement;mesh reduction techniques;multilevel finite element method;multilevel preconditioned conjugate gradient solver;multilevel representations;piecewise linear finite element approximation;ray-caster;rendering quality;scalar-valued functions;speedup;vector-valued functions;volume data visualization	Acceleration;Algorithm design and analysis;Approximation error;Data visualization;Error correction;Finite element methods;Linear approximation;Optimization methods;Piecewise linear approximation;Piecewise linear techniques		Multilevel representations and mesh reduction techniques have been used for accelerating the processing and the rendering of large datasets representing scalar- or vector-valued functions defined on complex 2D or 3D meshes. We present a method based on finite element approximations which combines these two approaches in a new and unique way that is conceptually simple and theoretically sound. The main idea is to consider mesh reduction as an approximation problem in appropriate finite element spaces. Starting with a very coarse triangulation of the functional domain, a hierarchy of highly non-uniform tetrahedral (or triangular in 2D) meshes is generated adaptively by local refinement. This process is driven by controlling the local error of the piecewise linear finite element approximation of the function on each mesh element. A reliable and efficient computation of the global approximation error and a multilevel preconditioned conjugate gradient solver are the key components of the implementation. In order to analyze the properties and advantages of the adaptively generated tetrahedral meshes, we implemented two volume visualization algorithms: an iso-surface extractor and a ray-caster. Both algorithms, while conceptually simple, show significant speedups over conventional methods delivering comparable rendering quality from adaptively compressed datasets.	Grosso, R.;Lurig, C.;Ertl, T.	Comput. Graphics Group, Erlangen-Nurnberg Univ., Germany|c|;;	37612367400;37372245300;37268023800
	SciVis	24-24 Oct. 1997	Simplifying polygonal models using successive mappings	10.1109/VISUAL.1997.663908	http://dx.doi.org/10.1109/VISUAL.1997.663908	395	402	663908	computational geometry;rendering (computer graphics);solid modelling	Ford Bronco;appropriate texture coordinates;computational geometry;edge collapse;edge collapse operations;error bounds;levels-of-detail;linear programming;local planar projections;mapping functions;model simplification;object modeling;piece-wise linear mapping function;polygonal model simplification;polygonal models;real-time rendering;simplification operation;successive mappings;surface approximation;surface deviation;textured lion model;textured wrinkled torus;vertex position	Chromium;Computational geometry;Computer graphics;Current measurement;Information geometry;Linear approximation;Linear programming;Piecewise linear techniques;Real time systems;Rendering (computer graphics);Solid modeling;Surface texture;Upper bound		We present the use of mapping functions to automatically generate levels of detail with known error bounds for polygonal models. We develop a piece-wise linear mapping function for each simplification operation and use this function to measure deviation of the new surface from both the previous level of detail and from the original surface. In addition, we use the mapping function to compute appropriate texture coordinates if the original map has texture coordinates at its vertices. Our overall algorithm uses edge collapse operations. We present rigorous procedures for the generation of local planar projections as well as for the selection of a new vertex position for the edge collapse operation. As compared to earlier methods, our algorithm is able to compute tight error bounds on surface deviation and produce an entire continuum of levels of detail with mappings between them. We demonstrate the effectiveness of our algorithm on several models: a Ford Bronco consisting of over 300 parts and 70,000 triangles, a textured lion model consisting of 49 parts and 86,000 triangles, and a textured, wrinkled torus consisting of 79,000 triangles.	Cohen, J.;Manocha, D.;Olano, M.	North Carolina Univ., Chapel Hill, NC, USA|c|;;	37275763600;37267825600;38181249400
	SciVis	24-24 Oct. 1997	Controlled simplification of genus for polygonal models	10.1109/VISUAL.1997.663909	http://dx.doi.org/10.1109/VISUAL.1997.663909	403	410	663909	computational geometry;data visualisation;rendering (computer graphics)	/spl alpha/-hulls;L/sub /spl infin// distance metric;cavities;computational geometry;controlled simplification;datasets;genus-preserving simplifications;genus-reducing simplifications;level-of-detail-based rendering;multiresolution hierarchies;object representations;polygonal meshes;polygonal models;simplification framework;small holes;triangulations;tunnels	Chromium;Computational geometry;Computer displays;Computer graphics;Hardware;Image generation;Rendering (computer graphics);Solid modeling;Switches;Three dimensional displays;Topology;Visualization			El-Sana, J.;Varshney, A.	State Univ. of New York, Stony Brook, NY, USA|c|;	37393584400;37282560200
	SciVis	24-24 Oct. 1997	Vortex identification-applications in aerodynamics: a case study	10.1109/VISUAL.1997.663910	http://dx.doi.org/10.1109/VISUAL.1997.663910	413	416	663910	aerodynamics;aerospace computing;data visualisation;flow visualisation;vortices	automatic grid refinement;disjointed line segments;eigenvector method;external flow aerodynamics;nonvortical flow features;spiral vortex breakdowns;vortex bursting;vortex core displacement;vortex cores;vortex diffusion;vortex identification	Aerodynamics;Computer aided software engineering;Computer vision;Eigenvalues and eigenfunctions;Electric breakdown;NASA;Space technology;Spirals;Tensile stress;Visualization		An eigenvector method for vortex identification has been applied to recent numerical and experimental studies in external flow aerodynamics. It is shown to be an effective way to extract and visualize features such as vortex cores, spiral vortex breakdowns, vortex bursting, and vortex diffusion. Several problems are reported and illustrated. These include: disjointed line segments, detecting non-vortical flow features, and vortex core displacement. Future research and applications are discussed, such as using vortex cores to guide automatic grid refinement.	Kenwright, D.;Haimes, R.	NASA Ames Res. Center, Moffett Field, CA, USA|c|;	37355295400;37282898700
	SciVis	24-24 Oct. 1997	exVis: developing a wind tunnel data visualization tool	10.1109/VISUAL.1997.663911	http://dx.doi.org/10.1109/VISUAL.1997.663911	417	420	663911	aerospace computing;data visualisation;flow visualisation;wind tunnels	NASA Ames;aeronautics data;aeroscientists;computational fluid dynamics;data acquisition;exVis;interactive computer-mediated analysis tools;software developers;visualization techniques;wind tunnel data visualization tool;wind tunnel experiments	Aerodynamics;Collaborative software;Computational fluid dynamics;Data acquisition;Data analysis;Data visualization;NASA;Object oriented modeling;Pressure measurement;Testing		Software has been developed to apply visualization techniques to aeronautics data collected during wind tunnel experiments. Interaction between the software developers and the aeroscientists has been crucial in making the software. The interaction has also been important in building the scientists' confidence in the use of interactive, computer-mediated analysis tools.	Uselton, S.P.	MRJ Technol. Solutions, NASA Ames Res. Center, Moffett Field, CA, USA|c|	37372247300
	SciVis	24-24 Oct. 1997	Strategies for effectively visualizing 3D flow with volume LIC	10.1109/VISUAL.1997.663912	http://dx.doi.org/10.1109/VISUAL.1997.663912	421	424	663912	convolution;data visualisation;flow visualisation;image texture;laminar flow;physics computing;rendering (computer graphics);supersonic flow	3D flow visualisation;3D visibility-impeding halos;advected texture elements;contiguous elements;depth discontinuities;hot supersonic laminar jet;input texture;integral convolution;line integral convolution;regions of interest;rendering;selective highlighting;subsonic coflow;volume LIC;volume line integral convolution	Aggregates;Character generation;Colored noise;Computer applications;Convolution;Data visualization;Impedance;Inspection;Streaming media;Three dimensional displays		"This paper discusses strategies for effectively portraying 3D flow using volume line integral convolution. Issues include defining an appropriate input texture, clarifying the distinct identities and relative depths of the advected texture elements, and selectively highlighting regions of interest in both the input and output volumes. Apart from offering insights into the greater potential of 3D LIC as a method for effectively representing flow in a volume, a principal contribution of this work is the suggestion of a technique for generating and rendering 3D visibility-impeding ""halos"" that can help to intuitively indicate the presence of depth discontinuities between contiguous elements in a projection and thereby clarify the 3D spatial organization of elements in the flow. The proposed techniques are applied to the visualization of a hot, supersonic, laminar jet exiting into a colder, subsonic coflow."	Interrante, V.;Grosch, C.	;	37282637800;37347332000
	SciVis	24-24 Oct. 1997	Towards efficient visualization support for single-block and multi-block datasets	10.1109/VISUAL.1997.663913	http://dx.doi.org/10.1109/VISUAL.1997.663913	425	428	663913	data visualisation;software packages	AVS/Express;RAM;case study;data access techniques;data processing;graphics primitives;large datasets;multi-block datasets;multi-grid configurations;resource sharing;simulation grids;single-block datasets;visualization environment;visualization software	Computational modeling;Data visualization;Electrostatic precipitators;Graphics;Hardware;Isosurfaces;Object oriented modeling;Resource management;Scientific computing;Tensile stress		Large simulation grids and multi-grid configurations impose many constraints on commercial visualization software. When available RAM is limited and graphics primitives are numbered in millions, alternative techniques for data access and processing are necessary. In this case study, we present our contributions to a visualization environment based on the AVS/Express software. We demonstrate how the efficient visualization of large datasets relies upon several forms of resource sharing, and alternate and efficient data access techniques.	Favre, J.M.	Swiss Center for Sci. Comput., Switzerland|c|	37284061600
	SciVis	24-24 Oct. 1997	Brushing techniques for exploring volume datasets	10.1109/VISUAL.1997.663914	http://dx.doi.org/10.1109/VISUAL.1997.663914	429	432	663914	data visualisation;information retrieval;microcomputer applications;natural sciences computing;public domain software;software libraries;solid modelling;visual databases	32 MByte;3D volume dataset browsing;Linux machine;data access;desktop computers;interactive time;multi-resolution brushing;public-domain software libraries;scientific visualization techniques	Computed tomography;Costs;Data visualization;Hardware;Isosurfaces;Magnetic heads;Magnetic resonance imaging;Software libraries;Software performance;Statistical analysis		Describes several visualization techniques based on the notion of multi-resolution brushing to browse large 3D volume datasets. Our software is implemented using public-domain libraries, and is designed to run on average-equipped desktop computers such as a Linux machine with 32 MBytes of memory. Empirically, our system allows scientists to obtain information from a large dataset with over 8.3 million numbers in interactive time. We show that very large scientific volume datasets can be accessed and utilized without expensive hardware and software.	Pak Chung Wong;Bergeron, R.D.	Dept. of Comput. Sci., New Hampshire Univ., Durham, NH, USA|c|;	;37342054500
	SciVis	24-24 Oct. 1997	Interactive volume rendering for virtual colonoscopy	10.1109/VISUAL.1997.663915	http://dx.doi.org/10.1109/VISUAL.1997.663915	433	436	663915	data visualisation;medical diagnostic computing;parallel algorithms;ray tracing;rendering (computer graphics);shared memory systems;solid modelling;virtual reality	3D virtual colonoscopy;benign structures;colon visualization;empty space skipping;endoscopy;interactive volume rendering;interior surface;malignant structures;parallel algorithm;patient data sets;perspective ray casting;rendering speed;resampling rates;shared-memory multiprocessing architecture;simulation data;sub-surface tissues;surface navigation;surface-assistant techniques;visibility	Biomedical optical imaging;Colon;Colonography;Computed tomography;Computer graphics;Navigation;Rendering (computer graphics);Solid modeling;Virtual colonoscopy;Visualization		3D virtual colonoscopy has recently been proposed as a non-invasive alternative procedure for the visualization of the human colon. Surface rendering is sufficient for implementing such a procedure to obtain an overview of the interior surface of the colon at interactive rendering speeds. Unfortunately, physicians can not use it to explore tissues beneath the surface to differentiate between benign and malignant structures. In this paper, we present a direct volume rendering approach based on perspective ray casting, as a supplement to the surface navigation. To accelerate the rendering speed, surface-assistant techniques are used to adapt the resampling rates by skipping the empty space inside the colon. In addition, a parallel version of the algorithm has been implemented on a shared-memory multiprocessing architecture. Experiments have been conducted on both simulation and patient data sets.	Suya You;Lichan Hong;Wan, M.;Kaufman, A.;Muraki, S.;Yong Zhou;Wax, M.;Zhengrong Liang	Center for Visual Comput., State Univ. of New York, Stony Brook, NY, USA|c|;;;;;;;	37611469100;37347459100;37362958300;37268052800;37612362200;37363214300;37354296000;37273115400
	SciVis	24-24 Oct. 1997	DNA visual and analytic data mining	10.1109/VISUAL.1997.663916	http://dx.doi.org/10.1109/VISUAL.1997.663916	437	441	663916	DNA;biology computing;data visualisation;deductive databases;knowledge acquisition;neural nets;pattern classification;sequences	AMI;DNA sequence classification;accuracy;analytic data mining;average mutual information;coding DNA sequences;data exploration techniques;data visualization;exons;fundamental position-dependent DNA nucleotide frequency values;introns;neural network classifiers;noncoding DNA sequences;nonlinear combination;rule-based classifiers;visual data mining	Biological cells;DNA;Data analysis;Data mining;Data visualization;Frequency;Neural networks;Organisms;Proteins;Sequences		Describes data exploration techniques designed to classify DNA sequences. Several visualization and data mining techniques were used to validate and attempt to discover new methods for distinguishing coding DNA sequences (exons) from non-coding DNA sequences (introns). The goal of the data mining was to see whether some other, possibly non-linear combination of the fundamental position-dependent DNA nucleotide frequency values could be a better predictor than the AMI (average mutual information). We tried many different classification techniques including rule-based classifiers and neural networks. We also used visualization of both the original data and the results of the data mining to help verify patterns and to understand the distinction between the different types of data and classifications. In particular, the visualization helped us develop refinements to neural network classifiers, which have accuracies as high as any known method. Finally, we discuss the interactions between visualization and data mining and suggest an integrated approach.	Hoffman, P.;Grinstein, G.;Marx, K.;Grosse, I.;Stanley, E.	Inst. for Visualization & Perception Res., Massachusetts Univ., Lowell, MA, USA|c|;;;;	37617749600;38470495600;38318555000;37622312400;38180962800
	SciVis	24-24 Oct. 1997	An interactive cerebral blood vessel exploration system	10.1109/VISUAL.1997.663917	http://dx.doi.org/10.1109/VISUAL.1997.663917	443	446	663917	bifurcation;biomedical NMR;blood;brain;data visualisation;image reconstruction;interactive systems;medical diagnostic computing;neurophysiology;rendering (computer graphics);surgery	aneurysms;automatic identification;automatic labelling;cerebral blood vessels;data visualization;individual magnetic resonance angiography slices;interactive exploration system;maximum intensity projection;medical applications;neurosurgeon requirements;shaded rendering;stenoses;surface reconstruction;symbolic model;symbolic schemes;vascular pathology diagnosis;vascular tree;vessel bifurcations;volume modelling;volume rendering	Aneurysm;Angiography;Bifurcation;Blood vessels;Labeling;Magnetic resonance;Neurosurgery;Pathology;Surface reconstruction;Visualization		An interactive cerebral blood vessel exploration system is described. It has been designed on the basis of neurosurgeons' requirements in order to assist them in the diagnosis of vascular pathologies. The system is based on the construction of a symbolic model of the vascular tree, with automatic identification and labelling of vessel bifurcations, aneurysms and stenoses. It provides several types of visualization: individual MRA (magnetic resonance angiography) slices, MIP (maximum intensity projection), shaded rendering, symbolic schemes and surface reconstruction.	Puig, A.;Tost, D.;Navazo, I.	Polytech. Univ. of Catalonia, Spain|c|;;	37615709500;37396861000;37612266700
	SciVis	24-24 Oct. 1997	Instructional software for visualizing optical phenomena	10.1109/VISUAL.1997.663918	http://dx.doi.org/10.1109/VISUAL.1997.663918	447	450	663918	Unix;courseware;data visualisation;graphical user interfaces;interactive systems;microcomputer applications;optics;physics computing;subroutines	Microsoft Windows;Open Inventor;Unix;courseware;instructional software;interactive 3D graphical modules;interactive 3D visualization techniques;multidisciplinary effort;optical phenomena visualization;program modules;upper-level undergraduate course	Computer graphics;Education;Educational institutions;Laboratories;Optical computing;Optical design;Optical filters;Physics;Stimulated emission;Visualization		We describe a multidisciplinary effort for creating interactive 3D graphical modules for visualizing optical phenomena. These modules are designed for use in an upper-level undergraduate course. The modules are developed in Open Inventor, which allows them to run under both Unix and Windows. The work is significant in that it applies contemporary interactive 3D visualization techniques to instructional courseware, which represents a considerable advance compared to the current state of the practice.	Banks, D.C.;Foley, J.T.;Vidimce, K.N.;Ming-Hoe Kiu	NSF Eng. Res. Center, Mississippi State Univ., MS, USA|c|;;;	37356983700;37393096300;37612298500;37612321600
	SciVis	24-24 Oct. 1997	Case study: wildfire visualization	10.1109/VISUAL.1997.663919	http://dx.doi.org/10.1109/VISUAL.1997.663919	451	454	663919	data visualisation;digital simulation;disasters;emergency services;environmental science computing;fires;forecasting theory;forestry	assessment expenditure;case study;computer models;crisis event progress forecasting;fire personnel;human suffering;loss of life;property destruction;realism;recovery expenditure;scientific accuracy;simulation quality assessment;wildfire spread simulation;wildfire visualization	Computer aided software engineering;Fires;Humans;Laboratories;Personnel;Predictive models;Read-write memory;Visualization;Weather forecasting;Wind		The ability to forecast the progress of crisis events would significantly reduce human suffering and loss of life, the destruction of property and expenditures for assessment and recovery. Los Alamos National Laboratory has established a scientific thrust in crisis forecasting to address this national challenge. In the initial phase of this project, scientists at Los Alamos are developing computer models to predict the spread of a wildfire. Visualization of the results of the wildfire simulation will be used by scientists to assess the quality of the simulation and eventually by fire personnel as a visual forecast of the wildfire's evolution. The fire personnel and scientists want the visualization to look as realistic as possible without compromising scientific accuracy. This paper describes how the visualization was created, analyzes the tools and approach that were used, and suggests directions for future work and research.	Ahrens, J.;McCormick, P.;Bossert, J.;Reisner, J.;Winterkamp, J.	Adv. Comput. Lab., Los Alamos Nat. Lab., NM, USA|c|;;;;	37282713700;37282708700;37617519400;37621785900;37612260000
	SciVis	24-24 Oct. 1997	Visualization of geometric algorithms in an electronic classroom	10.1109/VISUAL.1997.663920	http://dx.doi.org/10.1109/VISUAL.1997.663920	455	458	663920	computational geometry;computer aided instruction;computer animation;computer science education;data visualisation;distributed processing;interactive systems;visual programming	3D geometric algorithm visualization;GASP-II;algorithm presentation;conceptual model;distributed electronic classroom;geometric animation system;interactive exploration;naive users	Animation;Collaboration;Communication system traffic control;Libraries;Machine learning;Programming profession;Solid modeling;Traffic control;Visualization;Watches		This paper investigates the visualization and animation of geometric computing in a distributed electronic classroom. We show how focusing in a well-defined domain makes it possible to develop a compact system that is accessible to even naive users. We present a conceptual model and a system, GASP-II (Geometric Animation System, Princeton, II), that realizes this model in the geometric domain. The system allows the presentation and interactive exploration of 3D geometric algorithms over a network.	Shneerson, M.;Tal, A.	Dept. of Appl. Math., Weizmann Inst. of Sci., Rehovot, Israel|c|;	37612261200;37337567000
	SciVis	24-24 Oct. 1997	Collaborative augmented reality: exploring dynamical systems	10.1109/VISUAL.1997.663921	http://dx.doi.org/10.1109/VISUAL.1997.663921	459	462	663921	data visualisation;groupware;stereo image processing;virtual reality	3D-interaction;AVS;DynSys3D;STUDIERSTUBE;augmented reality system;collaborative augmented reality;collaborative scientific visualization;customized views;dynamical systems;dynamical systems exploration;individual viewpoints;natural collaboration;stereoscopy;visualization system	Augmented reality;Collaboration;Collaborative work;Computer displays;Computer graphics;Data visualization;Head;Three dimensional displays;Virtual environment;Virtual reality		We present collaborative scientific visualization in STUDIERSTUBE. STUDIERSTUBE is an augmented reality system that has several advantages over conventional desktop and other virtual reality environments, including true stereoscopy, 3D-interaction, individual viewpoints and customized views for multiple users, unhindered natural collaboration and low cost. We demonstrate the application of this concept for the interaction of multiple users and illustrate it with several visualizations of dynamical systems in DynSys3D, a visualization system running on top of AVS.	Fuhrmann, A.;Loffelmann, H.;Schmalstieg, D.	Wien Univ. of Technol., Austria|c|;;	37267442100;37427428300;37297103800
	SciVis	24-24 Oct. 1997	Case study: visualizing customer segmentations produced by self organizing maps	10.1109/VISUAL.1997.663922	http://dx.doi.org/10.1109/VISUAL.1997.663922	463	466	663922	data visualisation;knowledge acquisition;marketing data processing;self-organising feature maps;very large databases	SOM algorithm;abstract overviews;customer records;customer segmentations visualisations;data mining;decision makers;marketing campaign;marketing strategies;scatterplots;self organizing maps;very large databases;visualization programs	Business communication;Computer aided software engineering;Dairy products;Data mining;Data visualization;Demography;Neural networks;Scattering;Self organizing feature maps;Visual databases		We describe a set of visualization programs developed for understanding segmentations of customer records produced by a self organizing map (SOM) algorithm. A SOM produces segments of similar customer records that can then be used as the basis of a marketing campaign. Since the characteristics that each segment will have in common are not specified a priori, visualization is essential to understanding the segment to design specific marketing strategies. Two different styles of visualizations were found to be useful for the two types of observers of the data. Abstract overviews of the entire segmentation were designed for analysts applying the SOM algorithm. Detailed scatterplots of individual records were designed for communicating the results to decision makers specifying marketing strategy.	Rushmeier, H.;Lawrence, R.;Almasi, G.	IBM Thomas J. Watson Res. Center, Yorktown Heights, NY, USA|c|;;	38338818800;38182013000;38203048700
	SciVis	24-24 Oct. 1997	Pearls found on the way to the ideal interface for scanned probe microscopes	10.1109/VISUAL.1997.663923	http://dx.doi.org/10.1109/VISUAL.1997.663923	467	470	663923	computerised instrumentation;data visualisation;interactive devices;rendering (computer graphics);scanning probe microscopy;telecontrol;user interfaces;virtual reality	advanced rendering;atomic force microscopy;control techniques;force feedback;haptic force;ideal interface;interactive graphics;manipulation;natural viewpoint control;scanned probe microscopes;scanning tunneling microscopy;scientific discovery;teleoperation;telepresence;user interface;virtual environment;virtual-environment interface;visualization	Application software;Coatings;Computer interfaces;Force control;Force feedback;Microscopy;Physics computing;Probes;Rendering (computer graphics);Visualization		Since 1991, our team of computer scientists, chemists and physicists have worked together to develop an advanced, virtual-environment interface to scanned-probe microscopes. The interface has provided insights and useful capabilities well beyond those of the traditional interface. This paper lists the particular visualization and control techniques that have enabled actual scientific discovery, including specific examples of insight gained using each technique. This information can help scientists determine which features are likely to be useful in their particular application, and which would be just sugar coating. It can also guide computer scientists to suggest the appropriate type of interface to help solve a particular problem. We have found benefit in advanced rendering with natural viewpoint control (but not always), from semi-automatic control techniques, from force feedback during manipulation, and from storing/replaying data for an entire experiment. These benefits come when the system is well-integrated into the existing tool and allows export of the data to standard visualization packages.	Taylor, R.M.;Jun Chen;Okimoto, S.;Llopis-Artime, N.;Chi, V.L.;Brooks, F.P., Jr.;Falvo, M.;Paulson, S.;Glick, D.;Washburn, Sean;Superfine, Richard	Dept. of Comput. Sci., North Carolina Univ., Chapel Hill, NC, USA|c|;;;;;;;;;;	38183525100;37616309400;37612241200;37612237600;37623416900;37295063200;37305862000;37447873500;37620455200;37271671300;37271669900
	SciVis	24-24 Oct. 1997	Viewing IGES files through VRML	10.1109/VISUAL.1997.663924	http://dx.doi.org/10.1109/VISUAL.1997.663924	471	474	663924	CAD;data visualisation;electronic data interchange;engineering graphics;file organisation;hypermedia;page description languages;program interpreters;specification languages;virtual reality	CAD;HTML;Hypertext Markup Language;IGES file viewing;Initial Graphics Exchange Specification;Java-based translator;VRML;Virtual Reality Modeling Language;computer-aided design;data visualization applications;file format conversion	Chromium;Computer applications;Computer graphics;Design automation;Design engineering;HTML;Java;Portable computers;Spline;Virtual reality		This paper describes our experiences with using the Virtual Reality Modeling Language (VRML) to view files in the Initial Graphics Exchange Specification (IGES) format using a Java-based translator from IGES to VRML and HTML (Hypertext Markup Language). The paper examines the conversion problems between IGES and VRML and presents some results of the process.	Marti, J.	Defense Group Inc., Salt Lake City, UT, USA|c|	37614584100
	SciVis	24-24 Oct. 1997	Visualization of plant growth	10.1109/VISUAL.1997.663925	http://dx.doi.org/10.1109/VISUAL.1997.663925	475	478	663925	biology computing;biomedical imaging;biomedical measurement;botany;data visualisation;image sequences	differential growth measurement;flexible templates;image sequence analysis;image stream;input data stream;modelling;nonrigid motions;plant biology;plant growth visualization;plant root;plant stem;shape representation;time-varying quantity measurement	Cameras;Computational biology;Data visualization;Gravity;Humans;Image reconstruction;Length measurement;Motion analysis;Plants (biology);Streaming media		The measurement, analysis and visualization of plant growth is of primary interest to plant biologists. We are developing software tools to support such investigations. There are two parts in this investigation, namely growth visualization of (i) a plant root and (ii) a plant stem. For both domains, the input data is a stream of images taken by cameras. The tools being developed make it possible to measure various time-varying quantities, such as differential growth. For both domains, the plant is modeled by using flexible templates to represent non-rigid motions.	Loomis, J.J.;Xiuwen Liu;Zhaohua Ding;Fujimura, K.;Evans, M.L.;Ishikawa, H.	Dept. of Comput. & Inf. Sci., Ohio State Univ., Columbus, OH, USA|c|;;;;;	37619552000;37616244800;37615115900;37269959700;37614269500;37615793800
	SciVis	24-24 Oct. 1997	Determination of unknown particle charges in a thunder cloud based upon detected electric field vectors	10.1109/VISUAL.1997.663926	http://dx.doi.org/10.1109/VISUAL.1997.663926	479	482	663926	atmospheric electricity;atmospheric ionisation;atmospheric measuring apparatus;atmospheric techniques;charge measurement;climatology;clouds;data visualisation;electric field measurement;geophysics computing;thunderstorms;vectors	2D data representation;EM field vectors;VTK;Visualization Toolkit;arrow plot;climatological data;cloud microstructure scalar measurements;color;correlation;data sample organization;data visualization;electric field vectors;measuring devices;overloaded primitive;point charge location;positional data;sail plane;scalar values;simultaneous variables display;sparse spiraling sample;temperature measurement;thunder cloud;thunderstorms;transparent isosurfaces;unknown particle charge determination;updraft;vector values	Clouds;Current measurement;Data visualization;Electromagnetic fields;Electromagnetic measurements;Measurement standards;Microstructure;Position measurement;Storms;Temperature measurement		Climatological data about thunderstorms is traditionally collected by balloons or planes traveling through the storm along straight tracts. Such data lends itself to simple 2D representations. The data described in this paper was gathered by a sail plane spiraling in an updraft within a thundercloud. The more complex organization of data samples demands more complex representation methods. This paper describes a system developed using the Visualization Toolkit (VTK) to explore such data. The data consists of several scalar values and a set of vector values associated with positional data on the measuring devices. The goal of this visualization is to explore the location of point charges suggested by the electromagnetic field vectors and determine if any correlation exists between the point charge location and standard cloud microstructure scalar measurements such as temperature. There are several problems associated with visualizing this rather unique set of data. They stem from the fact that the data is a sparse spiraling sample of scalars and vectors. The system allows the track of the plane to be displayed as a line, a tube or a ribbon; scalar values can be displayed as transparent isosurfaces; and the vector data as an arrow plot along that track, given a color that is constant, based on orientation or related to the value of a scalar. Any combination of methods can be used to display the data. A single primitive can be overloaded in many ways, or several different variables can all be displayed simultaneously.	Drake, D.;Simpson, T.;Smithmier, L.;Rheingans, P.	Dept. of Comput. Sci., Mississippi Univ., MS, USA|c|;;;	37617709900;37608356300;37612252200;37282292000
	SciVis	24-24 Oct. 1997	Interactive visualization of aircraft and power generation engines	10.1109/VISUAL.1997.663927	http://dx.doi.org/10.1109/VISUAL.1997.663927	483	486	663927	CAD;aerospace computing;aerospace engines;aircraft;data visualisation;electric generators;electric machine CAD;engineering graphics;interactive systems;rendering (computer graphics)	CAD systems;aircraft engines;arbitrary dataset viewing;dynamic data loading strategy;interactive visualization;level-of-detail modeling;part motion;polygonal environments;power generation engines;system start-up delays;view frustum culling	Aircraft propulsion;Computer graphics;Data visualization;Engines;Hardware;Image databases;Navigation;Power generation;Power system modeling;Visual databases		Presents a system for interactively visualizing large polygonal environments such as those produced by CAD systems during the design of aircraft and power generation engines. Our method combines view frustum culling with level-of-detail modeling to create a visualization system that supports part motion and has the ability to view arbitrary sets of data. To avoid long system start-up delays due to data loading, we have implemented our system using a dynamic loading strategy. This also allows us to interactively visualize more data than could fit in memory at one time.	Sobierajski, L.;Schroeder, W.	GE Corp. Res. & Dev., Miskayuna, NY, USA|c|;	;37282730100
	SciVis	24-24 Oct. 1997	Case study: efficient visualization of physical and structural properties in crash-worthiness simulations	10.1109/VISUAL.1997.663928	http://dx.doi.org/10.1109/VISUAL.1997.663928	487	490	663928	automobiles;collision processes;data visualisation;engineering graphics;finite element analysis;rendering (computer graphics);structural engineering computing;traffic engineering computing	automotive development process;calculation costs;car body behaviour;car components;case study;collision scenarios;computer graphics technology;crash simulations;crashworthiness;data analysis;data visualization;design cycles;force tubing;numerical finite element simulations;physical properties;post-processing techniques;rendering costs;scientific engineering computations;structural properties;texture mapping;time to market	Automotive engineering;Computational modeling;Computer crashes;Computer graphics;Costs;Data visualization;Finite element methods;Numerical simulation;Standards development;Vehicle crash testing		Numerical finite element simulations of the behaviour of a car body in frontal, side or rear impact collision scenarios have become increasingly complex as well as reliable and precise. They are well-established as a standard evaluation tool in the automotive development process. Both the increased complexity and the advances in computer graphics technology have resulted in the need for new visualization techniques to facilitate the analysis of the immense amount of data originating from such scientific engineering computations. Expanding the effectiveness of traditional post-processing techniques is one key to achieve shorter design cycles and faster time-to-market. In this paper, we describe how the extensive use of texture mapping and new visualization mappings like force tubing can considerably enhance the post-processing of structural and physical properties of car components in crash simulations. We show that, using these techniques, both the calculation costs and the rendering costs are reduced, and the quality of the visualization is improved.	Kuschfeldt, S.;Ertl, T.;Holzner, M.	Comput. Graphics Group, Erlangen-Nurnberg Univ., Germany|c|;;	37612264300;37268023800;37612266200
	SciVis	24-24 Oct. 1997	Visualization of rotation fields	10.1109/VISUAL.1997.663929	http://dx.doi.org/10.1109/VISUAL.1997.663929	491	494	663929	calibration;classical field theory;data visualisation;electrical engineering computing;magnetic fields;natural sciences computing;rotation;tensors;tracking;vectors	calibration;magnetic tracking system;orientation error mapping;rotation field visualization;scientific visualization;stream surfaces;streamlines;tensor fields;tufts;vector fields	Computer errors;Computer graphics;Computer science;Ellipsoids;Magnetic domains;Matrix converters;Probes;Streaming media;Tensile stress;Visualization		We define a rotation field by extending the notion of a vector field to rotations. A vector field has a vector as a value at each point of its domain; a rotation field has a rotation as a value at each point of its domain. Rotation fields result from mapping the orientation error of tracking systems. We build upon previous methods for the visualization of vector fields, tensor fields and rotations at a point, to visualize a rotation field resulting from calibration of a commonly-used magnetic tracking system.	Livingston, M.A.	Dept. of Comput. Sci., North Carolina Univ., Chapel Hill, NC, USA|c|	37619718800
	SciVis	24-24 Oct. 1997	Isosurface extraction using particle systems	10.1109/VISUAL.1997.663930	http://dx.doi.org/10.1109/VISUAL.1997.663930	495	498	663930	computational geometry;data visualisation;force;mesh generation;physics computing	adjacent particle repulsion;artificial rules;birth-death process;dynamic particle behaviour;equilibrium;force calculations;isosurface extraction;level-of-detail control;neighbour information;particle concentration;particle systems;physics;repulsive forces;scaling factor;surface curvature;surface features;surface value attraction;triangular mesh generation;vertex densities;volume data	Animation;Computer science;Data mining;Deformable models;Force control;Isosurfaces;Mesh generation;Physics;Sampling methods;Solid modeling		Presents a new approach to isosurface extraction from volume data using particle systems. Particle behavior is dynamic and can be based on laws of physics or artificial rules. For isosurface extraction, we program particles to be attracted towards a specific surface value while simultaneously repelling adjacent particles. The repulsive forces are based on the curvature of the surface at that location. A birth-death process results in a denser concentration of particles in areas of high curvature and sparser populations in areas of lower curvature. The overall level of detail is controlled through a scaling factor that increases or decreases the repulsive forces of the particles. Once particles reach equilibrium, their locations are used as vertices in generating a triangular mesh of the surface. The advantages of our approach include: vertex densities are based on surface features rather than on the sampling rate of the volume; a single scaling factor simplifies level-of-detail control; and meshing is efficient because it uses neighbor information that has already been generated during the force calculations.	Crossno, P.;Angel, E.	Dept. of Comput. Sci., New Mexico Univ., Albuquerque, NM, USA|c|;	37282576500;37284249400
	SciVis	24-24 Oct. 1997	A visualization of music	10.1109/VISUAL.1997.663931	http://dx.doi.org/10.1109/VISUAL.1997.663931	499	503	663931	colour graphics;data visualisation;music	3D space;color;music data representation;music notation;music visualization	Computer science;Data visualization;Instruments;Multiple signal classification;Music;Rhythm;Shape;Timbre		Currently, the most popular method of visualizing music is music notation. Through music notation, an experienced musician can gain an impression of how a particular piece of music sounds simply by looking at the notes on paper. However, most listeners are unfamiliar or uncomfortable with the complex nature of music notation. The goal of this project is to present an alternate method for visualizing music that makes use of color and 3D space. This paper describes one method of visualizing music in 3D space. The implementation of this method shows that music visualization is an effective technique, although it is certainly not the only possible method for accomplishing the task. Throughout the course of this project, several variations and alternative approaches were discussed. The final version of this project reflects the decisions that were made in order to present the best possible representation of music data.	Smith, S.M.;Williams, G.	Dept. of Comput. Sci., Texas A&M Univ., College Station, TX, USA|c|;	;37344773600
	SciVis	24-24 Oct. 1997	Terascale Visualization: Approaches, Pitfalls And Issues	10.1109/VISUAL.1997.663932	http://dx.doi.org/10.1109/VISUAL.1997.663932	507	509	663932			Computational modeling;Computer architecture;Computer graphics;Concurrent computing;Data analysis;Data visualization;Earth Observing System;Laboratories;NASA;Supercomputers			Cox, M.	NASA Ames Research Center|c|	
	SciVis	24-24 Oct. 1997	Information Exploration Shootout Project And Benchmark Data Sets: Evaluating How Visualization Does In Analyzing Real-World Data Analysis Problems	10.1109/VISUAL.1997.663933	http://dx.doi.org/10.1109/VISUAL.1997.663933	511	513	663933			Artificial intelligence;Business communication;Data analysis;Data mining;Data visualization;Database systems;Humans;Information management;Machine learning algorithms;Visual databases			Laskowski, S.	National Institute for Standards and Technology|c|	
	SciVis	24-24 Oct. 1997	Perceptual Measures For Effective Visualizations	10.1109/VISUAL.1997.663934	http://dx.doi.org/10.1109/VISUAL.1997.663934	515	517	663934			Brightness;Data visualization;Displays;Feeds;Humans;NASA;Pixel;Space technology;Surface finishing;Testing			Barrett, H.	University of Arizona|c|	
