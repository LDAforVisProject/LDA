{
 "metadata": {
  "name": "",
  "signature": "sha256:17aab8892010a19b3435e309c85252145a3c452aa7ebd84265af1832fe2b222c"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from gensim.models import word2vec\n",
      "import logging\n",
      "logging.basicConfig(format= '%(asctime)s : %(levelnames)s : %(message)s', level=logging.INFO)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Load unzipped corpus from http://mattmahoney.net/dc/text8.zip\n",
      "sentences = word2vec.Text8Corpus('/Users/charleywu/Github/LDA/tmp/text8')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#train the hierarchical softmax skip-gram mode; default window = 5\n",
      "model = word2vec.Word2Vec(sentences, size = 200)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Pickle the entire model to disk so we can load it later\n",
      "model.save('/Users/charleywu/Github/LDA/tmp/text8.model')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#store the learned weights\n",
      "model.save_word2vec_format('/Users/charleywu/Github/LDA/tmp/text8.vectors.bin', binary=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#import word weights created by the (Faster) C word2vec\n",
      "model = word2vec.Word2Vec.load_word2vec_format('/Users/charleywu/Github/LDA/tmp/text8.vectors.bin', binary=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#TEST\n",
      "#boy is to father as girl is to....\n",
      "model.most_similar(positive = ['girl', 'father'], negative = ['boy'], topn=3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#more examples\n",
      "more_examples = [\"he his she\", \"big bigger bad\", \"going went being\"]\n",
      "for example in more_Examples:\n",
      "    a, b, x = example.split()\n",
      "    predicted = model.most_similar([x,b], [a])[0][0]\n",
      "    print \" '%s' is to '%s' as '%s' is to '%s'\" % (a,b,x,predicted)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#which word doesn't go with the others?\n",
      "model.doesnt_match(\"breakfast cereal dinner lunch\".split())\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Having deep learning available in Python allows us to plug in the multitude of NLP tools available in Python. More intelligent tokenization/sentence splitting, named entity recognition? Just use NLTK. Web crawling, lemmatization? Try pattern. Removing boilerplate HTML and extracting meaningful, plain text? jusText. Continue the learning pipeline with k-means or other machine learning algos? Scikit-learn has loads."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}